# Manage a Pipeline Project

Organize multi-schema pipelines for team collaboration.

## Overview

A production DataJoint pipeline typically involves:

- **Multiple schemas** â€” Organized by experimental modality or processing stage
- **Team of users** â€” With different roles and access levels
- **Shared infrastructure** â€” Database server, object storage, code repository
- **Coordination** â€” Between code, database, and storage permissions

This guide outlines the key considerations. For a fully managed solution, [request a DataJoint Platform account](https://www.datajoint.com/sign-up).

## Pipeline Architecture

A DataJoint pipeline integrates three core components:

![DataJoint Platform Architecture](../images/dj-platform.png)

**Core components:**

- **Code Repository** â€” Version-controlled pipeline definitions, analysis code, configuration
- **Relational Database** â€” Metadata store, system of record, integrity enforcement
- **Object Store** â€” Scalable storage for large scientific data (images, recordings, videos)

## Pipeline as a DAG

A DataJoint pipeline forms a **Directed Acyclic Graph (DAG)** at two levels:

![Pipeline DAG Structure](../images/pipeline-illustration.png)

**Nodes** represent Python modules, which correspond to database schemas.

**Edges** represent:

- Python import dependencies between modules
- Bundles of foreign key references between schemas

This dual structure ensures that both code dependencies and data dependencies flow in the same direction.

## Schema Organization

Each schema corresponds to a dedicated Python module:

![Schema Structure](../images/schema-illustration.png)

### Project Structure

Use a modern Python project layout with source code under `src/`:

```
my_pipeline/
â”œâ”€â”€ datajoint.json          # Shared settings (committed)
â”œâ”€â”€ .secrets/               # Local credentials (gitignored)
â”‚   â”œâ”€â”€ database.password
â”‚   â””â”€â”€ storage.credentials
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml          # Package metadata and dependencies
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â””â”€â”€ my_pipeline/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ subject.py      # subject schema
â”‚       â”œâ”€â”€ session.py      # session schema
â”‚       â”œâ”€â”€ ephys.py        # ephys schema
â”‚       â”œâ”€â”€ imaging.py      # imaging schema
â”‚       â”œâ”€â”€ analysis.py     # analysis schema
â”‚       â””â”€â”€ utils/
â”‚           â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_ephys.py
â””â”€â”€ docs/
    â””â”€â”€ ...
```

### One Module Per Schema

Each module defines and binds to its schema:

```python
# src/my_pipeline/ephys.py
import datajoint as dj
from . import session  # Import dependency

schema = dj.Schema('ephys')

@schema
class Probe(dj.Lookup):
    definition = """
    probe_type : varchar(32)
    ---
    num_channels : uint16
    """

@schema
class Recording(dj.Imported):
    definition = """
    -> session.Session
    -> Probe
    ---
    recording_path : varchar(255)
    """
```

### Import Dependencies Mirror Foreign Keys

Module imports reflect the schema DAG:

```python
# analysis.py depends on both ephys and imaging
from . import ephys
from . import imaging

schema = dj.Schema('analysis')

@schema
class MultiModalAnalysis(dj.Computed):
    definition = """
    -> ephys.Recording
    -> imaging.Scan
    ---
    correlation : float64
    """
```

### DAG Constraints

> **All foreign key relationships within a schema MUST form a DAG.**
>
> **Dependencies between schemas (foreign keys + imports) MUST also form a DAG.**

This ensures unidirectional flow of data and computational dependencies throughout the pipeline.

## Repository Configuration

### Shared Settings

Store non-secret configuration in `datajoint.json` at the project root:

**datajoint.json** (committed):
```json
{
  "database": {
    "host": "db.example.com",
    "port": 3306
  },
  "stores": {
    "main": {
      "protocol": "s3",
      "endpoint": "s3.example.com",
      "bucket": "my-org-data",
      "location": "my_pipeline"
    }
  }
}
```

### Credentials Management

Credentials are stored locally and never committed:

**Option 1: `.secrets/` directory**
```
.secrets/
â”œâ”€â”€ database.user
â”œâ”€â”€ database.password
â”œâ”€â”€ storage.access_key
â””â”€â”€ storage.secret_key
```

**Option 2: Environment variables**
```bash
export DJ_USER=alice
export DJ_PASS=alice_password
export DJ_STORES__MAIN__ACCESS_KEY=...
export DJ_STORES__MAIN__SECRET_KEY=...
```

### Essential `.gitignore`

```gitignore
# Credentials
.secrets/

# Python
__pycache__/
*.pyc
*.egg-info/
dist/
build/

# Environment
.env
.venv/

# IDE
.idea/
.vscode/
```

### `pyproject.toml` Example

```toml
[project]
name = "my-pipeline"
version = "1.0.0"
requires-python = ">=3.10"
dependencies = [
    "datajoint>=2.0",
    "numpy",
]

[project.optional-dependencies]
dev = ["pytest", "pytest-cov"]

[tool.setuptools.packages.find]
where = ["src"]
```

## Database Access Control

### The Complexity

Multi-user database access requires:

1. **User accounts** â€” Individual credentials per team member
2. **Schema permissions** â€” Which users can access which schemas
3. **Operation permissions** â€” SELECT, INSERT, UPDATE, DELETE, CREATE, DROP
4. **Role hierarchy** â€” Admin, developer, analyst, viewer
5. **Audit trail** â€” Who modified what and when

### Basic MySQL Grants

```sql
-- Create user
CREATE USER 'alice'@'%' IDENTIFIED BY 'password';

-- Grant read-only on specific schema
GRANT SELECT ON ephys.* TO 'alice'@'%';

-- Grant read-write on specific schema
GRANT SELECT, INSERT, UPDATE, DELETE ON analysis.* TO 'alice'@'%';

-- Grant full access (developers)
GRANT ALL PRIVILEGES ON my_pipeline_*.* TO 'bob'@'%';
```

### Role-Based Access Patterns

| Role | Permissions | Typical Use |
|------|-------------|-------------|
| Viewer | SELECT | Browse data, run queries |
| Analyst | SELECT, INSERT on analysis | Add analysis results |
| Operator | SELECT, INSERT, DELETE on data schemas | Run pipeline |
| Developer | ALL on development schemas | Schema changes |
| Admin | ALL + GRANT | User management |

### Considerations

- Users need SELECT on parent schemas to INSERT into child schemas (FK validation)
- Cascading deletes require DELETE on all dependent schemas
- Schema creation requires CREATE privilege
- Coordinating permissions across many schemas becomes complex

## Object Storage Access Control

### The Complexity

Object storage permissions must align with database permissions:

1. **Bucket/prefix policies** â€” Map to schema access
2. **Read vs write** â€” Match SELECT vs INSERT/UPDATE
3. **Credential distribution** â€” Per-user or shared service accounts
4. **Cross-schema objects** â€” When computed tables reference multiple inputs

### Hierarchical Storage Structure

A DataJoint project creates a structured storage pattern:

```
ğŸ“ project_name/
â”œâ”€â”€ ğŸ“ schema_name1/
â”œâ”€â”€ ğŸ“ schema_name2/
â”œâ”€â”€ ğŸ“ schema_name3/
â”‚   â”œâ”€â”€ objects/
â”‚   â”‚   â””â”€â”€ table1/
â”‚   â”‚       â””â”€â”€ key1-value1/
â”‚   â””â”€â”€ fields/
â”‚       â””â”€â”€ table1-field1/
â””â”€â”€ ...
```

### S3/MinIO Policy Example

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject"],
      "Resource": "arn:aws:s3:::my-lab-data/datajoint/ephys/*"
    },
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject", "s3:PutObject"],
      "Resource": "arn:aws:s3:::my-lab-data/datajoint/analysis/*"
    }
  ]
}
```

### Considerations

- Object paths include schema name: `{project}/{schema}/{table}/...`
- Users need read access to fetch blobs from upstream schemas
- Content-addressed storage (`<blob@>`) shares objects across tables
- Garbage collection requires coordinated delete permissions

## Pipeline Initialization

### Schema Creation Order

Initialize schemas in dependency order:

```python
# src/my_pipeline/__init__.py
from . import subject   # No dependencies
from . import session   # Depends on subject
from . import ephys     # Depends on session
from . import imaging   # Depends on session
from . import analysis  # Depends on ephys, imaging

def initialize():
    """Create all schemas in dependency order."""
    # Schemas are created when modules are imported
    # and tables are first accessed
    subject.Subject()
    session.Session()
    ephys.Recording()
    imaging.Scan()
    analysis.MultiModalAnalysis()
```

### Version Coordination

Track schema versions with your code:

```python
# src/my_pipeline/version.py
__version__ = "1.2.0"

SCHEMA_VERSIONS = {
    'subject': '1.0.0',
    'session': '1.1.0',
    'ephys': '1.2.0',
    'imaging': '1.2.0',
    'analysis': '1.2.0',
}
```

## Team Workflows

### Development vs Production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Development   â”‚     â”‚   Production    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dev_subject     â”‚     â”‚ subject         â”‚
â”‚ dev_session     â”‚     â”‚ session         â”‚
â”‚ dev_ephys       â”‚     â”‚ ephys           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚
        â”‚    Schema promotion   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Branching Strategy

```
main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
       â”‚              â”‚
       â”‚ feature/     â”‚ hotfix/
       â–¼              â–¼
    ephys-v2      fix-recording
       â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â–¶ main
```

## Summary of Complexities

Managing a team pipeline requires coordinating:

| Component | Challenges |
|-----------|------------|
| **Code** | Module dependencies, version control, deployment |
| **Database** | User accounts, schema permissions, role hierarchy |
| **Object Storage** | Bucket policies, credential distribution, path alignment |
| **Compute** | Worker deployment, job distribution, resource allocation |
| **Monitoring** | Progress tracking, error alerting, audit logging |

These challenges grow with team size and pipeline complexity. The [DataJoint Platform](https://www.datajoint.com/sign-up) provides integrated management for all these concerns.

## See Also

- [Configure Object Storage](configure-storage.md) â€” Storage setup
- [Distributed Computing](distributed-computing.md) â€” Multi-worker pipelines
- [Model Relationships](model-relationships.md) â€” Foreign key patterns
