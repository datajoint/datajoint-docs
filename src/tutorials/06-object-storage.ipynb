{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Object-Augmented Schemas\n\nThis tutorial covers DataJoint's Object-Augmented Schema (OAS) model. You'll learn:\n\n- **The OAS concept** — Unified relational + object storage\n- **Blobs** — Storing arrays and Python objects\n- **Object storage** — Scaling to large datasets\n- **Staged insert** — Writing directly to object storage (Zarr, HDF5)\n- **Attachments** — Preserving file names and formats\n- **Codecs** — How data is serialized and deserialized\n\nIn an Object-Augmented Schema, the relational database and object storage operate as a **single integrated system**—not as separate \"internal\" and \"external\" components."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "\n",
    "schema = dj.Schema('tutorial_oas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## The Object-Augmented Schema Model\n\nScientific data often combines:\n- **Structured metadata** — Subjects, sessions, parameters (relational)\n- **Large data objects** — Arrays, images, recordings (binary)\n\nDataJoint's OAS model manages both as a unified system:\n\n```mermaid\nblock-beta\n    columns 1\n    block:oas:1\n        columns 2\n        OAS[\"Object-Augmented Schema\"]:2\n        block:db:1\n            DB[\"Relational Database\"]\n            DB1[\"Metadata\"]\n            DB2[\"Keys\"]\n            DB3[\"Relationships\"]\n        end\n        block:os:1\n            OS[\"Object Storage (S3/File/etc)\"]\n            OS1[\"Large arrays\"]\n            OS2[\"Images/videos\"]\n            OS3[\"Recordings\"]\n        end\n    end\n```\n\nFrom the user's perspective, this is **one schema**—storage location is transparent."
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Blob Attributes\n",
    "\n",
    "Use `<blob>` to store arbitrary Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Recording(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    recording_id : int\n",
    "    ---\n",
    "    metadata : <blob>         # Dict, stored in database\n",
    "    waveform : <blob>         # NumPy array, stored in database\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert with blob data\n",
    "Recording.insert1({\n",
    "    'recording_id': 1,\n",
    "    'metadata': {'channels': 32, 'sample_rate': 30000, 'duration': 60.0},\n",
    "    'waveform': np.random.randn(32, 30000)  # 32 channels x 1 second\n",
    "})\n",
    "\n",
    "Recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch blob data\n",
    "data = (Recording & {'recording_id': 1}).fetch1()\n",
    "print(f\"Metadata: {data['metadata']}\")\n",
    "print(f\"Waveform shape: {data['waveform'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "### What Can Be Stored in Blobs?\n\nThe `<blob>` codec handles:\n\n- NumPy arrays (any dtype, any shape)\n- Python dicts, lists, tuples, sets\n- Strings, bytes, integers, floats\n- datetime objects and UUIDs\n- Nested combinations of the above\n\n**Note:** Pandas DataFrames should be converted before storage (e.g., `df.to_dict()` or `df.to_records()`)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "@schema\nclass AnalysisResult(dj.Manual):\n    definition = \"\"\"\n    result_id : int\n    ---\n    arrays : <blob>\n    nested_data : <blob>\n    \"\"\"\n\n# Store complex data structures\narrays = {'x': np.array([1, 2, 3]), 'y': np.array([4, 5, 6])}\nnested = {'arrays': [np.array([1, 2]), np.array([3, 4])], 'params': {'a': 1, 'b': 2}}\n\nAnalysisResult.insert1({\n    'result_id': 1,\n    'arrays': arrays,\n    'nested_data': nested\n})\n\n# Fetch back\nresult = (AnalysisResult & {'result_id': 1}).fetch1()\nprint(f\"Arrays type: {type(result['arrays'])}\")\nprint(f\"Arrays keys: {result['arrays'].keys()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Object Storage with `@`\n",
    "\n",
    "For large datasets, add `@` to route data to object storage. The schema remains unified—only the physical storage location changes.\n",
    "\n",
    "### Configure Object Storage\n",
    "\n",
    "First, configure a store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "import tempfile\nimport os\n\n# Create a store for this tutorial\nstore_path = tempfile.mkdtemp(prefix='dj_store_')\n\n# Configure object storage (global settings for staged_insert1)\ndj.config.object_storage.protocol = 'file'\ndj.config.object_storage.location = store_path\ndj.config.object_storage.project_name = 'tutorial'\n\n# Also configure as a named store for <blob@tutorial> syntax\ndj.config.object_storage.stores['tutorial'] = {\n    'protocol': 'file',\n    'location': store_path\n}\n\nprint(f\"Store configured at: {store_path}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Using Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class LargeRecording(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    recording_id : int\n",
    "    ---\n",
    "    small_data : <blob>            # In database (small)\n",
    "    large_data : <blob@tutorial>   # In object storage (large)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data - usage is identical regardless of storage\n",
    "small = np.random.randn(10, 10)\n",
    "large = np.random.randn(1000, 1000)  # ~8 MB array\n",
    "\n",
    "LargeRecording.insert1({\n",
    "    'recording_id': 1,\n",
    "    'small_data': small,\n",
    "    'large_data': large\n",
    "})\n",
    "\n",
    "LargeRecording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch is also identical - storage is transparent\n",
    "data = (LargeRecording & {'recording_id': 1}).fetch1()\n",
    "print(f\"Small data shape: {data['small_data'].shape}\")\n",
    "print(f\"Large data shape: {data['large_data'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects are stored in the configured location\n",
    "for root, dirs, files in os.walk(store_path):\n",
    "    for f in files:\n",
    "        path = os.path.join(root, f)\n",
    "        size = os.path.getsize(path)\n",
    "        print(f\"{os.path.relpath(path, store_path)}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Content-Addressed Storage\n",
    "\n",
    "`<blob@>` uses content-addressed (hash-based) storage. Identical data is stored only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the same data twice\n",
    "shared_data = np.ones((500, 500))\n",
    "\n",
    "LargeRecording.insert([\n",
    "    {'recording_id': 2, 'small_data': small, 'large_data': shared_data},\n",
    "    {'recording_id': 3, 'small_data': small, 'large_data': shared_data},  # Same!\n",
    "])\n",
    "\n",
    "print(f\"Rows in table: {len(LargeRecording())}\")\n",
    "\n",
    "# Deduplication: identical data stored once\n",
    "files = [f for _, _, fs in os.walk(store_path) for f in fs]\n",
    "print(f\"Files in store: {len(files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fzmvk89rx1k",
   "source": "## Path-Addressed Storage with `<object@>`\n\nWhile `<blob@>` uses content-addressed (hash-based) storage with deduplication, `<object@>` uses **path-addressed** storage where each row has its own dedicated storage path:\n\n| Aspect | `<blob@>` | `<object@>` |\n|--------|-----------|-------------|\n| Addressing | By content hash | By primary key |\n| Deduplication | Yes | No |\n| Deletion | Garbage collected | With row |\n| Use case | Arrays, serialized objects | Zarr, HDF5, multi-file outputs |\n\nUse `<object@>` when you need:\n- Hierarchical formats like Zarr or HDF5\n- Direct write access during data generation\n- Each row to have its own isolated storage location",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8q80k9agb3c",
   "source": "@schema\nclass ImagingSession(dj.Manual):\n    definition = \"\"\"\n    subject_id : int32\n    session_id : int32\n    ---\n    n_frames : int32\n    frame_rate : float32\n    frames : <object@tutorial>    # Zarr array stored at path derived from PK\n    \"\"\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8eb6bvjm1o",
   "source": "### Staged Insert for Direct Object Storage Writes\n\nFor large datasets like multi-GB imaging recordings, copying data from local storage to object storage is inefficient. The `staged_insert1` context manager lets you **write directly to object storage** before finalizing the database insert:\n\n1. Set primary key values in `staged.rec`\n2. Get a storage handle with `staged.store(field, extension)`\n3. Write data directly (e.g., with Zarr)\n4. On successful exit, metadata is computed and the record is inserted",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m1gzyoazrxd",
   "source": "import zarr\n\n# Simulate acquiring imaging data frame-by-frame\nn_frames = 100\nheight, width = 512, 512\n\nwith ImagingSession.staged_insert1 as staged:\n    # Set primary key values first\n    staged.rec['subject_id'] = 1\n    staged.rec['session_id'] = 1\n    \n    # Get storage handle for the object field\n    store = staged.store('frames', '.zarr')\n    \n    # Create Zarr array directly in object storage\n    z = zarr.open(store, mode='w', shape=(n_frames, height, width), \n                  chunks=(10, height, width), dtype='uint16')\n    \n    # Write frames as they are \"acquired\"\n    for i in range(n_frames):\n        z[i] = np.random.randint(0, 4096, (height, width), dtype='uint16')\n    \n    # Set remaining attributes\n    staged.rec['n_frames'] = n_frames\n    staged.rec['frame_rate'] = 30.0\n\n# Record is now inserted with metadata computed from the Zarr\nImagingSession()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "btl186xeyeb",
   "source": "# Fetch returns an ObjectRef for lazy access\nref = (ImagingSession & {'subject_id': 1, 'session_id': 1}).fetch1('frames')\nprint(f\"Type: {type(ref).__name__}\")\nprint(f\"Path: {ref.path}\")\n\n# Open as Zarr array (data stays in object storage)\nz = zarr.open(ref.fsmap, mode='r')\nprint(f\"Shape: {z.shape}\")\nprint(f\"Chunks: {z.chunks}\")\nprint(f\"First frame mean: {z[0].mean():.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "su8znwb7nl7",
   "source": "### Benefits of Staged Insert\n\n- **No intermediate copies** — Data flows directly to object storage\n- **Streaming writes** — Write frame-by-frame as data is acquired\n- **Atomic transactions** — If an error occurs, storage is cleaned up automatically\n- **Automatic metadata** — File sizes and manifests are computed on finalize\n\nUse `staged_insert1` when:\n- Data is too large to hold in memory\n- You're generating data incrementally (e.g., during acquisition)\n- You need direct control over storage format (Zarr chunks, HDF5 datasets)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Attachments\n",
    "\n",
    "Use `<attach>` to store files with their original names preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Document(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    doc_id : int\n",
    "    ---\n",
    "    report : <attach@tutorial>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Create a sample file\nsample_file = os.path.join(tempfile.gettempdir(), 'analysis_report.txt')\nwith open(sample_file, 'w') as f:\n    f.write('Analysis Results\\n')\n    f.write('================\\n')\n    f.write('Accuracy: 95.2%\\n')\n\n# Insert using file path directly\nDocument.insert1({\n    'doc_id': 1,\n    'report': sample_file  # Just pass the path\n})\n\nDocument()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Fetch returns path to extracted file\ndoc_path = (Document & {'doc_id': 1}).fetch1('report')\nprint(f\"Type: {type(doc_path)}\")\nprint(f\"Path: {doc_path}\")\n\n# Read the content\nwith open(doc_path, 'r') as f:\n    print(f\"Content:\\n{f.read()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Codec Summary\n",
    "\n",
    "| Codec | Syntax | Description |\n",
    "|-------|--------|-------------|\n",
    "| `<blob>` | In database | Python objects, arrays |\n",
    "| `<blob@>` | Default store | Large objects, hash-addressed |\n",
    "| `<blob@name>` | Named store | Specific storage tier |\n",
    "| `<attach>` | In database | Files with names |\n",
    "| `<attach@name>` | Named store | Large files with names |\n",
    "| `<object@name>` | Named store | Path-addressed (Zarr, etc.) |\n",
    "| `<filepath@name>` | Named store | References to existing files |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Computed Tables with Large Data\n",
    "\n",
    "Computed tables commonly produce large results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class ProcessedRecording(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> LargeRecording\n",
    "    ---\n",
    "    filtered : <blob@tutorial>     # Result in object storage\n",
    "    mean_value : float64\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        # Fetch source data\n",
    "        data = (LargeRecording & key).fetch1('large_data')\n",
    "        \n",
    "        # Process\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        filtered = gaussian_filter(data, sigma=2)\n",
    "        \n",
    "        self.insert1({\n",
    "            **key,\n",
    "            'filtered': filtered,\n",
    "            'mean_value': float(np.mean(filtered))\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedRecording.populate(display_progress=True)\n",
    "ProcessedRecording()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Efficient Data Access\n",
    "\n",
    "### Fetch Only What You Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch only scalar metadata (fast)\n",
    "meta = (ProcessedRecording & {'recording_id': 1}).fetch1('mean_value')\n",
    "print(f\"Mean value: {meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch large data only when needed\n",
    "filtered = (ProcessedRecording & {'recording_id': 1}).fetch1('filtered')\n",
    "print(f\"Filtered shape: {filtered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Project Away Large Columns Before Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient: project to scalar columns before join\n",
    "result = LargeRecording.proj('recording_id') * ProcessedRecording.proj('mean_value')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Choose Storage Based on Size\n",
    "\n",
    "```python\n",
    "# Small objects (< 1 MB): no @\n",
    "parameters : <blob>\n",
    "\n",
    "# Large objects (> 1 MB): use @\n",
    "raw_data : <blob@>\n",
    "```\n",
    "\n",
    "### 2. Use Named Stores for Different Tiers\n",
    "\n",
    "```python\n",
    "# Fast local storage for active data\n",
    "working_data : <blob@fast>\n",
    "\n",
    "# Cold storage for archives\n",
    "archived_data : <blob@archive>\n",
    "```\n",
    "\n",
    "### 3. Separate Queryable Metadata from Large Data\n",
    "\n",
    "```python\n",
    "@schema\n",
    "class Experiment(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    exp_id : int\n",
    "    ---\n",
    "    # Queryable metadata\n",
    "    date : date\n",
    "    duration : float\n",
    "    n_trials : int\n",
    "    # Large data\n",
    "    raw_data : <blob@>\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "### 4. Use Attachments for Files\n",
    "\n",
    "```python\n",
    "# Preserves filename\n",
    "video : <attach@>\n",
    "config_file : <attach@>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "| Pattern | Use Case |\n",
    "|---------|----------|\n",
    "| `<blob>` | Small Python objects |\n",
    "| `<blob@>` | Large arrays with deduplication |\n",
    "| `<blob@store>` | Large arrays in specific store |\n",
    "| `<attach@store>` | Files preserving names |\n",
    "| `<object@store>` | Path-addressed data (Zarr, HDF5) |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Configure Object Storage](../how-to/configure-storage.md) — Set up S3, MinIO, or filesystem stores\n",
    "- [Custom Codecs](advanced/custom-codecs.ipynb) — Define domain-specific types\n",
    "- [Manage Large Data](../how-to/manage-large-data.md) — Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "schema.drop(prompt=False)\n",
    "import shutil\n",
    "shutil.rmtree(store_path, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}