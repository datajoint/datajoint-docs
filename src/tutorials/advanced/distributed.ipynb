{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Distributed Computing\n",
    "\n",
    "This tutorial covers running computations across multiple workers. You'll learn:\n",
    "\n",
    "- **Jobs 2.0** — DataJoint's job coordination system\n",
    "- **Multi-process** — Parallel workers on one machine\n",
    "- **Multi-machine** — Cluster-scale computation\n",
    "- **Error handling** — Recovery and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import datajoint as dj\nimport numpy as np\nimport time\n\nschema = dj.Schema('tutorial_distributed')\n\n# Clean up from previous runs\nschema.drop(prompt=False)\nschema = dj.Schema('tutorial_distributed')"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@schema\n",
    "class Experiment(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    exp_id : int\n",
    "    ---\n",
    "    n_samples : int\n",
    "    \"\"\"\n",
    "\n",
    "@schema\n",
    "class Analysis(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Experiment\n",
    "    ---\n",
    "    result : float64\n",
    "    compute_time : float32\n",
    "    \"\"\"\n",
    "\n",
    "    def make(self, key):\n",
    "        start = time.time()\n",
    "        n = (Experiment & key).fetch1('n_samples')\n",
    "        result = float(np.mean(np.random.randn(n) ** 2))\n",
    "        time.sleep(0.1)\n",
    "        self.insert1({**key, 'result': result, 'compute_time': time.time() - start})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment.insert([{'exp_id': i, 'n_samples': 10000} for i in range(20)])\n",
    "print(f\"To compute: {len(Analysis.key_source - Analysis)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Direct vs Distributed Mode\n",
    "\n",
    "**Direct mode** (default): No coordination, suitable for single worker.\n",
    "\n",
    "**Distributed mode** (`reserve_jobs=True`): Workers coordinate via jobs table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed mode\n",
    "Analysis.populate(reserve_jobs=True, max_calls=5, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## The Jobs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh job queue\n",
    "result = Analysis.jobs.refresh()\n",
    "print(f\"Added: {result['added']}\")\n",
    "\n",
    "# Check status\n",
    "for status, count in Analysis.jobs.progress().items():\n",
    "    print(f\"{status}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Multi-Process and Multi-Machine\n\nThe `processes=N` parameter spawns multiple worker processes on one machine. However, this requires table classes to be defined in importable Python modules (not notebooks), because multiprocessing needs to pickle and transfer the class definitions to worker processes.\n\nFor production use, define your tables in a module and run workers as scripts:\n\n```python\n# pipeline.py - Define your tables\nimport datajoint as dj\nschema = dj.Schema('my_pipeline')\n\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"...\"\"\"\n    def make(self, key): ...\n```\n\n```python\n# worker.py - Run workers\nfrom pipeline import Analysis\n\n# Single machine, 4 processes\nAnalysis.populate(reserve_jobs=True, processes=4)\n\n# Or run this script on multiple machines\nwhile True:\n    result = Analysis.populate(reserve_jobs=True, max_calls=100, suppress_errors=True)\n    if result['success_count'] == 0:\n        break\n```\n\nIn this notebook, we'll demonstrate distributed coordination with a single process:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Complete remaining jobs with distributed coordination\nAnalysis.populate(reserve_jobs=True, display_progress=True)\nprint(f\"Computed: {len(Analysis())}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View errors\n",
    "print(f\"Errors: {len(Analysis.jobs.errors)}\")\n",
    "\n",
    "# Retry failed jobs\n",
    "Analysis.jobs.errors.delete()\n",
    "Analysis.populate(reserve_jobs=True, suppress_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "| Option | Description |\n",
    "|--------|-------------|\n",
    "| `reserve_jobs=True` | Enable coordination |\n",
    "| `processes=N` | N worker processes |\n",
    "| `max_calls=N` | Limit jobs per run |\n",
    "| `suppress_errors=True` | Continue on errors |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.drop(prompt=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}