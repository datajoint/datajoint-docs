{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DataJoint Documentation","text":"<p>Upgrading from 0.14.x?</p> <ul> <li>Migration guide: Migrate to 2.0</li> <li>Legacy docs: datajoint.github.io</li> </ul>"},{"location":"#about-datajoint","title":"About DataJoint","text":"<p>DataJoint implements the Relational Workflow Model\u2014a data model where your database schema defines an executable data pipeline. Tables represent workflow steps, foreign keys encode dependencies, and the system handles job management, parallel execution, and provenance tracking. Object storage integration enables seamless handling of large scientific data.</p> <p></p> <ul> <li> Concepts <p>Understand the Relational Workflow Model and DataJoint's core principles</p> <p> Learn the concepts</p> </li> </ul> <ul> <li> Tutorials <p>Build your first pipeline with hands-on Jupyter notebooks</p> <p> Start learning</p> </li> </ul> <ul> <li> How-To Guides <p>Practical guides for common tasks and patterns</p> <p> Find solutions</p> </li> </ul> <ul> <li> Reference <p>Specifications, API documentation, and technical details</p> <p> Look it up</p> </li> </ul> <ul> <li> DataJoint Elements <p>Reusable pipeline modules for neurophysiology experiments</p> <p> Explore Elements</p> </li> </ul> <ul> <li> DataJoint Platform <p>A cloud platform for automated analysis workflows. It relies on DataJoint Python and DataJoint Elements.</p> <p> Learn more | Sign-in</p> </li> </ul> <p>New to DataJoint? Start with the  Quick Start tutorial.</p>"},{"location":"about/","title":"About DataJoint","text":"<p>DataJoint is an open-source framework for building scientific data pipelines. It was created to address the challenges of managing complex, interconnected data in research laboratories.</p>"},{"location":"about/#what-is-datajoint","title":"What is DataJoint?","text":"<p>DataJoint implements the Relational Workflow Model\u2014a paradigm that extends relational databases with native support for computational workflows. Unlike traditional databases that only store data, DataJoint pipelines define how data flows through processing steps, when computations run, and how results depend on inputs.</p> <p>Key characteristics:</p> <ul> <li>Declarative schema design \u2014 Define tables and relationships in Python</li> <li>Automatic dependency tracking \u2014 Foreign keys encode workflow dependencies</li> <li>Built-in computation \u2014 Imported and Computed tables run automatically</li> <li>Data integrity \u2014 Referential integrity and transaction support</li> <li>Reproducibility \u2014 Immutable data with full provenance</li> </ul>"},{"location":"about/#history","title":"History","text":"<p>DataJoint was developed at Baylor College of Medicine starting in 2009 to support neuroscience research. It has since been adopted by laboratories worldwide for a variety of scientific applications.</p> <p> Read the full history</p>"},{"location":"about/#citation","title":"Citation","text":"<p>If you use DataJoint in your research, please cite it appropriately.</p> <p> Citation guidelines</p>"},{"location":"about/#contributing","title":"Contributing","text":"<p>DataJoint is developed openly on GitHub. Contributions are welcome.</p> <p> Contribution guidelines</p>"},{"location":"about/#license","title":"License","text":"<p>DataJoint is released under the Apache License 2.0.</p> <p>Copyright 2024 DataJoint Inc. and contributors.</p>"},{"location":"about/citation/","title":"Citation Guidelines","text":"<p>When your work uses the DataJoint Python, MATLAB, or Elements framework, please cite the respective manuscripts and include their associated Research Resource Identifiers (RRIDs). Proper citation helps credit the contributors and supports the broader scientific community by highlighting the tools used in research.</p>"},{"location":"about/citation/#citing-datajoint-elements","title":"Citing DataJoint Elements","text":"<p>If your work utilizes DataJoint Elements, please cite the following manuscript:</p> <ul> <li>Manuscript: Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki   M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for   Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</li> </ul> <ul> <li>RRID: RRID:SCR_021894</li> </ul> <p>You should also cite the DataJoint Core manuscript detailed below.</p>"},{"location":"about/citation/#citing-the-datajoint-relational-model","title":"Citing the DataJoint Relational Model","text":"<p>For any work relying on the DataJoint Relational Model, include the following citation:</p> <ul> <li>Manuscript: Yatsenko D, Walker EY, Tolias AS. DataJoint: A simpler relational data   model. arXiv:1807.11104. 2018 Jul 29. doi: https://doi.org/10.48550/arXiv.1807.11104</li> </ul> <ul> <li>RRID: RRID:SCR_014543</li> </ul>"},{"location":"about/citation/#citing-datajoint-python-and-matlab","title":"Citing DataJoint Python and MATLAB","text":"<p>For work using DataJoint Python or DataJoint MATLAB, cite the following manuscript:</p> <ul> <li>Manuscript: Yatsenko D, Reimer J, Ecker AS, Walker EY, Sinz F, Berens P,   Hoenselaar A, Cotton RJ, Siapas AS, Tolias AS. DataJoint: Managing big scientific data   using MATLAB or Python. bioRxiv. 2015 Jan 1:031658. doi:   https://doi.org/10.1101/031658</li> </ul> <ul> <li>RRID: RRID:SCR_014543</li> </ul>"},{"location":"about/citation/#citing-sciops-and-capability-maturity-model","title":"Citing SciOps and Capability Maturity Model","text":"<p>If your work references SciOps or the Capability Maturity Model for Data-Intensive Research, please use the following citation:</p> <ul> <li>Manuscript: Johnson EC, Nguyen TT, Dichter BK, Zappulla F, Kosma M, Gunalan K,   Halchenko YO, Neufeld SQ, Schirner M, Ritter P, Martone ME. SciOps: Achieving   Productivity and Reliability in Data-Intensive Research. arXiv preprint   arXiv:2401.00077v2. 2023 Dec 29.</li> </ul> <ul> <li>RRID: TBD</li> </ul>"},{"location":"about/citation/#why-cite-datajoint","title":"Why Cite DataJoint?","text":"<p>By citing DataJoint and its associated resources:</p> <p>You give credit to the authors and contributors who developed these tools.</p> <p>You help other researchers identify and use these tools effectively.</p> <p>You strengthen the visibility and impact of open-source tools in scientific research.</p> <p>For further questions or assistance with citations, please reach out to the DataJoint support team (support@datajoint.com).</p>"},{"location":"about/contributing/","title":"Contributing to DataJoint","text":"<p>DataJoint is developed openly and welcomes contributions from the community.</p>"},{"location":"about/contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"about/contributing/#report-issues","title":"Report Issues","text":"<p>Found a bug or have a feature request? Open an issue on GitHub:</p> <ul> <li>datajoint-python issues</li> <li>datajoint-docs issues</li> </ul>"},{"location":"about/contributing/#propose-enhancements-rfc-process","title":"Propose Enhancements (RFC Process)","text":"<p>For significant changes to DataJoint\u2014new features, API changes, or specification updates\u2014we use an RFC (Request for Comments) process via GitHub Discussions.</p> <p>When to use an RFC:</p> <ul> <li>API changes or new features in datajoint-python</li> <li>Changes to the DataJoint specification</li> <li>Breaking changes or deprecations</li> <li>Major documentation restructuring</li> </ul> <p>RFC Process:</p> <ol> <li> <p>Propose \u2014 Create a new Discussion using the RFC template in the appropriate repository:</p> <ul> <li>datajoint-python Discussions</li> <li>datajoint-docs Discussions</li> </ul> </li> <li> <p>Discuss \u2014 Community and maintainers provide feedback (2-4 weeks). Use \ud83d\udc4d/\ud83d\udc4e reactions to signal support. Prototyping in parallel is encouraged.</p> </li> <li> <p>Final Comment Period \u2014 Once consensus emerges, maintainers announce a 1-2 week final comment period. No changes during this time.</p> </li> <li> <p>Decision \u2014 RFC is accepted, rejected, or postponed. Accepted RFCs become tracking issues for implementation.</p> </li> </ol> <p>RFC Labels:</p> Label Meaning <code>rfc</code> All enhancement proposals <code>status: proposed</code> Initial submission <code>status: under-review</code> Active discussion <code>status: final-comment</code> Final comment period <code>status: accepted</code> Approved for implementation <code>status: rejected</code> Not accepted <code>status: postponed</code> Deferred to future <p>Tips for a good RFC:</p> <ul> <li>Search existing discussions first</li> <li>Include concrete use cases and code examples</li> <li>Consider backwards compatibility</li> <li>Start with motivation before diving into design</li> </ul>"},{"location":"about/contributing/#improve-documentation","title":"Improve Documentation","text":"<p>Documentation improvements are valuable contributions:</p> <ol> <li>Fork the datajoint-docs repository</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"about/contributing/#contribute-code","title":"Contribute Code","text":"<p>For code contributions to datajoint-python:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Write tests for your changes</li> <li>Ensure all tests pass</li> <li>Submit a pull request</li> </ol> <p>See the Developer Guide for detailed instructions.</p>"},{"location":"about/contributing/#development-setup","title":"Development Setup","text":""},{"location":"about/contributing/#datajoint-python","title":"datajoint-python","text":"<pre><code>git clone https://github.com/datajoint/datajoint-python.git\ncd datajoint-python\npip install -e \".[dev]\"\npre-commit install\n</code></pre>"},{"location":"about/contributing/#datajoint-docs","title":"datajoint-docs","text":"<pre><code>git clone https://github.com/datajoint/datajoint-docs.git\ncd datajoint-docs\npip install -r pip_requirements.txt\nmkdocs serve\n</code></pre>"},{"location":"about/contributing/#code-style","title":"Code Style","text":"<ul> <li>Python code follows PEP 8</li> <li>Docstrings use NumPy style</li> <li>Pre-commit hooks enforce formatting</li> </ul>"},{"location":"about/contributing/#testing","title":"Testing","text":"<p>See the Developer Guide for current testing instructions using <code>pixi</code> and <code>testcontainers</code>.</p>"},{"location":"about/contributing/#questions","title":"Questions?","text":"<ul> <li>Open a GitHub Discussion</li> </ul>"},{"location":"about/history/","title":"History","text":"<p>Dimitri Yatsenko began development of DataJoint in Andreas S. Tolias' lab in the Neuroscience Department at Baylor College of Medicine in the fall of 2009. Initially implemented as a thin MySQL API in MATLAB, it defined the major principles of the DataJoint model. The original DataJoint project is archived on Google Code.</p> <p>In 2015, additional contributors joined to develop the Python implementation, resulting in the foundational publication describing the DataJoint framework.</p> <p>In 2016, Vathes LLC was founded to provide support to groups using DataJoint.</p> <p>In 2017, DARPA awarded a Phase I SBIR grant (Contract D17PC00162, PI: Dimitri Yatsenko, $150,000, 2017\u20132018) titled \"Tools for Sharing and Analyzing Neuroscience Data\" to further develop and publicize the DataJoint framework.</p> <p>In 2018, the key theoretical framework was formulated in \"DataJoint: A Simpler Relational Data Model\", establishing the formal basis for DataJoint's approach to scientific data management.</p> <p>In 2022, NIH awarded a Phase II SBIR grant (R44 NS129492, PI: Dimitri Yatsenko, $2,124,457, 2022\u20132024) titled \"DataJoint SciOps: A Managed Service for Neuroscience Data Workflows\" to DataJoint (then Vathes LLC) in collaboration with the Johns Hopkins University Applied Physics Laboratory (Co-PI: Erik C. Johnson) to build a scalable cloud platform for DataJoint pipelines.</p>"},{"location":"about/history/#datajoint-elements","title":"DataJoint Elements","text":"<p>DataJoint Elements is an NIH-funded project (U24 NS116470, PI: Dimitri Yatsenko, $3,780,000, 2020\u20132025) titled \"DataJoint Pipelines for Neurophysiology.\" The project developed standard, open-source data pipelines for neurophysiology research (Press Release).</p> <p>Building on DataJoint's workflow framework, Elements provides curated, modular components for common experimental modalities including calcium imaging, electrophysiology, pose estimation, and optogenetics. The project distilled best practices from leading neuroscience labs into reusable pipeline modules that integrate with third-party analysis tools (Suite2p, DeepLabCut, Kilosort, etc.) and data standards (NWB, DANDI).</p> <p>The project is described in the position paper \"DataJoint Elements: Data Workflows for Neurophysiology\".</p>"},{"location":"about/history/#recent-developments","title":"Recent Developments","text":"<p>In January 2024, Vathes LLC was re-incorporated as DataJoint Inc.</p> <p>In 2025, Jim Olson was appointed as CEO of DataJoint (Press Release).</p> <p>In August 2025, DataJoint closed a $4.9M seed funding round to expand data management and AI capabilities in academic and life sciences (Press Release).</p> <p>Today, DataJoint is used in hundreds of research labs worldwide for managing scientific data pipelines.</p>"},{"location":"about/publications/","title":"Publications","text":"<p>The following publications relied on DataJoint open-source software for data analysis. If your work uses DataJoint or DataJoint Elements, please cite the respective manuscripts and RRIDs.</p>"},{"location":"about/publications/#2025","title":"2025","text":"<ul> <li>Campagner, D., Bhagat, J., Lopes, G., Calcaterra, L., Pouget, A. G., Almeida, A., ... &amp; SWC GCNU Experimental Neuroethology Group. (2025). Aeon: an open-source platform to study the neural basis of ethological behaviours over naturalistic timescales. bioRxiv.</li> </ul> <ul> <li>Bae, J. A., Baptiste, M., Bodor, A. L., Brittain, D., Buchanan, J., Bumbarger, D. J., Castro, M. A., Celii, B., Cobos, E., Collman, F., ... (2025). Functional connectomics spanning multiple areas of mouse visual cortex. Nature, 640(8058), 435-447.</li> </ul> <ul> <li>Celii, B., Papadopoulos, S., Ding, Z., Fahey, P. G., Wang, E., Papadopoulos, C., ... &amp; Reimer, J. (2025). NEURD offers automated proofreading and feature extraction for connectomics. Nature, 640(8058), 487-496.</li> </ul> <ul> <li>Ding, Z., Fahey, P.G., Papadopoulos, S., Wang, E.Y., Celii, B., Papadopoulos, C., Chang, A., Kunin, A.B., Tran, D., Fu, J. ... &amp; Tolias, A. S. (2025). Functional connectomics reveals general wiring rule in mouse visual cortex. Nature, 640(8058), 459-469.</li> </ul> <ul> <li>Dyszkant, N., Oesterle, J., Qiu, Y., Harrer, M., Schubert, T., Gonschorek, D. &amp; Euler, T. (2025). Photoreceptor degeneration has heterogeneous effects on functional retinal ganglion cell types. The Journal of Physiology, 603(21), 6599-6621.</li> </ul> <ul> <li>Finkelstein, A., Daie, K., R\u00f3zsa, M., Darshan, R. &amp; Svoboda, K. (2025). Connectivity underlying motor cortex activity during goal-directed behaviour. Nature.</li> </ul> <ul> <li>Gillon, C.J., Baker, C., Ly, R., Balzani, E., Brunton, B.W., Schottdorf, M., Ghosh, S. and Dehghani, N.(2025). Open data in neurophysiology: Advancements, solutions &amp; challenges. eNeuro, 12(11).</li> </ul> <ul> <li>Huang, J.Y., Hess, M., Bajpai, A., Li, X., Hobson, L.N., Xu, A.J., Barton, S.J. and Lu, H.C.(2025). From initial formation to developmental refinement: GABAergic inputs shape neuronal subnetworks in the primary somatosensory cortex. iScience, 28(3).</li> </ul> <ul> <li>Lee, K. H., Denovellis, E. L., Ly, R., Magland, J., Soules, J., Comrie, A. E., Gramling, D. P., Guidera, J. A., Nevers, R., Adenekan, P., Brozdowski, C., Bray, S. R., Monroe, E., Bak, J. H., Coulter, M. E., Sun, X., Broyles, E., Shin, D., Chiang, S., Holobetz, C., ... Frank, L. M. (2025). Spyglass: a data analysis framework for reproducible and shareable neuroscience research. eLife.</li> </ul> <ul> <li>Lees, R.M., Bianco, I.H., Campbell, R.A.A., Orlova, N., Peterka, D.S., Pichler, B., Smith, S.L., Yatsenko, D., Yu, C.H. &amp; Packer, A.M. (2025). Standardized measurements for monitoring and comparing multiphoton microscope systems. Nature Protocols, 20, 2171\u20132208.</li> </ul> <ul> <li>Schmors, L., Kotkat, A.H., Bauer, Y., Huang, Z., Crombie, D., Meyerolbersleben, L.S., Sokoloski, S., Berens, P. &amp; Busse, L. (2025). Effects of corticothalamic feedback depend on visual responsiveness and stimulus type. iScience, 28, 112481.</li> </ul> <ul> <li>Sibener, L.J., Mosberger, A.C., Chen, T.X., Athalye, V.R., Murray, J.M. &amp; Costa, R.M. (2025). Dissociable roles of distinct thalamic circuits in learning reaches to spatial targets. Nature Communications, 16, 2962.</li> </ul>"},{"location":"about/publications/#2024","title":"2024","text":"<ul> <li>Chen, S., Liu, Y., Wang, Z. A., Colonell, J., Liu, L. D., Hou, H., ... &amp; Svoboda, K. (2024). Brain-wide neural activity underlying memory-guided movement. Cell, 187(3), 676-691.</li> </ul> <ul> <li>Gonzalo Cogno, S., Obenhaus, H. A., Lautrup, A., Jacobsen, R. I., Clopath, C., Andersson, S. O., ... &amp; Moser, E. I. (2024). Minute-scale oscillatory sequences in medial entorhinal cortex. Nature, 625(7994), 338-344.</li> </ul> <ul> <li>Korympidou, M.M., Strauss, S., Schubert, T., Franke, K., Berens, P., Euler, T. &amp; Vlasits, A.L. (2024). GABAergic amacrine cells balance biased chromatic information in the mouse retina. Cell Reports, 43(11), 114953.</li> </ul> <ul> <li>Mosberger, A.C., Sibener, L.J., Chen, T.X., Rodrigues, H.F., Hormigo, R., Ingram, J.N., Athalye, V.R., Tabachnik, T., Wolpert, D.M., Murray, J.M. and Costa, R.M. (2024). Exploration biases forelimb reaching strategies. Cell Reports, 43(4).</li> </ul> <ul> <li>Reimer, M. L., Kauer, S. D., Benson, C. A., King, J. F., Patwa, S., Feng, S., Estacion, M. A., Bangalore, L., Waxman, S. G., &amp; Tan, A. M. (2024). A FAIR, open-source virtual reality platform for dendritic spine analysis. Patterns, 5(9).</li> </ul>"},{"location":"about/publications/#2023","title":"2023","text":"<ul> <li>Willeke, K.F., Restivo, K., Franke, K., Nix, A.F., Cadena, S.A., Shinn, T., Nealley, C., Rodriguez, G., Patel, S., Ecker, A.S., Sinz, F.H. &amp; Tolias, A.S. (2023). Deep learning-driven characterization of single cell tuning in primate visual area V4 supports topological organization. bioRxiv.</li> </ul> <ul> <li>Laboratory, I. B., Bonacchi, N., Chapuis, G. A., Churchland, A. K., DeWitt, E. E., Faulkner, M., ... &amp; Wells, M. J. (2023). A modular architecture for organizing, processing and sharing neurophysiology data. Nature Methods. 1-5.</li> </ul>"},{"location":"about/publications/#2022","title":"2022","text":"<ul> <li>Franke, K., Willeke, K. F., Ponder, K., Galdamez, M., Zhou, N., Muhammad, T., ... &amp; Tolias, A. S. (2022). State-dependent pupil dilation rapidly shifts visual feature selectivity. Nature, 610(7930), 128-134.</li> </ul> <ul> <li>Wang, Y., Chiola, S., Yang, G., Russell, C., Armstrong, C.J., Wu, Y., ... &amp; Shcheglovitov, A. (2022). Modeling human telencephalic development and autism-associated SHANK3 deficiency using organoids generated from single neural rosettes. Nature Communications, 13, 5688.</li> </ul> <ul> <li>Goetz, J., Jessen, Z. F., Jacobi, A., Mani, A., Cooler, S., Greer, D., ... &amp; Schwartz, G. W. (2022). Unified classification of mouse retinal ganglion cells using function, morphology, and gene expression. Cell Reports, 40(2), 111040.</li> </ul> <ul> <li>Obenhaus, H.A., Zong, W., Jacobsen, R.I., Rose, T., Donato, F., Chen, L., Cheng, H., Bonhoeffer, T., Moser, M.B. &amp; Moser, E.I. (2022). Functional network topography of the medial entorhinal cortex. Proceedings of the National Academy of Sciences, 119(7).</li> </ul> <ul> <li>Pettit, N. H., Yap, E., Greenberg, M. E., Harvey, C. D. (2022). Fos ensembles encode and shape stable spatial maps in the hippocampus. Nature.</li> </ul> <ul> <li>Tseng, S. Y., Chettih, S. N., Arlt, C., Barroso-Luque, R., &amp; Harvey, C. D. (2022). Shared and specialized coding across posterior cortical areas for dynamic navigation decisions. Neuron.</li> </ul> <ul> <li>Turner, N. L., Macrina, T., Bae, J. A., Yang, R., Wilson, A. M., Schneider-Mizell, C., ... &amp; Seung, H. S. (2022). Reconstruction of neocortex: Organelles, compartments, cells, circuits, and activity. Cell, 185(6), 1082-1100.</li> </ul> <ul> <li>Zong, W., Obenhaus, H.A., Skytoen, E.R., Eneqvist, H., de Jong, N.L., Vale, R., Jorge, M.R., Moser, M.B. and Moser, E.I. (2022). Large-scale two-photon calcium imaging in freely moving mice. Cell, 185(7), 1240-1256.</li> </ul>"},{"location":"about/publications/#2021","title":"2021","text":"<ul> <li>Dennis, E.J., El Hady, A., Michaiel, A., Clemens, A., Tervo, D.R.G., Voigts, J. &amp; Datta, S.R. (2021). Systems Neuroscience of Natural Behaviors in Rodents. Journal of Neuroscience, 41(5), 911-919.</li> </ul> <ul> <li>Born, G., Schneider-Soupiadis, F. A., Erisken, S., Vaiceliunaite, A., Lao, C. L., Mobarhan, M. H., Spacek, M. A., Einevoll, G. T., &amp; Busse, L. (2021). Corticothalamic feedback sculpts visual spatial integration in mouse thalamus. Nature Neuroscience, 24(12), 1711-1720.</li> </ul> <ul> <li>Finkelstein, A., Fontolan, L., Economo, M. N., Li, N., Romani, S., &amp; Svoboda, K. (2021). Attractor dynamics gate cortical information flow during decision-making. Nature Neuroscience, 24(6), 843-850.</li> </ul> <ul> <li>Laboratory, T. I. B., Aguillon-Rodriguez, V., Angelaki, D., Bayer, H., Bonacchi, N., Carandini, M., Cazettes, F., Chapuis, G., Churchland, A. K., Dan, Y., ... (2021). Standardized and reproducible measurement of decision-making in mice. eLife, 10.</li> </ul>"},{"location":"about/publications/#2020","title":"2020","text":"<ul> <li>Angelaki, D. E., Ng, J., Abrego, A. M., Cham, H. X., Asprodini, E. K., Dickman, J. D., &amp; Laurens, J. (2020). A gravity-based three-dimensional compass in the mouse brain. Nature Communications, 11(1), 1-13.</li> </ul> <ul> <li>Heath, S. L., Christenson, M. P., Oriol, E., Saavedra-Weisenhaus, M., Kohn, J. R., &amp; Behnia, R. (2020). Circuit mechanisms underlying chromatic encoding in drosophila photoreceptors. Current Biology.</li> </ul> <ul> <li>Yatsenko, D., Moreaux, L. C., Choi, J., Tolias, A., Shepard, K. L., &amp; Roukes, M. L. (2020). Signal separability in integrated neurophotonics. bioRxiv.</li> </ul>"},{"location":"about/publications/#2019","title":"2019","text":"<ul> <li>Chettih, S. N., &amp; Harvey, C. D. (2019). Single-neuron perturbations reveal feature-specific competition in V1. Nature, 567(7748), 334-340.</li> </ul> <ul> <li>Walker, E. Y., Sinz, F. H., Cobos, E., Muhammad, T., Froudarakis, E., Fahey, P. G., Ecker, A. S., Reimer, J., Pitkow, X., &amp; Tolias, A. S. (2019). Inception loops discover what excites neurons most using deep predictive models. Nature Neuroscience, 22(12), 2060-2065.</li> </ul>"},{"location":"about/publications/#2018","title":"2018","text":"<ul> <li>Denfield, G. H., Ecker, A. S., Shinn, T. J., Bethge, M., &amp; Tolias, A. S. (2018). Attentional fluctuations induce shared variability in macaque primary visual cortex. Nature Communications, 9(1), 2654.</li> </ul>"},{"location":"about/publications/#2017","title":"2017","text":"<ul> <li>Franke, K., Berens, P., Schubert, T., Bethge, M., Euler, T., &amp; Baden, T. (2017). Inhibition decorrelates visual feature representations in the inner retina. Nature, 542(7642), 439.</li> </ul>"},{"location":"about/publications/#2016","title":"2016","text":"<ul> <li>Baden, T., Berens, P., Franke, K., Rosen, M. R., Bethge, M., &amp; Euler, T. (2016). The functional diversity of retinal ganglion cells in the mouse. Nature, 529(7586), 345-350.</li> </ul> <ul> <li>Reimer, J., McGinley, M. J., Liu, Y., Rodenkirch, C., Wang, Q., McCormick, D. A., &amp; Tolias, A. S. (2016). Pupil fluctuations track rapid changes in adrenergic and cholinergic activity in cortex. Nature Communications, 7, 13289.</li> </ul>"},{"location":"about/publications/#2015","title":"2015","text":"<ul> <li>Jiang, X., Shen, S., Cadwell, C. R., Berens, P., Sinz, F., Ecker, A. S., Patel, S., &amp; Tolias, A. S. (2015). Principles of connectivity among morphologically defined cell types in adult neocortex. Science, 350(6264), aac9462.</li> </ul>"},{"location":"about/publications/#2014","title":"2014","text":"<ul> <li>Froudarakis, E., Berens, P., Ecker, A. S., Cotton, R. J., Sinz, F. H., Yatsenko, D., Saggau, P., Bethge, M., &amp; Tolias, A. S. (2014). Population code in mouse V1 facilitates readout of natural scenes through increased sparseness. Nature Neuroscience, 17(6), 851-857.</li> </ul> <ul> <li>Reimer, J., Froudarakis, E., Cadwell, C. R., Yatsenko, D., Denfield, G. H., &amp; Tolias, A. S. (2014). Pupil fluctuations track fast switching of cortical states during quiet wakefulness. Neuron, 84(2), 355-362.</li> </ul>"},{"location":"about/versioning/","title":"Documentation Versioning","text":"<p>This page explains how version information is indicated throughout the DataJoint documentation.</p>"},{"location":"about/versioning/#documentation-scope","title":"Documentation Scope","text":"<p>This documentation covers DataJoint 2.0 and later. All code examples and tutorials use DataJoint 2.0+ syntax and APIs.</p> <p>DataJoint 2.0 is the baseline. Features and APIs introduced in 2.0 are documented without version markers, as they are the standard for this documentation.</p> <p>If you're using legacy DataJoint (version 0.14.x or earlier), please visit the legacy documentation or follow the Migration Guide to upgrade.</p>"},{"location":"about/versioning/#version-indicators","title":"Version Indicators","text":""},{"location":"about/versioning/#global-indicators","title":"Global Indicators","text":"<p>Site-wide banner: Every page displays a banner indicating you're viewing documentation for DataJoint 2.0+, with a link to the migration guide for legacy users.</p>"},{"location":"about/versioning/#feature-level-indicators","title":"Feature-Level Indicators","text":"<p>Version admonitions are used for features introduced after 2.0 (i.e., version 2.1 and later):</p>"},{"location":"about/versioning/#new-features","title":"New Features","text":"<p>New in 2.1</p> <p>This indicates a feature that was introduced after the 2.0 baseline.</p> <p>Example usage:</p> <p>New in 2.1</p> <p>The <code>dj.Top</code> operator with ordering support was introduced in DataJoint 2.1.</p> <p>Note: Features present in DataJoint 2.0 (the baseline) are not marked with version indicators.</p>"},{"location":"about/versioning/#changed-behavior","title":"Changed Behavior","text":"<p>Changed in 2.1</p> <p>This indicates behavior that changed in a post-2.0 release.</p> <p>Example usage:</p> <p>Changed in 2.1</p> <p>The <code>populate()</code> method now supports priority-based scheduling by default.</p> <p>Use <code>priority=50</code> to control execution order when using <code>reserve_jobs=True</code>.</p>"},{"location":"about/versioning/#deprecated-features","title":"Deprecated Features","text":"<p>Deprecated in 2.1, removed in 3.0</p> <p>This indicates features that are deprecated and will be removed in future versions.</p> <p>Example usage:</p> <p>Deprecated in 2.1, removed in 3.0</p> <p>The <code>allow_direct_insert</code> parameter is deprecated. Use <code>dj.config['safemode']</code> instead.</p> <p>Note: Features deprecated at the 2.0 baseline (coming from pre-2.0) are documented in the Migration Guide rather than with admonitions, since this documentation assumes 2.0 as the baseline.</p>"},{"location":"about/versioning/#inline-version-badges","title":"Inline Version Badges","text":"<p>For features introduced after 2.0, inline version badges may appear in API reference:</p> <ul> <li><code>dj.Top()</code> v2.1+ - Top N restriction with ordering</li> <li><code>some_method()</code> deprecated - Legacy method</li> </ul> <p>Note: Methods and features present in DataJoint 2.0 (the baseline) do not have version badges.</p>"},{"location":"about/versioning/#checking-your-version","title":"Checking Your Version","text":"<p>To check which version of DataJoint you're using:</p> <pre><code>import datajoint as dj\nprint(dj.__version__)\n</code></pre> <ul> <li>Version 2.0 or higher: You're on the current version</li> <li>Version 0.14.x or lower: You're on legacy DataJoint</li> </ul>"},{"location":"about/versioning/#migration-path","title":"Migration Path","text":"<p>If you're upgrading from legacy DataJoint (pre-2.0):</p> <ol> <li>Review the What's New in 2.0 page to understand major changes</li> <li>Follow the Migration Guide for step-by-step upgrade instructions</li> <li>Reference this documentation for updated syntax and APIs</li> </ol>"},{"location":"about/versioning/#legacy-documentation","title":"Legacy Documentation","text":"<p>For DataJoint 0.x documentation, visit:</p> <p>datajoint.github.io/datajoint-python</p>"},{"location":"about/versioning/#version-history","title":"Version History","text":"Version Release Date Major Changes 2.1 2026 PostgreSQL multi-backend support 2.0 2026 Redesigned fetch API, unified stores, per-table jobs, semantic matching 0.14.x 2020-2025 Legacy version with external storage 0.13.x 2019 Legacy version <p>For complete version history, see the changelog.</p>"},{"location":"api/","title":"API Reference","text":"<p>Auto-generated documentation from DataJoint source code.</p>"},{"location":"api/#modules","title":"Modules","text":"Module Description Package Main datajoint package exports Connection Database connection management Schema Schema and VirtualModule classes Table Base Table and FreeTable classes Table Types Manual, Lookup, Imported, Computed, Part Expressions Query expressions and operators Heading Table heading and attributes Diagram Schema visualization Settings Configuration management Errors Exception classes Codecs Type codec system Blob Binary serialization Hash Registry Content hashing for external storage Jobs Job queue for AutoPopulate Migrate Schema migration utilities"},{"location":"api/datajoint/","title":"Package","text":"<p>Main datajoint package exports</p> <p>DataJoint for Python \u2014 a framework for scientific data pipelines.</p> <p>DataJoint introduces the Relational Workflow Model, where your database schema is an executable specification of your workflow. Tables represent workflow steps, foreign keys encode dependencies, and computations are declarative.</p> <p>Documentation: https://docs.datajoint.com Source: https://github.com/datajoint/datajoint-python</p> <p>Copyright 2014-2026 DataJoint Inc. and contributors. Licensed under the Apache License, Version 2.0.</p> <p>If DataJoint contributes to a publication, please cite: https://doi.org/10.1101/031658</p>"},{"location":"api/datajoint/#datajoint.Codec","title":"Codec","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for codec types. Subclasses auto-register by name.</p> <p>Requires Python 3.10+.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str or None</code> <p>Unique identifier used in <code>&lt;name&gt;</code> syntax. Must be set by subclasses.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class GraphCodec(dj.Codec):\n...     name = \"graph\"\n...\n...     def get_dtype(self, is_store: bool) -&gt; str:\n...         return \"&lt;blob&gt;\"\n...\n...     def encode(self, graph, *, key=None, store_name=None):\n...         return {'nodes': list(graph.nodes()), 'edges': list(graph.edges())}\n...\n...     def decode(self, stored, *, key=None):\n...         import networkx as nx\n...         G = nx.Graph()\n...         G.add_nodes_from(stored['nodes'])\n...         G.add_edges_from(stored['edges'])\n...         return G\n</code></pre> <p>Use in table definitions::</p> <pre><code>class Connectivity(dj.Manual):\n    definition = '''\n    id : uint16\n    ---\n    graph_data : &lt;graph&gt;\n    '''\n</code></pre> <p>Skip auto-registration for abstract base classes::</p> <pre><code>class ExternalOnlyCodec(dj.Codec, register=False):\n    '''Abstract base - not registered.'''\n    ...\n</code></pre>"},{"location":"api/datajoint/#datajoint.Codec.get_dtype","title":"get_dtype  <code>abstractmethod</code>","text":"<pre><code>get_dtype(is_store)\n</code></pre> <p>Return the storage dtype for this codec.</p> <p>Parameters:</p> Name Type Description Default <code>is_store</code> <code>bool</code> <p>True if <code>@</code> modifier present (object store vs inline).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A core type (e.g., <code>\"bytes\"</code>, <code>\"json\"</code>) or another codec (e.g., <code>\"&lt;hash&gt;\"</code>).</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If store mode not supported but requested.</p>"},{"location":"api/datajoint/#datajoint.Codec.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(value, *, key=None, store_name=None)\n</code></pre> <p>Encode Python value for storage.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>any</code> <p>The Python object to store.</p> required <code>key</code> <code>dict</code> <p>Primary key values. May be needed for path construction.</p> <code>None</code> <code>store_name</code> <code>str</code> <p>Target store name for object storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>any</code> <p>Value in the format expected by the dtype.</p>"},{"location":"api/datajoint/#datajoint.Codec.decode","title":"decode  <code>abstractmethod</code>","text":"<pre><code>decode(stored, *, key=None)\n</code></pre> <p>Decode stored value back to Python.</p> <p>Parameters:</p> Name Type Description Default <code>stored</code> <code>any</code> <p>Data retrieved from storage.</p> required <code>key</code> <code>dict</code> <p>Primary key values.</p> <code>None</code> <p>Returns:</p> Type Description <code>any</code> <p>The reconstructed Python object.</p>"},{"location":"api/datajoint/#datajoint.Codec.validate","title":"validate","text":"<pre><code>validate(value)\n</code></pre> <p>Validate a value before encoding.</p> <p>Override this method to add type checking or domain constraints. Called automatically before <code>encode()</code> during INSERT operations. The default implementation accepts any value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>any</code> <p>The value to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the value has an incompatible type.</p> <code>ValueError</code> <p>If the value fails domain validation.</p>"},{"location":"api/datajoint/#datajoint.get_codec","title":"get_codec","text":"<pre><code>get_codec(name)\n</code></pre> <p>Retrieve a registered codec by name.</p> <p>Looks up the codec in the explicit registry first, then attempts to load from installed packages via entry points.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The codec name, with or without angle brackets. Store parameters (e.g., <code>\"&lt;blob@cold&gt;\"</code>) are stripped.</p> required <p>Returns:</p> Type Description <code>Codec</code> <p>The registered Codec instance.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If the codec is not found.</p>"},{"location":"api/datajoint/#datajoint.list_codecs","title":"list_codecs","text":"<pre><code>list_codecs()\n</code></pre> <p>List all registered codec names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of registered codec names.</p>"},{"location":"api/datajoint/#datajoint.SchemaCodec","title":"SchemaCodec","text":"<p>               Bases: <code>Codec</code></p> <p>Abstract base class for schema-addressed codecs.</p> <p>Schema-addressed storage is an OAS (Object-Augmented Schema) addressing scheme where paths mirror the database schema structure: <code>{schema}/{table}/{pk}/{attribute}</code>. This creates a browsable organization in object storage that reflects the schema design.</p> <p>Subclasses must implement:     - <code>name</code>: Codec name for <code>&lt;name@&gt;</code> syntax     - <code>encode()</code>: Serialize and upload content     - <code>decode()</code>: Create lazy reference from metadata     - <code>validate()</code>: Validate input values</p> <p>Helper Methods:     - <code>_extract_context()</code>: Parse key dict into schema/table/field/pk     - <code>_build_path()</code>: Construct storage path from context     - <code>_get_backend()</code>: Get storage backend by name</p> <p>Comparison with Hash-addressed:     - Schema-addressed (this): Path from schema structure, no dedup     - Hash-addressed: Path from content hash, automatic dedup</p> <p>Example::</p> <pre><code>class MyCodec(SchemaCodec):\n    name = \"my\"\n\n    def encode(self, value, *, key=None, store_name=None):\n        schema, table, field, pk = self._extract_context(key)\n        path, _ = self._build_path(schema, table, field, pk, ext=\".dat\")\n        backend = self._get_backend(store_name)\n        backend.put_buffer(serialize(value), path)\n        return {\"path\": path, \"store\": store_name, ...}\n\n    def decode(self, stored, *, key=None):\n        backend = self._get_backend(stored.get(\"store\"))\n        return MyRef(stored, backend)\n</code></pre> See Also <p>HashCodec : Hash-addressed storage with content deduplication. ObjectCodec : Schema-addressed storage for files/folders. NpyCodec : Schema-addressed storage for numpy arrays.</p>"},{"location":"api/datajoint/#datajoint.SchemaCodec.get_dtype","title":"get_dtype","text":"<pre><code>get_dtype(is_store)\n</code></pre> <p>Return storage dtype. Schema-addressed codecs require @ modifier.</p> <p>Parameters:</p> Name Type Description Default <code>is_store</code> <code>bool</code> <p>Must be True for schema-addressed codecs.</p> required <p>Returns:</p> Type Description <code>str</code> <p>\"json\" for metadata storage.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If is_store is False (@ modifier missing).</p>"},{"location":"api/datajoint/#datajoint.NpyRef","title":"NpyRef","text":"<p>Lazy reference to a numpy array stored as a .npy file.</p> <p>This class provides metadata access without I/O and transparent integration with numpy operations via the <code>__array__</code> protocol.</p> <p>Attributes:</p> Name Type Description <code>shape</code> <code>tuple[int, ...]</code> <p>Array shape (from metadata, no I/O).</p> <code>dtype</code> <code>dtype</code> <p>Array dtype (from metadata, no I/O).</p> <code>path</code> <code>str</code> <p>Storage path within the store.</p> <code>store</code> <code>str or None</code> <p>Store name (None for default).</p> <p>Examples:</p> <p>Metadata access without download::</p> <pre><code>ref = (Recording &amp; key).fetch1('waveform')\nprint(ref.shape)  # (1000, 32) - no download\nprint(ref.dtype)  # float64 - no download\n</code></pre> <p>Explicit loading::</p> <pre><code>arr = ref.load()  # Downloads and returns np.ndarray\n</code></pre> <p>Transparent numpy integration::</p> <pre><code># These all trigger automatic download via __array__\nresult = ref + 1\nresult = np.mean(ref)\nresult = ref[0:100]  # Slicing works too\n</code></pre>"},{"location":"api/datajoint/#datajoint.NpyRef.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>Array shape (no I/O required).</p>"},{"location":"api/datajoint/#datajoint.NpyRef.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype\n</code></pre> <p>Array dtype (no I/O required).</p>"},{"location":"api/datajoint/#datajoint.NpyRef.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim\n</code></pre> <p>Number of dimensions (no I/O required).</p>"},{"location":"api/datajoint/#datajoint.NpyRef.size","title":"size  <code>property</code>","text":"<pre><code>size\n</code></pre> <p>Total number of elements (no I/O required).</p>"},{"location":"api/datajoint/#datajoint.NpyRef.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes\n</code></pre> <p>Total bytes (estimated from shape and dtype, no I/O required).</p>"},{"location":"api/datajoint/#datajoint.NpyRef.path","title":"path  <code>property</code>","text":"<pre><code>path\n</code></pre> <p>Storage path within the store.</p>"},{"location":"api/datajoint/#datajoint.NpyRef.store","title":"store  <code>property</code>","text":"<pre><code>store\n</code></pre> <p>Store name (None for default store).</p>"},{"location":"api/datajoint/#datajoint.NpyRef.is_loaded","title":"is_loaded  <code>property</code>","text":"<pre><code>is_loaded\n</code></pre> <p>True if array data has been downloaded and cached.</p>"},{"location":"api/datajoint/#datajoint.NpyRef.load","title":"load","text":"<pre><code>load(mmap_mode=None)\n</code></pre> <p>Download and return the array.</p> <p>Parameters:</p> Name Type Description Default <code>mmap_mode</code> <code>str</code> <p>Memory-map mode for lazy, random-access loading of large arrays:</p> <ul> <li><code>'r'</code>: Read-only</li> <li><code>'r+'</code>: Read-write</li> <li><code>'c'</code>: Copy-on-write (changes not saved to disk)</li> </ul> <p>If None (default), loads entire array into memory.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray or memmap</code> <p>The array data. Returns <code>numpy.memmap</code> if mmap_mode is specified.</p> Notes <p>When <code>mmap_mode</code> is None, the array is cached after first load.</p> <p>For local filesystem stores, memory mapping accesses the file directly with no download. For remote stores (S3, etc.), the file is downloaded to a local cache (<code>{tempdir}/datajoint_mmap/</code>) before memory mapping.</p> <p>Examples:</p> <p>Standard loading::</p> <pre><code>arr = ref.load()  # Loads entire array into memory\n</code></pre> <p>Memory-mapped for random access to large arrays::</p> <pre><code>arr = ref.load(mmap_mode='r')\nslice = arr[1000:2000]  # Only reads the needed portion from disk\n</code></pre>"},{"location":"api/datajoint/#datajoint.MatCell","title":"MatCell","text":"<p>               Bases: <code>ndarray</code></p> <p>NumPy ndarray subclass representing a MATLAB cell array.</p> <p>Used to distinguish cell arrays from regular arrays during serialization for MATLAB compatibility.</p>"},{"location":"api/datajoint/#datajoint.MatStruct","title":"MatStruct","text":"<p>               Bases: <code>recarray</code></p> <p>NumPy recarray subclass representing a MATLAB struct array.</p> <p>Used to distinguish struct arrays from regular recarrays during serialization for MATLAB compatibility.</p>"},{"location":"api/datajoint/#datajoint.Connection","title":"Connection","text":"<p>Manages a connection to a database server.</p> <p>Catalogues schemas, tables, and their dependencies (foreign keys). Most parameters should be set in the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Hostname, may include port as <code>hostname:port</code>.</p> required <code>user</code> <code>str</code> <p>Database username.</p> required <code>password</code> <code>str</code> <p>Database password.</p> required <code>port</code> <code>int</code> <p>Port number. Overridden if specified in host.</p> <code>None</code> <code>init_fun</code> <code>str</code> <p>SQL initialization command.</p> <code>None</code> <code>use_tls</code> <code>bool or dict</code> <p>TLS encryption option.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>schemas</code> <code>dict</code> <p>Registered schema objects.</p> <code>dependencies</code> <code>Dependencies</code> <p>Foreign key dependency graph.</p>"},{"location":"api/datajoint/#datajoint.Connection.connect","title":"connect","text":"<pre><code>connect()\n</code></pre> <p>Establish or re-establish connection to the database server.</p>"},{"location":"api/datajoint/#datajoint.Connection.set_query_cache","title":"set_query_cache","text":"<pre><code>set_query_cache(query_cache=None)\n</code></pre> <p>Enable query caching mode.</p> <p>When enabled: 1. Only SELECT queries are allowed 2. Results are cached under <code>dj.config['query_cache']</code> 3. Cache key differentiates cache states</p> <p>Parameters:</p> Name Type Description Default <code>query_cache</code> <code>str</code> <p>String to initialize the hash for query results. None disables caching.</p> <code>None</code>"},{"location":"api/datajoint/#datajoint.Connection.purge_query_cache","title":"purge_query_cache","text":"<pre><code>purge_query_cache()\n</code></pre> <p>Delete all cached query results.</p>"},{"location":"api/datajoint/#datajoint.Connection.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the database connection.</p>"},{"location":"api/datajoint/#datajoint.Connection.register","title":"register","text":"<pre><code>register(schema)\n</code></pre> <p>Register a schema with this connection.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>Schema object to register.</p> required"},{"location":"api/datajoint/#datajoint.Connection.ping","title":"ping","text":"<pre><code>ping()\n</code></pre> <p>Ping the server to verify connection is alive.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the connection is closed.</p>"},{"location":"api/datajoint/#datajoint.Connection.is_connected","title":"is_connected  <code>property</code>","text":"<pre><code>is_connected\n</code></pre> <p>Check if connected to the database server.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connected.</p>"},{"location":"api/datajoint/#datajoint.Connection.query","title":"query","text":"<pre><code>query(query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None)\n</code></pre> <p>Execute a SQL query and return the cursor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query to execute.</p> required <code>args</code> <code>tuple</code> <p>Query parameters for prepared statement.</p> <code>()</code> <code>as_dict</code> <code>bool</code> <p>If True, return rows as dictionaries. Default False.</p> <code>False</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress SQL library warnings. Default True.</p> <code>True</code> <code>reconnect</code> <code>bool</code> <p>If True, reconnect if disconnected. None uses config setting.</p> <code>None</code> <p>Returns:</p> Type Description <code>cursor</code> <p>Database cursor with query results.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If non-SELECT query during query caching mode.</p>"},{"location":"api/datajoint/#datajoint.Connection.get_user","title":"get_user","text":"<pre><code>get_user()\n</code></pre> <p>Get the current user and host.</p> <p>Returns:</p> Type Description <code>str</code> <p>User name and host as <code>'user@host'</code>.</p>"},{"location":"api/datajoint/#datajoint.Connection.in_transaction","title":"in_transaction  <code>property</code>","text":"<pre><code>in_transaction\n</code></pre> <p>Check if a transaction is open.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if a transaction is in progress.</p>"},{"location":"api/datajoint/#datajoint.Connection.start_transaction","title":"start_transaction","text":"<pre><code>start_transaction()\n</code></pre> <p>Start a new transaction.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If a transaction is already in progress.</p>"},{"location":"api/datajoint/#datajoint.Connection.cancel_transaction","title":"cancel_transaction","text":"<pre><code>cancel_transaction()\n</code></pre> <p>Cancel the current transaction and roll back all changes.</p>"},{"location":"api/datajoint/#datajoint.Connection.commit_transaction","title":"commit_transaction","text":"<pre><code>commit_transaction()\n</code></pre> <p>Commit all changes and close the transaction.</p>"},{"location":"api/datajoint/#datajoint.Connection.transaction","title":"transaction  <code>property</code>","text":"<pre><code>transaction\n</code></pre> <p>Context manager for transactions.</p> <p>Opens a transaction and automatically commits on success or rolls back on exception.</p> <p>Yields:</p> Type Description <code>Connection</code> <p>This connection object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with dj.conn().transaction:\n...     # All operations here are in one transaction\n...     table.insert(data)\n</code></pre>"},{"location":"api/datajoint/#datajoint.conn","title":"conn","text":"<pre><code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)\n</code></pre> <p>Return a persistent connection object shared by multiple modules.</p> <p>If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Database hostname.</p> <code>None</code> <code>user</code> <code>str</code> <p>Database username. Required if not set in config.</p> <code>None</code> <code>password</code> <code>str</code> <p>Database password. Required if not set in config.</p> <code>None</code> <code>init_fun</code> <code>callable</code> <p>Initialization function called after connection.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, reset existing connection. Default False.</p> <code>False</code> <code>use_tls</code> <code>bool or dict</code> <p>TLS encryption option: True (required), False (no TLS), None (preferred, default), or dict for manual configuration.</p> <code>None</code> <p>Returns:</p> Type Description <code>Connection</code> <p>Persistent database connection.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If user or password is not provided and not set in config.</p>"},{"location":"api/datajoint/#datajoint.DataJointError","title":"DataJointError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p>"},{"location":"api/datajoint/#datajoint.DataJointError.suggest","title":"suggest","text":"<pre><code>suggest(*args)\n</code></pre> <p>Regenerate the exception with additional arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>object</code> <p>Additional arguments to append to the exception.</p> <code>()</code> <p>Returns:</p> Type Description <code>DataJointError</code> <p>A new exception of the same type with the additional arguments.</p>"},{"location":"api/datajoint/#datajoint.AndList","title":"AndList","text":"<p>               Bases: <code>list</code></p> <p>List of conditions combined with logical AND.</p> <p>All conditions in the list are AND-ed together. Other collections (lists, sets, QueryExpressions) are OR-ed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr &amp; dj.AndList((cond1, cond2, cond3))\n# equivalent to\n&gt;&gt;&gt; expr &amp; cond1 &amp; cond2 &amp; cond3\n</code></pre>"},{"location":"api/datajoint/#datajoint.Not","title":"Not","text":"<p>Invert a restriction condition.</p> <p>Parameters:</p> Name Type Description Default <code>restriction</code> <code>any</code> <p>Restriction condition to negate.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; table - condition  # equivalent to table &amp; Not(condition)\n</code></pre>"},{"location":"api/datajoint/#datajoint.Top","title":"Top  <code>dataclass</code>","text":"<p>Restrict query to top N entities with ordering.</p> <p>In SQL, corresponds to <code>ORDER BY ... LIMIT ... OFFSET</code>.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of rows to return. Default 1.</p> <code>1</code> <code>order_by</code> <code>str or list[str] or None</code> <p>Attributes to order by. <code>\"KEY\"</code> for primary key order. <code>None</code> means inherit ordering from an existing Top (or default to KEY). Default <code>\"KEY\"</code>.</p> <code>'KEY'</code> <code>offset</code> <code>int</code> <p>Number of rows to skip. Default 0.</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; query &amp; dj.Top(5)                    # Top 5 by primary key\n&gt;&gt;&gt; query &amp; dj.Top(10, 'score DESC')     # Top 10 by score descending\n&gt;&gt;&gt; query &amp; dj.Top(10, order_by=None)    # Top 10, inherit existing order\n&gt;&gt;&gt; query &amp; dj.Top(5, offset=10)         # Skip 10, take 5\n</code></pre>"},{"location":"api/datajoint/#datajoint.Top.merge","title":"merge","text":"<pre><code>merge(other)\n</code></pre> <p>Merge another Top into this one (when other inherits ordering).</p> <p>Used when <code>other.order_by</code> is None or matches <code>self.order_by</code>.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Top</code> <p>The Top to merge. Its order_by should be None or equal to self.order_by.</p> required <p>Returns:</p> Type Description <code>Top</code> <p>New Top with merged limit/offset and preserved ordering.</p>"},{"location":"api/datajoint/#datajoint.U","title":"U","text":"<p>dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes.</p> <p>Restriction:</p> <p>dj.U can be used to enumerate unique combinations of values of attributes from other expressions.</p> <p>The following expression yields all unique combinations of contrast and brightness found in the <code>stimulus</code> set:</p> <p>dj.U('contrast', 'brightness') &amp; stimulus</p> <p>Aggregation:</p> <p>In aggregation, dj.U is used for summary calculation over an entire set:</p> <p>The following expression yields one element with one attribute <code>s</code> containing the total number of elements in query expression <code>expr</code>:</p> <p>dj.U().aggr(expr, n='count(*)')</p> <p>The following expressions both yield one element containing the number <code>n</code> of distinct values of attribute <code>attr</code> in query expression <code>expr</code>.</p> <p>dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')</p> <p>The following expression yields one element and one attribute <code>s</code> containing the sum of values of attribute <code>attr</code> over entire result set of expression <code>expr</code>:</p> <p>dj.U().aggr(expr, s='sum(attr)')</p> <p>The following expression yields the set of all unique combinations of attributes <code>attr1</code>, <code>attr2</code> and the number of their occurrences in the result set of query expression <code>expr</code>.</p> <p>dj.U(attr1,attr2).aggr(expr, n='count(*)')</p> <p>Joins:</p> <p>If expression <code>expr</code> has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as <code>expr</code> but <code>attr1</code> and <code>attr2</code> are promoted to the the primary key.  This is useful for producing a join on non-primary key attributes. For example, if <code>attr</code> is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename <code>attr</code> in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.</p>"},{"location":"api/datajoint/#datajoint.U.aggr","title":"aggr","text":"<pre><code>aggr(group, **named_attributes)\n</code></pre> <p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Note: exclude_nonmatching is always True for dj.U (cannot keep all rows from infinite set).</p> <p>:param group:  The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression</p>"},{"location":"api/datajoint/#datajoint.ObjectRef","title":"ObjectRef  <code>dataclass</code>","text":"<p>Handle to a file or folder stored in the pipeline's object storage backend.</p> <p>This class is returned when fetching object-type attributes. It provides metadata access without I/O, and methods for reading content directly from the storage backend.</p> <p>Attributes:     path: Relative path within the store (includes token)     url: Full URI to the object (e.g., 's3://bucket/path/to/object.dat')     store: Store name (None for default store)     size: Total size in bytes (sum for folders), or None if not computed.         For large hierarchical data like Zarr stores, size computation can         be expensive and is optional.     hash: Content hash with algorithm prefix, or None if not computed     ext: File extension as tooling hint (e.g., \".dat\", \".zarr\") or None.         This is a conventional suffix for tooling, not a content-type declaration.     is_dir: True if stored content is a directory/key-prefix (e.g., Zarr store)     timestamp: ISO 8601 upload timestamp     mime_type: MIME type (files only, auto-detected from extension)     item_count: Number of files (folders only), or None if not computed</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(json_data, backend=None)\n</code></pre> <p>Create an ObjectRef from JSON metadata stored in the database.</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>dict or str</code> <p>JSON string or dict containing object metadata.</p> required <code>backend</code> <code>StorageBackend</code> <p>StorageBackend instance for file operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>ObjectRef</code> <p>ObjectRef instance.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.to_json","title":"to_json","text":"<pre><code>to_json()\n</code></pre> <p>Convert ObjectRef to JSON-serializable dict for database storage.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dict suitable for JSON serialization.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.fs","title":"fs  <code>property</code>","text":"<pre><code>fs\n</code></pre> <p>Return fsspec filesystem for direct access.</p> <p>This allows integration with libraries like Zarr and xarray that work with fsspec filesystems.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.fsmap","title":"fsmap  <code>property</code>","text":"<pre><code>fsmap\n</code></pre> <p>Return FSMap suitable for Zarr/xarray.</p> <p>This provides a dict-like interface to the storage location, compatible with zarr.open() and xarray.open_zarr().</p> <p>Example:     &gt;&gt;&gt; z = zarr.open(obj_ref.fsmap, mode='r')</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.full_path","title":"full_path  <code>property</code>","text":"<pre><code>full_path\n</code></pre> <p>Return full URI (e.g., 's3://bucket/path').</p> <p>This is the complete path including protocol and bucket/location.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.read","title":"read","text":"<pre><code>read()\n</code></pre> <p>Read entire file content as bytes.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>File contents as bytes.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If object is a directory.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.open","title":"open","text":"<pre><code>open(subpath=None, mode='rb')\n</code></pre> <p>Open file for reading.</p> <p>Parameters:</p> Name Type Description Default <code>subpath</code> <code>str</code> <p>Path within directory (for folder objects).</p> <code>None</code> <code>mode</code> <code>str</code> <p>File mode ('rb' for binary read, 'r' for text). Default 'rb'.</p> <code>'rb'</code> <p>Returns:</p> Type Description <code>IO</code> <p>File-like object.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.listdir","title":"listdir","text":"<pre><code>listdir(subpath='')\n</code></pre> <p>List contents of directory.</p> <p>Parameters:</p> Name Type Description Default <code>subpath</code> <code>str</code> <p>Subdirectory path. Default empty string (root).</p> <code>''</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of filenames/directory names.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.walk","title":"walk","text":"<pre><code>walk()\n</code></pre> <p>Walk directory tree, similar to os.walk().</p> <p>Yields:</p> Type Description <code>tuple[str, list[str], list[str]]</code> <p>Tuples of (dirpath, dirnames, filenames).</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.download","title":"download","text":"<pre><code>download(destination, subpath=None)\n</code></pre> <p>Download object to local filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>Path or str</code> <p>Local directory or file path.</p> required <code>subpath</code> <code>str</code> <p>Path within directory (for folder objects).</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to downloaded file/directory.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.exists","title":"exists","text":"<pre><code>exists(subpath=None)\n</code></pre> <p>Check if object (or subpath within it) exists.</p> <p>Parameters:</p> Name Type Description Default <code>subpath</code> <code>str</code> <p>Path within directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if exists.</p>"},{"location":"api/datajoint/#datajoint.ObjectRef.verify","title":"verify","text":"<pre><code>verify()\n</code></pre> <p>Verify object integrity.</p> <p>For files: checks size matches, and hash if available. For folders: validates manifest (all files exist with correct sizes).</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if valid.</p> <p>Raises:</p> Type Description <code>IntegrityError</code> <p>If verification fails with details.</p>"},{"location":"api/datajoint/#datajoint.Schema","title":"Schema","text":"<p>Decorator that binds table classes to a database schema.</p> <p>Schema objects associate Python table classes with database schemas and provide the namespace context for foreign key resolution.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Database schema name. If omitted, call <code>activate()</code> later.</p> <code>None</code> <code>context</code> <code>dict</code> <p>Namespace for foreign key lookup. None uses caller's context.</p> <code>None</code> <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If False, raise error if schema doesn't exist. Default True.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If False, raise error when accessing missing tables. Default from <code>dj.config.database.create_tables</code> (True unless configured).</p> <code>None</code> <code>add_objects</code> <code>dict</code> <p>Additional objects for the declaration context.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; schema = dj.Schema('my_schema')\n&gt;&gt;&gt; @schema\n... class Session(dj.Manual):\n...     definition = '''\n...     session_id : int\n...     '''\n</code></pre>"},{"location":"api/datajoint/#datajoint.Schema.is_activated","title":"is_activated","text":"<pre><code>is_activated()\n</code></pre> <p>Check if the schema has been activated.</p>"},{"location":"api/datajoint/#datajoint.Schema.activate","title":"activate","text":"<pre><code>activate(schema_name=None, *, connection=None, create_schema=None, create_tables=None, add_objects=None)\n</code></pre> <p>Associate with a database schema.</p> <p>If the schema does not exist, attempts to create it on the server.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Database schema name. None asserts schema is already activated.</p> <code>None</code> <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If False, raise error if schema doesn't exist.</p> <code>None</code> <code>create_tables</code> <code>bool</code> <p>If False, raise error when accessing missing tables.</p> <code>None</code> <code>add_objects</code> <code>dict</code> <p>Additional objects for the declaration context.</p> <code>None</code> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If schema_name is None and schema not yet activated, or if schema already activated for a different database.</p>"},{"location":"api/datajoint/#datajoint.Schema.size_on_disk","title":"size_on_disk  <code>property</code>","text":"<pre><code>size_on_disk\n</code></pre> <p>Return the total size of all tables in the schema.</p> <p>Returns:</p> Type Description <code>int</code> <p>Size in bytes (data + indices).</p>"},{"location":"api/datajoint/#datajoint.Schema.make_classes","title":"make_classes","text":"<pre><code>make_classes(into=None)\n</code></pre> <p>Create Python table classes for tables in the schema.</p> <p>Introspects the database schema and creates appropriate Python classes (Lookup, Manual, Imported, Computed, Part) for tables that don't have corresponding classes in the target namespace.</p> <p>Parameters:</p> Name Type Description Default <code>into</code> <code>dict</code> <p>Namespace to place created classes into. Defaults to caller's local namespace.</p> <code>None</code>"},{"location":"api/datajoint/#datajoint.Schema.drop","title":"drop","text":"<pre><code>drop(prompt=None)\n</code></pre> <p>Drop the associated schema and all its tables.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>bool</code> <p>If True, show confirmation prompt before dropping. If False, drop without confirmation. If None (default), use <code>dj.config['safemode']</code> setting.</p> <code>None</code> <p>Raises:</p> Type Description <code>AccessError</code> <p>If insufficient permissions to drop the schema.</p>"},{"location":"api/datajoint/#datajoint.Schema.exists","title":"exists  <code>property</code>","text":"<pre><code>exists\n</code></pre> <p>Check if the associated schema exists on the server.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the schema exists.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If schema has not been activated.</p>"},{"location":"api/datajoint/#datajoint.Schema.lineage_table_exists","title":"lineage_table_exists  <code>property</code>","text":"<pre><code>lineage_table_exists\n</code></pre> <p>Check if the ~lineage table exists in this schema.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the lineage table exists.</p>"},{"location":"api/datajoint/#datajoint.Schema.lineage","title":"lineage  <code>property</code>","text":"<pre><code>lineage\n</code></pre> <p>Get all lineages for tables in this schema.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Mapping of <code>'schema.table.attribute'</code> to its lineage origin.</p>"},{"location":"api/datajoint/#datajoint.Schema.rebuild_lineage","title":"rebuild_lineage","text":"<pre><code>rebuild_lineage()\n</code></pre> <p>Rebuild the ~lineage table for all tables in this schema.</p> <p>Recomputes lineage for all attributes by querying FK relationships from the information_schema. Use to restore lineage for schemas that predate the lineage system or after corruption.</p> Notes <p>After rebuilding, restart the Python kernel and reimport to pick up the new lineage information.</p> <p>Upstream schemas (referenced via cross-schema foreign keys) must have their lineage rebuilt first.</p>"},{"location":"api/datajoint/#datajoint.Schema.jobs","title":"jobs  <code>property</code>","text":"<pre><code>jobs\n</code></pre> <p>Return Job objects for auto-populated tables with job tables.</p> <p>Only returns Job objects when both the target table and its <code>~~table_name</code> job table exist in the database. Job tables are created lazily on first access to <code>table.jobs</code> or <code>populate(reserve_jobs=True)</code>.</p> <p>Returns:</p> Type Description <code>list[Job]</code> <p>Job objects for existing job tables.</p>"},{"location":"api/datajoint/#datajoint.Schema.save","title":"save","text":"<pre><code>save(python_filename=None)\n</code></pre> <p>Generate Python code that recreates this schema.</p> <p>Parameters:</p> Name Type Description Default <code>python_filename</code> <code>str</code> <p>If provided, write the code to this file.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Python module source code defining this schema.</p> Notes <p>This method is in preparation for a future release and is not officially supported.</p>"},{"location":"api/datajoint/#datajoint.Schema.list_tables","title":"list_tables","text":"<pre><code>list_tables()\n</code></pre> <p>Return all user tables in the schema.</p> <p>Excludes hidden tables (starting with <code>~</code>) such as <code>~lineage</code> and job tables (<code>~~</code>).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Table names in topological order.</p>"},{"location":"api/datajoint/#datajoint.Schema.get_table","title":"get_table","text":"<pre><code>get_table(name)\n</code></pre> <p>Get a table instance by name.</p> <p>Returns a FreeTable instance for the given table name. This is useful for accessing tables when you don't have the Python class available.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name (e.g., 'experiment', 'session__trial' for parts). Can be snake_case (SQL name) or CamelCase (class name). Tier prefixes are optional and will be auto-detected.</p> required <p>Returns:</p> Type Description <code>FreeTable</code> <p>A FreeTable instance for the table.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If the table does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; schema = dj.Schema('my_schema')\n&gt;&gt;&gt; experiment = schema.get_table('experiment')\n&gt;&gt;&gt; experiment.fetch()\n</code></pre>"},{"location":"api/datajoint/#datajoint.VirtualModule","title":"VirtualModule","text":"<p>               Bases: <code>ModuleType</code></p> <p>A virtual module representing a DataJoint schema from database tables.</p> <p>Creates a Python module with table classes automatically generated from the database schema. Useful for accessing schemas without Python source.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Display name for the module.</p> required <code>schema_name</code> <code>str</code> <p>Database schema name.</p> required <code>create_schema</code> <code>bool</code> <p>If True, create the schema if it doesn't exist. Default False.</p> <code>False</code> <code>create_tables</code> <code>bool</code> <p>If True, allow declaring new tables. Default False.</p> <code>False</code> <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>add_objects</code> <code>dict</code> <p>Additional objects to add to the module namespace.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lab = dj.VirtualModule('lab', 'my_lab_schema')\n&gt;&gt;&gt; lab.Subject.fetch()\n</code></pre>"},{"location":"api/datajoint/#datajoint.list_schemas","title":"list_schemas","text":"<pre><code>list_schemas(connection=None)\n</code></pre> <p>List all accessible schemas on the server.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Names of all accessible schemas.</p>"},{"location":"api/datajoint/#datajoint.virtual_schema","title":"virtual_schema","text":"<pre><code>virtual_schema(schema_name, *, connection=None, create_schema=False, create_tables=False, add_objects=None)\n</code></pre> <p>Create a virtual module for an existing database schema.</p> <p>This is the recommended way to access database schemas when you don't have the Python source code that defined them. Returns a module-like object with table classes as attributes.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Database schema name.</p> required <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, create the schema if it doesn't exist. Default False.</p> <code>False</code> <code>create_tables</code> <code>bool</code> <p>If True, allow declaring new tables. Default False.</p> <code>False</code> <code>add_objects</code> <code>dict</code> <p>Additional objects to add to the module namespace.</p> <code>None</code> <p>Returns:</p> Type Description <code>VirtualModule</code> <p>A module-like object with table classes as attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lab = dj.virtual_schema('my_lab')\n&gt;&gt;&gt; lab.Subject.fetch()\n&gt;&gt;&gt; lab.Session &amp; \"subject_id='M001'\"\n</code></pre> See Also <p>Schema : For defining new schemas with Python classes. VirtualModule : The underlying class (prefer virtual_schema function).</p>"},{"location":"api/datajoint/#datajoint.FreeTable","title":"FreeTable","text":"<p>               Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>:param conn:  a dj.Connection object :param full_table_name: in format <code>database</code>.<code>table_name</code></p>"},{"location":"api/datajoint/#datajoint.Table","title":"Table","text":"<p>               Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p>"},{"location":"api/datajoint/#datajoint.Table.heading","title":"heading  <code>property</code>","text":"<pre><code>heading\n</code></pre> <p>Return the table's heading, or raise a helpful error if not configured.</p> <p>Overrides QueryExpression.heading to provide a clear error message when the table is not properly associated with an activated schema. For base tier classes (Lookup, Manual, etc.), returns None to support introspection (e.g., help()).</p>"},{"location":"api/datajoint/#datajoint.Table.declare","title":"declare","text":"<pre><code>declare(context=None)\n</code></pre> <p>Declare the table in the schema based on self.definition.</p> <p>:param context: the context for foreign key resolution. If None, foreign keys are     not allowed.</p>"},{"location":"api/datajoint/#datajoint.Table.alter","title":"alter","text":"<pre><code>alter(prompt=True, context=None)\n</code></pre> <p>Alter the table definition from self.definition</p>"},{"location":"api/datajoint/#datajoint.Table.from_clause","title":"from_clause","text":"<pre><code>from_clause()\n</code></pre> <p>:return: the FROM clause of SQL SELECT statements.</p>"},{"location":"api/datajoint/#datajoint.Table.get_select_fields","title":"get_select_fields","text":"<pre><code>get_select_fields(select_fields=None)\n</code></pre> <p>:return: the selected attributes from the SQL SELECT statement.</p>"},{"location":"api/datajoint/#datajoint.Table.parents","title":"parents","text":"<pre><code>parents(primary=None, as_objects=False, foreign_key_info=False)\n</code></pre> <p>:param primary: if None, then all parents are returned. If True, then only foreign keys composed of     primary key attributes are considered.  If False, return foreign keys including at least one     secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects     with (optional) foreign key information.</p>"},{"location":"api/datajoint/#datajoint.Table.children","title":"children","text":"<pre><code>children(primary=None, as_objects=False, foreign_key_info=False)\n</code></pre> <p>:param primary: if None, then all children are returned. If True, then only foreign keys composed of     primary key attributes are considered.  If False, return foreign keys including at least one     secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects     with (optional) foreign key information.</p>"},{"location":"api/datajoint/#datajoint.Table.descendants","title":"descendants","text":"<pre><code>descendants(as_objects=False)\n</code></pre> <p>:param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order.</p>"},{"location":"api/datajoint/#datajoint.Table.ancestors","title":"ancestors","text":"<pre><code>ancestors(as_objects=False)\n</code></pre> <p>:param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order.</p>"},{"location":"api/datajoint/#datajoint.Table.parts","title":"parts","text":"<pre><code>parts(as_objects=False)\n</code></pre> <p>return part tables either as entries in a dict with foreign key information or a list of objects</p> <p>:param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p>"},{"location":"api/datajoint/#datajoint.Table.is_declared","title":"is_declared  <code>property</code>","text":"<pre><code>is_declared\n</code></pre> <p>:return: True is the table is declared in the schema.</p>"},{"location":"api/datajoint/#datajoint.Table.full_table_name","title":"full_table_name  <code>property</code>","text":"<pre><code>full_table_name\n</code></pre> <p>:return: full table name in the schema</p>"},{"location":"api/datajoint/#datajoint.Table.adapter","title":"adapter  <code>property</code>","text":"<pre><code>adapter\n</code></pre> <p>Database adapter for backend-agnostic SQL generation.</p>"},{"location":"api/datajoint/#datajoint.Table.update1","title":"update1","text":"<pre><code>update1(row)\n</code></pre> <p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>:param row: a <code>dict</code> containing the primary key values and the attributes to update.     Setting an attribute value to None will reset it to the default value (if any).</p> <p>The primary key attributes must always be provided.</p> <p>Examples:</p> <p>table.update1({'id': 1, 'value': 3})  # update value in record with id=1 table.update1({'id': 1, 'value': None})  # reset value to default</p>"},{"location":"api/datajoint/#datajoint.Table.validate","title":"validate","text":"<pre><code>validate(rows, *, ignore_extra_fields=False)\n</code></pre> <p>Validate rows without inserting them.</p> <p>:param rows: Same format as insert() - iterable of dicts, tuples, numpy records,     or a pandas DataFrame. :param ignore_extra_fields: If True, ignore fields not in the table heading. :return: ValidationResult with is_valid, errors list, and rows_checked count.</p> <p>Validates:     - Field existence (all fields must be in table heading)     - Row format (correct number of attributes for positional inserts)     - Codec validation (type checking via codec.validate())     - NULL constraints (non-nullable fields must have values)     - Primary key completeness (all PK fields must be present)     - UUID format and JSON serializability</p> <p>Cannot validate (database-enforced):     - Foreign key constraints     - Unique constraints (other than PK)     - Custom MySQL constraints</p> <p>Example::</p> <pre><code>result = table.validate(rows)\nif result:\n    table.insert(rows)\nelse:\n    print(result.summary())\n</code></pre>"},{"location":"api/datajoint/#datajoint.Table.insert1","title":"insert1","text":"<pre><code>insert1(row, **kwargs)\n</code></pre> <p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>:param row: a numpy record, a dict-like object, or an ordered sequence to be inserted     as one row.</p>"},{"location":"api/datajoint/#datajoint.Table.staged_insert1","title":"staged_insert1  <code>property</code>","text":"<pre><code>staged_insert1\n</code></pre> <p>Context manager for staged insert with direct object storage writes.</p> <p>Use this for large objects like Zarr arrays where copying from local storage is inefficient. Allows writing directly to the destination storage before finalizing the database insert.</p> <p>Example:     with table.staged_insert1 as staged:         staged.rec['subject_id'] = 123         staged.rec['session_id'] = 45</p> <pre><code>    # Create object storage directly\n    z = zarr.open(staged.store('raw_data', '.zarr'), mode='w', shape=(1000, 1000))\n    z[:] = data\n\n    # Assign to record\n    staged.rec['raw_data'] = z\n\n# On successful exit: metadata computed, record inserted\n# On exception: storage cleaned up, no record inserted\n</code></pre> <p>Yields:     StagedInsert: Context for setting record values and getting storage handles</p>"},{"location":"api/datajoint/#datajoint.Table.insert","title":"insert","text":"<pre><code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None, chunk_size=None)\n</code></pre> <p>Insert a collection of rows.</p> <p>:param rows: Either (a) an iterable where an element is a numpy record, a     dict-like object, a pandas.DataFrame, a polars.DataFrame, a pyarrow.Table,     a sequence, or a query expression with the same heading as self, or     (b) a pathlib.Path object specifying a path relative to the current     directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default),     insert may only be called from inside the make callback. :param chunk_size: If set, insert rows in batches of this size. Useful for very     large inserts to avoid memory issues. Each chunk is a separate transaction.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; Table.insert([\n&gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n&gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n\n# Large insert with chunking\n&gt;&gt;&gt; Table.insert(large_dataset, chunk_size=10000)\n</code></pre>"},{"location":"api/datajoint/#datajoint.Table.insert_dataframe","title":"insert_dataframe","text":"<pre><code>insert_dataframe(df, index_as_pk=None, **insert_kwargs)\n</code></pre> <p>Insert DataFrame with explicit index handling.</p> <p>This method provides symmetry with to_pandas(): data fetched with to_pandas() (which sets primary key as index) can be modified and re-inserted using insert_dataframe() without manual index manipulation.</p> <p>:param df: pandas DataFrame to insert :param index_as_pk: How to handle DataFrame index:     - None (default): Auto-detect. Use index as primary key if index names       match primary_key columns. Drop if unnamed RangeIndex.     - True: Treat index as primary key columns. Raises if index names don't       match table primary key.     - False: Ignore index entirely (drop it). :param **insert_kwargs: Passed to insert() - replace, skip_duplicates,     ignore_extra_fields, allow_direct_insert, chunk_size</p> <p>Example::</p> <pre><code># Round-trip with to_pandas()\ndf = table.to_pandas()           # PK becomes index\ndf['value'] = df['value'] * 2    # Modify data\ntable.insert_dataframe(df)       # Auto-detects index as PK\n\n# Explicit control\ntable.insert_dataframe(df, index_as_pk=True)   # Use index\ntable.insert_dataframe(df, index_as_pk=False)  # Ignore index\n</code></pre>"},{"location":"api/datajoint/#datajoint.Table.delete_quick","title":"delete_quick","text":"<pre><code>delete_quick(get_count=False)\n</code></pre> <p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p>"},{"location":"api/datajoint/#datajoint.Table.delete","title":"delete","text":"<pre><code>delete(transaction=True, prompt=None, part_integrity='enforce')\n</code></pre> <p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     prompt: If <code>True</code>, show what will be deleted and ask for confirmation.         If <code>False</code>, delete without confirmation. Default is <code>dj.config['safemode']</code>.     part_integrity: Policy for master-part integrity. One of:         - <code>\"enforce\"</code> (default): Error if parts would be deleted without masters.         - <code>\"ignore\"</code>: Allow deleting parts without masters (breaks integrity).         - <code>\"cascade\"</code>: Also delete masters when parts are deleted (maintains integrity).</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master (when part_integrity=\"enforce\").     ValueError: Invalid part_integrity value.</p>"},{"location":"api/datajoint/#datajoint.Table.drop_quick","title":"drop_quick","text":"<pre><code>drop_quick()\n</code></pre> <p>Drops the table without cascading to dependent tables and without user prompt.</p>"},{"location":"api/datajoint/#datajoint.Table.drop","title":"drop","text":"<pre><code>drop(prompt=None)\n</code></pre> <p>Drop the table and all tables that reference it, recursively.</p> <p>Args:     prompt: If <code>True</code>, show what will be dropped and ask for confirmation.         If <code>False</code>, drop without confirmation. Default is <code>dj.config['safemode']</code>.</p>"},{"location":"api/datajoint/#datajoint.Table.size_on_disk","title":"size_on_disk  <code>property</code>","text":"<pre><code>size_on_disk\n</code></pre> <p>:return: size of data and indices in bytes on the storage device</p>"},{"location":"api/datajoint/#datajoint.Table.describe","title":"describe","text":"<pre><code>describe(context=None, printout=False)\n</code></pre> <p>:return:  the definition string for the query using DataJoint DDL.</p>"},{"location":"api/datajoint/#datajoint.ValidationResult","title":"ValidationResult  <code>dataclass</code>","text":"<p>Result of table.validate() call.</p> <p>Attributes:     is_valid: True if all rows passed validation     errors: List of (row_index, field_name, error_message) tuples     rows_checked: Number of rows that were validated</p>"},{"location":"api/datajoint/#datajoint.ValidationResult.raise_if_invalid","title":"raise_if_invalid","text":"<pre><code>raise_if_invalid()\n</code></pre> <p>Raise DataJointError if validation failed.</p>"},{"location":"api/datajoint/#datajoint.ValidationResult.summary","title":"summary","text":"<pre><code>summary()\n</code></pre> <p>Return formatted error summary.</p>"},{"location":"api/datajoint/#datajoint.Computed","title":"Computed","text":"<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p>"},{"location":"api/datajoint/#datajoint.Imported","title":"Imported","text":"<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p>"},{"location":"api/datajoint/#datajoint.Lookup","title":"Lookup","text":"<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p>"},{"location":"api/datajoint/#datajoint.Manual","title":"Manual","text":"<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p>"},{"location":"api/datajoint/#datajoint.Part","title":"Part","text":"<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p>"},{"location":"api/datajoint/#datajoint.Part.delete","title":"delete","text":"<pre><code>delete(part_integrity='enforce', **kwargs)\n</code></pre> <p>Delete from a Part table.</p> <p>Args:     part_integrity: Policy for master-part integrity. One of:         - <code>\"enforce\"</code> (default): Error - delete from master instead.         - <code>\"ignore\"</code>: Allow direct deletion (breaks master-part integrity).         - <code>\"cascade\"</code>: Delete parts AND cascade up to delete master.     **kwargs: Additional arguments passed to Table.delete()               (transaction, prompt)</p> <p>Raises:     DataJointError: If part_integrity=\"enforce\" (direct Part deletes prohibited)</p>"},{"location":"api/datajoint/#datajoint.Part.drop","title":"drop","text":"<pre><code>drop(part_integrity='enforce')\n</code></pre> <p>Drop a Part table.</p> <p>Args:     part_integrity: Policy for master-part integrity. One of:         - <code>\"enforce\"</code> (default): Error - drop master instead.         - <code>\"ignore\"</code>: Allow direct drop (breaks master-part structure).         Note: <code>\"cascade\"</code> is not supported for drop (too destructive).</p> <p>Raises:     DataJointError: If part_integrity=\"enforce\" (direct Part drops prohibited)</p>"},{"location":"api/datajoint/","title":"datajoint","text":"<p>DataJoint Python library modules.</p>"},{"location":"api/datajoint/#submodules","title":"Submodules","text":"Module Description Connection Database connection management Schema Schema and VirtualModule classes Table Base Table and FreeTable classes Table Types Manual, Lookup, Imported, Computed, Part Expressions Query expressions and operators Heading Table heading and attributes Diagram Schema visualization Settings Configuration management Errors Exception classes Codecs Type codec system Blob Binary serialization Hash Registry Content hashing for external storage Jobs Job queue for AutoPopulate Migrate Schema migration utilities"},{"location":"api/datajoint/blob/","title":"Blob","text":"<p>Binary serialization</p> <p>Binary serialization for DataJoint blob storage.</p> <p>Provides (de)serialization for Python/NumPy objects with backward compatibility for MATLAB mYm-format blobs. Supports arrays, scalars, structs, cells, and Python built-in types (dict, list, tuple, set, datetime, UUID, Decimal).</p>"},{"location":"api/datajoint/blob/#datajoint.blob.MatCell","title":"MatCell","text":"<p>               Bases: <code>ndarray</code></p> <p>NumPy ndarray subclass representing a MATLAB cell array.</p> <p>Used to distinguish cell arrays from regular arrays during serialization for MATLAB compatibility.</p>"},{"location":"api/datajoint/blob/#datajoint.blob.MatStruct","title":"MatStruct","text":"<p>               Bases: <code>recarray</code></p> <p>NumPy recarray subclass representing a MATLAB struct array.</p> <p>Used to distinguish struct arrays from regular recarrays during serialization for MATLAB compatibility.</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob","title":"Blob","text":"<p>Binary serializer/deserializer for DataJoint blob storage.</p> <p>Handles packing Python objects into binary format and unpacking binary data back to Python objects. Supports two protocols:</p> <ul> <li><code>mYm</code>: Original MATLAB-compatible format (default)</li> <li><code>dj0</code>: Extended format for Python-specific types</li> </ul> <p>Parameters:</p> Name Type Description Default <code>squeeze</code> <code>bool</code> <p>If True, remove singleton dimensions from arrays and convert 0-dimensional arrays to scalars. Default False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>protocol</code> <code>bytes or None</code> <p>Current serialization protocol (<code>b\"mYm\\0\"</code> or <code>b\"dj0\\0\"</code>).</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.set_dj0","title":"set_dj0","text":"<pre><code>set_dj0()\n</code></pre> <p>Switch to dj0 protocol for extended type support.</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.squeeze","title":"squeeze","text":"<pre><code>squeeze(array, convert_to_scalar=True)\n</code></pre> <p>Remove singleton dimensions from an array.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array.</p> required <code>convert_to_scalar</code> <code>bool</code> <p>If True, convert 0-dimensional arrays to Python scalars. Default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray or scalar</code> <p>Squeezed array or scalar value.</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.pack_array","title":"pack_array","text":"<pre><code>pack_array(array)\n</code></pre> <p>Serialize a NumPy array into bytes.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Array to serialize. Scalars are encoded with ndim=0.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Serialized array data.</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.read_recarray","title":"read_recarray","text":"<pre><code>read_recarray()\n</code></pre> <p>Serialize an np.ndarray with fields, including recarrays</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.pack_recarray","title":"pack_recarray","text":"<pre><code>pack_recarray(array)\n</code></pre> <p>Serialize a Matlab struct array</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.read_struct","title":"read_struct","text":"<pre><code>read_struct()\n</code></pre> <p>deserialize matlab struct</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.pack_struct","title":"pack_struct","text":"<pre><code>pack_struct(array)\n</code></pre> <p>Serialize a Matlab struct array</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.read_cell_array","title":"read_cell_array","text":"<pre><code>read_cell_array()\n</code></pre> <p>Deserialize MATLAB cell array.</p> <p>Handles edge cases from MATLAB: - Empty cell arrays ({}) - Cell arrays with empty elements ({[], [], []}) - Nested arrays ({[1,2], [3,4,5]}) - ragged arrays - Cell matrices with mixed content</p>"},{"location":"api/datajoint/blob/#datajoint.blob.Blob.read_datetime","title":"read_datetime","text":"<pre><code>read_datetime()\n</code></pre> <p>deserialize datetime.date, .time, or .datetime</p>"},{"location":"api/datajoint/blob/#datajoint.blob.pack","title":"pack","text":"<pre><code>pack(obj, compress=True)\n</code></pre> <p>Serialize a Python object to binary blob format.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>any</code> <p>Object to serialize. Supports NumPy arrays, Python scalars, collections (dict, list, tuple, set), datetime objects, UUID, Decimal, and MATLAB-compatible MatCell/MatStruct.</p> required <code>compress</code> <code>bool</code> <p>If True (default), compress blobs larger than 1000 bytes using zlib.</p> <code>True</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Serialized binary data.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If the object type is not supported.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = np.array([1, 2, 3])\n&gt;&gt;&gt; blob = pack(data)\n&gt;&gt;&gt; unpacked = unpack(blob)\n</code></pre>"},{"location":"api/datajoint/blob/#datajoint.blob.unpack","title":"unpack","text":"<pre><code>unpack(blob, squeeze=False)\n</code></pre> <p>Deserialize a binary blob to a Python object.</p> <p>Parameters:</p> Name Type Description Default <code>blob</code> <code>bytes</code> <p>Binary data from <code>pack()</code> or MATLAB mYm serialization.</p> required <code>squeeze</code> <code>bool</code> <p>If True, remove singleton dimensions from arrays. Default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>any</code> <p>Deserialized Python object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; blob = pack({'a': 1, 'b': [1, 2, 3]})\n&gt;&gt;&gt; data = unpack(blob)\n&gt;&gt;&gt; data['b']\n[1, 2, 3]\n</code></pre>"},{"location":"api/datajoint/codecs/","title":"Codecs","text":"<p>Type codec system</p> <p>Codec type system for DataJoint.</p> <p>This module provides the Codec base class for creating custom data types that extend DataJoint's native type system. Codecs provide encode/decode semantics for complex Python objects.</p> <p>Codecs auto-register when subclassed - no decorator needed (Python 3.10+).</p> <p>Example:     class GraphCodec(dj.Codec):         name = \"graph\"</p> <pre><code>    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n\n    def encode(self, graph, *, key=None, store_name=None):\n        return {'nodes': list(graph.nodes()), 'edges': list(graph.edges())}\n\n    def decode(self, stored, *, key=None):\n        import networkx as nx\n        G = nx.Graph()\n        G.add_nodes_from(stored['nodes'])\n        G.add_edges_from(stored['edges'])\n        return G\n\n# Then use in table definitions:\nclass MyTable(dj.Manual):\n    definition = '''\n</code></pre> <pre><code>    id : uint16\n</code></pre> <pre><code>    data : &lt;graph&gt;\n    '''\n</code></pre>"},{"location":"api/datajoint/codecs/#datajoint.codecs.Codec","title":"Codec","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for codec types. Subclasses auto-register by name.</p> <p>Requires Python 3.10+.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str or None</code> <p>Unique identifier used in <code>&lt;name&gt;</code> syntax. Must be set by subclasses.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class GraphCodec(dj.Codec):\n...     name = \"graph\"\n...\n...     def get_dtype(self, is_store: bool) -&gt; str:\n...         return \"&lt;blob&gt;\"\n...\n...     def encode(self, graph, *, key=None, store_name=None):\n...         return {'nodes': list(graph.nodes()), 'edges': list(graph.edges())}\n...\n...     def decode(self, stored, *, key=None):\n...         import networkx as nx\n...         G = nx.Graph()\n...         G.add_nodes_from(stored['nodes'])\n...         G.add_edges_from(stored['edges'])\n...         return G\n</code></pre> <p>Use in table definitions::</p> <pre><code>class Connectivity(dj.Manual):\n    definition = '''\n    id : uint16\n    ---\n    graph_data : &lt;graph&gt;\n    '''\n</code></pre> <p>Skip auto-registration for abstract base classes::</p> <pre><code>class ExternalOnlyCodec(dj.Codec, register=False):\n    '''Abstract base - not registered.'''\n    ...\n</code></pre>"},{"location":"api/datajoint/codecs/#datajoint.codecs.Codec.get_dtype","title":"get_dtype  <code>abstractmethod</code>","text":"<pre><code>get_dtype(is_store)\n</code></pre> <p>Return the storage dtype for this codec.</p> <p>Parameters:</p> Name Type Description Default <code>is_store</code> <code>bool</code> <p>True if <code>@</code> modifier present (object store vs inline).</p> required <p>Returns:</p> Type Description <code>str</code> <p>A core type (e.g., <code>\"bytes\"</code>, <code>\"json\"</code>) or another codec (e.g., <code>\"&lt;hash&gt;\"</code>).</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If store mode not supported but requested.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.Codec.encode","title":"encode  <code>abstractmethod</code>","text":"<pre><code>encode(value, *, key=None, store_name=None)\n</code></pre> <p>Encode Python value for storage.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>any</code> <p>The Python object to store.</p> required <code>key</code> <code>dict</code> <p>Primary key values. May be needed for path construction.</p> <code>None</code> <code>store_name</code> <code>str</code> <p>Target store name for object storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>any</code> <p>Value in the format expected by the dtype.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.Codec.decode","title":"decode  <code>abstractmethod</code>","text":"<pre><code>decode(stored, *, key=None)\n</code></pre> <p>Decode stored value back to Python.</p> <p>Parameters:</p> Name Type Description Default <code>stored</code> <code>any</code> <p>Data retrieved from storage.</p> required <code>key</code> <code>dict</code> <p>Primary key values.</p> <code>None</code> <p>Returns:</p> Type Description <code>any</code> <p>The reconstructed Python object.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.Codec.validate","title":"validate","text":"<pre><code>validate(value)\n</code></pre> <p>Validate a value before encoding.</p> <p>Override this method to add type checking or domain constraints. Called automatically before <code>encode()</code> during INSERT operations. The default implementation accepts any value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>any</code> <p>The value to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the value has an incompatible type.</p> <code>ValueError</code> <p>If the value fails domain validation.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.parse_type_spec","title":"parse_type_spec","text":"<pre><code>parse_type_spec(spec)\n</code></pre> <p>Parse a type specification into type name and optional store parameter.</p> <p>Parameters:</p> Name Type Description Default <code>spec</code> <code>str</code> <p>Type specification string, with or without angle brackets.</p> required <p>Returns:</p> Type Description <code>tuple[str, str | None]</code> <p><code>(type_name, store_name)</code>. <code>store_name</code> is None if not specified, empty string if <code>@</code> present without name (default store).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; parse_type_spec(\"&lt;blob&gt;\")\n(\"blob\", None)\n&gt;&gt;&gt; parse_type_spec(\"&lt;blob@cold&gt;\")\n(\"blob\", \"cold\")\n&gt;&gt;&gt; parse_type_spec(\"&lt;blob@&gt;\")\n(\"blob\", \"\")\n</code></pre>"},{"location":"api/datajoint/codecs/#datajoint.codecs.unregister_codec","title":"unregister_codec","text":"<pre><code>unregister_codec(name)\n</code></pre> <p>Remove a codec from the registry.</p> <p>Primarily useful for testing. Use with caution in production code.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The codec name to unregister.</p> required <p>Raises:</p> Type Description <code>DataJointError</code> <p>If the codec is not registered.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.get_codec","title":"get_codec","text":"<pre><code>get_codec(name)\n</code></pre> <p>Retrieve a registered codec by name.</p> <p>Looks up the codec in the explicit registry first, then attempts to load from installed packages via entry points.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The codec name, with or without angle brackets. Store parameters (e.g., <code>\"&lt;blob@cold&gt;\"</code>) are stripped.</p> required <p>Returns:</p> Type Description <code>Codec</code> <p>The registered Codec instance.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If the codec is not found.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.list_codecs","title":"list_codecs","text":"<pre><code>list_codecs()\n</code></pre> <p>List all registered codec names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of registered codec names.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.is_codec_registered","title":"is_codec_registered","text":"<pre><code>is_codec_registered(name)\n</code></pre> <p>Check if a codec name is registered.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The codec name to check (store parameters are ignored).</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the codec is registered.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.resolve_dtype","title":"resolve_dtype","text":"<pre><code>resolve_dtype(dtype, seen=None, store_name=None)\n</code></pre> <p>Resolve a dtype string, following codec chains.</p> <p>If dtype references another codec (e.g., <code>\"&lt;hash&gt;\"</code>), recursively resolves to find the ultimate storage type. Store parameters are propagated through the chain.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>str</code> <p>The dtype string to resolve (e.g., <code>\"&lt;blob&gt;\"</code>, <code>\"&lt;blob@cold&gt;\"</code>, <code>\"bytes\"</code>).</p> required <code>seen</code> <code>set[str]</code> <p>Set of already-seen codec names (for cycle detection).</p> <code>None</code> <code>store_name</code> <code>str</code> <p>Store name from outer type specification (propagated inward).</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, list[Codec], str | None]</code> <p><code>(final_storage_type, codec_chain, resolved_store_name)</code>. Chain is ordered from outermost to innermost codec.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If a circular type reference is detected.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; resolve_dtype(\"&lt;blob&gt;\")\n(\"bytes\", [BlobCodec], None)\n&gt;&gt;&gt; resolve_dtype(\"&lt;blob@cold&gt;\")\n(\"&lt;hash&gt;\", [BlobCodec], \"cold\")\n&gt;&gt;&gt; resolve_dtype(\"bytes\")\n(\"bytes\", [], None)\n</code></pre>"},{"location":"api/datajoint/codecs/#datajoint.codecs.lookup_codec","title":"lookup_codec","text":"<pre><code>lookup_codec(codec_spec)\n</code></pre> <p>Look up a codec from a type specification string.</p> <p>Parses a codec specification (e.g., <code>\"&lt;blob@store&gt;\"</code>) and returns the codec instance along with any store name.</p> <p>Parameters:</p> Name Type Description Default <code>codec_spec</code> <code>str</code> <p>The codec specification, with or without angle brackets. May include store parameter (e.g., <code>\"&lt;blob@cold&gt;\"</code>).</p> required <p>Returns:</p> Type Description <code>tuple[Codec, str | None]</code> <p><code>(codec_instance, store_name)</code> or <code>(codec_instance, None)</code>.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If the codec is not found.</p>"},{"location":"api/datajoint/codecs/#datajoint.codecs.decode_attribute","title":"decode_attribute","text":"<pre><code>decode_attribute(attr, data, squeeze=False)\n</code></pre> <p>Decode raw database value using attribute's codec or native type handling.</p> <p>This is the central decode function used by all fetch methods. It handles: - Codec chains (e.g., blob@store \u2192  \u2192 bytes) - Native type conversions (JSON, UUID) - Object storage downloads (via config[\"download_path\"]) <p>Args:     attr: Attribute from the table's heading.     data: Raw value fetched from the database.     squeeze: If True, remove singleton dimensions from numpy arrays.</p> <p>Returns:     Decoded Python value.</p>"},{"location":"api/datajoint/connection/","title":"Connection","text":"<p>Database connection management</p> <p>This module contains the Connection class that manages the connection to the database, and the <code>conn</code> function that provides access to a persistent connection in datajoint.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.translate_query_error","title":"translate_query_error","text":"<pre><code>translate_query_error(client_error, query, adapter)\n</code></pre> <p>Translate client error to the corresponding DataJoint exception.</p> <p>Parameters:</p> Name Type Description Default <code>client_error</code> <code>Exception</code> <p>The exception raised by the client interface.</p> required <code>query</code> <code>str</code> <p>SQL query with placeholders.</p> required <code>adapter</code> <code>DatabaseAdapter</code> <p>The database adapter instance.</p> required <p>Returns:</p> Type Description <code>Exception</code> <p>An instance of the corresponding DataJoint error subclass, or the original error if no mapping exists.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.conn","title":"conn","text":"<pre><code>conn(host=None, user=None, password=None, *, init_fun=None, reset=False, use_tls=None)\n</code></pre> <p>Return a persistent connection object shared by multiple modules.</p> <p>If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Database hostname.</p> <code>None</code> <code>user</code> <code>str</code> <p>Database username. Required if not set in config.</p> <code>None</code> <code>password</code> <code>str</code> <p>Database password. Required if not set in config.</p> <code>None</code> <code>init_fun</code> <code>callable</code> <p>Initialization function called after connection.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>If True, reset existing connection. Default False.</p> <code>False</code> <code>use_tls</code> <code>bool or dict</code> <p>TLS encryption option: True (required), False (no TLS), None (preferred, default), or dict for manual configuration.</p> <code>None</code> <p>Returns:</p> Type Description <code>Connection</code> <p>Persistent database connection.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If user or password is not provided and not set in config.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.EmulatedCursor","title":"EmulatedCursor","text":"<p>acts like a cursor</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection","title":"Connection","text":"<p>Manages a connection to a database server.</p> <p>Catalogues schemas, tables, and their dependencies (foreign keys). Most parameters should be set in the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Hostname, may include port as <code>hostname:port</code>.</p> required <code>user</code> <code>str</code> <p>Database username.</p> required <code>password</code> <code>str</code> <p>Database password.</p> required <code>port</code> <code>int</code> <p>Port number. Overridden if specified in host.</p> <code>None</code> <code>init_fun</code> <code>str</code> <p>SQL initialization command.</p> <code>None</code> <code>use_tls</code> <code>bool or dict</code> <p>TLS encryption option.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>schemas</code> <code>dict</code> <p>Registered schema objects.</p> <code>dependencies</code> <code>Dependencies</code> <p>Foreign key dependency graph.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.connect","title":"connect","text":"<pre><code>connect()\n</code></pre> <p>Establish or re-establish connection to the database server.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.set_query_cache","title":"set_query_cache","text":"<pre><code>set_query_cache(query_cache=None)\n</code></pre> <p>Enable query caching mode.</p> <p>When enabled: 1. Only SELECT queries are allowed 2. Results are cached under <code>dj.config['query_cache']</code> 3. Cache key differentiates cache states</p> <p>Parameters:</p> Name Type Description Default <code>query_cache</code> <code>str</code> <p>String to initialize the hash for query results. None disables caching.</p> <code>None</code>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.purge_query_cache","title":"purge_query_cache","text":"<pre><code>purge_query_cache()\n</code></pre> <p>Delete all cached query results.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close the database connection.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.register","title":"register","text":"<pre><code>register(schema)\n</code></pre> <p>Register a schema with this connection.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>Schema object to register.</p> required"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.ping","title":"ping","text":"<pre><code>ping()\n</code></pre> <p>Ping the server to verify connection is alive.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the connection is closed.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.is_connected","title":"is_connected  <code>property</code>","text":"<pre><code>is_connected\n</code></pre> <p>Check if connected to the database server.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connected.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.query","title":"query","text":"<pre><code>query(query, args=(), *, as_dict=False, suppress_warnings=True, reconnect=None)\n</code></pre> <p>Execute a SQL query and return the cursor.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query to execute.</p> required <code>args</code> <code>tuple</code> <p>Query parameters for prepared statement.</p> <code>()</code> <code>as_dict</code> <code>bool</code> <p>If True, return rows as dictionaries. Default False.</p> <code>False</code> <code>suppress_warnings</code> <code>bool</code> <p>If True, suppress SQL library warnings. Default True.</p> <code>True</code> <code>reconnect</code> <code>bool</code> <p>If True, reconnect if disconnected. None uses config setting.</p> <code>None</code> <p>Returns:</p> Type Description <code>cursor</code> <p>Database cursor with query results.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If non-SELECT query during query caching mode.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.get_user","title":"get_user","text":"<pre><code>get_user()\n</code></pre> <p>Get the current user and host.</p> <p>Returns:</p> Type Description <code>str</code> <p>User name and host as <code>'user@host'</code>.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.in_transaction","title":"in_transaction  <code>property</code>","text":"<pre><code>in_transaction\n</code></pre> <p>Check if a transaction is open.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if a transaction is in progress.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.start_transaction","title":"start_transaction","text":"<pre><code>start_transaction()\n</code></pre> <p>Start a new transaction.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If a transaction is already in progress.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.cancel_transaction","title":"cancel_transaction","text":"<pre><code>cancel_transaction()\n</code></pre> <p>Cancel the current transaction and roll back all changes.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.commit_transaction","title":"commit_transaction","text":"<pre><code>commit_transaction()\n</code></pre> <p>Commit all changes and close the transaction.</p>"},{"location":"api/datajoint/connection/#datajoint.connection.Connection.transaction","title":"transaction  <code>property</code>","text":"<pre><code>transaction\n</code></pre> <p>Context manager for transactions.</p> <p>Opens a transaction and automatically commits on success or rolls back on exception.</p> <p>Yields:</p> Type Description <code>Connection</code> <p>This connection object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with dj.conn().transaction:\n...     # All operations here are in one transaction\n...     table.insert(data)\n</code></pre>"},{"location":"api/datajoint/diagram/","title":"Diagram","text":"<p>Schema visualization</p> <p>Diagram visualization for DataJoint schemas.</p> <p>This module provides the Diagram class for visualizing schema structure as directed acyclic graphs showing tables and their foreign key relationships.</p>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram","title":"Diagram","text":"<p>               Bases: <code>DiGraph</code></p> <p>Schema diagram as a directed acyclic graph (DAG).</p> <p>Visualizes tables and foreign key relationships derived from <code>connection.dependencies</code>.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Table, Schema, or module</code> <p>A table object, table class, schema, or module with a schema.</p> required <code>context</code> <code>dict</code> <p>Namespace for resolving table class names. If None, uses caller's frame globals/locals.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; diag = dj.Diagram(schema.MyTable)\n&gt;&gt;&gt; diag.draw()\n</code></pre> <p>Operators:</p> <ul> <li><code>diag1 + diag2</code> - union of diagrams</li> <li><code>diag1 - diag2</code> - difference of diagrams</li> <li><code>diag1 * diag2</code> - intersection of diagrams</li> <li><code>diag + n</code> - expand n levels of successors (children)</li> <li><code>diag - n</code> - expand n levels of predecessors (parents)</li> </ul> <pre><code>&gt;&gt;&gt; dj.Diagram(schema.Table) + 1 - 1  # immediate ancestors and descendants\n</code></pre> Notes <p><code>diagram + 1 - 1</code> may differ from <code>diagram - 1 + 1</code>. Only tables loaded in the connection are displayed.</p> <p>Layout direction is controlled via <code>dj.config.display.diagram_direction</code> (default <code>\"TB\"</code>). Use <code>dj.config.override()</code> to change temporarily::</p> <pre><code>with dj.config.override(display_diagram_direction=\"LR\"):\n    dj.Diagram(schema).draw()\n</code></pre>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram.from_sequence","title":"from_sequence  <code>classmethod</code>","text":"<pre><code>from_sequence(sequence)\n</code></pre> <p>Create combined Diagram from a sequence of sources.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>iterable</code> <p>Sequence of table objects, classes, or schemas.</p> required <p>Returns:</p> Type Description <code>Diagram</code> <p>Union of diagrams: <code>Diagram(arg1) + ... + Diagram(argn)</code>.</p>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram.add_parts","title":"add_parts","text":"<pre><code>add_parts()\n</code></pre> <p>Add part tables of all masters already in the diagram.</p> <p>Returns:</p> Type Description <code>Diagram</code> <p>New diagram with part tables included.</p>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram.collapse","title":"collapse","text":"<pre><code>collapse()\n</code></pre> <p>Mark all nodes in this diagram as collapsed.</p> <p>Collapsed nodes are shown as a single node per schema. When combined with other diagrams using <code>+</code>, expanded nodes win: if a node is expanded in either operand, it remains expanded in the result.</p> <p>Returns:</p> Type Description <code>Diagram</code> <p>A copy of this diagram with all nodes collapsed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Show schema1 expanded, schema2 collapsed into single nodes\n&gt;&gt;&gt; dj.Diagram(schema1) + dj.Diagram(schema2).collapse()\n</code></pre> <pre><code>&gt;&gt;&gt; # Collapse all three schemas together\n&gt;&gt;&gt; (dj.Diagram(schema1) + dj.Diagram(schema2) + dj.Diagram(schema3)).collapse()\n</code></pre> <pre><code>&gt;&gt;&gt; # Expand one table from collapsed schema\n&gt;&gt;&gt; dj.Diagram(schema).collapse() + dj.Diagram(SingleTable)\n</code></pre>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram.topo_sort","title":"topo_sort","text":"<pre><code>topo_sort()\n</code></pre> <p>Return nodes in topological order.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Node names in topological order.</p>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram.make_dot","title":"make_dot","text":"<pre><code>make_dot()\n</code></pre> <p>Generate a pydot graph object.</p> <p>Returns:</p> Type Description <code>Dot</code> <p>The graph object ready for rendering.</p> Notes <p>Layout direction is controlled via <code>dj.config.display.diagram_direction</code>. Tables are grouped by schema, with the Python module name shown as the group label when available.</p>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram.make_mermaid","title":"make_mermaid","text":"<pre><code>make_mermaid()\n</code></pre> <p>Generate Mermaid diagram syntax.</p> <p>Produces a flowchart in Mermaid syntax that can be rendered in Markdown documentation, GitHub, or https://mermaid.live.</p> <p>Returns:</p> Type Description <code>str</code> <p>Mermaid flowchart syntax.</p> Notes <p>Layout direction is controlled via <code>dj.config.display.diagram_direction</code>. Tables are grouped by schema using Mermaid subgraphs, with the Python module name shown as the group label when available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(dj.Diagram(schema).make_mermaid())\nflowchart TB\n    subgraph my_pipeline\n        Mouse[Mouse]:::manual\n        Session[Session]:::manual\n        Neuron([Neuron]):::computed\n    end\n    Mouse --&gt; Session\n    Session --&gt; Neuron\n</code></pre>"},{"location":"api/datajoint/diagram/#datajoint.diagram.Diagram.save","title":"save","text":"<pre><code>save(filename, format=None)\n</code></pre> <p>Save diagram to file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output filename.</p> required <code>format</code> <code>str</code> <p>File format (<code>'png'</code>, <code>'svg'</code>, or <code>'mermaid'</code>). Inferred from extension if None.</p> <code>None</code> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If format is unsupported.</p> Notes <p>Layout direction is controlled via <code>dj.config.display.diagram_direction</code>. Tables are grouped by schema, with the Python module name shown as the group label when available.</p>"},{"location":"api/datajoint/errors/","title":"Errors","text":"<p>Exception classes</p> <p>Exception classes for the DataJoint library.</p> <p>This module defines the exception hierarchy for DataJoint errors.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.DataJointError","title":"DataJointError","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for errors specific to DataJoint internal operation.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.DataJointError.suggest","title":"suggest","text":"<pre><code>suggest(*args)\n</code></pre> <p>Regenerate the exception with additional arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>object</code> <p>Additional arguments to append to the exception.</p> <code>()</code> <p>Returns:</p> Type Description <code>DataJointError</code> <p>A new exception of the same type with the additional arguments.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.LostConnectionError","title":"LostConnectionError","text":"<p>               Bases: <code>DataJointError</code></p> <p>Loss of server connection.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.QueryError","title":"QueryError","text":"<p>               Bases: <code>DataJointError</code></p> <p>Errors arising from queries to the database.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.QuerySyntaxError","title":"QuerySyntaxError","text":"<p>               Bases: <code>QueryError</code></p> <p>Errors arising from incorrect query syntax.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.AccessError","title":"AccessError","text":"<p>               Bases: <code>QueryError</code></p> <p>User access error: insufficient privileges.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.MissingTableError","title":"MissingTableError","text":"<p>               Bases: <code>DataJointError</code></p> <p>Query on a table that has not been declared.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.DuplicateError","title":"DuplicateError","text":"<p>               Bases: <code>QueryError</code></p> <p>Integrity error caused by a duplicate entry into a unique key.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.IntegrityError","title":"IntegrityError","text":"<p>               Bases: <code>QueryError</code></p> <p>Integrity error triggered by foreign key constraints.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.UnknownAttributeError","title":"UnknownAttributeError","text":"<p>               Bases: <code>QueryError</code></p> <p>User requests an attribute name not found in query heading.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.MissingAttributeError","title":"MissingAttributeError","text":"<p>               Bases: <code>QueryError</code></p> <p>Required attribute value not provided in INSERT.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.MissingExternalFile","title":"MissingExternalFile","text":"<p>               Bases: <code>DataJointError</code></p> <p>External file managed by DataJoint is no longer accessible.</p>"},{"location":"api/datajoint/errors/#datajoint.errors.BucketInaccessible","title":"BucketInaccessible","text":"<p>               Bases: <code>DataJointError</code></p> <p>S3 bucket is inaccessible.</p>"},{"location":"api/datajoint/expression/","title":"Expressions","text":"<p>Query expressions and operators</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression","title":"QueryExpression","text":"<p>QueryExpression implements query operators to derive new entity set from its input. A QueryExpression object generates a SELECT statement in SQL. QueryExpression operators are restrict, join, proj, aggr, and union.</p> <p>A QueryExpression object has a support, a restriction (an AndList), and heading. Property <code>heading</code> (type dj.Heading) contains information about the attributes. It is loaded from the database and updated by proj.</p> <p>Property <code>support</code> is the list of table names or other QueryExpressions to be joined.</p> <p>The restriction is applied first without having access to the attributes generated by the projection. Then projection is applied by selecting modifying the heading attribute.</p> <p>Application of operators does not always lead to the creation of a subquery. A subquery is generated when:     1. A restriction is applied on any computed or renamed attributes     2. A projection is applied remapping remapped attributes     3. Subclasses: Join, Aggregation, and Union have additional specific rules.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.connection","title":"connection  <code>property</code>","text":"<pre><code>connection\n</code></pre> <p>a dj.Connection object</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.support","title":"support  <code>property</code>","text":"<pre><code>support\n</code></pre> <p>A list of table names or subqueries to from the FROM clause</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.heading","title":"heading  <code>property</code>","text":"<pre><code>heading\n</code></pre> <p>a dj.Heading object, reflects the effects of the projection operator .proj</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.original_heading","title":"original_heading  <code>property</code>","text":"<pre><code>original_heading\n</code></pre> <p>a dj.Heading object reflecting the attributes before projection</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.restriction","title":"restriction  <code>property</code>","text":"<pre><code>restriction\n</code></pre> <p>a AndList object of restrictions applied to input to produce the result</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.restriction_attributes","title":"restriction_attributes  <code>property</code>","text":"<pre><code>restriction_attributes\n</code></pre> <p>the set of attribute names invoked in the WHERE clause</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.make_sql","title":"make_sql","text":"<pre><code>make_sql(fields=None)\n</code></pre> <p>Make the SQL SELECT statement.</p> <p>:param fields: used to explicitly set the select attributes</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.make_subquery","title":"make_subquery","text":"<pre><code>make_subquery()\n</code></pre> <p>create a new SELECT statement where self is the FROM clause</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.restrict","title":"restrict","text":"<pre><code>restrict(restriction, semantic_check=True)\n</code></pre> <p>Produces a new expression with the new restriction applied.</p> <p>:param restriction: a sequence or an array (treated as OR list), another QueryExpression,     an SQL condition string, or an AndList. :param semantic_check: If True (default), use semantic matching - only match on     homologous namesakes and error on non-homologous namesakes.     If False, use natural matching on all namesakes (no lineage checking). :return: A new QueryExpression with the restriction applied.</p> <p>rel.restrict(restriction) is equivalent to rel &amp; restriction. rel.restrict(Not(restriction)) is equivalent to rel - restriction</p> <p>The primary key of the result is unaffected. Successive restrictions are combined as logical AND: r &amp; a &amp; b is equivalent to r &amp; AndList((a, b)) Any QueryExpression, collection, or sequence other than an AndList are treated as OrLists (logical disjunction of conditions) Inverse restriction is accomplished by either using the subtraction operator or the Not class.</p> <p>The expressions in each row equivalent:</p> <p>rel &amp; True                          rel rel &amp; False                         the empty entity set rel &amp; 'TRUE'                        rel rel &amp; 'FALSE'                       the empty entity set rel - cond                          rel &amp; Not(cond) rel - 'TRUE'                        rel &amp; False rel - 'FALSE'                       rel rel &amp; AndList((cond1,cond2))        rel &amp; cond1 &amp; cond2 rel &amp; AndList()                     rel rel &amp; [cond1, cond2]                rel &amp; OrList((cond1, cond2)) rel &amp; []                            rel &amp; False rel &amp; None                          rel &amp; False rel &amp; any_empty_entity_set          rel &amp; False rel - AndList((cond1,cond2))        rel &amp; [Not(cond1), Not(cond2)] rel - [cond1, cond2]                rel &amp; Not(cond1) &amp; Not(cond2) rel - AndList()                     rel &amp; False rel - []                            rel rel - None                          rel rel - any_empty_entity_set          rel</p> <p>When arg is another QueryExpression, the restriction rel &amp; arg restricts rel to elements that match at least one element in arg (hence arg is treated as an OrList). Conversely, rel - arg restricts rel to elements that do not match any elements in arg. Two elements match when their common attributes have equal values or when they have no common attributes. All shared attributes must be in the primary key of either rel or arg or both or an error will be raised.</p> <p>QueryExpression.restrict is the only access point that modifies restrictions. All other operators must ultimately call restrict()</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.join","title":"join","text":"<pre><code>join(other, semantic_check=True, left=False, allow_nullable_pk=False)\n</code></pre> <p>Create the joined QueryExpression.</p> <p>:param other: QueryExpression to join with :param semantic_check: If True (default), use semantic matching - only match on     homologous namesakes (same lineage) and error on non-homologous namesakes.     If False, use natural join on all namesakes (no lineage checking). :param left: If True, perform a left join (retain all rows from self) :param allow_nullable_pk: If True, bypass the left join constraint that requires     self to determine other. When bypassed, the result PK is the union of both     operands' PKs, and PK attributes from the right operand could be NULL.     Used internally by aggregation when exclude_nonmatching=False. :return: The joined QueryExpression</p> <p>a * b is short for a.join(b)</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.extend","title":"extend","text":"<pre><code>extend(other, semantic_check=True)\n</code></pre> <p>Extend self with attributes from other.</p> <p>The extend operation adds attributes from <code>other</code> to <code>self</code> while preserving self's entity identity. It is semantically equivalent to <code>self.join(other, left=True)</code> but expresses a clearer intent: extending an entity set with additional attributes rather than combining two entity sets.</p> <p>Requirements:     self \u2192 other: Every attribute in other's primary key must exist in self.     This ensures:     - All rows of self are preserved (no filtering)     - Self's primary key remains the result's primary key (no NULL PKs)     - The operation is a true extension, not a Cartesian product</p> <p>Conceptual model:     Unlike a general join (Cartesian product restricted by matching attributes),     extend is closer to projection\u2014it adds new attributes to existing entities     without changing which entities are in the result.</p> <p>Example:     # Session determines Trial (session_id is in Trial's PK)     # But Trial does NOT determine Session (trial_num not in Session)</p> <pre><code># Valid: extend trials with session info\nTrial.extend(Session)  # Adds 'date' from Session to each Trial\n\n# Invalid: Session cannot extend to Trial\nSession.extend(Trial)  # Error: trial_num not in Session\n</code></pre> <p>:param other: QueryExpression whose attributes will extend self :param semantic_check: If True (default), require homologous namesakes.     If False, match on all namesakes without lineage checking. :return: Extended QueryExpression with self's PK and combined attributes :raises DataJointError: If self does not determine other</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.proj","title":"proj","text":"<pre><code>proj(*attributes, **named_attributes)\n</code></pre> <p>Projection operator.</p> <p>:param attributes:  attributes to be included in the result. (The primary key is already included). :param named_attributes: new attributes computed or renamed from existing attributes. :return: the projected expression. Primary key attributes cannot be excluded but may be renamed. If the attribute list contains an Ellipsis ..., then all secondary attributes are included too Prefixing an attribute name with a dash '-attr' removes the attribute from the list if present. Keyword arguments can be used to rename attributes as in name='attr', duplicate them as in name='(attr)', or self.proj(...) or self.proj(Ellipsis) -- include all attributes (return self) self.proj() -- include only primary key self.proj('attr1', 'attr2')  -- include primary key and attributes attr1 and attr2 self.proj(..., '-attr1', '-attr2')  -- include all attributes except attr1 and attr2 self.proj(name1='attr1') -- include primary key and 'attr1' renamed as name1 self.proj('attr1', dup='(attr1)') -- include primary key and attribute attr1 twice, with the duplicate 'dup' self.proj(k='abs(attr1)') adds the new attribute k with the value computed as an expression (SQL syntax) from other attributes available before the projection. Each attribute name can only be used once.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.aggr","title":"aggr","text":"<pre><code>aggr(group, *attributes, exclude_nonmatching=False, **named_attributes)\n</code></pre> <p>Aggregation/grouping operation, similar to proj but with computations over a grouped relation.</p> <p>By default, keeps all rows from self (like proj). Use exclude_nonmatching=True to keep only rows that have matches in group.</p> <p>:param group: The query expression to be aggregated. :param exclude_nonmatching: If True, exclude rows from self that have no matching     entries in group (INNER JOIN). Default False keeps all rows (LEFT JOIN). :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression</p> <p>Example::</p> <pre><code># Count sessions per subject (keeps all subjects, even those with 0 sessions)\nSubject.aggr(Session, n=\"count(*)\")\n\n# Count sessions per subject (only subjects with at least one session)\nSubject.aggr(Session, n=\"count(*)\", exclude_nonmatching=True)\n</code></pre>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.fetch","title":"fetch","text":"<pre><code>fetch(*attrs, offset=None, limit=None, order_by=None, format=None, as_dict=None, squeeze=False)\n</code></pre> <p>Fetch data from the table (backward-compatible with DataJoint 0.14).</p> <p>.. deprecated:: 2.0     Use the new explicit output methods instead:     - <code>to_dicts()</code> for list of dictionaries     - <code>to_pandas()</code> for pandas DataFrame     - <code>to_arrays()</code> for numpy structured array     - <code>to_arrays('a', 'b')</code> for tuple of arrays     - <code>keys()</code> for primary keys</p> <p>Parameters:</p> Name Type Description Default <code>*attrs</code> <code>str</code> <p>Attributes to fetch. If empty, fetches all.</p> <code>()</code> <code>offset</code> <code>int</code> <p>Number of tuples to skip.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Maximum number of tuples to return.</p> <code>None</code> <code>order_by</code> <code>str or list</code> <p>Attribute(s) for ordering results.</p> <code>None</code> <code>format</code> <code>str</code> <p>Output format: 'array' or 'frame' (pandas DataFrame).</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>Return as list of dicts instead of structured array.</p> <code>None</code> <code>squeeze</code> <code>bool</code> <p>Remove extra dimensions from arrays. Default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>np.recarray, list[dict], or pd.DataFrame</code> <p>Query results in requested format.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.fetch1","title":"fetch1","text":"<pre><code>fetch1(*attrs, squeeze=False)\n</code></pre> <p>Fetch exactly one row from the query result.</p> <p>If no attributes are specified, returns the result as a dict. If attributes are specified, returns the corresponding values as a tuple.</p> <p>:param attrs: attribute names to fetch (if empty, fetch all as dict) :param squeeze: if True, remove extra dimensions from arrays :return: dict (no attrs) or tuple/value (with attrs) :raises DataJointError: if not exactly one row in result</p> <p>Examples::</p> <pre><code>d = table.fetch1()              # returns dict with all attributes\na, b = table.fetch1('a', 'b')   # returns tuple of attribute values\nvalue = table.fetch1('a')       # returns single value\n</code></pre>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.to_dicts","title":"to_dicts","text":"<pre><code>to_dicts(order_by=None, limit=None, offset=None, squeeze=False)\n</code></pre> <p>Fetch all rows as a list of dictionaries.</p> <p>:param order_by: attribute(s) to order by, or \"KEY\"/\"KEY DESC\" :param limit: maximum number of rows to return :param offset: number of rows to skip :param squeeze: if True, remove extra dimensions from arrays :return: list of dictionaries, one per row</p> <p>For object storage types (attachments, filepaths), files are downloaded to config[\"download_path\"]. Use config.override() to change::</p> <pre><code>with dj.config.override(download_path=\"/data\"):\n    data = table.to_dicts()\n</code></pre>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas(order_by=None, limit=None, offset=None, squeeze=False)\n</code></pre> <p>Fetch all rows as a pandas DataFrame with primary key as index.</p> <p>:param order_by: attribute(s) to order by, or \"KEY\"/\"KEY DESC\" :param limit: maximum number of rows to return :param offset: number of rows to skip :param squeeze: if True, remove extra dimensions from arrays :return: pandas DataFrame with primary key columns as index</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.to_polars","title":"to_polars","text":"<pre><code>to_polars(order_by=None, limit=None, offset=None, squeeze=False)\n</code></pre> <p>Fetch all rows as a polars DataFrame.</p> <p>Requires polars: pip install datajoint[polars]</p> <p>:param order_by: attribute(s) to order by, or \"KEY\"/\"KEY DESC\" :param limit: maximum number of rows to return :param offset: number of rows to skip :param squeeze: if True, remove extra dimensions from arrays :return: polars DataFrame</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow(order_by=None, limit=None, offset=None, squeeze=False)\n</code></pre> <p>Fetch all rows as a PyArrow Table.</p> <p>Requires pyarrow: pip install datajoint[arrow]</p> <p>:param order_by: attribute(s) to order by, or \"KEY\"/\"KEY DESC\" :param limit: maximum number of rows to return :param offset: number of rows to skip :param squeeze: if True, remove extra dimensions from arrays :return: pyarrow Table</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.to_arrays","title":"to_arrays","text":"<pre><code>to_arrays(*attrs, include_key=False, order_by=None, limit=None, offset=None, squeeze=False)\n</code></pre> <p>Fetch data as numpy arrays.</p> <p>If no attrs specified, returns a numpy structured array (recarray) of all columns. If attrs specified, returns a tuple of numpy arrays (one per attribute).</p> <p>:param attrs: attribute names to fetch (if empty, fetch all) :param include_key: if True and attrs specified, prepend primary keys as list of dicts :param order_by: attribute(s) to order by, or \"KEY\"/\"KEY DESC\" :param limit: maximum number of rows to return :param offset: number of rows to skip :param squeeze: if True, remove extra dimensions from arrays :return: numpy recarray (no attrs) or tuple of arrays (with attrs).     With include_key=True: (keys, *arrays) where keys is list[dict]</p> <p>Examples::</p> <pre><code># Fetch as structured array\ndata = table.to_arrays()\n\n# Fetch specific columns as separate arrays\na, b = table.to_arrays('a', 'b')\n\n# Fetch with primary keys for later restrictions\nkeys, a, b = table.to_arrays('a', 'b', include_key=True)\n# keys = [{'id': 1}, {'id': 2}, ...]  # same format as table.keys()\n</code></pre>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.keys","title":"keys","text":"<pre><code>keys(order_by=None, limit=None, offset=None)\n</code></pre> <p>Fetch primary key values as a list of dictionaries.</p> <p>:param order_by: attribute(s) to order by, or \"KEY\"/\"KEY DESC\" :param limit: maximum number of rows to return :param offset: number of rows to skip :return: list of dictionaries containing only primary key columns</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.head","title":"head","text":"<pre><code>head(limit=25)\n</code></pre> <p>Preview the first few entries from query expression.</p> <p>:param limit: number of entries (default 25) :return: list of dictionaries</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.tail","title":"tail","text":"<pre><code>tail(limit=25)\n</code></pre> <p>Preview the last few entries from query expression.</p> <p>:param limit: number of entries (default 25) :return: list of dictionaries</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.cursor","title":"cursor","text":"<pre><code>cursor(as_dict=False)\n</code></pre> <p>Execute the query and return a database cursor.</p> <p>:param as_dict: if True, rows are returned as dictionaries :return: database query cursor</p>"},{"location":"api/datajoint/expression/#datajoint.expression.QueryExpression.preview","title":"preview","text":"<pre><code>preview(limit=None, width=None)\n</code></pre> <p>:return: a string of preview of the contents of the query.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.Aggregation","title":"Aggregation","text":"<p>               Bases: <code>QueryExpression</code></p> <p>Aggregation.create(arg, group, comp1='calc1', ..., compn='calcn')  yields an entity set with primary key from arg. The computed arguments comp1, ..., compn use aggregation calculations on the attributes of group or simple projections and calculations on the attributes of arg. Aggregation is used QueryExpression.aggr and U.aggr. Aggregation is a private class in DataJoint, not exposed to users.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.Aggregation.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(groupby, group, keep_all_rows=False)\n</code></pre> <p>Create an aggregation expression.</p> <p>:param groupby: The expression to GROUP BY (determines the result's primary key) :param group: The expression to aggregate over :param keep_all_rows: If True, use left join to keep all rows from groupby</p>"},{"location":"api/datajoint/expression/#datajoint.expression.Union","title":"Union","text":"<p>               Bases: <code>QueryExpression</code></p> <p>Union is the private DataJoint class that implements the union operator.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.Union.from_clause","title":"from_clause","text":"<pre><code>from_clause()\n</code></pre> <p>The union does not use a FROM clause.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.Union.where_clause","title":"where_clause","text":"<pre><code>where_clause()\n</code></pre> <p>The union does not use a WHERE clause.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.U","title":"U","text":"<p>dj.U objects are the universal sets representing all possible values of their attributes. dj.U objects cannot be queried on their own but are useful for forming some queries. dj.U('attr1', ..., 'attrn') represents the universal set with the primary key attributes attr1 ... attrn. The universal set is the set of all possible combinations of values of the attributes. Without any attributes, dj.U() represents the set with one element that has no attributes.</p> <p>Restriction:</p> <p>dj.U can be used to enumerate unique combinations of values of attributes from other expressions.</p> <p>The following expression yields all unique combinations of contrast and brightness found in the <code>stimulus</code> set:</p> <p>dj.U('contrast', 'brightness') &amp; stimulus</p> <p>Aggregation:</p> <p>In aggregation, dj.U is used for summary calculation over an entire set:</p> <p>The following expression yields one element with one attribute <code>s</code> containing the total number of elements in query expression <code>expr</code>:</p> <p>dj.U().aggr(expr, n='count(*)')</p> <p>The following expressions both yield one element containing the number <code>n</code> of distinct values of attribute <code>attr</code> in query expression <code>expr</code>.</p> <p>dj.U().aggr(expr, n='count(distinct attr)') dj.U().aggr(dj.U('attr').aggr(expr), 'n=count(*)')</p> <p>The following expression yields one element and one attribute <code>s</code> containing the sum of values of attribute <code>attr</code> over entire result set of expression <code>expr</code>:</p> <p>dj.U().aggr(expr, s='sum(attr)')</p> <p>The following expression yields the set of all unique combinations of attributes <code>attr1</code>, <code>attr2</code> and the number of their occurrences in the result set of query expression <code>expr</code>.</p> <p>dj.U(attr1,attr2).aggr(expr, n='count(*)')</p> <p>Joins:</p> <p>If expression <code>expr</code> has attributes 'attr1' and 'attr2', then expr * dj.U('attr1','attr2') yields the same result as <code>expr</code> but <code>attr1</code> and <code>attr2</code> are promoted to the the primary key.  This is useful for producing a join on non-primary key attributes. For example, if <code>attr</code> is in both expr1 and expr2 but not in their primary keys, then expr1 * expr2 will throw an error because in most cases, it does not make sense to join on non-primary key attributes and users must first rename <code>attr</code> in one of the operands.  The expression dj.U('attr') * rel1 * rel2 overrides this constraint.</p>"},{"location":"api/datajoint/expression/#datajoint.expression.U.aggr","title":"aggr","text":"<pre><code>aggr(group, **named_attributes)\n</code></pre> <p>Aggregation of the type U('attr1','attr2').aggr(group, computation=\"QueryExpression\") has the primary key ('attr1','attr2') and performs aggregation computations for all matching elements of <code>group</code>.</p> <p>Note: exclude_nonmatching is always True for dj.U (cannot keep all rows from infinite set).</p> <p>:param group:  The query expression to be aggregated. :param named_attributes: computations of the form new_attribute=\"sql expression on attributes of group\" :return: The derived query expression</p>"},{"location":"api/datajoint/hash_registry/","title":"Hash Registry","text":"<p>Content hashing for external storage</p> <p>Hash-addressed storage registry for DataJoint.</p> <p>This module provides hash-addressed storage with deduplication for the <code>&lt;hash&gt;</code> codec. Content is identified by a Base32-encoded MD5 hash and stored with per-schema isolation::</p> <pre><code>_hash/{schema}/{hash}\n</code></pre> <p>With optional subfolding (configured per-store)::</p> <pre><code>_hash/{schema}/{fold1}/{fold2}/{hash}\n</code></pre> <p>Subfolding creates directory hierarchies to improve performance on filesystems that struggle with large directories (ext3, FAT32, NFS). Modern filesystems (ext4, XFS, ZFS, S3) handle flat directories efficiently.</p> <p>Storage Model:</p> <ul> <li>Hash is used for content identification (deduplication, integrity verification)</li> <li>Path is always stored in metadata and used for all file operations</li> </ul> <p>This design protects against configuration changes (e.g., subfolding) affecting existing data. The path stored at insert time is always used for retrieval.</p> <p>Hash-addressed storage is used by <code>&lt;hash@&gt;</code>, <code>&lt;blob@&gt;</code>, and <code>&lt;attach@&gt;</code> types. Deduplication occurs within each schema. Deletion requires garbage collection via <code>dj.gc.collect()</code>.</p> See Also <p>datajoint.gc : Garbage collection for orphaned storage items.</p>"},{"location":"api/datajoint/hash_registry/#datajoint.hash_registry.compute_hash","title":"compute_hash","text":"<pre><code>compute_hash(data)\n</code></pre> <p>Compute Base32-encoded MD5 hash of content.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Content bytes.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Base32-encoded hash (26 lowercase characters, no padding).</p>"},{"location":"api/datajoint/hash_registry/#datajoint.hash_registry.build_hash_path","title":"build_hash_path","text":"<pre><code>build_hash_path(content_hash, schema_name, subfolding=None)\n</code></pre> <p>Build the storage path for hash-addressed storage.</p> <p>Path structure without subfolding::</p> <pre><code>_hash/{schema}/{hash}\n</code></pre> <p>Path structure with subfolding (e.g., (2, 2))::</p> <pre><code>_hash/{schema}/{fold1}/{fold2}/{hash}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>content_hash</code> <code>str</code> <p>Base32-encoded hash (26 characters).</p> required <code>schema_name</code> <code>str</code> <p>Database/schema name for isolation.</p> required <code>subfolding</code> <code>tuple[int, ...]</code> <p>Subfolding pattern from store config. None means flat (no subfolding).</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Relative path within the store.</p>"},{"location":"api/datajoint/hash_registry/#datajoint.hash_registry.get_store_backend","title":"get_store_backend","text":"<pre><code>get_store_backend(store_name=None)\n</code></pre> <p>Get a StorageBackend for hash-addressed storage.</p> <p>Parameters:</p> Name Type Description Default <code>store_name</code> <code>str</code> <p>Name of the store to use. If None, uses stores.default.</p> <code>None</code> <p>Returns:</p> Type Description <code>StorageBackend</code> <p>StorageBackend instance.</p>"},{"location":"api/datajoint/hash_registry/#datajoint.hash_registry.get_store_subfolding","title":"get_store_subfolding","text":"<pre><code>get_store_subfolding(store_name=None)\n</code></pre> <p>Get the subfolding configuration for a store.</p> <p>Parameters:</p> Name Type Description Default <code>store_name</code> <code>str</code> <p>Name of the store. If None, uses stores.default.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[int, ...] | None</code> <p>Subfolding pattern (e.g., (2, 2)) or None for flat storage.</p>"},{"location":"api/datajoint/hash_registry/#datajoint.hash_registry.put_hash","title":"put_hash","text":"<pre><code>put_hash(data, schema_name, store_name=None)\n</code></pre> <p>Store content using hash-addressed storage.</p> <p>If the content already exists (same hash in same schema), it is not re-uploaded. Returns metadata including the hash, path, store, and size.</p> <p>The path is always stored in metadata and used for retrieval, protecting against configuration changes (e.g., subfolding) affecting existing data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>Content bytes to store.</p> required <code>schema_name</code> <code>str</code> <p>Database/schema name for path isolation.</p> required <code>store_name</code> <code>str</code> <p>Name of the store. If None, uses default store.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Metadata dict with keys: hash, path, schema, store, size.</p>"},{"location":"api/datajoint/hash_registry/#datajoint.hash_registry.get_hash","title":"get_hash","text":"<pre><code>get_hash(metadata)\n</code></pre> <p>Retrieve content using stored metadata.</p> <p>Uses the stored path directly (not derived from hash) to protect against configuration changes affecting existing data.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>dict</code> <p>Metadata dict with keys: path, hash, store (optional).</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Content bytes.</p> <p>Raises:</p> Type Description <code>MissingExternalFile</code> <p>If content is not found at the stored path.</p> <code>DataJointError</code> <p>If hash verification fails (data corruption).</p>"},{"location":"api/datajoint/hash_registry/#datajoint.hash_registry.delete_path","title":"delete_path","text":"<pre><code>delete_path(path, store_name=None)\n</code></pre> <p>Delete content at the specified path from storage.</p> <p>This should only be called after verifying no references exist. Use garbage collection to safely remove unreferenced content.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Storage path (as stored in metadata).</p> required <code>store_name</code> <code>str</code> <p>Name of the store. If None, uses default store.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if content was deleted, False if it didn't exist.</p> Warnings <p>This permanently deletes content. Ensure no references exist first.</p>"},{"location":"api/datajoint/heading/","title":"Heading","text":"<p>Table heading and attributes</p> <p>Heading management for DataJoint tables.</p> <p>This module provides the Heading class for managing table column metadata, including attribute types, constraints, and lineage information.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Attribute","title":"Attribute","text":"<p>               Bases: <code>namedtuple('_Attribute', default_attribute_properties)</code></p> <p>Properties of a table column (attribute).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Attribute name.</p> <code>type</code> <code>str</code> <p>Database type string.</p> <code>in_key</code> <code>bool</code> <p>True if part of primary key.</p> <code>nullable</code> <code>bool</code> <p>True if NULL values allowed.</p> <code>default</code> <code>any</code> <p>Default value.</p> <code>comment</code> <code>str</code> <p>Attribute comment/description.</p> <code>codec</code> <code>Codec</code> <p>Codec for encoding/decoding values.</p> <code>lineage</code> <code>str</code> <p>Origin of attribute for semantic matching.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Attribute.todict","title":"todict","text":"<pre><code>todict()\n</code></pre> <p>Convert to dictionary.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Attribute.sql_type","title":"sql_type  <code>property</code>","text":"<pre><code>sql_type\n</code></pre> <p>Return the SQL datatype string.</p> <p>Returns:</p> Type Description <code>str</code> <p>Database type (usually same as self.type).</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Attribute.sql_comment","title":"sql_comment  <code>property</code>","text":"<pre><code>sql_comment\n</code></pre> <p>Return the full SQL comment including type markers.</p> <p>Returns:</p> Type Description <code>str</code> <p>Comment with optional <code>:uuid:</code> prefix.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Attribute.sql","title":"sql  <code>property</code>","text":"<pre><code>sql\n</code></pre> <p>Generate SQL clause for this attribute in CREATE TABLE.</p> <p>Used for declaring foreign keys in referencing tables. Default values are not included.</p> <p>Returns:</p> Type Description <code>str</code> <p>SQL attribute declaration.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Attribute.original_name","title":"original_name  <code>property</code>","text":"<pre><code>original_name\n</code></pre> <p>Return the original attribute name before any renaming.</p> <p>Returns:</p> Type Description <code>str</code> <p>Original name from attribute_expression or current name.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading","title":"Heading","text":"<p>Table heading containing column metadata.</p> <p>Manages attribute information including names, types, constraints, and lineage for semantic matching.</p> <p>Parameters:</p> Name Type Description Default <code>attribute_specs</code> <code>list</code> <p>List of attribute specification dictionaries.</p> <code>None</code> <code>table_info</code> <code>dict</code> <p>Database table information for lazy loading.</p> <code>None</code> <code>lineage_available</code> <code>bool</code> <p>Whether lineage information is available. Default True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>attributes</code> <code>dict</code> <p>Mapping of attribute names to Attribute objects.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.lineage_available","title":"lineage_available  <code>property</code>","text":"<pre><code>lineage_available\n</code></pre> <p>Whether lineage tracking is available for this heading's schema.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.table_status","title":"table_status  <code>property</code>","text":"<pre><code>table_status\n</code></pre> <p>Table status information from database.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.attributes","title":"attributes  <code>property</code>","text":"<pre><code>attributes\n</code></pre> <p>Mapping of attribute names to Attribute objects.</p> <p>Excludes hidden attributes (names starting with <code>_</code>).</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.names","title":"names  <code>property</code>","text":"<pre><code>names\n</code></pre> <p>List of visible attribute names.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.primary_key","title":"primary_key  <code>property</code>","text":"<pre><code>primary_key\n</code></pre> <p>List of primary key attribute names.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.secondary_attributes","title":"secondary_attributes  <code>property</code>","text":"<pre><code>secondary_attributes\n</code></pre> <p>List of non-primary-key attribute names.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.determines","title":"determines","text":"<pre><code>determines(other)\n</code></pre> <p>Check if self determines other (self \u2192 other).</p> <p>A determines B iff every attribute in PK(B) is in A. This means knowing A's primary key is sufficient to determine B's primary key through functional dependencies.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Heading</code> <p>Another Heading object.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if self determines other.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.blobs","title":"blobs  <code>property</code>","text":"<pre><code>blobs\n</code></pre> <p>List of blob attribute names.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.non_blobs","title":"non_blobs  <code>property</code>","text":"<pre><code>non_blobs\n</code></pre> <p>Attributes that are not blobs or JSON.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.new_attributes","title":"new_attributes  <code>property</code>","text":"<pre><code>new_attributes\n</code></pre> <p>Attributes with computed expressions (projections).</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.has_autoincrement","title":"has_autoincrement  <code>property</code>","text":"<pre><code>has_autoincrement\n</code></pre> <p>Check if any attribute has auto_increment.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.as_dtype","title":"as_dtype  <code>property</code>","text":"<pre><code>as_dtype\n</code></pre> <p>Return heading as a numpy dtype.</p> <p>Returns:</p> Type Description <code>dtype</code> <p>Structured dtype for creating numpy arrays.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.as_sql","title":"as_sql","text":"<pre><code>as_sql(fields, include_aliases=True, adapter=None)\n</code></pre> <p>Generate SQL SELECT clause for specified fields.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[str]</code> <p>Attribute names to include.</p> required <code>include_aliases</code> <code>bool</code> <p>Include AS clauses for computed attributes. Default True.</p> <code>True</code> <code>adapter</code> <code>DatabaseAdapter</code> <p>Database adapter for identifier quoting. If not provided, attempts to get from table_info connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Comma-separated SQL field list.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.select","title":"select","text":"<pre><code>select(select_list, rename_map=None, compute_map=None)\n</code></pre> <p>derive a new heading by selecting, renaming, or computing attributes. In relational algebra these operators are known as project, rename, and extend.</p> <p>:param select_list:  the full list of existing attributes to include :param rename_map:  dictionary of renamed attributes: keys=new names, values=old names :param compute_map: a direction of computed attributes This low-level method performs no error checking.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.join","title":"join","text":"<pre><code>join(other, nullable_pk=False)\n</code></pre> <p>Join two headings into a new one.</p> <p>The primary key of the result depends on functional dependencies: - A \u2192 B: PK = PK(A), A's attributes first - B \u2192 A (not A \u2192 B): PK = PK(B), B's attributes first - Both: PK = PK(A), left operand takes precedence - Neither: PK = PK(A) \u222a PK(B), A's PK first then B's new PK attrs</p> <p>:param nullable_pk: If True, skip PK optimization and use combined PK from both     operands. Used for left joins that bypass the A \u2192 B constraint, where the     right operand's PK attributes could be NULL.</p> <p>It assumes that self and other are headings that share no common dependent attributes.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.set_primary_key","title":"set_primary_key","text":"<pre><code>set_primary_key(primary_key)\n</code></pre> <p>Create a new heading with the specified primary key. This low-level method performs no error checking.</p>"},{"location":"api/datajoint/heading/#datajoint.heading.Heading.make_subquery_heading","title":"make_subquery_heading","text":"<pre><code>make_subquery_heading()\n</code></pre> <p>Create a new heading with removed attribute sql_expressions. Used by subqueries, which resolve the sql_expressions.</p>"},{"location":"api/datajoint/jobs/","title":"Jobs","text":"<p>Job queue for AutoPopulate</p> <p>Job queue management for AutoPopulate 2.0.</p> <p>Each auto-populated table (Computed/Imported) has an associated jobs table with the naming pattern <code>~~table_name</code>. The jobs table tracks job status, priority, scheduling, and error information.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job","title":"Job","text":"<p>               Bases: <code>Table</code></p> <p>Per-table job queue for AutoPopulate 2.0.</p> <p>Each auto-populated table (Computed/Imported) has an associated job table with the naming pattern <code>~~table_name</code>. The job table tracks job status, priority, scheduling, and error information.</p> <p>Parameters:</p> Name Type Description Default <code>target_table</code> <code>Table</code> <p>The Computed/Imported table instance this jobs table manages.</p> required <p>Attributes:</p> Name Type Description <code>target</code> <code>Table</code> <p>The auto-populated table this jobs table manages.</p> <code>pending</code> <code>QueryExpression</code> <p>Query for jobs with <code>status='pending'</code>.</p> <code>reserved</code> <code>QueryExpression</code> <p>Query for jobs with <code>status='reserved'</code>.</p> <code>errors</code> <code>QueryExpression</code> <p>Query for jobs with <code>status='error'</code>.</p> <code>completed</code> <code>QueryExpression</code> <p>Query for jobs with <code>status='success'</code>.</p> <code>ignored</code> <code>QueryExpression</code> <p>Query for jobs with <code>status='ignore'</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; MyTable.jobs.refresh()      # Add new jobs, clean up stale ones\n&gt;&gt;&gt; MyTable.jobs.pending        # Query pending jobs\n&gt;&gt;&gt; MyTable.jobs.errors         # Query failed jobs\n</code></pre>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.target","title":"target  <code>property</code>","text":"<pre><code>target\n</code></pre> <p>The auto-populated table this jobs table manages.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.delete","title":"delete","text":"<pre><code>delete()\n</code></pre> <p>Delete all entries, bypassing interactive prompts and dependencies.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.drop","title":"drop","text":"<pre><code>drop()\n</code></pre> <p>Drop the table, bypassing interactive prompts and dependencies.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.pending","title":"pending  <code>property</code>","text":"<pre><code>pending\n</code></pre> <p>Query for pending jobs awaiting processing.</p> <p>Returns:</p> Type Description <code>Job</code> <p>Restricted query with <code>status='pending'</code>.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.reserved","title":"reserved  <code>property</code>","text":"<pre><code>reserved\n</code></pre> <p>Query for jobs currently being processed.</p> <p>Returns:</p> Type Description <code>Job</code> <p>Restricted query with <code>status='reserved'</code>.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.errors","title":"errors  <code>property</code>","text":"<pre><code>errors\n</code></pre> <p>Query for jobs that failed with errors.</p> <p>Returns:</p> Type Description <code>Job</code> <p>Restricted query with <code>status='error'</code>.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.ignored","title":"ignored  <code>property</code>","text":"<pre><code>ignored\n</code></pre> <p>Query for jobs marked to be skipped.</p> <p>Returns:</p> Type Description <code>Job</code> <p>Restricted query with <code>status='ignore'</code>.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.completed","title":"completed  <code>property</code>","text":"<pre><code>completed\n</code></pre> <p>Query for successfully completed jobs.</p> <p>Returns:</p> Type Description <code>Job</code> <p>Restricted query with <code>status='success'</code>.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.refresh","title":"refresh","text":"<pre><code>refresh(*restrictions, delay=0, priority=None, stale_timeout=None, orphan_timeout=None)\n</code></pre> <p>Refresh the jobs queue: add new jobs and clean up stale/orphaned jobs.</p> <p>Parameters:</p> Name Type Description Default <code>*restrictions</code> <code>any</code> <p>Conditions to filter key_source (for adding new jobs).</p> <code>()</code> <code>delay</code> <code>float</code> <p>Seconds from now until new jobs become available for processing. Default 0 (immediately available). Uses database server time.</p> <code>0</code> <code>priority</code> <code>int</code> <p>Priority for new jobs (lower = more urgent). Default from <code>config.jobs.default_priority</code>.</p> <code>None</code> <code>stale_timeout</code> <code>float</code> <p>Seconds after which jobs are checked for staleness. Jobs older than this are removed if key not in key_source. Default from <code>config.jobs.stale_timeout</code>. Set to 0 to skip.</p> <code>None</code> <code>orphan_timeout</code> <code>float</code> <p>Seconds after which reserved jobs are considered orphaned. Reserved jobs older than this are deleted and re-added as pending. Default None (no orphan cleanup).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Status counts with keys: <code>'added'</code>, <code>'removed'</code>, <code>'orphaned'</code>, <code>'re_pended'</code>.</p> Notes <p>Operations performed:</p> <ol> <li>Add new jobs: <code>(key_source &amp; restrictions) - target - jobs</code> \u2192 insert as pending</li> <li>Re-pend success jobs: if <code>keep_completed=True</code> and key in key_source but not in target</li> <li>Remove stale jobs: jobs older than stale_timeout whose keys not in key_source</li> <li>Remove orphaned jobs: reserved jobs older than orphan_timeout (if specified)</li> </ol>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.reserve","title":"reserve","text":"<pre><code>reserve(key)\n</code></pre> <p>Attempt to reserve a pending job for processing.</p> <p>Updates status to <code>'reserved'</code> if currently <code>'pending'</code> and <code>scheduled_time &lt;= now</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key dict of the job to reserve.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if reservation successful, False if job not available.</p>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.complete","title":"complete","text":"<pre><code>complete(key, duration=None)\n</code></pre> <p>Mark a job as successfully completed.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key dict of the job.</p> required <code>duration</code> <code>float</code> <p>Execution duration in seconds.</p> <code>None</code> Notes <p>Based on <code>config.jobs.keep_completed</code>:</p> <ul> <li>If True: updates status to <code>'success'</code> with completion time and duration</li> <li>If False: deletes the job entry</li> </ul>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.error","title":"error","text":"<pre><code>error(key, error_message, error_stack=None)\n</code></pre> <p>Mark a job as failed with error details.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key dict of the job.</p> required <code>error_message</code> <code>str</code> <p>Error message (truncated to 2047 chars if longer).</p> required <code>error_stack</code> <code>str</code> <p>Full stack trace.</p> <code>None</code>"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.ignore","title":"ignore","text":"<pre><code>ignore(key)\n</code></pre> <p>Mark a job to be ignored (skipped during populate).</p> <p>If the key doesn't exist in the jobs table, inserts it with <code>status='ignore'</code>. If it exists, updates the status to <code>'ignore'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key dict of the job.</p> required"},{"location":"api/datajoint/jobs/#datajoint.jobs.Job.progress","title":"progress","text":"<pre><code>progress()\n</code></pre> <p>Return job status breakdown.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Counts by status with keys: <code>'pending'</code>, <code>'reserved'</code>, <code>'success'</code>, <code>'error'</code>, <code>'ignore'</code>, <code>'total'</code>.</p>"},{"location":"api/datajoint/migrate/","title":"Migrate","text":"<p>Schema migration utilities</p> <p>Migration utilities for DataJoint schema updates.</p> <p>This module provides tools for migrating existing schemas to use the new Codec system, particularly for upgrading blob columns to use explicit <code>&lt;blob&gt;</code> type declarations.</p> <p>.. note::     This module is provided temporarily to assist with migration from pre-2.0.     It will be deprecated in DataJoint 2.1 and removed in 2.2.     Complete your migrations while on DataJoint 2.0.</p> Note on Terminology <p>This module uses \"external storage\" because that was the term in DataJoint 0.14.6. In DataJoint 2.0 documentation, this is called \"object storage\" (general term) or \"in-store storage\" (specific to the @ modifier).</p>"},{"location":"api/datajoint/migrate/#datajoint.migrate.analyze_columns","title":"analyze_columns","text":"<pre><code>analyze_columns(schema)\n</code></pre> <p>Analyze a schema to find columns that need type labels in comments.</p> <p>This identifies columns that:</p> <ol> <li>Use native MySQL types that should be labeled with core types</li> <li>Are blob columns without codec markers</li> <li>Use external storage (requiring Phase 3-4 migration)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to analyze.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>needs_migration: list of columns needing type labels</li> <li>already_migrated: list of columns with existing type labels</li> <li>external_storage: list of columns requiring Phase 3-4</li> </ul> <p>Each column entry has: table, column, native_type, core_type, comment</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import datajoint as dj\n&gt;&gt;&gt; from datajoint.migrate import analyze_columns\n&gt;&gt;&gt; schema = dj.Schema('my_database')\n&gt;&gt;&gt; result = analyze_columns(schema)\n&gt;&gt;&gt; for col in result['needs_migration']:\n...     print(f\"{col['table']}.{col['column']}: {col['native_type']} \u2192 {col['core_type']}\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.migrate_columns","title":"migrate_columns","text":"<pre><code>migrate_columns(schema, dry_run=True)\n</code></pre> <p>Add type labels to column comments for Phase 2 migration.</p> <p>This updates column comments to include type labels, enabling DataJoint 2.0 to recognize column types without relying on native MySQL types.</p> <p>Migrates:</p> <ul> <li>Numeric types: int unsigned \u2192 :uint32:, smallint \u2192 :int16:, etc.</li> <li>Blob types: longblob \u2192 :: <p>Does NOT migrate external storage columns (external-, attach@, filepath@*) - those require Phase 3-4.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to migrate.</p> required <code>dry_run</code> <code>bool</code> <p>If True, only preview changes without applying. Default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>columns_analyzed: total columns checked</li> <li>columns_migrated: number of columns updated</li> <li>columns_skipped: number already migrated or external</li> <li>sql_statements: list of SQL executed (or to be executed)</li> <li>details: per-column results</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datajoint.migrate import migrate_columns\n&gt;&gt;&gt; # Preview\n&gt;&gt;&gt; result = migrate_columns(schema, dry_run=True)\n&gt;&gt;&gt; print(f\"Would migrate {len(result['sql_statements'])} columns\")\n&gt;&gt;&gt; # Apply\n&gt;&gt;&gt; result = migrate_columns(schema, dry_run=False)\n&gt;&gt;&gt; print(f\"Migrated {result['columns_migrated']} columns\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.analyze_blob_columns","title":"analyze_blob_columns","text":"<pre><code>analyze_blob_columns(schema)\n</code></pre> <p>Analyze a schema to find blob columns that could be migrated to . <p>This function identifies blob columns that:</p> <ol> <li>Have a MySQL blob type (tinyblob, blob, mediumblob, longblob)</li> <li>Do NOT already have a codec/type specified in their comment</li> </ol> <p>All blob size variants are included in the analysis.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to analyze.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of dicts with keys:</p> <ul> <li>table_name: Full table name (database.table)</li> <li>column_name: Name of the blob column</li> <li>column_type: MySQL column type (tinyblob, blob, mediumblob, longblob)</li> <li>current_comment: Current column comment</li> <li>needs_migration: True if column should be migrated</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import datajoint as dj\n&gt;&gt;&gt; schema = dj.Schema('my_database')\n&gt;&gt;&gt; columns = dj.migrate.analyze_blob_columns(schema)\n&gt;&gt;&gt; for col in columns:\n...     if col['needs_migration']:\n...         print(f\"{col['table_name']}.{col['column_name']} ({col['column_type']})\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.generate_migration_sql","title":"generate_migration_sql","text":"<pre><code>generate_migration_sql(schema, target_type='blob', dry_run=True)\n</code></pre> <p>Generate SQL statements to migrate blob columns to use . <p>This generates ALTER TABLE statements that update column comments to include the <code>:&lt;blob&gt;:</code> prefix, marking them as using explicit DataJoint blob serialization.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to migrate.</p> required <code>target_type</code> <code>str</code> <p>The type name to migrate to. Default \"blob\".</p> <code>'blob'</code> <code>dry_run</code> <code>bool</code> <p>If True, only return SQL without executing.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of SQL ALTER TABLE statements.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sql_statements = dj.migrate.generate_migration_sql(schema)\n&gt;&gt;&gt; for sql in sql_statements:\n...     print(sql)\n</code></pre> Notes <p>This is a metadata-only migration. The actual blob data format remains unchanged - only the column comments are updated to indicate explicit type handling.</p>"},{"location":"api/datajoint/migrate/#datajoint.migrate.migrate_blob_columns","title":"migrate_blob_columns","text":"<pre><code>migrate_blob_columns(schema, target_type='blob', dry_run=True)\n</code></pre> <p>Migrate blob columns in a schema to use explicit  type. <p>This updates column comments in the database to include the type declaration. The data format remains unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to migrate.</p> required <code>target_type</code> <code>str</code> <p>The type name to migrate to. Default \"blob\".</p> <code>'blob'</code> <code>dry_run</code> <code>bool</code> <p>If True, only preview changes without applying. Default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>analyzed: Number of blob columns analyzed</li> <li>needs_migration: Number of columns that need migration</li> <li>migrated: Number of columns migrated (0 if dry_run)</li> <li>sql_statements: List of SQL statements (executed or to be executed)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Preview migration\n&gt;&gt;&gt; result = dj.migrate.migrate_blob_columns(schema, dry_run=True)\n&gt;&gt;&gt; print(f\"Would migrate {result['needs_migration']} columns\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Apply migration\n&gt;&gt;&gt; result = dj.migrate.migrate_blob_columns(schema, dry_run=False)\n&gt;&gt;&gt; print(f\"Migrated {result['migrated']} columns\")\n</code></pre> Warnings <p>After migration, table definitions should be updated to use <code>&lt;blob&gt;</code> instead of <code>longblob</code> for consistency. The migration only updates database metadata; source code changes are manual.</p>"},{"location":"api/datajoint/migrate/#datajoint.migrate.check_migration_status","title":"check_migration_status","text":"<pre><code>check_migration_status(schema)\n</code></pre> <p>Check the migration status of blob columns in a schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to check.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>total_blob_columns: Total number of blob columns</li> <li>migrated: Number of columns with explicit type</li> <li>pending: Number of columns using implicit serialization</li> <li>columns: List of column details</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; status = dj.migrate.check_migration_status(schema)\n&gt;&gt;&gt; print(f\"Migration progress: {status['migrated']}/{status['total_blob_columns']}\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.add_job_metadata_columns","title":"add_job_metadata_columns","text":"<pre><code>add_job_metadata_columns(target, dry_run=True)\n</code></pre> <p>Add hidden job metadata columns to existing Computed/Imported tables.</p> <p>This migration utility adds the hidden columns (_job_start_time, _job_duration, _job_version) to tables that were created before config.jobs.add_job_metadata was enabled.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Table or Schema</code> <p>Either a table class/instance (dj.Computed or dj.Imported) or a Schema object. If a Schema, all Computed/Imported tables in the schema will be processed.</p> required <code>dry_run</code> <code>bool</code> <p>If True, only preview changes without applying. Default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>tables_analyzed: Number of tables checked</li> <li>tables_modified: Number of tables that were/would be modified</li> <li>columns_added: Total columns added across all tables</li> <li>details: List of dicts with per-table information</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import datajoint as dj\n&gt;&gt;&gt; from datajoint.migrate import add_job_metadata_columns\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Preview migration for a single table\n&gt;&gt;&gt; result = add_job_metadata_columns(MyComputedTable, dry_run=True)\n&gt;&gt;&gt; print(f\"Would add {result['columns_added']} columns\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Apply migration to all tables in a schema\n&gt;&gt;&gt; result = add_job_metadata_columns(schema, dry_run=False)\n&gt;&gt;&gt; print(f\"Modified {result['tables_modified']} tables\")\n</code></pre> Notes <ul> <li>Only Computed and Imported tables are modified (not Manual, Lookup, or Part)</li> <li>Existing rows will have NULL values for _job_start_time and _job_duration</li> <li>Future populate() calls will fill in metadata for new rows</li> <li>This does NOT retroactively populate metadata for existing rows</li> </ul>"},{"location":"api/datajoint/migrate/#datajoint.migrate.migrate_external","title":"migrate_external","text":"<pre><code>migrate_external(schema, dry_run=True, finalize=False)\n</code></pre> <p>Migrate external storage columns from 0.x to 2.0 format.</p> <p>This migration uses a safe, multi-step approach:</p> <ol> <li> <p>Initial run (dry_run=False): Adds new <code>&lt;column&gt;_v2</code> columns with JSON    type and copies data from the old columns, converting UUID references to    JSON metadata.</p> </li> <li> <p>Verification: You verify all data is accessible via DataJoint 2.0.</p> </li> <li> <p>Finalize (finalize=True): Renames columns (old \u2192 <code>_v1</code>, new \u2192 original    name) and optionally drops the old columns.</p> </li> </ol> <p>This allows 0.x and 2.0 to coexist during migration and provides a rollback path if issues are discovered.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to migrate.</p> required <code>dry_run</code> <code>bool</code> <p>If True, only preview changes without applying. Default True.</p> <code>True</code> <code>finalize</code> <code>bool</code> <p>If True, rename migrated columns to original names and drop old columns. Only run after verifying migration succeeded. Default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Migration results with keys:</p> <ul> <li>columns_found: Number of external columns found</li> <li>columns_migrated: Number of columns processed</li> <li>rows_migrated: Number of rows with data converted</li> <li>details: Per-column migration details</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datajoint.migration import migrate_external\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Step 1: Preview\n&gt;&gt;&gt; result = migrate_external(schema, dry_run=True)\n&gt;&gt;&gt; print(f\"Found {result['columns_found']} columns to migrate\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Step 2: Run migration (adds new columns)\n&gt;&gt;&gt; result = migrate_external(schema, dry_run=False)\n&gt;&gt;&gt; print(f\"Migrated {result['rows_migrated']} rows\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Step 3: Verify data is accessible via DataJoint 2.0\n&gt;&gt;&gt; # ... manual verification ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Step 4: Finalize (rename columns, drop old)\n&gt;&gt;&gt; result = migrate_external(schema, finalize=True)\n</code></pre> Notes <p>The migration reads from the hidden <code>~external_&lt;store&gt;</code> tables to build JSON metadata. Ensure store configuration in datajoint.json matches the paths stored in these tables.</p>"},{"location":"api/datajoint/migrate/#datajoint.migrate.check_store_configuration","title":"check_store_configuration","text":"<pre><code>check_store_configuration(schema)\n</code></pre> <p>Verify external stores are properly configured.</p> <p>Checks that all external storage stores referenced in the schema's tables are configured in settings and accessible.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to check.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>stores_configured: list of store names with valid config</li> <li>stores_missing: list of stores referenced but not configured</li> <li>stores_unreachable: list of stores that failed connection test</li> <li>details: per-store details</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datajoint.migrate import check_store_configuration\n&gt;&gt;&gt; result = check_store_configuration(schema)\n&gt;&gt;&gt; if result['stores_missing']:\n...     print(f\"Missing stores: {result['stores_missing']}\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.verify_external_integrity","title":"verify_external_integrity","text":"<pre><code>verify_external_integrity(schema, store_name=None)\n</code></pre> <p>Check that all external references point to existing files.</p> <p>Verifies integrity of external storage by checking that each reference in the ~external_* tables points to an accessible file.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to check.</p> required <code>store_name</code> <code>str</code> <p>Specific store to check. If None, checks all stores.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>total_references: count of external entries</li> <li>valid: count with accessible files</li> <li>missing: list of entries with inaccessible files</li> <li>stores_checked: list of store names checked</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datajoint.migrate import verify_external_integrity\n&gt;&gt;&gt; result = verify_external_integrity(schema)\n&gt;&gt;&gt; if result['missing']:\n...     print(f\"Missing files: {len(result['missing'])}\")\n...     for entry in result['missing'][:5]:\n...         print(f\"  {entry['filepath']}\")\n</code></pre> Notes <p>For S3/MinIO stores, this function does not verify file existence (would require network calls). Only local file stores are fully verified.</p>"},{"location":"api/datajoint/migrate/#datajoint.migrate.rebuild_lineage","title":"rebuild_lineage","text":"<pre><code>rebuild_lineage(schema, dry_run=True)\n</code></pre> <p>Rebuild ~lineage table from current table definitions.</p> <p>Use after schema changes or to repair corrupted lineage data. The lineage table tracks foreign key relationships for semantic matching.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to rebuild lineage for.</p> required <code>dry_run</code> <code>bool</code> <p>If True, only preview changes without applying. Default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <ul> <li>tables_analyzed: number of tables in schema</li> <li>lineage_entries: number of lineage entries created</li> <li>status: 'dry_run', 'rebuilt', or 'error'</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datajoint.migrate import rebuild_lineage\n&gt;&gt;&gt; result = rebuild_lineage(schema, dry_run=True)\n&gt;&gt;&gt; print(f\"Would create {result['lineage_entries']} lineage entries\")\n&gt;&gt;&gt; result = rebuild_lineage(schema, dry_run=False)\n&gt;&gt;&gt; print(f\"Rebuilt lineage: {result['status']}\")\n</code></pre> Notes <p>This function wraps schema.rebuild_lineage() with dry_run support and additional reporting.</p>"},{"location":"api/datajoint/migrate/#datajoint.migrate.migrate_filepath","title":"migrate_filepath","text":"<pre><code>migrate_filepath(schema, dry_run=True, finalize=False)\n</code></pre> <p>Migrate filepath columns from 0.x to 2.0 format.</p> <p>Same multi-step approach as migrate_external:</p> <ol> <li>Initial run: Adds new <code>&lt;column&gt;_v2</code> columns with JSON type</li> <li>Verification: Verify files accessible via DataJoint 2.0</li> <li>Finalize: Rename columns and drop old</li> </ol> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The DataJoint schema to migrate.</p> required <code>dry_run</code> <code>bool</code> <p>If True, only preview changes. Default True.</p> <code>True</code> <code>finalize</code> <code>bool</code> <p>If True, finalize migration. Default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Migration results (same format as migrate_external).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datajoint.migration import migrate_filepath\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Preview\n&gt;&gt;&gt; result = migrate_filepath(schema, dry_run=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Run migration\n&gt;&gt;&gt; result = migrate_filepath(schema, dry_run=False)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Finalize after verification\n&gt;&gt;&gt; result = migrate_filepath(schema, finalize=True)\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.create_parallel_schema","title":"create_parallel_schema","text":"<pre><code>create_parallel_schema(source, dest, copy_data=False, connection=None)\n</code></pre> <p>Create a parallel _v20 schema for migration testing.</p> <p>This creates a copy of a production schema (source) into a test schema (dest) for safely testing DataJoint 2.0 migration without affecting production.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Production schema name (e.g., 'my_pipeline')</p> required <code>dest</code> <code>str</code> <p>Test schema name (e.g., 'my_pipeline_v20')</p> required <code>copy_data</code> <code>bool</code> <p>If True, copy all table data. If False (default), create empty tables.</p> <code>False</code> <code>connection</code> <code>Connection</code> <p>Database connection. If None, uses default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <ul> <li>tables_created: int - number of tables created</li> <li>data_copied: bool - whether data was copied</li> <li>tables: list - list of table names created</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datajoint.migrate import create_parallel_schema\n&gt;&gt;&gt; result = create_parallel_schema('my_pipeline', 'my_pipeline_v20')\n&gt;&gt;&gt; print(f\"Created {result['tables_created']} tables\")\n</code></pre> See Also <p>copy_table_data : Copy data between schemas</p>"},{"location":"api/datajoint/migrate/#datajoint.migrate.copy_table_data","title":"copy_table_data","text":"<pre><code>copy_table_data(source_schema, dest_schema, table, limit=None, where_clause=None, connection=None)\n</code></pre> <p>Copy data from production table to test table.</p> <p>Parameters:</p> Name Type Description Default <code>source_schema</code> <code>str</code> <p>Production schema name</p> required <code>dest_schema</code> <code>str</code> <p>Test schema name (_v20)</p> required <code>table</code> <code>str</code> <p>Table name</p> required <code>limit</code> <code>int</code> <p>Maximum number of rows to copy</p> <code>None</code> <code>where_clause</code> <code>str</code> <p>SQL WHERE clause for filtering (without 'WHERE' keyword)</p> <code>None</code> <code>connection</code> <code>Connection</code> <p>Database connection. If None, uses default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <ul> <li>rows_copied: int - number of rows copied</li> <li>time_taken: float - seconds elapsed</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Copy all data\n&gt;&gt;&gt; result = copy_table_data('my_pipeline', 'my_pipeline_v20', 'Mouse')\n</code></pre> <pre><code>&gt;&gt;&gt; # Copy sample\n&gt;&gt;&gt; result = copy_table_data(\n...     'my_pipeline', 'my_pipeline_v20', 'Session',\n...     limit=100,\n...     where_clause=\"session_date &gt;= '2024-01-01'\"\n... )\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.compare_query_results","title":"compare_query_results","text":"<pre><code>compare_query_results(prod_schema, test_schema, table, tolerance=1e-06, connection=None)\n</code></pre> <p>Compare query results between production and test schemas.</p> <p>Parameters:</p> Name Type Description Default <code>prod_schema</code> <code>str</code> <p>Production schema name</p> required <code>test_schema</code> <code>str</code> <p>Test schema name (_v20)</p> required <code>table</code> <code>str</code> <p>Table name to compare</p> required <code>tolerance</code> <code>float</code> <p>Tolerance for floating-point comparison. Default 1e-6.</p> <code>1e-06</code> <code>connection</code> <code>Connection</code> <p>Database connection. If None, uses default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <ul> <li>match: bool - whether all rows match</li> <li>row_count: int - number of rows compared</li> <li>discrepancies: list - list of mismatches (if any)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = compare_query_results('my_pipeline', 'my_pipeline_v20', 'neuron')\n&gt;&gt;&gt; if result['match']:\n...     print(f\"\u2713 All {result['row_count']} rows match\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.backup_schema","title":"backup_schema","text":"<pre><code>backup_schema(schema, backup_name, connection=None)\n</code></pre> <p>Create full backup of a schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name to backup</p> required <code>backup_name</code> <code>str</code> <p>Backup schema name (e.g., 'my_pipeline_backup_20250114')</p> required <code>connection</code> <code>Connection</code> <p>Database connection. If None, uses default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <ul> <li>tables_backed_up: int</li> <li>rows_backed_up: int</li> <li>backup_location: str</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = backup_schema('my_pipeline', 'my_pipeline_backup_20250114')\n&gt;&gt;&gt; print(f\"Backed up {result['tables_backed_up']} tables\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.restore_schema","title":"restore_schema","text":"<pre><code>restore_schema(backup, dest, connection=None)\n</code></pre> <p>Restore schema from backup.</p> <p>Parameters:</p> Name Type Description Default <code>backup</code> <code>str</code> <p>Backup schema name</p> required <code>dest</code> <code>str</code> <p>Destination schema name</p> required <code>connection</code> <code>Connection</code> <p>Database connection. If None, uses default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <ul> <li>tables_restored: int</li> <li>rows_restored: int</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; restore_schema('my_pipeline_backup_20250114', 'my_pipeline')\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.verify_schema_v20","title":"verify_schema_v20","text":"<pre><code>verify_schema_v20(schema, connection=None)\n</code></pre> <p>Verify schema is fully migrated to DataJoint 2.0.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name to verify</p> required <code>connection</code> <code>Connection</code> <p>Database connection. If None, uses default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <ul> <li>compatible: bool - True if fully compatible with 2.0</li> <li>blob_markers: bool - All blob columns have :: markers <li>lineage_exists: bool - ~lineage table exists</li> <li>issues: list - List of compatibility issues found</li> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = verify_schema_v20('my_pipeline')\n&gt;&gt;&gt; if result['compatible']:\n...     print(\"\u2713 Schema fully migrated to 2.0\")\n</code></pre>"},{"location":"api/datajoint/migrate/#datajoint.migrate.migrate_external_pointers_v2","title":"migrate_external_pointers_v2","text":"<pre><code>migrate_external_pointers_v2(schema, table, attribute, source_store, dest_store, copy_files=False, connection=None)\n</code></pre> <p>Migrate external storage pointers from 0.14.6 to 2.0 format.</p> <p>Converts BINARY(16) UUID references to JSON metadata format. Optionally copies blob files to new storage location.</p> <p>This is useful when copying production data to _v2 schemas and you need to access external storage attributes but don't want to move the files yet.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Schema name (e.g., 'my_pipeline_v2')</p> required <code>table</code> <code>str</code> <p>Table name</p> required <code>attribute</code> <code>str</code> <p>External attribute name (e.g., 'signal')</p> required <code>source_store</code> <code>str</code> <p>0.14.6 store name (e.g., 'external-raw')</p> required <code>dest_store</code> <code>str</code> <p>2.0 store name (e.g., 'raw')</p> required <code>copy_files</code> <code>bool</code> <p>If True, copy blob files to new location. If False (default), JSON points to existing files.</p> <code>False</code> <code>connection</code> <code>Connection</code> <p>Database connection. If None, uses default connection.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <ul> <li>rows_migrated: int - number of pointers migrated</li> <li>files_copied: int - number of files copied (if copy_files=True)</li> <li>errors: list - any errors encountered</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Migrate pointers without moving files\n&gt;&gt;&gt; result = migrate_external_pointers_v2(\n...     schema='my_pipeline_v2',\n...     table='recording',\n...     attribute='signal',\n...     source_store='external-raw',\n...     dest_store='raw',\n...     copy_files=False\n... )\n&gt;&gt;&gt; print(f\"Migrated {result['rows_migrated']} pointers\")\n</code></pre> Notes <p>This function: 1. Reads BINARY(16) UUID from table column 2. Looks up file in ~external_{source_store} table 3. Creates JSON metadata with file path 4. Optionally copies file to new store location 5. Updates column with JSON metadata</p> <p>The JSON format is: {   \"path\": \"schema/table/key_hash/file.ext\",   \"size\": 12345,   \"hash\": null,   \"ext\": \".dat\",   \"is_dir\": false,   \"timestamp\": \"2025-01-14T10:30:00+00:00\" }</p>"},{"location":"api/datajoint/schemas/","title":"Schema","text":"<p>Schema and VirtualModule classes</p> <p>Schema management for DataJoint.</p> <p>This module provides the Schema class for binding Python table classes to database schemas, and utilities for schema introspection and management.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.ordered_dir","title":"ordered_dir","text":"<pre><code>ordered_dir(class_)\n</code></pre> <p>List class attributes respecting declaration order.</p> <p>Similar to the <code>dir()</code> built-in, but preserves attribute declaration order as much as possible.</p> <p>Parameters:</p> Name Type Description Default <code>class_</code> <code>type</code> <p>Class to list members for.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Attributes declared in class_ and its superclasses.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema","title":"Schema","text":"<p>Decorator that binds table classes to a database schema.</p> <p>Schema objects associate Python table classes with database schemas and provide the namespace context for foreign key resolution.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Database schema name. If omitted, call <code>activate()</code> later.</p> <code>None</code> <code>context</code> <code>dict</code> <p>Namespace for foreign key lookup. None uses caller's context.</p> <code>None</code> <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If False, raise error if schema doesn't exist. Default True.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>If False, raise error when accessing missing tables. Default from <code>dj.config.database.create_tables</code> (True unless configured).</p> <code>None</code> <code>add_objects</code> <code>dict</code> <p>Additional objects for the declaration context.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; schema = dj.Schema('my_schema')\n&gt;&gt;&gt; @schema\n... class Session(dj.Manual):\n...     definition = '''\n...     session_id : int\n...     '''\n</code></pre>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.is_activated","title":"is_activated","text":"<pre><code>is_activated()\n</code></pre> <p>Check if the schema has been activated.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.activate","title":"activate","text":"<pre><code>activate(schema_name=None, *, connection=None, create_schema=None, create_tables=None, add_objects=None)\n</code></pre> <p>Associate with a database schema.</p> <p>If the schema does not exist, attempts to create it on the server.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Database schema name. None asserts schema is already activated.</p> <code>None</code> <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If False, raise error if schema doesn't exist.</p> <code>None</code> <code>create_tables</code> <code>bool</code> <p>If False, raise error when accessing missing tables.</p> <code>None</code> <code>add_objects</code> <code>dict</code> <p>Additional objects for the declaration context.</p> <code>None</code> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If schema_name is None and schema not yet activated, or if schema already activated for a different database.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.size_on_disk","title":"size_on_disk  <code>property</code>","text":"<pre><code>size_on_disk\n</code></pre> <p>Return the total size of all tables in the schema.</p> <p>Returns:</p> Type Description <code>int</code> <p>Size in bytes (data + indices).</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.make_classes","title":"make_classes","text":"<pre><code>make_classes(into=None)\n</code></pre> <p>Create Python table classes for tables in the schema.</p> <p>Introspects the database schema and creates appropriate Python classes (Lookup, Manual, Imported, Computed, Part) for tables that don't have corresponding classes in the target namespace.</p> <p>Parameters:</p> Name Type Description Default <code>into</code> <code>dict</code> <p>Namespace to place created classes into. Defaults to caller's local namespace.</p> <code>None</code>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.drop","title":"drop","text":"<pre><code>drop(prompt=None)\n</code></pre> <p>Drop the associated schema and all its tables.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>bool</code> <p>If True, show confirmation prompt before dropping. If False, drop without confirmation. If None (default), use <code>dj.config['safemode']</code> setting.</p> <code>None</code> <p>Raises:</p> Type Description <code>AccessError</code> <p>If insufficient permissions to drop the schema.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.exists","title":"exists  <code>property</code>","text":"<pre><code>exists\n</code></pre> <p>Check if the associated schema exists on the server.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the schema exists.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If schema has not been activated.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.lineage_table_exists","title":"lineage_table_exists  <code>property</code>","text":"<pre><code>lineage_table_exists\n</code></pre> <p>Check if the ~lineage table exists in this schema.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the lineage table exists.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.lineage","title":"lineage  <code>property</code>","text":"<pre><code>lineage\n</code></pre> <p>Get all lineages for tables in this schema.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Mapping of <code>'schema.table.attribute'</code> to its lineage origin.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.rebuild_lineage","title":"rebuild_lineage","text":"<pre><code>rebuild_lineage()\n</code></pre> <p>Rebuild the ~lineage table for all tables in this schema.</p> <p>Recomputes lineage for all attributes by querying FK relationships from the information_schema. Use to restore lineage for schemas that predate the lineage system or after corruption.</p> Notes <p>After rebuilding, restart the Python kernel and reimport to pick up the new lineage information.</p> <p>Upstream schemas (referenced via cross-schema foreign keys) must have their lineage rebuilt first.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.jobs","title":"jobs  <code>property</code>","text":"<pre><code>jobs\n</code></pre> <p>Return Job objects for auto-populated tables with job tables.</p> <p>Only returns Job objects when both the target table and its <code>~~table_name</code> job table exist in the database. Job tables are created lazily on first access to <code>table.jobs</code> or <code>populate(reserve_jobs=True)</code>.</p> <p>Returns:</p> Type Description <code>list[Job]</code> <p>Job objects for existing job tables.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.save","title":"save","text":"<pre><code>save(python_filename=None)\n</code></pre> <p>Generate Python code that recreates this schema.</p> <p>Parameters:</p> Name Type Description Default <code>python_filename</code> <code>str</code> <p>If provided, write the code to this file.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Python module source code defining this schema.</p> Notes <p>This method is in preparation for a future release and is not officially supported.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.list_tables","title":"list_tables","text":"<pre><code>list_tables()\n</code></pre> <p>Return all user tables in the schema.</p> <p>Excludes hidden tables (starting with <code>~</code>) such as <code>~lineage</code> and job tables (<code>~~</code>).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Table names in topological order.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.Schema.get_table","title":"get_table","text":"<pre><code>get_table(name)\n</code></pre> <p>Get a table instance by name.</p> <p>Returns a FreeTable instance for the given table name. This is useful for accessing tables when you don't have the Python class available.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name (e.g., 'experiment', 'session__trial' for parts). Can be snake_case (SQL name) or CamelCase (class name). Tier prefixes are optional and will be auto-detected.</p> required <p>Returns:</p> Type Description <code>FreeTable</code> <p>A FreeTable instance for the table.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If the table does not exist.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; schema = dj.Schema('my_schema')\n&gt;&gt;&gt; experiment = schema.get_table('experiment')\n&gt;&gt;&gt; experiment.fetch()\n</code></pre>"},{"location":"api/datajoint/schemas/#datajoint.schemas.VirtualModule","title":"VirtualModule","text":"<p>               Bases: <code>ModuleType</code></p> <p>A virtual module representing a DataJoint schema from database tables.</p> <p>Creates a Python module with table classes automatically generated from the database schema. Useful for accessing schemas without Python source.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>Display name for the module.</p> required <code>schema_name</code> <code>str</code> <p>Database schema name.</p> required <code>create_schema</code> <code>bool</code> <p>If True, create the schema if it doesn't exist. Default False.</p> <code>False</code> <code>create_tables</code> <code>bool</code> <p>If True, allow declaring new tables. Default False.</p> <code>False</code> <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>add_objects</code> <code>dict</code> <p>Additional objects to add to the module namespace.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lab = dj.VirtualModule('lab', 'my_lab_schema')\n&gt;&gt;&gt; lab.Subject.fetch()\n</code></pre>"},{"location":"api/datajoint/schemas/#datajoint.schemas.list_schemas","title":"list_schemas","text":"<pre><code>list_schemas(connection=None)\n</code></pre> <p>List all accessible schemas on the server.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Names of all accessible schemas.</p>"},{"location":"api/datajoint/schemas/#datajoint.schemas.virtual_schema","title":"virtual_schema","text":"<pre><code>virtual_schema(schema_name, *, connection=None, create_schema=False, create_tables=False, add_objects=None)\n</code></pre> <p>Create a virtual module for an existing database schema.</p> <p>This is the recommended way to access database schemas when you don't have the Python source code that defined them. Returns a module-like object with table classes as attributes.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Database schema name.</p> required <code>connection</code> <code>Connection</code> <p>Database connection. Defaults to <code>dj.conn()</code>.</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>If True, create the schema if it doesn't exist. Default False.</p> <code>False</code> <code>create_tables</code> <code>bool</code> <p>If True, allow declaring new tables. Default False.</p> <code>False</code> <code>add_objects</code> <code>dict</code> <p>Additional objects to add to the module namespace.</p> <code>None</code> <p>Returns:</p> Type Description <code>VirtualModule</code> <p>A module-like object with table classes as attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lab = dj.virtual_schema('my_lab')\n&gt;&gt;&gt; lab.Subject.fetch()\n&gt;&gt;&gt; lab.Session &amp; \"subject_id='M001'\"\n</code></pre> See Also <p>Schema : For defining new schemas with Python classes. VirtualModule : The underlying class (prefer virtual_schema function).</p>"},{"location":"api/datajoint/settings/","title":"Settings","text":"<p>Configuration management</p> <p>DataJoint configuration system using pydantic-settings.</p> <p>This module provides strongly-typed configuration with automatic loading from environment variables, secrets directories, and JSON config files.</p> <p>Configuration sources (in priority order):</p> <ol> <li>Environment variables (<code>DJ_*</code>)</li> <li>Secrets directories (<code>.secrets/</code> in project, <code>/run/secrets/datajoint/</code>)</li> <li>Project config file (<code>datajoint.json</code>, searched recursively up to <code>.git/.hg</code>)</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import datajoint as dj\n&gt;&gt;&gt; dj.config.database.host\n'localhost'\n&gt;&gt;&gt; dj.config.database.backend\n'mysql'\n&gt;&gt;&gt; dj.config.database.port  # Auto-detects: 3306 for MySQL, 5432 for PostgreSQL\n3306\n&gt;&gt;&gt; with dj.config.override(safemode=False):\n...     # dangerous operations here\n...     pass\n</code></pre> <p>Project structure::</p> <pre><code>myproject/\n\u251c\u2500\u2500 .git/\n\u251c\u2500\u2500 datajoint.json      # Project config (commit this)\n\u251c\u2500\u2500 .secrets/           # Local secrets (gitignore this)\n\u2502   \u251c\u2500\u2500 database.password\n\u2502   \u2514\u2500\u2500 aws.secret_access_key\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 analysis.py     # Config found via parent search\n</code></pre>"},{"location":"api/datajoint/settings/#datajoint.settings.find_config_file","title":"find_config_file","text":"<pre><code>find_config_file(start=None)\n</code></pre> <p>Search for datajoint.json in current and parent directories.</p> <p>Searches upward from <code>start</code> until finding the config file or hitting a project boundary (<code>.git</code>, <code>.hg</code>) or filesystem root.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Path</code> <p>Directory to start search from. Defaults to current working directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path or None</code> <p>Path to config file if found, None otherwise.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.find_secrets_dir","title":"find_secrets_dir","text":"<pre><code>find_secrets_dir(config_path=None)\n</code></pre> <p>Find the secrets directory.</p> <p>Priority:</p> <ol> <li><code>.secrets/</code> in same directory as datajoint.json (project secrets)</li> <li><code>/run/secrets/datajoint/</code> (Docker/Kubernetes secrets)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Path</code> <p>Path to datajoint.json if found.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path or None</code> <p>Path to secrets directory if found, None otherwise.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.read_secret_file","title":"read_secret_file","text":"<pre><code>read_secret_file(secrets_dir, name)\n</code></pre> <p>Read a secret value from a file in the secrets directory.</p> <p>Parameters:</p> Name Type Description Default <code>secrets_dir</code> <code>Path or None</code> <p>Path to secrets directory.</p> required <code>name</code> <code>str</code> <p>Name of the secret file (e.g., <code>'database.password'</code>).</p> required <p>Returns:</p> Type Description <code>str or None</code> <p>Secret value as string, or None if not found.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.DatabaseSettings","title":"DatabaseSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Database connection settings.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.DatabaseSettings.set_default_port_from_backend","title":"set_default_port_from_backend","text":"<pre><code>set_default_port_from_backend()\n</code></pre> <p>Set default port based on backend if not explicitly provided.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.ConnectionSettings","title":"ConnectionSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Connection behavior settings.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.DisplaySettings","title":"DisplaySettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Display and preview settings.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.StoresSettings","title":"StoresSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Unified object storage configuration.</p> <p>Stores configuration supports both hash-addressed and schema-addressed storage using the same named stores with _hash and _schema sections.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.JobsSettings","title":"JobsSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Job queue configuration for AutoPopulate 2.0.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.Config","title":"Config","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Main DataJoint configuration.</p> <p>Settings are loaded from (in priority order):</p> <ol> <li>Environment variables (<code>DJ_*</code>)</li> <li>Secrets directory (<code>.secrets/</code> or <code>/run/secrets/datajoint/</code>)</li> <li>Config file (<code>datajoint.json</code>, searched in parent directories)</li> <li>Default values</li> </ol> <p>Examples:</p> <p>Access settings via attributes:</p> <pre><code>&gt;&gt;&gt; config.database.host\n&gt;&gt;&gt; config.safemode\n</code></pre> <p>Override temporarily with context manager:</p> <pre><code>&gt;&gt;&gt; with config.override(safemode=False):\n...     pass\n</code></pre>"},{"location":"api/datajoint/settings/#datajoint.settings.Config.set_logger_level","title":"set_logger_level  <code>classmethod</code>","text":"<pre><code>set_logger_level(v)\n</code></pre> <p>Update logger level when loglevel changes.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.Config.convert_path","title":"convert_path  <code>classmethod</code>","text":"<pre><code>convert_path(v)\n</code></pre> <p>Convert string paths to Path objects.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.Config.get_store_spec","title":"get_store_spec","text":"<pre><code>get_store_spec(store=None, *, use_filepath_default=False)\n</code></pre> <p>Get configuration for a storage store.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>str</code> <p>Name of the store to retrieve. If None, uses the appropriate default.</p> <code>None</code> <code>use_filepath_default</code> <code>bool</code> <p>If True and store is None, uses stores.filepath_default instead of stores.default. Use for filepath references which are not part of OAS. Default: False (use stores.default for integrated storage).</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Store configuration dict with validated fields.</p> <p>Raises:</p> Type Description <code>DataJointError</code> <p>If store is not configured or has invalid config.</p>"},{"location":"api/datajoint/settings/#datajoint.settings.Config.load","title":"load","text":"<pre><code>load(filename)\n</code></pre> <p>Load settings from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str or Path</code> <p>Path to load configuration from.</p> required"},{"location":"api/datajoint/settings/#datajoint.settings.Config.override","title":"override","text":"<pre><code>override(**kwargs)\n</code></pre> <p>Temporarily override configuration values.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Settings to override. Use double underscore for nested settings (e.g., <code>database__host=\"localhost\"</code>).</p> <code>{}</code> <p>Yields:</p> Type Description <code>Config</code> <p>The config instance with overridden values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with config.override(safemode=False, database__host=\"test\"):\n...     # config.safemode is False here\n...     pass\n&gt;&gt;&gt; # config.safemode is restored\n</code></pre>"},{"location":"api/datajoint/settings/#datajoint.settings.Config.save_template","title":"save_template  <code>staticmethod</code>","text":"<pre><code>save_template(path='datajoint.json', minimal=True, create_secrets_dir=True)\n</code></pre> <p>Create a template datajoint.json configuration file.</p> <p>Credentials should NOT be stored in datajoint.json. Instead, use either:</p> <ul> <li>Environment variables (<code>DJ_USER</code>, <code>DJ_PASS</code>, <code>DJ_HOST</code>, etc.)</li> <li>The <code>.secrets/</code> directory (created alongside datajoint.json)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Where to save the template. Default <code>'datajoint.json'</code>.</p> <code>'datajoint.json'</code> <code>minimal</code> <code>bool</code> <p>If True (default), create minimal template with just database settings. If False, create full template with all available settings.</p> <code>True</code> <code>create_secrets_dir</code> <code>bool</code> <p>If True (default), also create a <code>.secrets/</code> directory with template files for credentials.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>Absolute path to the created config file.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If config file already exists (won't overwrite).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import datajoint as dj\n&gt;&gt;&gt; dj.config.save_template()  # Creates minimal template + .secrets/\n&gt;&gt;&gt; dj.config.save_template(\"full-config.json\", minimal=False)\n</code></pre>"},{"location":"api/datajoint/settings/#datajoint.settings.Config.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>Get setting with optional default value.</p>"},{"location":"api/datajoint/table/","title":"Table","text":"<p>Base Table and FreeTable classes</p>"},{"location":"api/datajoint/table/#datajoint.table.ValidationResult","title":"ValidationResult  <code>dataclass</code>","text":"<p>Result of table.validate() call.</p> <p>Attributes:     is_valid: True if all rows passed validation     errors: List of (row_index, field_name, error_message) tuples     rows_checked: Number of rows that were validated</p>"},{"location":"api/datajoint/table/#datajoint.table.ValidationResult.raise_if_invalid","title":"raise_if_invalid","text":"<pre><code>raise_if_invalid()\n</code></pre> <p>Raise DataJointError if validation failed.</p>"},{"location":"api/datajoint/table/#datajoint.table.ValidationResult.summary","title":"summary","text":"<pre><code>summary()\n</code></pre> <p>Return formatted error summary.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table","title":"Table","text":"<p>               Bases: <code>QueryExpression</code></p> <p>Table is an abstract class that represents a table in the schema. It implements insert and delete methods and inherits query functionality. To make it a concrete class, override the abstract properties specifying the connection, table name, database, and definition.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.heading","title":"heading  <code>property</code>","text":"<pre><code>heading\n</code></pre> <p>Return the table's heading, or raise a helpful error if not configured.</p> <p>Overrides QueryExpression.heading to provide a clear error message when the table is not properly associated with an activated schema. For base tier classes (Lookup, Manual, etc.), returns None to support introspection (e.g., help()).</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.declare","title":"declare","text":"<pre><code>declare(context=None)\n</code></pre> <p>Declare the table in the schema based on self.definition.</p> <p>:param context: the context for foreign key resolution. If None, foreign keys are     not allowed.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.alter","title":"alter","text":"<pre><code>alter(prompt=True, context=None)\n</code></pre> <p>Alter the table definition from self.definition</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.from_clause","title":"from_clause","text":"<pre><code>from_clause()\n</code></pre> <p>:return: the FROM clause of SQL SELECT statements.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.get_select_fields","title":"get_select_fields","text":"<pre><code>get_select_fields(select_fields=None)\n</code></pre> <p>:return: the selected attributes from the SQL SELECT statement.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.parents","title":"parents","text":"<pre><code>parents(primary=None, as_objects=False, foreign_key_info=False)\n</code></pre> <p>:param primary: if None, then all parents are returned. If True, then only foreign keys composed of     primary key attributes are considered.  If False, return foreign keys including at least one     secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of parents as table names or table objects     with (optional) foreign key information.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.children","title":"children","text":"<pre><code>children(primary=None, as_objects=False, foreign_key_info=False)\n</code></pre> <p>:param primary: if None, then all children are returned. If True, then only foreign keys composed of     primary key attributes are considered.  If False, return foreign keys including at least one     secondary attribute. :param as_objects: if False, return table names. If True, return table objects. :param foreign_key_info: if True, each element in result also includes foreign key info. :return: list of children as table names or table objects     with (optional) foreign key information.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.descendants","title":"descendants","text":"<pre><code>descendants(as_objects=False)\n</code></pre> <p>:param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables descendants in topological order.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.ancestors","title":"ancestors","text":"<pre><code>ancestors(as_objects=False)\n</code></pre> <p>:param as_objects: False - a list of table names; True - a list of table objects. :return: list of tables ancestors in topological order.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.parts","title":"parts","text":"<pre><code>parts(as_objects=False)\n</code></pre> <p>return part tables either as entries in a dict with foreign key information or a list of objects</p> <p>:param as_objects: if False (default), the output is a dict describing the foreign keys. If True, return table objects.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.is_declared","title":"is_declared  <code>property</code>","text":"<pre><code>is_declared\n</code></pre> <p>:return: True is the table is declared in the schema.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.full_table_name","title":"full_table_name  <code>property</code>","text":"<pre><code>full_table_name\n</code></pre> <p>:return: full table name in the schema</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.adapter","title":"adapter  <code>property</code>","text":"<pre><code>adapter\n</code></pre> <p>Database adapter for backend-agnostic SQL generation.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.update1","title":"update1","text":"<pre><code>update1(row)\n</code></pre> <p><code>update1</code> updates one existing entry in the table. Caution: In DataJoint the primary modes for data manipulation is to <code>insert</code> and <code>delete</code> entire records since referential integrity works on the level of records, not fields. Therefore, updates are reserved for corrective operations outside of main workflow. Use UPDATE methods sparingly with full awareness of potential violations of assumptions.</p> <p>:param row: a <code>dict</code> containing the primary key values and the attributes to update.     Setting an attribute value to None will reset it to the default value (if any).</p> <p>The primary key attributes must always be provided.</p> <p>Examples:</p> <p>table.update1({'id': 1, 'value': 3})  # update value in record with id=1 table.update1({'id': 1, 'value': None})  # reset value to default</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.validate","title":"validate","text":"<pre><code>validate(rows, *, ignore_extra_fields=False)\n</code></pre> <p>Validate rows without inserting them.</p> <p>:param rows: Same format as insert() - iterable of dicts, tuples, numpy records,     or a pandas DataFrame. :param ignore_extra_fields: If True, ignore fields not in the table heading. :return: ValidationResult with is_valid, errors list, and rows_checked count.</p> <p>Validates:     - Field existence (all fields must be in table heading)     - Row format (correct number of attributes for positional inserts)     - Codec validation (type checking via codec.validate())     - NULL constraints (non-nullable fields must have values)     - Primary key completeness (all PK fields must be present)     - UUID format and JSON serializability</p> <p>Cannot validate (database-enforced):     - Foreign key constraints     - Unique constraints (other than PK)     - Custom MySQL constraints</p> <p>Example::</p> <pre><code>result = table.validate(rows)\nif result:\n    table.insert(rows)\nelse:\n    print(result.summary())\n</code></pre>"},{"location":"api/datajoint/table/#datajoint.table.Table.insert1","title":"insert1","text":"<pre><code>insert1(row, **kwargs)\n</code></pre> <p>Insert one data record into the table. For <code>kwargs</code>, see <code>insert()</code>.</p> <p>:param row: a numpy record, a dict-like object, or an ordered sequence to be inserted     as one row.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.staged_insert1","title":"staged_insert1  <code>property</code>","text":"<pre><code>staged_insert1\n</code></pre> <p>Context manager for staged insert with direct object storage writes.</p> <p>Use this for large objects like Zarr arrays where copying from local storage is inefficient. Allows writing directly to the destination storage before finalizing the database insert.</p> <p>Example:     with table.staged_insert1 as staged:         staged.rec['subject_id'] = 123         staged.rec['session_id'] = 45</p> <pre><code>    # Create object storage directly\n    z = zarr.open(staged.store('raw_data', '.zarr'), mode='w', shape=(1000, 1000))\n    z[:] = data\n\n    # Assign to record\n    staged.rec['raw_data'] = z\n\n# On successful exit: metadata computed, record inserted\n# On exception: storage cleaned up, no record inserted\n</code></pre> <p>Yields:     StagedInsert: Context for setting record values and getting storage handles</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.insert","title":"insert","text":"<pre><code>insert(rows, replace=False, skip_duplicates=False, ignore_extra_fields=False, allow_direct_insert=None, chunk_size=None)\n</code></pre> <p>Insert a collection of rows.</p> <p>:param rows: Either (a) an iterable where an element is a numpy record, a     dict-like object, a pandas.DataFrame, a polars.DataFrame, a pyarrow.Table,     a sequence, or a query expression with the same heading as self, or     (b) a pathlib.Path object specifying a path relative to the current     directory with a CSV file, the contents of which will be inserted. :param replace: If True, replaces the existing tuple. :param skip_duplicates: If True, silently skip duplicate inserts. :param ignore_extra_fields: If False, fields that are not in the heading raise error. :param allow_direct_insert: Only applies in auto-populated tables. If False (default),     insert may only be called from inside the make callback. :param chunk_size: If set, insert rows in batches of this size. Useful for very     large inserts to avoid memory issues. Each chunk is a separate transaction.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; Table.insert([\n&gt;&gt;&gt;     dict(subject_id=7, species=\"mouse\", date_of_birth=\"2014-09-01\"),\n&gt;&gt;&gt;     dict(subject_id=8, species=\"mouse\", date_of_birth=\"2014-09-02\")])\n\n# Large insert with chunking\n&gt;&gt;&gt; Table.insert(large_dataset, chunk_size=10000)\n</code></pre>"},{"location":"api/datajoint/table/#datajoint.table.Table.insert_dataframe","title":"insert_dataframe","text":"<pre><code>insert_dataframe(df, index_as_pk=None, **insert_kwargs)\n</code></pre> <p>Insert DataFrame with explicit index handling.</p> <p>This method provides symmetry with to_pandas(): data fetched with to_pandas() (which sets primary key as index) can be modified and re-inserted using insert_dataframe() without manual index manipulation.</p> <p>:param df: pandas DataFrame to insert :param index_as_pk: How to handle DataFrame index:     - None (default): Auto-detect. Use index as primary key if index names       match primary_key columns. Drop if unnamed RangeIndex.     - True: Treat index as primary key columns. Raises if index names don't       match table primary key.     - False: Ignore index entirely (drop it). :param **insert_kwargs: Passed to insert() - replace, skip_duplicates,     ignore_extra_fields, allow_direct_insert, chunk_size</p> <p>Example::</p> <pre><code># Round-trip with to_pandas()\ndf = table.to_pandas()           # PK becomes index\ndf['value'] = df['value'] * 2    # Modify data\ntable.insert_dataframe(df)       # Auto-detects index as PK\n\n# Explicit control\ntable.insert_dataframe(df, index_as_pk=True)   # Use index\ntable.insert_dataframe(df, index_as_pk=False)  # Ignore index\n</code></pre>"},{"location":"api/datajoint/table/#datajoint.table.Table.delete_quick","title":"delete_quick","text":"<pre><code>delete_quick(get_count=False)\n</code></pre> <p>Deletes the table without cascading and without user prompt. If this table has populated dependent tables, this will fail.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.delete","title":"delete","text":"<pre><code>delete(transaction=True, prompt=None, part_integrity='enforce')\n</code></pre> <p>Deletes the contents of the table and its dependent tables, recursively.</p> <p>Args:     transaction: If <code>True</code>, use of the entire delete becomes an atomic transaction.         This is the default and recommended behavior. Set to <code>False</code> if this delete is         nested within another transaction.     prompt: If <code>True</code>, show what will be deleted and ask for confirmation.         If <code>False</code>, delete without confirmation. Default is <code>dj.config['safemode']</code>.     part_integrity: Policy for master-part integrity. One of:         - <code>\"enforce\"</code> (default): Error if parts would be deleted without masters.         - <code>\"ignore\"</code>: Allow deleting parts without masters (breaks integrity).         - <code>\"cascade\"</code>: Also delete masters when parts are deleted (maintains integrity).</p> <p>Returns:     Number of deleted rows (excluding those from dependent tables).</p> <p>Raises:     DataJointError: Delete exceeds maximum number of delete attempts.     DataJointError: When deleting within an existing transaction.     DataJointError: Deleting a part table before its master (when part_integrity=\"enforce\").     ValueError: Invalid part_integrity value.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.drop_quick","title":"drop_quick","text":"<pre><code>drop_quick()\n</code></pre> <p>Drops the table without cascading to dependent tables and without user prompt.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.drop","title":"drop","text":"<pre><code>drop(prompt=None)\n</code></pre> <p>Drop the table and all tables that reference it, recursively.</p> <p>Args:     prompt: If <code>True</code>, show what will be dropped and ask for confirmation.         If <code>False</code>, drop without confirmation. Default is <code>dj.config['safemode']</code>.</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.size_on_disk","title":"size_on_disk  <code>property</code>","text":"<pre><code>size_on_disk\n</code></pre> <p>:return: size of data and indices in bytes on the storage device</p>"},{"location":"api/datajoint/table/#datajoint.table.Table.describe","title":"describe","text":"<pre><code>describe(context=None, printout=False)\n</code></pre> <p>:return:  the definition string for the query using DataJoint DDL.</p>"},{"location":"api/datajoint/table/#datajoint.table.lookup_class_name","title":"lookup_class_name","text":"<pre><code>lookup_class_name(name, context, depth=3)\n</code></pre> <p>given a table name in the form <code>schema_name</code>.<code>table_name</code>, find its class in the context.</p> <p>:param name: <code>schema_name</code>.<code>table_name</code> :param context: dictionary representing the namespace :param depth: search depth into imported modules, helps avoid infinite recursion. :return: class name found in the context or None if not found</p>"},{"location":"api/datajoint/table/#datajoint.table.FreeTable","title":"FreeTable","text":"<p>               Bases: <code>Table</code></p> <p>A base table without a dedicated class. Each instance is associated with a table specified by full_table_name.</p> <p>:param conn:  a dj.Connection object :param full_table_name: in format <code>database</code>.<code>table_name</code></p>"},{"location":"api/datajoint/user_tables/","title":"Table Types","text":"<p>Manual, Lookup, Imported, Computed, Part</p> <p>Hosts the table tiers, user tables should be derived from.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.TableMeta","title":"TableMeta","text":"<p>               Bases: <code>type</code></p> <p>TableMeta subclasses allow applying some instance methods and properties directly at class level. For example, this allows Table.to_dicts() instead of Table().to_dicts().</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.TableMeta.connection","title":"connection  <code>property</code>","text":"<pre><code>connection\n</code></pre> <p>The database connection for this table.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.TableMeta.table_name","title":"table_name  <code>property</code>","text":"<pre><code>table_name\n</code></pre> <p>The table name formatted for MySQL.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.TableMeta.full_table_name","title":"full_table_name  <code>property</code>","text":"<pre><code>full_table_name\n</code></pre> <p>The fully qualified table name (quoted per backend).</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.UserTable","title":"UserTable","text":"<p>               Bases: <code>Table</code></p> <p>A subclass of UserTable is a dedicated class interfacing a base table. UserTable is initialized by the decorator generated by schema().</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.UserTable.definition","title":"definition  <code>property</code>","text":"<pre><code>definition\n</code></pre> <p>:return: a string containing the table definition using the DataJoint DDL.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.Manual","title":"Manual","text":"<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are entered manually.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.Lookup","title":"Lookup","text":"<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are for lookup. This is currently equivalent to defining the table as Manual and serves semantic purposes only.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.Imported","title":"Imported","text":"<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are imported from external data sources. The inherited class must at least provide the function <code>_make_tuples</code>.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.Computed","title":"Computed","text":"<p>               Bases: <code>UserTable</code>, <code>AutoPopulate</code></p> <p>Inherit from this class if the table's values are computed from other tables in the schema. The inherited class must at least provide the function <code>_make_tuples</code>.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.PartMeta","title":"PartMeta","text":"<p>               Bases: <code>TableMeta</code></p> <p>Metaclass for Part tables with overridden class properties.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.PartMeta.table_name","title":"table_name  <code>property</code>","text":"<pre><code>table_name\n</code></pre> <p>The table name for a Part is derived from its master table.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.PartMeta.full_table_name","title":"full_table_name  <code>property</code>","text":"<pre><code>full_table_name\n</code></pre> <p>The fully qualified table name (quoted per backend).</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.PartMeta.master","title":"master  <code>property</code>","text":"<pre><code>master\n</code></pre> <p>The master table for this Part table.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.Part","title":"Part","text":"<p>               Bases: <code>UserTable</code></p> <p>Inherit from this class if the table's values are details of an entry in another table and if this table is populated by the other table. For example, the entries inheriting from dj.Part could be single entries of a matrix, while the parent table refers to the entire matrix. Part tables are implemented as classes inside classes.</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.Part.delete","title":"delete","text":"<pre><code>delete(part_integrity='enforce', **kwargs)\n</code></pre> <p>Delete from a Part table.</p> <p>Args:     part_integrity: Policy for master-part integrity. One of:         - <code>\"enforce\"</code> (default): Error - delete from master instead.         - <code>\"ignore\"</code>: Allow direct deletion (breaks master-part integrity).         - <code>\"cascade\"</code>: Delete parts AND cascade up to delete master.     **kwargs: Additional arguments passed to Table.delete()               (transaction, prompt)</p> <p>Raises:     DataJointError: If part_integrity=\"enforce\" (direct Part deletes prohibited)</p>"},{"location":"api/datajoint/user_tables/#datajoint.user_tables.Part.drop","title":"drop","text":"<pre><code>drop(part_integrity='enforce')\n</code></pre> <p>Drop a Part table.</p> <p>Args:     part_integrity: Policy for master-part integrity. One of:         - <code>\"enforce\"</code> (default): Error - drop master instead.         - <code>\"ignore\"</code>: Allow direct drop (breaks master-part structure).         Note: <code>\"cascade\"</code> is not supported for drop (too destructive).</p> <p>Raises:     DataJointError: If part_integrity=\"enforce\" (direct Part drops prohibited)</p>"},{"location":"elements/","title":"DataJoint Elements","text":"<p>DataJoint Elements are curated data pipeline modules for neurophysiology experiments. Each Element implements common patterns for specific data modalities and integrates seamlessly with other Elements and custom DataJoint pipelines.</p> <p>For comprehensive documentation, tutorials, and API reference for each Element, visit the Element's repository.</p>"},{"location":"elements/#background","title":"Background","text":"<p>DataJoint Elements was developed as part of an NIH BRAIN Initiative project to disseminate open-source data management tools for neuroscience research.</p> <ul> <li>Grant: U24 NS116470 \u2014 DataJoint Pipelines for Neurophysiology</li> <li>Institute: National Institute of Neurological Disorders and Stroke (NINDS)</li> <li>Period: September 2020 -- August 2025</li> <li>PI: Dimitri Yatsenko, DataJoint</li> </ul> <p>The project compiled and systematized data pipeline designs from leading neuroscience laboratories into a library of reusable modules. These modules automate data ingestion and processing for common experimental modalities including extracellular electrophysiology, calcium imaging, pose estimation, and optogenetics.</p> <p>DataJoint Elements is listed in the BRAIN Initiative Alliance Resource Catalog (SciCrunch ID: SCR_021894).</p>"},{"location":"elements/#neurophysiology","title":"Neurophysiology","text":"<ul> <li>Element Calcium Imaging <p>Two-photon and widefield calcium imaging analysis with Suite2p, CaImAn, and EXTRACT.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Array Electrophysiology <p>High-density probe recordings (Neuropixels) with Kilosort and spike sorting.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Miniscope <p>Miniscope calcium imaging with UCLA Miniscope and Inscopix systems.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Electrode Localization <p>Anatomical localization of Neuropixels probe electrodes.</p> <p> Documentation  Repository</p> </li> </ul>"},{"location":"elements/#behavior","title":"Behavior","text":"<ul> <li>Element DeepLabCut <p>Markerless pose estimation with DeepLabCut.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Facemap <p>Orofacial behavior tracking with Facemap.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element MoSeq <p>Behavioral syllable analysis with Keypoint-MoSeq.</p> <p> Documentation  Repository</p> </li> </ul>"},{"location":"elements/#stimulation-imaging","title":"Stimulation &amp; Imaging","text":"<ul> <li>Element Optogenetics <p>Optogenetic stimulation experiments.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Visual Stimulus <p>Visual stimulation with Psychtoolbox.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element ZStack <p>Volumetric microscopy with Cellpose segmentation and BossDB integration.</p> <p> Documentation  Repository</p> </li> </ul>"},{"location":"elements/#core-elements","title":"Core Elements","text":"<ul> <li>Element Lab <p>Lab, project, and protocol management.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Animal <p>Subject and genotype management.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Session <p>Experimental session management.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Event <p>Event- and trial-based experiment structure.</p> <p> Documentation  Repository</p> </li> </ul> <ul> <li>Element Interface <p>Common utilities for DataJoint Elements.</p> <p> Documentation  Repository</p> </li> </ul>"},{"location":"elements/#citation","title":"Citation","text":"<p>If you use DataJoint Elements in your research, please cite:</p> <p>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021.</p>"},{"location":"explanation/","title":"Concepts","text":"<p>Understanding the principles behind DataJoint.</p> <p>DataJoint implements the Relational Workflow Model\u2014a paradigm that extends relational databases with native support for computational workflows. This section explains the core concepts that make DataJoint pipelines reliable, reproducible, and scalable.</p>"},{"location":"explanation/#core-concepts","title":"Core Concepts","text":"<ul> <li> Relational Workflow Model<p>How DataJoint differs from traditional databases. The paradigm shift from   storage to workflow.</p> </li> </ul> <ul> <li> Entity Integrity<p>Primary keys and the three questions. Ensuring one-to-one correspondence   between entities and records.</p> </li> </ul> <ul> <li> Normalization<p>Schema design principles. Organizing tables around workflow steps to   minimize redundancy.</p> </li> </ul> <ul> <li> Query Algebra<p>The five operators: restriction, join, projection, aggregation, union.   Workflow-aware query semantics.</p> </li> </ul> <ul> <li> Type System<p>Three-layer architecture: native, core, and codec types. In-table and   in-store storage modes.</p> </li> </ul> <ul> <li> Computation Model<p>AutoPopulate and Jobs 2.0. Automated, reproducible, distributed computation.</p> </li> </ul> <ul> <li> Custom Codecs<p>Extend DataJoint with domain-specific types. The codec extensibility system.</p> </li> </ul> <ul> <li> Data Pipelines<p>From workflows to complete data operations systems. Project structure and   object-augmented schemas.</p> </li> </ul> <ul> <li> Semantic Matching<p>How DataJoint ensures safe joins through attribute lineage tracking.</p> </li> </ul> <ul> <li> What's New in 2.0<p>Major changes, new features, and migration guidance for DataJoint 2.0.</p> </li> </ul> <ul> <li> FAQ<p>How DataJoint compares to ORMs, workflow managers, and lakehouses.   Common questions answered.</p> </li> </ul>"},{"location":"explanation/#why-these-concepts-matter","title":"Why These Concepts Matter","text":"<p>Traditional databases store data. DataJoint pipelines process data. Understanding the Relational Workflow Model helps you:</p> <ul> <li>Design schemas that naturally express your workflow</li> <li>Write queries that are both powerful and intuitive</li> <li>Build computations that scale from laptop to cluster</li> <li>Maintain data integrity throughout the pipeline lifecycle</li> </ul>"},{"location":"explanation/computation-model/","title":"Computation Model","text":"<p>DataJoint's computation model enables automated, reproducible data processing through the <code>populate()</code> mechanism and Jobs 2.0 system.</p>"},{"location":"explanation/computation-model/#autopopulate-the-core-concept","title":"AutoPopulate: The Core Concept","text":"<p>Tables that inherit from <code>dj.Imported</code> or <code>dj.Computed</code> can automatically populate themselves based on upstream data.</p> <pre><code>@schema\nclass Segmentation(dj.Computed):\n    definition = \"\"\"\n    -&gt; Scan\n    ---\n    num_cells : int64\n    cell_masks : &lt;blob@&gt;\n    \"\"\"\n\n    def make(self, key):\n        # key contains primary key of one Scan\n        scan_data = (Scan &amp; key).fetch1('image_data')\n\n        # Your computation\n        masks, num_cells = segment_cells(scan_data)\n\n        # Insert result\n        self.insert1({\n            **key,\n            'num_cells': num_cells,\n            'cell_masks': masks\n        })\n</code></pre>"},{"location":"explanation/computation-model/#the-make-contract","title":"The <code>make()</code> Contract","text":"<p>The <code>make(self, key)</code> method:</p> <ol> <li>Receives the primary key of one upstream entity</li> <li>Computes results for that entity</li> <li>Inserts results into the table</li> </ol> <p>DataJoint guarantees:</p> <ul> <li><code>make()</code> is called once per upstream entity</li> <li>Failed computations can be retried</li> <li>Parallel execution is safe</li> </ul>"},{"location":"explanation/computation-model/#key-source","title":"Key Source","text":"<p>The key source determines what needs to be computed:</p> <pre><code># Default: all upstream keys not yet in this table\nkey_source = Scan - Segmentation\n\n# Custom key source\n@property\ndef key_source(self):\n    return (Scan &amp; 'quality &gt; 0.8') - self\n</code></pre>"},{"location":"explanation/computation-model/#calling-populate","title":"Calling <code>populate()</code>","text":"<pre><code># Populate all missing entries\nSegmentation.populate()\n\n# Populate specific subset\nSegmentation.populate(restriction)\n\n# Limit number of jobs\nSegmentation.populate(limit=100)\n\n# Show progress\nSegmentation.populate(display_progress=True)\n\n# Suppress errors, continue processing\nSegmentation.populate(suppress_errors=True)\n</code></pre>"},{"location":"explanation/computation-model/#jobs-20-distributed-computing","title":"Jobs 2.0: Distributed Computing","text":"<p>For parallel and distributed execution, Jobs 2.0 provides:</p>"},{"location":"explanation/computation-model/#job-states","title":"Job States","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; pending : key_source - table\n    pending --&gt; reserved : reserve()\n    reserved --&gt; success : complete()\n    reserved --&gt; error : error()\n    reserved --&gt; pending : timeout\n    success --&gt; [*]\n    error --&gt; pending : ignore/clear\n</code></pre>"},{"location":"explanation/computation-model/#job-table","title":"Job Table","text":"<p>Each auto-populated table has an associated jobs table:</p> <pre><code># View job status\nSegmentation.jobs()\n\n# View errors\nSegmentation.jobs &amp; \"status = 'error'\"\n\n# Clear errors to retry\n(Segmentation.jobs &amp; \"status = 'error'\").delete()\n</code></pre>"},{"location":"explanation/computation-model/#parallel-execution","title":"Parallel Execution","text":"<pre><code># Multiple workers can run simultaneously\n# Each reserves different keys\n\n# Worker 1\nSegmentation.populate(reserve_jobs=True)\n\n# Worker 2 (different process/machine)\nSegmentation.populate(reserve_jobs=True)\n</code></pre> <p>Jobs are reserved atomically\u2014no two workers process the same key.</p>"},{"location":"explanation/computation-model/#error-handling","title":"Error Handling","text":"<pre><code># Populate with error suppression\nSegmentation.populate(suppress_errors=True)\n\n# Check what failed\nerrors = (Segmentation.jobs &amp; \"status = 'error'\").to_dicts()\n\n# Clear specific error to retry\n(Segmentation.jobs &amp; error_key).delete()\n\n# Clear all errors\n(Segmentation.jobs &amp; \"status = 'error'\").delete()\n</code></pre>"},{"location":"explanation/computation-model/#imported-vs-computed","title":"Imported vs. Computed","text":"Aspect <code>dj.Imported</code> <code>dj.Computed</code> Data source External (files, APIs) Other tables Typical use Load raw data Derive results Diagram color Blue Red <p>Both use the same <code>make()</code> mechanism.</p>"},{"location":"explanation/computation-model/#workflow-integrity","title":"Workflow Integrity","text":"<p>The computation model maintains workflow integrity:</p> <ol> <li>Dependency order \u2014 Upstream tables populate before downstream</li> <li>Cascade deletes \u2014 Deleting upstream deletes downstream</li> <li>Recomputation \u2014 Delete and re-populate to update results</li> </ol> <pre><code># Correct an upstream error\n(Scan &amp; problem_key).delete()  # Cascades to Segmentation\n\n# Reinsert corrected data\nScan.insert1(corrected_data)\n\n# Recompute\nSegmentation.populate()\n</code></pre>"},{"location":"explanation/computation-model/#job-metadata-optional","title":"Job Metadata (Optional)","text":"<p>Track computation metadata with hidden columns:</p> <pre><code>dj.config['jobs.add_job_metadata'] = True\n</code></pre> <p>This adds to computed tables:</p> <ul> <li><code>_job_start_time</code> \u2014 When computation started</li> <li><code>_job_duration</code> \u2014 How long it took</li> <li><code>_job_version</code> \u2014 Code version (if configured)</li> </ul>"},{"location":"explanation/computation-model/#the-three-part-make-model","title":"The Three-Part Make Model","text":"<p>For long-running computations (hours or days), holding a database transaction open for the entire duration causes problems:</p> <ul> <li>Database locks block other operations</li> <li>Transaction timeouts may occur</li> <li>Resources are held unnecessarily</li> </ul> <p>The three-part make pattern solves this by separating the computation from the transaction:</p> <pre><code>@schema\nclass SignalAverage(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawSignal\n    ---\n    avg_signal : float64\n    \"\"\"\n\n    def make_fetch(self, key, **kwargs):\n        \"\"\"Step 1: Fetch input data (outside transaction).\n\n        kwargs are passed from populate(make_kwargs={...}).\n        \"\"\"\n        raw_signal = (RawSignal &amp; key).fetch1(\"signal\")\n        return (raw_signal,)\n\n    def make_compute(self, key, fetched):\n        \"\"\"Step 2: Perform computation (outside transaction)\"\"\"\n        (raw_signal,) = fetched\n        avg = raw_signal.mean()\n        return (avg,)\n\n    def make_insert(self, key, fetched, computed):\n        \"\"\"Step 3: Insert results (inside brief transaction)\"\"\"\n        (avg,) = computed\n        self.insert1({**key, \"avg_signal\": avg})\n</code></pre>"},{"location":"explanation/computation-model/#how-it-works","title":"How It Works","text":"<p>DataJoint executes the three parts with verification:</p> <pre><code>fetched = make_fetch(key)              # Outside transaction\ncomputed = make_compute(key, fetched)  # Outside transaction\n\n&lt;begin transaction&gt;\nfetched_again = make_fetch(key)        # Re-fetch to verify\nif fetched != fetched_again:\n    &lt;rollback&gt;                         # Inputs changed\u2014abort\nelse:\n    make_insert(key, fetched, computed)\n    &lt;commit&gt;\n</code></pre> <p>The key insight: the computation runs outside any transaction, but referential integrity is preserved by re-fetching and verifying inputs before insertion. If upstream data changed during computation, the job is cancelled rather than inserting inconsistent results.</p>"},{"location":"explanation/computation-model/#benefits","title":"Benefits","text":"Aspect Standard <code>make()</code> Three-Part Pattern Transaction duration Entire computation Only final insert Database locks Held throughout Minimal Suitable for Short computations Hours/days Integrity guarantee Transaction Re-fetch verification"},{"location":"explanation/computation-model/#when-to-use-each-pattern","title":"When to Use Each Pattern","text":"Computation Time Pattern Rationale Seconds to minutes Standard <code>make()</code> Simple, transaction overhead acceptable Minutes to hours Three-part Avoid long transactions Hours to days Three-part Essential for stability <p>The three-part pattern trades off fetching data twice for dramatically reduced transaction duration. Use it when computation time significantly exceeds fetch time.</p>"},{"location":"explanation/computation-model/#best-practices","title":"Best Practices","text":""},{"location":"explanation/computation-model/#1-keep-make-focused","title":"1. Keep <code>make()</code> Focused","text":"<pre><code>def make(self, key):\n    # Good: One clear computation\n    data = (UpstreamTable &amp; key).fetch1('data')\n    result = process(data)\n    self.insert1({**key, 'result': result})\n</code></pre>"},{"location":"explanation/computation-model/#2-handle-large-data-efficiently","title":"2. Handle Large Data Efficiently","text":"<pre><code>def make(self, key):\n    # Stream large data instead of loading all at once\n    for row in (LargeTable &amp; key):\n        process_chunk(row['data'])\n</code></pre>"},{"location":"explanation/computation-model/#3-use-transactions-for-multi-row-inserts","title":"3. Use Transactions for Multi-Row Inserts","text":"<pre><code>def make(self, key):\n    results = compute_multiple_results(key)\n\n    # All-or-nothing insertion\n    with dj.conn().transaction:\n        self.insert(results)\n</code></pre>"},{"location":"explanation/computation-model/#4-test-with-single-keys-first","title":"4. Test with Single Keys First","text":"<pre><code># Test make() on one key\nkey = (Scan - Segmentation).fetch1('KEY')\nSegmentation().make(key)\n\n# Then populate all\nSegmentation.populate()\n</code></pre>"},{"location":"explanation/computation-model/#summary","title":"Summary","text":"<ol> <li><code>make(key)</code> \u2014 Computes one entity at a time</li> <li><code>populate()</code> \u2014 Executes <code>make()</code> for all missing entities</li> <li>Jobs 2.0 \u2014 Enables parallel, distributed execution</li> <li>Three-part make \u2014 For long computations without long transactions</li> <li>Cascade deletes \u2014 Maintain workflow integrity</li> <li>Error handling \u2014 Robust retry mechanisms</li> </ol>"},{"location":"explanation/custom-codecs/","title":"Extending DataJoint with Custom Codecs","text":"<p>DataJoint's type system is extensible through codecs\u2014plugins that define how domain-specific Python objects are stored and retrieved. This enables seamless integration of specialized data types without modifying DataJoint itself.</p>"},{"location":"explanation/custom-codecs/#why-codecs","title":"Why Codecs?","text":"<p>Scientific computing involves diverse data types:</p> <ul> <li>Neuroscience: Spike trains, neural networks, connectivity graphs</li> <li>Imaging: Medical images, microscopy stacks, point clouds</li> <li>Genomics: Sequence alignments, phylogenetic trees, variant calls</li> <li>Physics: Simulation meshes, particle systems, field data</li> </ul> <p>Rather than forcing everything into NumPy arrays or JSON, codecs let you work with native data structures while DataJoint handles storage transparently.</p>"},{"location":"explanation/custom-codecs/#the-codec-contract","title":"The Codec Contract","text":"<p>A codec defines two operations:</p> <pre><code>graph LR\n    A[Python Object] --&gt;|encode| B[Storable Form]\n    B --&gt;|decode| A\n</code></pre> Method Input Output When Called <code>encode()</code> Python object bytes, dict, or another codec's input On <code>insert()</code> <code>decode()</code> Stored data Python object On <code>fetch()</code>"},{"location":"explanation/custom-codecs/#creating-a-custom-codec","title":"Creating a Custom Codec","text":""},{"location":"explanation/custom-codecs/#basic-structure","title":"Basic Structure","text":"<pre><code>import datajoint as dj\n\nclass MyCodec(dj.Codec):\n    \"\"\"Store custom objects.\"\"\"\n    name = \"mytype\"  # Used as &lt;mytype&gt; in definitions\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        \"\"\"Return storage type.\"\"\"\n        return \"&lt;blob&gt;\"  # Chain to blob serialization\n\n    def encode(self, value, *, key=None, store_name=None):\n        \"\"\"Convert Python object to storable form.\"\"\"\n        return serialize(value)\n\n    def decode(self, stored, *, key=None):\n        \"\"\"Convert stored form back to Python object.\"\"\"\n        return deserialize(stored)\n</code></pre>"},{"location":"explanation/custom-codecs/#auto-registration","title":"Auto-Registration","text":"<p>Codecs register automatically when the class is defined\u2014no decorator needed:</p> <pre><code>class GraphCodec(dj.Codec):\n    name = \"graph\"  # Immediately available as &lt;graph&gt;\n    ...\n\n# Check registration\nassert \"graph\" in dj.list_codecs()\n</code></pre>"},{"location":"explanation/custom-codecs/#example-networkx-graphs","title":"Example: NetworkX Graphs","text":"<pre><code>import networkx as nx\nimport datajoint as dj\n\nclass GraphCodec(dj.Codec):\n    \"\"\"Store NetworkX graphs as adjacency data.\"\"\"\n    name = \"graph\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        # Store as blob (internal) or blob@ (external)\n        return \"&lt;blob@&gt;\" if is_store else \"&lt;blob&gt;\"\n\n    def encode(self, graph, *, key=None, store_name=None):\n        \"\"\"Serialize graph to dict.\"\"\"\n        return {\n            'directed': graph.is_directed(),\n            'nodes': list(graph.nodes(data=True)),\n            'edges': list(graph.edges(data=True)),\n        }\n\n    def decode(self, stored, *, key=None):\n        \"\"\"Reconstruct graph from dict.\"\"\"\n        cls = nx.DiGraph if stored['directed'] else nx.Graph\n        G = cls()\n        G.add_nodes_from(stored['nodes'])\n        G.add_edges_from(stored['edges'])\n        return G\n</code></pre> <p>Usage:</p> <pre><code>@schema\nclass Connectivity(dj.Computed):\n    definition = \"\"\"\n    -&gt; Neurons\n    ---\n    network : &lt;graph&gt;        # Small graphs in database\n    full_network : &lt;graph@&gt;  # Large graphs in object storage\n    \"\"\"\n\n    def make(self, key):\n        # Build connectivity graph\n        G = nx.DiGraph()\n        G.add_edges_from(compute_connections(key))\n\n        self.insert1({**key, 'network': G, 'full_network': G})\n\n# Fetch returns NetworkX graph directly\ngraph = (Connectivity &amp; key).fetch1('network')\nprint(f\"Nodes: {graph.number_of_nodes()}\")\n</code></pre>"},{"location":"explanation/custom-codecs/#example-domain-specific-formats","title":"Example: Domain-Specific Formats","text":""},{"location":"explanation/custom-codecs/#genomics-pysam-alignments","title":"Genomics: Pysam Alignments","text":"<pre><code>import pysam\nimport tempfile\nfrom pathlib import Path\n\nclass BamCodec(dj.Codec):\n    \"\"\"Store BAM alignments.\"\"\"\n    name = \"bam\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        if not is_store:\n            raise dj.DataJointError(\"&lt;bam&gt; requires in-store storage: use &lt;bam@&gt;\")\n        return \"&lt;object@&gt;\"  # Path-addressed storage for file structure\n\n    def encode(self, alignments, *, key=None, store_name=None):\n        \"\"\"Write alignments to BAM format.\"\"\"\n        # alignments is a pysam.AlignmentFile or list of reads\n        # Storage handled by &lt;object&gt; codec\n        return alignments\n\n    def decode(self, stored, *, key=None):\n        \"\"\"Return ObjectRef for lazy BAM access.\"\"\"\n        return stored  # ObjectRef with .open() method\n</code></pre>"},{"location":"explanation/custom-codecs/#medical-imaging-simpleitk","title":"Medical Imaging: SimpleITK","text":"<pre><code>import SimpleITK as sitk\nimport io\n\nclass MedicalImageCodec(dj.Codec):\n    \"\"\"Store medical images with metadata.\"\"\"\n    name = \"medimg\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob@&gt;\" if is_store else \"&lt;blob&gt;\"\n\n    def encode(self, image, *, key=None, store_name=None):\n        \"\"\"Serialize SimpleITK image.\"\"\"\n        # Preserve spacing, origin, direction\n        buffer = io.BytesIO()\n        sitk.WriteImage(image, buffer, imageIO='NrrdImageIO')\n        return {\n            'data': buffer.getvalue(),\n            'spacing': image.GetSpacing(),\n            'origin': image.GetOrigin(),\n        }\n\n    def decode(self, stored, *, key=None):\n        \"\"\"Reconstruct SimpleITK image.\"\"\"\n        buffer = io.BytesIO(stored['data'])\n        return sitk.ReadImage(buffer)\n</code></pre>"},{"location":"explanation/custom-codecs/#codec-chaining","title":"Codec Chaining","text":"<p>Codecs can chain to other codecs via <code>get_dtype()</code>:</p> <pre><code>graph LR\n    A[\"\u2039graph\u203a\"] --&gt;|get_dtype| B[\"\u2039blob\u203a\"]\n    B --&gt;|get_dtype| C[\"bytes\"]\n    C --&gt;|MySQL| D[\"LONGBLOB\"]\n</code></pre> <pre><code>class CompressedGraphCodec(dj.Codec):\n    name = \"cgraph\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;graph&gt;\"  # Chain to graph codec\n\n    def encode(self, graph, *, key=None, store_name=None):\n        # Simplify before passing to graph codec\n        return nx.to_sparse6_bytes(graph)\n\n    def decode(self, stored, *, key=None):\n        return nx.from_sparse6_bytes(stored)\n</code></pre>"},{"location":"explanation/custom-codecs/#storage-mode-support","title":"Storage Mode Support","text":""},{"location":"explanation/custom-codecs/#internal-only","title":"Internal Only","text":"<pre><code>class SmallDataCodec(dj.Codec):\n    name = \"small\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        if is_store:\n            raise dj.DataJointError(\"&lt;small&gt; is internal-only\")\n        return \"json\"\n</code></pre>"},{"location":"explanation/custom-codecs/#external-only","title":"External Only","text":"<pre><code>class LargeDataCodec(dj.Codec):\n    name = \"large\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        if not is_store:\n            raise dj.DataJointError(\"&lt;large&gt; requires @: use &lt;large@&gt;\")\n        return \"&lt;object@&gt;\"\n</code></pre>"},{"location":"explanation/custom-codecs/#both-modes","title":"Both Modes","text":"<pre><code>class FlexibleCodec(dj.Codec):\n    name = \"flex\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob@&gt;\" if is_store else \"&lt;blob&gt;\"\n</code></pre>"},{"location":"explanation/custom-codecs/#validation","title":"Validation","text":"<p>Add validation to catch errors early:</p> <pre><code>class StrictGraphCodec(dj.Codec):\n    name = \"strictgraph\"\n\n    def validate(self, value):\n        \"\"\"Called before encode().\"\"\"\n        if not isinstance(value, nx.Graph):\n            raise dj.DataJointError(\n                f\"Expected NetworkX graph, got {type(value).__name__}\"\n            )\n        if value.number_of_nodes() == 0:\n            raise dj.DataJointError(\"Graph must have at least one node\")\n\n    def encode(self, graph, *, key=None, store_name=None):\n        self.validate(graph)\n        return {...}\n</code></pre>"},{"location":"explanation/custom-codecs/#best-practices","title":"Best Practices","text":""},{"location":"explanation/custom-codecs/#1-choose-appropriate-storage","title":"1. Choose Appropriate Storage","text":"Data Size Recommendation &lt; 1 KB <code>json</code> or <code>&lt;blob&gt;</code> 1 KB - 10 MB <code>&lt;blob&gt;</code> or <code>&lt;blob@&gt;</code> &gt; 10 MB <code>&lt;blob@&gt;</code> or <code>&lt;object@&gt;</code> File structures <code>&lt;object@&gt;</code>"},{"location":"explanation/custom-codecs/#2-preserve-metadata","title":"2. Preserve Metadata","text":"<pre><code>def encode(self, obj, *, key=None, store_name=None):\n    return {\n        'data': serialize(obj),\n        'version': '1.0',  # For future compatibility\n        'dtype': str(obj.dtype),\n        'shape': obj.shape,\n    }\n</code></pre>"},{"location":"explanation/custom-codecs/#3-handle-versioning","title":"3. Handle Versioning","text":"<pre><code>def decode(self, stored, *, key=None):\n    version = stored.get('version', '0.9')\n    if version == '1.0':\n        return deserialize_v1(stored)\n    else:\n        return deserialize_legacy(stored)\n</code></pre>"},{"location":"explanation/custom-codecs/#4-document-your-codec","title":"4. Document Your Codec","text":"<pre><code>class WellDocumentedCodec(dj.Codec):\n    \"\"\"\n    Store XYZ data structures.\n\n    Supports both in-table (&lt;xyz&gt;) and in-store (&lt;xyz@&gt;) storage.\n\n    Examples\n    --------\n    &gt;&gt;&gt; @schema\n    ... class Results(dj.Computed):\n    ...     definition = '''\n    ...     -&gt; Experiment\n    ...     ---\n    ...     output : &lt;xyz@&gt;\n    ...     '''\n    \"\"\"\n    name = \"xyz\"\n</code></pre>"},{"location":"explanation/custom-codecs/#summary","title":"Summary","text":"<p>Custom codecs enable:</p> <ol> <li>Domain-specific types \u2014 Work with native data structures</li> <li>Transparent storage \u2014 DataJoint handles serialization</li> <li>Flexible backends \u2014 Internal, external, or both</li> <li>Composability \u2014 Chain codecs for complex transformations</li> <li>Validation \u2014 Catch errors before storage</li> </ol> <p>The codec system makes DataJoint extensible to any scientific domain without modifying the core framework.</p>"},{"location":"explanation/custom-codecs/#before-creating-your-own","title":"Before Creating Your Own","text":"<p>Check for existing plugin codecs that may already solve your needs:</p> <ul> <li>dj-zarr-codecs \u2014 General numpy arrays with Zarr storage</li> <li>dj-photon-codecs \u2014 Photon-limited movies with Anscombe transformation and compression</li> </ul> <p>See the Use Plugin Codecs guide for installation and usage of existing codec packages. Creating a custom codec is straightforward, but reusing existing ones saves time and ensures compatibility.</p>"},{"location":"explanation/data-pipelines/","title":"Scientific Data Pipelines","text":"<p>A scientific data pipeline extends beyond a database with computations. It is a comprehensive system that:</p> <ul> <li>Manages the complete lifecycle of scientific data from acquisition to delivery</li> <li>Integrates diverse tools for data entry, visualization, and analysis</li> <li>Provides infrastructure for secure, scalable computation</li> <li>Enables collaboration across teams and institutions</li> <li>Supports reproducibility and provenance tracking throughout</li> </ul>"},{"location":"explanation/data-pipelines/#pipeline-architecture","title":"Pipeline Architecture","text":"<p>A DataJoint pipeline integrates three core components:</p> <p></p> Component Purpose Code Repository Version-controlled pipeline definitions, <code>make</code> methods, configuration Relational Database System of record for metadata, relationships, and integrity enforcement Object Store Scalable storage for large scientific data (images, recordings, signals) <p>These components work together: code defines the schema and computations, the database tracks all metadata and relationships, and object storage holds the large scientific data files.</p>"},{"location":"explanation/data-pipelines/#pipeline-as-a-dag","title":"Pipeline as a DAG","text":"<p>A DataJoint pipeline forms a Directed Acyclic Graph (DAG) at two levels:</p> <p></p> <p>Nodes represent Python modules, which correspond to database schemas.</p> <p>Edges represent:</p> <ul> <li>Python import dependencies between modules</li> <li>Bundles of foreign key references between schemas</li> </ul> <p>This dual structure ensures that both code dependencies and data dependencies flow in the same direction.</p>"},{"location":"explanation/data-pipelines/#dag-constraints","title":"DAG Constraints","text":"<p>All foreign key relationships within a schema MUST form a DAG.</p> <p>Dependencies between schemas (foreign keys + imports) MUST also form a DAG.</p> <p>This constraint is fundamental to DataJoint's design. It ensures:</p> <ul> <li>Unidirectional data flow \u2014 Data enters at the top and flows downstream</li> <li>Clear provenance \u2014 Every result traces back to its inputs</li> <li>Safe deletion \u2014 Cascading deletes follow the DAG without cycles</li> <li>Predictable computation \u2014 <code>populate()</code> can determine correct execution order</li> </ul>"},{"location":"explanation/data-pipelines/#the-relational-workflow-model","title":"The Relational Workflow Model","text":"<p>DataJoint pipelines are built on the Relational Workflow Model\u2014a paradigm that extends relational databases with native support for computational workflows. In this model:</p> <ul> <li>Tables represent workflow steps, not just data storage</li> <li>Foreign keys encode dependencies, prescribing the order of operations</li> <li>Table tiers (Lookup, Manual, Imported, Computed) classify how data enters the pipeline</li> <li>The schema forms a DAG that defines valid execution sequences</li> </ul> <p>This model treats the database schema as an executable workflow specification\u2014defining not just what data exists but when and how it comes into existence.</p>"},{"location":"explanation/data-pipelines/#schema-organization","title":"Schema Organization","text":"<p>Each schema corresponds to a dedicated Python module. The module import structure mirrors the foreign key dependencies between schemas:</p> <p></p> <pre><code>my_pipeline/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_pipeline/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 subject.py      # subject schema (no dependencies)\n\u2502       \u251c\u2500\u2500 session.py      # session schema (depends on subject)\n\u2502       \u251c\u2500\u2500 acquisition.py  # acquisition schema (depends on session)\n\u2502       \u2514\u2500\u2500 analysis.py     # analysis schema (depends on acquisition)\n</code></pre> <p>For practical guidance on organizing multi-schema pipelines, configuring repositories, and managing team access, see Manage a Pipeline Project.</p>"},{"location":"explanation/data-pipelines/#object-augmented-schemas","title":"Object-Augmented Schemas","text":"<p>Scientific data often includes large objects\u2014images, recordings, time series, instrument outputs\u2014that don't fit efficiently in relational tables. DataJoint addresses this through Object-Augmented Schemas (OAS), a hybrid storage architecture that preserves relational semantics while handling arbitrarily large data.</p>"},{"location":"explanation/data-pipelines/#the-oas-philosophy","title":"The OAS Philosophy","text":"<p>1. The database remains the system of record.</p> <p>All metadata, relationships, and query logic live in the relational database. The schema defines what data exists, how entities relate, and what computations produce them. Queries operate on the relational structure; results are consistent and reproducible.</p> <p>2. Large objects live in object stores.</p> <p>Object storage (filesystems, S3, GCS, Azure Blob, MinIO) holds the actual bytes\u2014arrays, images, files. The database stores only lightweight references (paths, checksums, metadata). This separation lets the database stay fast while data scales to terabytes.</p> <p>3. Transparent access through codecs.</p> <p>DataJoint's type system provides codec types that bridge Python objects and storage:</p> Codec Purpose <code>&lt;blob&gt;</code> Serialize Python objects (NumPy arrays, dicts) <code>&lt;blob@store&gt;</code> Same, but stored in object store <code>&lt;attach&gt;</code> Store files with preserved filenames <code>&lt;object@store&gt;</code> Path-addressed storage for complex structures (Zarr, HDF5) <code>&lt;filepath@store&gt;</code> References to user-managed files <p>Users work with native Python objects; serialization and storage routing are invisible.</p> <p>4. Referential integrity extends to objects.</p> <p>When a database row is deleted, its associated stored objects are garbage-collected. Foreign key cascades work correctly\u2014delete upstream data and downstream results (including their objects) disappear. The database and object store remain synchronized without manual cleanup.</p> <p>5. Multiple storage tiers support diverse access patterns.</p> <p>Different attributes can route to different stores:</p> <pre><code>class Recording(dj.Imported):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    raw_data : &lt;blob@fast&gt;       # Hot storage for active analysis\n    archive : &lt;blob@cold&gt;        # Cold storage for long-term retention\n    \"\"\"\n</code></pre> <p>This architecture lets teams work with terabyte-scale datasets while retaining the query power, integrity guarantees, and reproducibility of the relational model.</p>"},{"location":"explanation/data-pipelines/#pipeline-workflow","title":"Pipeline Workflow","text":"<p>A typical data pipeline workflow:</p> <ol> <li> <p>Acquisition \u2014 Data is collected from instruments, experiments, or external sources. Raw files land in object storage; metadata populates Manual tables.</p> </li> <li> <p>Import \u2014 Automated processes parse raw data, extract signals, and populate Imported tables with structured results.</p> </li> <li> <p>Computation \u2014 The <code>populate()</code> mechanism identifies new data and triggers downstream processing. Compute resources execute transformations and populate Computed tables.</p> </li> <li> <p>Query &amp; Analysis \u2014 Users query results across the pipeline, combining data from multiple stages to generate insights, reports, or visualizations.</p> </li> <li> <p>Collaboration \u2014 Team members access the same database concurrently, building on shared results. Foreign key constraints maintain consistency.</p> </li> <li> <p>Delivery \u2014 Processed results are exported, integrated into downstream systems, or archived according to project requirements.</p> </li> </ol> <p>Throughout this process, the schema definition remains the single source of truth.</p>"},{"location":"explanation/data-pipelines/#comparing-approaches","title":"Comparing Approaches","text":"Aspect File-Based Approach DataJoint Pipeline Data Structure Implicit in filenames/folders Explicit in schema definition Dependencies Encoded in scripts Declared through foreign keys Provenance Manual tracking Automatic through referential integrity Reproducibility Requires careful discipline Built into the model Collaboration File sharing/conflicts Concurrent database access Queries Custom scripts per question Composable query algebra Scalability Limited by filesystem Database + object-augmented storage <p>The pipeline approach requires upfront investment in schema design. This investment pays dividends through reduced errors, improved reproducibility, and efficient collaboration as projects scale.</p>"},{"location":"explanation/data-pipelines/#summary","title":"Summary","text":"<p>Scientific data pipelines extend the Relational Workflow Model into complete data operations systems:</p> <ul> <li>Pipeline Architecture \u2014 Code repository, relational database, and object store working together</li> <li>DAG Structure \u2014 Unidirectional flow of data and dependencies</li> <li>Object-Augmented Schemas \u2014 Scalable storage with relational semantics</li> </ul> <p>The schema remains central\u2014defining data structures, dependencies, and computational flow. This pipeline-centric approach lets teams focus on their science while the system handles data integrity, provenance, and reproducibility automatically.</p>"},{"location":"explanation/data-pipelines/#see-also","title":"See Also","text":"<ul> <li>Relational Workflow Model \u2014 The conceptual foundation</li> <li>Entity Integrity \u2014 Primary keys and dimensions</li> <li>Type System \u2014 Codec types and storage modes</li> <li>Manage a Pipeline Project \u2014 Practical project organization</li> </ul>"},{"location":"explanation/entity-integrity/","title":"Entity Integrity","text":"<p>Entity integrity ensures a one-to-one correspondence between real-world entities and their database records. This is the foundation of reliable data management.</p>"},{"location":"explanation/entity-integrity/#the-core-guarantee","title":"The Core Guarantee","text":"<ul> <li>Each real-world entity \u2192 exactly one database record</li> <li>Each database record \u2192 exactly one real-world entity</li> </ul> <p>Without entity integrity, databases become unreliable:</p> Integrity Failure Consequence Same entity, multiple records Fragmented data, conflicting information Multiple entities, same record Mixed data, privacy violations Cannot match entity to record Lost data, broken workflows"},{"location":"explanation/entity-integrity/#the-three-questions","title":"The Three Questions","text":"<p>When designing a primary key, answer these three questions:</p>"},{"location":"explanation/entity-integrity/#1-how-do-i-prevent-duplicate-records","title":"1. How do I prevent duplicate records?","text":"<p>Ensure the same entity cannot appear twice in the table.</p>"},{"location":"explanation/entity-integrity/#2-how-do-i-prevent-record-sharing","title":"2. How do I prevent record sharing?","text":"<p>Ensure different entities cannot share the same record.</p>"},{"location":"explanation/entity-integrity/#3-how-do-i-match-entities-to-records","title":"3. How do I match entities to records?","text":"<p>When an entity arrives, how do I find its corresponding record?</p>"},{"location":"explanation/entity-integrity/#example-laboratory-mouse-database","title":"Example: Laboratory Mouse Database","text":"<p>Consider a neuroscience lab tracking mice:</p> Question Answer Prevent duplicates? Each mouse gets a unique ear tag at arrival; database rejects duplicate tags Prevent sharing? Ear tags are never reused; retired tags are archived Match entities? Read the ear tag \u2192 look up record by primary key <pre><code>@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    ear_tag : char(6)   # unique ear tag (e.g., 'M00142')\n    ---\n    date_of_birth : date\n    sex : enum('M', 'F', 'U')\n    strain : varchar(50)\n    \"\"\"\n</code></pre> <p>The database enforces the first two questions through the primary key constraint. The third question requires a physical identification system\u2014ear tags, barcodes, or RFID chips that link physical entities to database records.</p>"},{"location":"explanation/entity-integrity/#primary-key-requirements","title":"Primary Key Requirements","text":"<p>In DataJoint, every table must have a primary key. Primary key attributes:</p> <ul> <li>Cannot be NULL \u2014 Every entity must be identifiable</li> <li>Must be unique \u2014 No two entities share the same key</li> <li>Cannot be changed \u2014 Keys are immutable after insertion</li> <li>Declared above the <code>---</code> line \u2014 Syntactic convention</li> </ul>"},{"location":"explanation/entity-integrity/#natural-keys-vs-surrogate-keys","title":"Natural Keys vs. Surrogate Keys","text":""},{"location":"explanation/entity-integrity/#natural-keys","title":"Natural Keys","text":"<p>Use attributes that naturally identify entities in your domain:</p> <pre><code>@schema\nclass Gene(dj.Lookup):\n    definition = \"\"\"\n    gene_symbol : varchar(20)   # Official gene symbol (e.g., 'BRCA1')\n    ---\n    full_name : varchar(200)\n    chromosome : varchar(5)\n    \"\"\"\n</code></pre> <p>Advantages:</p> <ul> <li>Meaningful to humans</li> <li>Self-documenting</li> <li>No additional lookup needed</li> </ul>"},{"location":"explanation/entity-integrity/#surrogate-keys","title":"Surrogate Keys","text":"<p>A surrogate key is an identifier used primarily inside the database, with minimal or no exposure to end users. Users typically don't search for entities by surrogate keys or use them in conversation.</p> <pre><code>@schema\nclass InternalRecord(dj.Manual):\n    definition = \"\"\"\n    record_id : uuid    # internal identifier, not exposed to users\n    ---\n    created_timestamp : datetime(3)\n    data : &lt;blob&gt;\n    \"\"\"\n</code></pre> <p>Key distinction from natural keys: Surrogate keys don't require external identification systems because users don't need to match physical entities to records by these keys.</p> <p>When surrogate keys are appropriate:</p> <ul> <li>Entities that exist only within the system (no physical counterpart)</li> <li>Privacy-sensitive contexts where natural identifiers shouldn't be stored</li> <li>Internal system records that users never reference directly</li> </ul> <p>Generating surrogate keys: DataJoint requires explicit key values rather than database-generated auto-increment. This is intentional:</p> <ul> <li>Auto-increment encourages treating keys as \"row numbers\" rather than entity identifiers</li> <li>It's incompatible with composite keys, which DataJoint uses extensively</li> <li>It breaks reproducibility (different IDs when rebuilding pipelines)</li> <li>It prevents the client-server handshake needed for proper entity integrity</li> </ul> <p>Use client-side generation instead:</p> <ul> <li>UUIDs \u2014 Generate with <code>uuid.uuid4()</code> before insertion</li> <li>ULIDs \u2014 Sortable unique IDs</li> <li>Client-side counters \u2014 Query max value and increment</li> </ul> <p>DataJoint recommendation: Prefer natural keys when they're stable and meaningful. Use surrogates only when no natural identifier exists or for privacy-sensitive contexts.</p>"},{"location":"explanation/entity-integrity/#composite-keys","title":"Composite Keys","text":"<p>When no single attribute uniquely identifies an entity, combine multiple attributes:</p> <pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    recording_idx : int32   # Recording number within session\n    ---\n    duration : float32       # seconds\n    \"\"\"\n</code></pre> <p>Here, <code>(subject_id, session_idx, recording_idx)</code> together form the primary key. Neither alone would be unique.</p>"},{"location":"explanation/entity-integrity/#foreign-keys-and-dependencies","title":"Foreign Keys and Dependencies","text":"<p>Foreign keys in DataJoint serve dual purposes:</p> <ol> <li>Referential integrity \u2014 Ensures referenced entities exist</li> <li>Workflow dependency \u2014 Declares that this entity depends on another</li> </ol> <pre><code>@schema\nclass Segmentation(dj.Computed):\n    definition = \"\"\"\n    -&gt; Scan           # Depends on Scan\n    ---\n    num_cells : int64\n    \"\"\"\n</code></pre> <p>The arrow <code>-&gt;</code> inherits the primary key from <code>Scan</code> and establishes both referential integrity and workflow dependency.</p>"},{"location":"explanation/entity-integrity/#schema-dimensions","title":"Schema Dimensions","text":"<p>A dimension is an independent axis of variation in your data. The fundamental principle:</p> <p>Any table that introduces a new primary key attribute introduces a new dimension.</p> <p>This is true whether the table has only new attributes or also inherits attributes from foreign keys. The key is simply: new primary key attribute = new dimension.</p>"},{"location":"explanation/entity-integrity/#tables-that-introduce-dimensions","title":"Tables That Introduce Dimensions","text":"<pre><code>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)   # NEW dimension: subject_id\n    ---\n    species : varchar(50)\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject                 # Inherits subject_id\n    session_idx : int32       # NEW dimension: session_idx\n    ---\n    session_date : date\n    \"\"\"\n\n@schema\nclass Trial(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session                 # Inherits subject_id, session_idx\n    trial_idx : int32         # NEW dimension: trial_idx\n    ---\n    outcome : enum('success', 'fail')\n    \"\"\"\n</code></pre> <p>All three tables introduce dimensions:</p> <ul> <li><code>Subject</code> introduces <code>subject_id</code> dimension</li> <li><code>Session</code> introduces <code>session_idx</code> dimension (even though it also inherits <code>subject_id</code>)</li> <li><code>Trial</code> introduces <code>trial_idx</code> dimension (even though it also inherits <code>subject_id</code>, <code>session_idx</code>)</li> </ul> <p>In schema diagrams, tables that introduce at least one new dimension have underlined names.</p>"},{"location":"explanation/entity-integrity/#tables-that-dont-introduce-dimensions","title":"Tables That Don't Introduce Dimensions","text":"<p>A table introduces no dimensions when its entire primary key comes from foreign keys:</p> <pre><code>@schema\nclass SubjectProfile(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject                 # Inherits subject_id only\n    ---\n    weight : float32\n    \"\"\"\n</code></pre> <p><code>SubjectProfile</code> doesn't introduce any new primary key attribute\u2014it extends the <code>Subject</code> dimension with additional attributes. There's exactly one profile per subject.</p> <p>In schema diagrams, these tables have non-underlined names.</p>"},{"location":"explanation/entity-integrity/#computed-tables-and-dimensions","title":"Computed Tables and Dimensions","text":"<p>Computed tables never introduce dimensions. Their primary key is entirely inherited from their dependencies:</p> <pre><code>@schema\nclass SessionSummary(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session                 # PK = (subject_id, session_idx)\n    ---\n    num_trials : int64\n    accuracy : float32\n    \"\"\"\n</code></pre> <p>This makes sense\u2014computed tables derive data from existing entities rather than introducing new ones.</p>"},{"location":"explanation/entity-integrity/#part-tables-can-introduce-dimensions","title":"Part Tables CAN Introduce Dimensions","text":"<p>Unlike computed tables, part tables can introduce new dimensions:</p> <pre><code>@schema\nclass Detection(dj.Computed):\n    definition = \"\"\"\n    -&gt; Image                   # Inherits image_id\n    -&gt; DetectionParams         # Inherits params_id\n    ---\n    num_blobs : int64\n    \"\"\"\n\n    class Blob(dj.Part):\n        definition = \"\"\"\n        -&gt; master              # Inherits (image_id, params_id)\n        blob_idx : int32      # NEW dimension within detection\n        ---\n        x : float32\n        y : float32\n        \"\"\"\n</code></pre> <p><code>Detection</code> inherits dimensions (no underline in diagram), but <code>Detection.Blob</code> introduces a new dimension (<code>blob_idx</code>) for individual blobs within each detection.</p>"},{"location":"explanation/entity-integrity/#dimensions-and-attribute-lineage","title":"Dimensions and Attribute Lineage","text":"<p>Every foreign key attribute traces back to the dimension where it was first defined. This is called attribute lineage:</p> <pre><code>Subject.subject_id      \u2192 myschema.subject.subject_id (origin)\nSession.subject_id      \u2192 myschema.subject.subject_id (inherited via foreign key)\nSession.session_idx     \u2192 myschema.session.session_idx (origin)\nTrial.subject_id        \u2192 myschema.subject.subject_id (inherited via foreign key)\nTrial.session_idx       \u2192 myschema.session.session_idx (inherited via foreign key)\nTrial.trial_idx         \u2192 myschema.trial.trial_idx (origin)\n</code></pre> <p>Lineage enables semantic matching\u2014DataJoint only joins attributes that trace back to the same dimension. Two attributes named <code>id</code> from different dimensions cannot be accidentally joined.</p> <p>See Semantic Matching for details.</p>"},{"location":"explanation/entity-integrity/#recognizing-dimensions-in-diagrams","title":"Recognizing Dimensions in Diagrams","text":"<p>In schema diagrams:</p> Visual Meaning Underlined name Introduces at least one new dimension Non-underlined name All PK attributes inherited (no new dimensions) Thick solid line One-to-one extension (no new dimension) Thin solid line Containment (may introduce dimension) <p>Common dimensions in neuroscience:</p> <ul> <li>Subject \u2014 Who/what is being studied</li> <li>Session \u2014 When data was collected</li> <li>Trial \u2014 Individual experimental unit</li> <li>Modality \u2014 Type of data (ephys, imaging, behavior)</li> <li>Parameter set \u2014 Configuration for analysis</li> </ul> <p>Understanding dimensions helps design schemas that naturally express your experimental structure and ensures correct joins through semantic matching.</p>"},{"location":"explanation/entity-integrity/#best-practices","title":"Best Practices","text":"<ol> <li>Answer the three questions before designing any table</li> <li>Choose stable identifiers that won't need to change</li> <li>Keep keys minimal \u2014 Include only what's necessary for uniqueness</li> <li>Document key semantics \u2014 Explain what the key represents</li> <li>Consider downstream queries \u2014 Keys affect join performance</li> </ol>"},{"location":"explanation/entity-integrity/#common-mistakes","title":"Common Mistakes","text":""},{"location":"explanation/entity-integrity/#too-few-key-attributes","title":"Too few key attributes","text":"<pre><code># Wrong: experiment_id alone isn't unique\nclass Trial(dj.Manual):\n    definition = \"\"\"\n    experiment_id : int64\n    ---\n    trial_number : int32   # Should be part of key!\n    result : float32\n    \"\"\"\n</code></pre>"},{"location":"explanation/entity-integrity/#too-many-key-attributes","title":"Too many key attributes","text":"<pre><code># Wrong: timestamp makes every row unique, losing entity semantics\nclass Measurement(dj.Manual):\n    definition = \"\"\"\n    subject_id : int64\n    timestamp : datetime(6)   # Microsecond precision\n    ---\n    value : float32\n    \"\"\"\n</code></pre>"},{"location":"explanation/entity-integrity/#mutable-natural-keys","title":"Mutable natural keys","text":"<pre><code># Risky: names can change\nclass Patient(dj.Manual):\n    definition = \"\"\"\n    patient_name : varchar(100)   # What if they change their name?\n    ---\n    date_of_birth : date\n    \"\"\"\n</code></pre>"},{"location":"explanation/entity-integrity/#summary","title":"Summary","text":"<p>Entity integrity is maintained by:</p> <ol> <li>Primary keys that uniquely identify each entity</li> <li>Foreign keys that establish valid references</li> <li>Physical systems that link real-world entities to records</li> </ol> <p>The three questions framework ensures your primary keys provide meaningful, stable identification for your domain entities.</p>"},{"location":"explanation/faq/","title":"Frequently Asked Questions","text":""},{"location":"explanation/faq/#why-does-datajoint-have-its-own-definition-and-query-language","title":"Why Does DataJoint Have Its Own Definition and Query Language?","text":"<p>DataJoint provides a custom data definition language and query algebra rather than using raw SQL or Object-Relational Mapping (ORM) patterns. This design reflects DataJoint's purpose: enabling research teams to build relational workflows with embedded computations with maximum clarity. These concepts were first formalized in Yatsenko et al., 2018.</p>"},{"location":"explanation/faq/#the-definition-language","title":"The Definition Language","text":"<p>DataJoint's definition language is a standalone scripting language for declaring table schemas \u2014 not Python syntax embedded in strings. It is designed for uniform support across multiple host languages (Python, MATLAB, and potentially others). The same definition works identically regardless of which language you use.</p>"},{"location":"explanation/faq/#composite-primary-keys-a-clarity-comparison","title":"Composite Primary Keys: A Clarity Comparison","text":"<p>Scientific workflows frequently use composite primary keys built from foreign keys. Compare how different approaches handle this common pattern:</p> <pre><code># DataJoint - two characters declare dependency, foreign key, and inherit primary key\nclass Scan(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    scan_idx : int16\n    ---\n    depth : float32\n    \"\"\"\n</code></pre> <pre><code># SQLAlchemy - verbose, scattered, error-prone\nclass Scan(Base):\n    subject_id = Column(Integer, primary_key=True)\n    session_date = Column(Date, primary_key=True)\n    scan_idx = Column(SmallInteger, primary_key=True)\n    depth = Column(Float)\n    __table_args__ = (\n        ForeignKeyConstraint(\n            ['subject_id', 'session_date'],\n            ['session.subject_id', 'session.session_date']\n        ),\n    )\n</code></pre> <pre><code>-- Raw SQL - maximum verbosity\nCREATE TABLE scan (\n    subject_id INT NOT NULL,\n    session_date DATE NOT NULL,\n    scan_idx SMALLINT NOT NULL,\n    depth FLOAT,\n    PRIMARY KEY (subject_id, session_date, scan_idx),\n    FOREIGN KEY (subject_id, session_date)\n        REFERENCES session(subject_id, session_date)\n);\n</code></pre> <p>The <code>-&gt; Session</code> syntax in DataJoint:</p> <ul> <li>Inherits all primary key attributes from Session</li> <li>Declares the foreign key constraint</li> <li>Establishes the computational dependency (for <code>populate()</code>)</li> <li>Documents the data lineage</li> </ul> <p>All in two characters. As pipelines grow to dozens of tables with deep dependency chains, this clarity compounds.</p>"},{"location":"explanation/faq/#why-multiline-strings","title":"Why Multiline Strings?","text":"Aspect Benefit Readable Looks like a specification: <code>---</code> separates primary from secondary attributes, <code>#</code> for comments Concise <code>mouse_id : int32</code> vs <code>mouse_id = Column(Integer, primary_key=True)</code> Database-first <code>table.describe()</code> shows the same format; virtual schemas reconstruct definitions from database metadata Language-agnostic Same syntax for Python, MATLAB, future implementations Separation of concerns Definition string = structure (what); class = behavior (how: <code>make()</code> methods) <p>The definition string is the specification \u2014 a declarative language that describes entities and their relationships, independent of any host language's syntax.</p>"},{"location":"explanation/faq/#why-custom-query-algebra","title":"Why Custom Query Algebra?","text":"<p>DataJoint's operators implement semantic matching \u2014 joins and restrictions match only on attributes connected through the foreign key graph, not arbitrary columns that happen to share a name. This prevents:</p> <ul> <li>Accidental Cartesian products</li> <li>Joins on unrelated columns</li> <li>Silent incorrect results</li> </ul> <p>Every query result has a defined entity type with a specific primary key (algebraic closure). SQL results are untyped bags of rows; DataJoint results are entity sets you can continue to query and compose.</p>"},{"location":"explanation/faq/#object-augmented-schemas","title":"Object-Augmented Schemas","text":"<p>Object-Relational Mappers treat large objects as opaque binary blobs or leave file management to the application. DataJoint's object store extends the relational schema (see Type System):</p> <ul> <li>Relational semantics apply: referential integrity, cascading deletes, query filtering</li> <li>Multiple access patterns: lazy <code>ObjectRef</code>, streaming via fsspec, explicit download</li> <li>Two addressing modes: schema-addressed (by primary key) and hash-addressed (deduplicated)</li> </ul> <p>The object store is part of the relational model \u2014 queryable and integrity-protected like any other attribute.</p>"},{"location":"explanation/faq/#summary","title":"Summary","text":"Aspect Raw SQL Object-Relational Mappers DataJoint Schema definition SQL Data Definition Language Host language classes Standalone definition language Composite foreign keys Verbose, repetitive Verbose, scattered <code>-&gt; TableName</code> Query model SQL strings Object navigation Relational algebra operators Dependencies Implicit in application Implicit in application Explicit in schema Large objects Binary blobs / manual Binary blobs / manual Object-Augmented Schema Computation External to database External to database First-class (Computed tables) Target audience Database administrators Web developers Research teams"},{"location":"explanation/faq/#is-datajoint-an-orm","title":"Is DataJoint an ORM?","text":"<p>Object-Relational Mapping (ORM) is a technique for interacting with relational databases through object-oriented programming, abstracting direct SQL queries. Popular Python ORMs include SQLAlchemy, Django ORM, and Peewee, often used in web development.</p> <p>DataJoint shares certain ORM characteristics\u2014tables are defined as Python classes, and queries return Python objects. However, DataJoint is fundamentally a computational database framework designed for scientific workflows:</p> Aspect Traditional ORMs DataJoint Primary use case Web applications Scientific data pipelines Focus Simplify database CRUD Data integrity + computation Dependencies Implicit (application logic) Explicit (foreign keys define data flow) Computation External to database First-class citizen in schema Query model Object navigation Relational algebra <p>DataJoint can be considered an ORM specialized for scientific databases\u2014purpose-built for structured experimental data and computational workflows where reproducibility and data integrity are paramount.</p>"},{"location":"explanation/faq/#is-datajoint-a-workflow-management-system","title":"Is DataJoint a Workflow Management System?","text":"<p>Not exactly. DataJoint and workflow management systems (Airflow, Prefect, Flyte, Nextflow, Snakemake) solve related but distinct problems:</p> Aspect Workflow Managers DataJoint Core abstraction Tasks and DAGs Tables and dependencies State management External (files, databases) Integrated (relational database) Scheduling Built-in schedulers External (or manual <code>populate()</code>) Distributed execution Built-in Via external tools Data model Unstructured (files, blobs) Structured (relational schema) Query capability Limited Full relational algebra <p>DataJoint excels at:</p> <ul> <li>Defining what needs to be computed based on data dependencies</li> <li>Ensuring computations are never duplicated</li> <li>Maintaining referential integrity across pipeline stages</li> <li>Querying intermediate and final results</li> </ul> <p>Workflow managers excel at:</p> <ul> <li>Scheduling and orchestrating job execution</li> <li>Distributing work across clusters</li> <li>Retry logic and failure handling</li> <li>Resource management</li> </ul> <p>They complement each other. DataJoint formalizes data dependencies so that external schedulers can effectively manage computational tasks. A common pattern:</p> <ol> <li>DataJoint defines the pipeline structure and tracks what's computed</li> <li>A workflow manager (or simple cron/SLURM scripts) calls <code>populate()</code> on a schedule</li> <li>DataJoint determines what work remains and executes it</li> </ol>"},{"location":"explanation/faq/#is-datajoint-a-lakehouse","title":"Is DataJoint a Lakehouse?","text":"<p>DataJoint and lakehouses share goals\u2014integrating structured data management with scalable storage and computation. However, they differ in approach:</p> Aspect Lakehouse DataJoint Data model Flexible (structured + semi-structured) Strict relational schema Schema enforcement Schema-on-read optional Schema-on-write enforced Primary use Analytics on diverse data Scientific workflows Computation model SQL/Spark queries Declarative <code>make()</code> methods Dependency tracking Limited Explicit via foreign keys <p>A lakehouse merges data lake flexibility with data warehouse structure, optimized for analytics workloads.</p> <p>DataJoint prioritizes:</p> <ul> <li>Rigorous schema definitions</li> <li>Explicit computational dependencies</li> <li>Data integrity and reproducibility</li> <li>Traceability within structured scientific datasets</li> </ul> <p>DataJoint can complement lakehouse architectures\u2014using object storage for large files while maintaining relational structure for metadata and provenance.</p>"},{"location":"explanation/faq/#does-datajoint-require-sql-knowledge","title":"Does DataJoint Require SQL Knowledge?","text":"<p>No. DataJoint provides a Python API that abstracts SQL:</p> SQL DataJoint <code>CREATE TABLE</code> Define tables as Python classes <code>INSERT INTO</code> <code>.insert()</code> method <code>SELECT * FROM</code> <code>.to_arrays()</code>, <code>.to_dicts()</code>, <code>.to_pandas()</code> <code>JOIN</code> <code>table1 * table2</code> <code>WHERE</code> <code>table &amp; condition</code> <code>GROUP BY</code> <code>.aggr()</code> <p>Understanding relational concepts (primary keys, foreign keys, normalization) is helpful but not required to start. The tutorials teach these concepts progressively.</p> <p>Since DataJoint uses standard database backends (MySQL, PostgreSQL), data remains accessible via SQL for users who prefer it or need integration with other tools.</p>"},{"location":"explanation/faq/#how-does-datajoint-handle-large-files","title":"How Does DataJoint Handle Large Files?","text":"<p>DataJoint uses a hybrid storage model called Object-Augmented Schemas (OAS):</p> <ul> <li>Relational database: Stores metadata, parameters, and relationships</li> <li>Object storage: Stores large files (images, recordings, arrays)</li> </ul> <p>The database maintains references to external objects, preserving:</p> <ul> <li>Referential integrity (files deleted with their parent records)</li> <li>Query capability (filter by metadata, join across tables)</li> <li>Deduplication (identical content stored once)</li> </ul> <p>See Object Storage for details.</p>"},{"location":"explanation/faq/#can-multiple-users-share-a-pipeline","title":"Can Multiple Users Share a Pipeline?","text":"<p>Yes. DataJoint pipelines are inherently collaborative:</p> <ul> <li>Shared database: All users connect to the same MySQL/PostgreSQL instance</li> <li>Shared schema: Table definitions are stored in the database</li> <li>Concurrent access: ACID transactions prevent conflicts</li> <li>Job reservation: <code>populate()</code> coordinates work across processes</li> </ul> <p>Teams typically:</p> <ol> <li>Share pipeline code via Git</li> <li>Connect to a shared database server</li> <li>Run <code>populate()</code> from multiple machines simultaneously</li> </ol> <p>See Distributed Computing for multi-process patterns.</p>"},{"location":"explanation/normalization/","title":"Schema Normalization","text":"<p>Schema normalization ensures data integrity by organizing tables to minimize redundancy and prevent update anomalies. DataJoint's workflow-centric approach makes normalization intuitive.</p>"},{"location":"explanation/normalization/#the-workflow-normalization-principle","title":"The Workflow Normalization Principle","text":"<p>\"Every table represents an entity type that is created at a specific step in a workflow, and all attributes describe that entity as it exists at that workflow step.\"</p> <p>This principle naturally leads to well-normalized schemas.</p>"},{"location":"explanation/normalization/#the-intrinsic-attributes-principle","title":"The Intrinsic Attributes Principle","text":"<p>\"Each entity should contain only its intrinsic attributes\u2014properties that are inherent to the entity itself. Relationships, assignments, and events that happen over time belong in separate tables.\"</p> <p>Full workflow entity normalization is achieved when:</p> <ol> <li>Each row represents a single, well-defined entity</li> <li>Each entity is entered once when first tracked</li> <li>Events that happen at later stages belong in separate tables</li> </ol>"},{"location":"explanation/normalization/#why-normalization-matters","title":"Why Normalization Matters","text":"<p>Without normalization, databases suffer from:</p> <ul> <li>Redundancy \u2014 Same information stored multiple times</li> <li>Update anomalies \u2014 Changes require updating multiple rows</li> <li>Insertion anomalies \u2014 Can't add data without unrelated data</li> <li>Deletion anomalies \u2014 Deleting data loses unrelated information</li> </ul>"},{"location":"explanation/normalization/#datajoints-approach","title":"DataJoint's Approach","text":"<p>Traditional normalization analyzes functional dependencies to determine table structure. DataJoint takes a different approach: design tables around workflow steps.</p>"},{"location":"explanation/normalization/#example-mouse-housing","title":"Example: Mouse Housing","text":"<p>Problem: Cage is not intrinsic to a mouse. A mouse's cage can change over time. The cage assignment is an event that happens after the mouse is first tracked.</p> <p>Denormalized (problematic):</p> <pre><code># Wrong: cage info repeated for every mouse\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int32\n    ---\n    cage_id : int32\n    cage_location : varchar(50)    # Redundant!\n    cage_temperature : float32     # Redundant!\n    weight : float32\n    \"\"\"\n</code></pre> <p>Partially normalized (better, but not complete):</p> <pre><code>@schema\nclass Cage(dj.Manual):\n    definition = \"\"\"\n    cage_id : int32\n    ---\n    location : varchar(50)\n    \"\"\"\n\n@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int32\n    ---\n    -&gt; Cage    # Still treats cage as static attribute\n    \"\"\"\n</code></pre> <p>Fully normalized (correct):</p> <pre><code>@schema\nclass Cage(dj.Manual):\n    definition = \"\"\"\n    cage_id : int32\n    ---\n    location : varchar(50)\n    \"\"\"\n\n@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int32\n    ---\n    date_of_birth : date\n    sex : enum('M', 'F')\n    # Note: NO cage reference here!\n    # Cage is not intrinsic to the mouse\n    \"\"\"\n\n@schema\nclass CageAssignment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Mouse\n    assignment_date : date\n    ---\n    -&gt; Cage\n    removal_date=null : date\n    \"\"\"\n\n@schema\nclass MouseWeight(dj.Manual):\n    definition = \"\"\"\n    -&gt; Mouse\n    weigh_date : date\n    ---\n    weight : float32\n    \"\"\"\n</code></pre> <p>This fully normalized design:</p> <ul> <li>Intrinsic attributes only \u2014 <code>Mouse</code> contains only attributes determined at creation (birth date, sex)</li> <li>Cage assignment as event \u2014 <code>CageAssignment</code> tracks the temporal relationship between mice and cages</li> <li>Single entity per row \u2014 Each mouse is entered once when first tracked</li> <li>Later events separate \u2014 Cage assignments, weight measurements happen after initial tracking</li> <li>History preserved \u2014 Can track cage moves over time without data loss</li> </ul>"},{"location":"explanation/normalization/#the-workflow-test","title":"The Workflow Test","text":"<p>Ask these questions to determine table structure:</p>"},{"location":"explanation/normalization/#1-is-this-an-intrinsic-attribute-of-the-entity","title":"1. \"Is this an intrinsic attribute of the entity?\"","text":"<p>An intrinsic attribute is inherent to the entity itself and determined when the entity is first created.</p> <ul> <li>Intrinsic: Mouse's date of birth, sex, genetic strain</li> <li>Not intrinsic: Mouse's cage (assignment that changes), weight (temporal measurement)</li> </ul> <p>If not intrinsic \u2192 separate table for the relationship or event</p>"},{"location":"explanation/normalization/#2-at-which-workflow-step-is-this-attribute-determined","title":"2. \"At which workflow step is this attribute determined?\"","text":"<ul> <li>If an attribute is determined at a different step, it belongs in a different table</li> <li>If an attribute changes over time, it needs its own table with a temporal key</li> </ul>"},{"location":"explanation/normalization/#3-is-this-a-relationship-or-event","title":"3. \"Is this a relationship or event?\"","text":"<ul> <li>Relationships (cage assignment, group membership) \u2192 association table with temporal keys</li> <li>Events (measurements, observations) \u2192 separate table with event date/time</li> <li>States (approval status, processing stage) \u2192 state transition table</li> </ul>"},{"location":"explanation/normalization/#common-patterns","title":"Common Patterns","text":""},{"location":"explanation/normalization/#lookup-tables","title":"Lookup Tables","text":"<p>Store reference data that doesn't change:</p> <pre><code>@schema\nclass Species(dj.Lookup):\n    definition = \"\"\"\n    species : varchar(50)\n    ---\n    common_name : varchar(100)\n    \"\"\"\n    contents = [\n        ('Mus musculus', 'House mouse'),\n        ('Rattus norvegicus', 'Brown rat'),\n    ]\n</code></pre>"},{"location":"explanation/normalization/#parameter-sets","title":"Parameter Sets","text":"<p>Store versioned configurations:</p> <pre><code>@schema\nclass AnalysisParams(dj.Lookup):\n    definition = \"\"\"\n    params_id : int32\n    ---\n    threshold : float32\n    window_size : int32\n    \"\"\"\n</code></pre>"},{"location":"explanation/normalization/#temporal-tracking","title":"Temporal Tracking","text":"<p>Track measurements or observations over time:</p> <pre><code>@schema\nclass SubjectWeight(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    weight_date : date\n    ---\n    weight : float32   # grams\n    \"\"\"\n</code></pre>"},{"location":"explanation/normalization/#temporal-associations","title":"Temporal Associations","text":"<p>Track relationships or assignments that change over time:</p> <pre><code>@schema\nclass GroupAssignment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    assignment_date : date\n    ---\n    -&gt; ExperimentalGroup\n    removal_date=null : date\n    \"\"\"\n\n@schema\nclass HousingAssignment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Animal\n    move_date : date\n    ---\n    -&gt; Cage\n    move_reason : varchar(200)\n    \"\"\"\n</code></pre> <p>Key pattern: The relationship itself (subject-to-group, animal-to-cage) is not intrinsic to either entity. It's a temporal event that happens during the workflow.</p>"},{"location":"explanation/normalization/#benefits-in-datajoint","title":"Benefits in DataJoint","text":"<ol> <li> <p>Natural from workflow thinking \u2014 Designing around workflow steps    naturally produces normalized schemas</p> </li> <li> <p>Cascade deletes \u2014 Normalization + foreign keys enable safe cascade    deletes that maintain consistency</p> </li> <li> <p>Join efficiency \u2014 Normalized tables with proper keys enable efficient    joins through the workflow graph</p> </li> <li> <p>Clear provenance \u2014 Each table represents a distinct workflow step,    making data lineage clear</p> </li> </ol>"},{"location":"explanation/normalization/#summary","title":"Summary","text":"<p>Core principles:</p> <ol> <li>Intrinsic attributes only \u2014 Each entity contains only properties inherent to itself</li> <li>One entity, one entry \u2014 Each entity entered once when first tracked</li> <li>Events separate \u2014 Relationships, assignments, measurements that happen later belong in separate tables</li> <li>Workflow steps \u2014 Design tables around the workflow step that creates each entity</li> <li>Temporal keys \u2014 Relationships and observations that change over time need temporal keys (dates, timestamps)</li> </ol> <p>Ask yourself:</p> <ul> <li>Is this attribute intrinsic to the entity? (No \u2192 separate table)</li> <li>Does this attribute change over time? (Yes \u2192 temporal table)</li> <li>Is this a relationship or event? (Yes \u2192 association/event table)</li> </ul> <p>Following these principles achieves full workflow entity normalization where each table represents a single, well-defined entity type entered at a specific workflow step.</p>"},{"location":"explanation/query-algebra/","title":"Query Algebra","text":"<p>DataJoint provides a powerful query algebra built on five core operators: restriction, join, projection, aggregation, and union. These operators work on entity sets (query expressions) and always return entity sets, enabling arbitrary composition.</p>"},{"location":"explanation/query-algebra/#algebraic-closure","title":"Algebraic Closure","text":"<p>A fundamental property of DataJoint's query algebra is algebraic closure: every query result is itself a valid entity set with a well-defined entity type \u2014 you always know what kind of entity the result represents, identified by a specific primary key. Unlike SQL where query results are unstructured \"bags of rows,\" DataJoint determines the entity type of each result based on the operator and the functional dependencies between operands.</p> <p>This means operators can be chained indefinitely \u2014 the output of any operation is a valid input to any other operation. See Primary Keys for the precise rules.</p>"},{"location":"explanation/query-algebra/#core-operators","title":"Core Operators","text":"<pre><code>graph LR\n    A[Entity Set] --&gt; R[Restriction &amp;]\n    A --&gt; J[Join *]\n    A --&gt; E[Extend .extend]\n    A --&gt; P[Projection .proj]\n    A --&gt; G[Aggregation .aggr]\n    A --&gt; U[Union +]\n    R --&gt; B[Entity Set]\n    J --&gt; B\n    E --&gt; B\n    P --&gt; B\n    G --&gt; B\n    U --&gt; B\n</code></pre>"},{"location":"explanation/query-algebra/#restriction-and-","title":"Restriction (<code>&amp;</code> and <code>-</code>)","text":"<p>Filter entities based on conditions.</p>"},{"location":"explanation/query-algebra/#include","title":"Include (<code>&amp;</code>)","text":"<pre><code># Mice born after 2024\nMouse &amp; \"date_of_birth &gt; '2024-01-01'\"\n\n# Sessions for a specific mouse\nSession &amp; {'mouse_id': 42}\n\n# Sessions matching a query\nSession &amp; (Mouse &amp; \"strain = 'C57BL/6'\")\n</code></pre>"},{"location":"explanation/query-algebra/#exclude-","title":"Exclude (<code>-</code>)","text":"<pre><code># Mice NOT in the study\nMouse - StudyMouse\n\n# Sessions without recordings\nSession - Recording\n</code></pre>"},{"location":"explanation/query-algebra/#top-n-djtop","title":"Top N (<code>dj.Top</code>)","text":"<p>Select a limited number of entities with ordering:</p> <pre><code># Most recent 10 sessions\nSession &amp; dj.Top(10, 'session_date DESC')\n\n# First session by primary key\nSession &amp; dj.Top()\n</code></pre> <p>The <code>order_by</code> parameter accepts attribute names with optional <code>DESC</code>/<code>ASC</code>. The special value <code>\"KEY\"</code> is an alias for all primary key attributes (e.g., <code>\"KEY DESC\"</code> for reverse primary key order).</p>"},{"location":"explanation/query-algebra/#join","title":"Join (<code>*</code>)","text":"<p>Combine entity sets along shared attributes.</p> <pre><code># All session-recording pairs\nSession * Recording\n\n# Chain through workflow\nMouse * Session * Scan * Segmentation\n</code></pre> <p>DataJoint joins are natural joins that:</p> <ul> <li>Match on attributes with the same name and lineage</li> <li>Respect declared dependencies (no accidental matches)</li> <li>Produce the intersection of matching entities</li> </ul>"},{"location":"explanation/query-algebra/#extend-extend","title":"Extend (<code>.extend()</code>)","text":"<p>Add attributes from another entity set while preserving all entities in the original set.</p> <pre><code># Add session info to each trial\nTrial.extend(Session)  # Adds session_date, subject_id to Trial\n\n# Add neuron properties to spike times\nSpikeTime.extend(Neuron)  # Adds cell_type, depth to SpikeTime\n</code></pre> <p>How it differs from join:</p> <ul> <li>Join (<code>*</code>): Returns only matching entities (inner join), primary key is the union of both PKs</li> <li>Extend: Returns all entities from the left side (left join), primary key stays as the left side's PK</li> </ul> <p>Primary key formation:</p> <pre><code># Join: PK is union of both primary keys\nresult = Session * Trial\n# PK: (session_id, trial_num)\n\n# Extend: PK stays as left side's PK\nresult = Trial.extend(Session)\n# PK: (session_id, trial_num) - same as Trial\n# session_date is added as a non-primary attribute\n</code></pre> <p>Requirement: The left side must determine the right side. This means all primary key attributes from the right side must exist in the left side. This requirement ensures:</p> <ol> <li>Every entity in the left side can match at most one entity in the right side</li> <li>The left side's primary key uniquely identifies entities in the result</li> <li>No NULL values appear in the result's primary key</li> </ol> <pre><code># Valid: Trial determines Session\n# (session_id is in Trial's primary key)\nTrial.extend(Session)  \u2713\n# Each trial belongs to exactly one session\n# Result PK: (session_id, trial_num)\n\n# Invalid: Session does NOT determine Trial\n# (trial_num is not in Session)\nSession.extend(Trial)  \u2717  # Error: trial_num not in Session\n# A session has multiple trials - PK would be ambiguous\n</code></pre> <p>Why use extend?</p> <ol> <li>Preserve all entities: When you need attributes from a parent but want to keep all children (even orphans)</li> <li>Clear intent: Expresses \"add attributes\" rather than \"combine entity sets\"</li> <li>No filtering: Guarantees the same number of entities in the result</li> </ol> <p>Think of extend as projection-like: it adds attributes to existing entities without changing which entities are present.</p>"},{"location":"explanation/query-algebra/#projection-proj","title":"Projection (<code>.proj()</code>)","text":"<p>Select and transform attributes.</p>"},{"location":"explanation/query-algebra/#select-attributes","title":"Select attributes","text":"<pre><code># Only mouse_id and strain\nMouse.proj('strain')\n\n# Rename attributes\nMouse.proj(animal_id='mouse_id')\n</code></pre>"},{"location":"explanation/query-algebra/#compute-new-attributes","title":"Compute new attributes","text":"<pre><code># Calculate age\nMouse.proj(\n    age='DATEDIFF(CURDATE(), date_of_birth)'\n)\n\n# Combine attributes\nSession.proj(\n    session_label='CONCAT(subject_id, \"-\", session_date)'\n)\n</code></pre>"},{"location":"explanation/query-algebra/#aggregate-in-projection","title":"Aggregate in projection","text":"<pre><code># Count recordings per session\nSession.aggr(Recording, n_recordings='COUNT(*)')\n</code></pre>"},{"location":"explanation/query-algebra/#aggregation-aggr","title":"Aggregation (<code>.aggr()</code>)","text":"<p>Summarize across groups.</p> <pre><code># Average spike rate per neuron\nNeuron.aggr(\n    SpikeTime,\n    avg_rate='AVG(spike_rate)',\n    total_spikes='COUNT(*)'\n)\n\n# Statistics per session\nSession.aggr(\n    Trial,\n    n_trials='COUNT(*)',\n    success_rate='AVG(success)'\n)\n</code></pre>"},{"location":"explanation/query-algebra/#union","title":"Union (<code>+</code>)","text":"<p>Combine entity sets with the same attributes.</p> <pre><code># All subjects from two studies\nStudyA_Subjects + StudyB_Subjects\n\n# Combine results from different analyses\nAnalysisV1 + AnalysisV2\n</code></pre> <p>Requirements:</p> <ul> <li>Same primary key structure</li> <li>Compatible attribute types</li> </ul>"},{"location":"explanation/query-algebra/#operator-composition","title":"Operator Composition","text":"<p>Operators compose freely:</p> <pre><code># Complex query\nresult = (\n    (Mouse &amp; \"strain = 'C57BL/6'\")  # Filter mice\n    * Session                        # Join sessions\n    * Scan                           # Join scans\n    .proj('scan_date', 'depth')      # Select attributes\n    &amp; 'depth &gt; 200'                  # Filter by depth\n)\n</code></pre>"},{"location":"explanation/query-algebra/#semantic-matching","title":"Semantic Matching","text":"<p>All binary operators in DataJoint rely on semantic matching of attributes. Unlike SQL's natural joins that match on any shared column name, DataJoint verifies that namesake attributes (those with the same name) are homologous\u2014they trace back to the same original definition.</p> <p>This prevents accidental matches on coincidentally-named columns, a pitfall that has been understood since the Entity-Relationship Model was introduced in the 1970s but was never addressed in SQL or traditional RDBMS implementations.</p> <p>See Semantic Matching for details.</p> <p>These concepts were first introduced in Yatsenko et al., 2018<sup>1</sup>.</p>"},{"location":"explanation/query-algebra/#fetching-results","title":"Fetching Results","text":"<p>Query expressions are lazy\u2014they build SQL but don't execute until you fetch:</p> <pre><code># Fetch as NumPy recarray\ndata = query.to_arrays()\n\n# Fetch as list of dicts\ndata = query.to_dicts()\n\n# Fetch as pandas DataFrame\ndf = query.to_pandas()\n\n# Fetch specific attributes\nids, dates = query.to_arrays('mouse_id', 'session_date')\n\n# Fetch single row\nrow = (query &amp; key).fetch1()\n</code></pre>"},{"location":"explanation/query-algebra/#summary","title":"Summary","text":"Operator Symbol/Method Purpose Restriction <code>&amp;</code>, <code>-</code> Filter entities Join <code>*</code> Combine entity sets (inner join) Extend <code>.extend()</code> Add attributes (left join) Projection <code>.proj()</code> Select/transform attributes Aggregation <code>.aggr()</code> Summarize groups Union <code>+</code> Combine parallel sets <p>These core operators, combined with workflow-aware join semantics, provide complete query capability for scientific data pipelines.</p> <ol> <li> <p>Yatsenko D, Walker EY, Tolias AS (2018). DataJoint Elements: Data Workflows for Neurophysiology. arXiv:1807.11104 \u21a9</p> </li> </ol>"},{"location":"explanation/relational-workflow-model/","title":"The Relational Workflow Model","text":"<p>DataJoint implements the Relational Workflow Model\u2014a paradigm that extends relational databases with native support for computational workflows. This model defines a new class of databases called Computational Databases, where computational transformations are first-class citizens of the data model.</p> <p>These concepts, along with DataJoint's schema definition language and query algebra, were first formalized in Yatsenko et al., 2018.</p>"},{"location":"explanation/relational-workflow-model/#the-problem-with-traditional-approaches","title":"The Problem with Traditional Approaches","text":"<p>Traditional relational databases excel at storing and querying data but struggle with computational workflows. They can store inputs and outputs, but:</p> <ul> <li>The database doesn't understand that outputs were computed from inputs</li> <li>It doesn't automatically recompute when inputs change</li> <li>It doesn't track provenance</li> </ul> <p>DataJoint solves these problems by treating your database schema as an executable workflow specification.</p>"},{"location":"explanation/relational-workflow-model/#three-paradigms-compared","title":"Three Paradigms Compared","text":"<p>The relational data model has been interpreted through different conceptual frameworks, each with distinct strengths and limitations:</p> Aspect Mathematical (Codd) Entity-Relationship (Chen) Relational Workflow (DataJoint) Core Question \"What functional dependencies exist?\" \"What entity types exist?\" \"When/how are entities created?\" Time Dimension Not addressed Not central Fundamental Implementation Gap High (abstract to SQL) High (ERM to SQL) None (unified approach) Workflow Support None None Native workflow modeling"},{"location":"explanation/relational-workflow-model/#codds-mathematical-foundation","title":"Codd's Mathematical Foundation","text":"<p>Edgar F. Codd's original relational model is rooted in predicate calculus and set theory. Tables represent logical predicates; rows assert true propositions. While mathematically rigorous, this approach requires abstract reasoning that doesn't map to intuitive domain thinking.</p>"},{"location":"explanation/relational-workflow-model/#chens-entity-relationship-model","title":"Chen's Entity-Relationship Model","text":"<p>Peter Chen's Entity-Relationship Model (ERM) shifted focus to concrete domain modeling\u2014entities and relationships visualized in diagrams. However, ERM:</p> <ul> <li>Creates a gap between conceptual design and SQL implementation</li> <li>Lacks temporal dimension (\"when\" entities are created)</li> <li>Treats relationships as static connections, not dynamic processes</li> </ul>"},{"location":"explanation/relational-workflow-model/#the-relational-workflow-model_1","title":"The Relational Workflow Model","text":"<p>The Relational Workflow Model introduces four fundamental concepts:</p>"},{"location":"explanation/relational-workflow-model/#1-workflow-entities","title":"1. Workflow Entities","text":"<p>Unlike traditional entities that exist independently, workflow entities are artifacts of workflow execution\u2014they represent the products of specific operations. This temporal dimension allows us to understand not just what exists, but when and how it came to exist.</p>"},{"location":"explanation/relational-workflow-model/#2-workflow-dependencies","title":"2. Workflow Dependencies","text":"<p>Workflow dependencies extend foreign keys with operational semantics. They don't just ensure referential integrity\u2014they prescribe the order of operations. Parent entities must be created before child entities.</p> <pre><code>graph LR\n    A[Session] --&gt; B[Scan]\n    B --&gt; C[Segmentation]\n    C --&gt; D[Analysis]\n</code></pre>"},{"location":"explanation/relational-workflow-model/#3-workflow-steps-table-tiers","title":"3. Workflow Steps (Table Tiers)","text":"<p>Each table represents a distinct workflow step with a specific role:</p> <pre><code>graph TD\n    subgraph \"Lookup (Gray)\"\n        L[Parameters]\n    end\n    subgraph \"Manual (Green)\"\n        M[Subject]\n        S[Session]\n    end\n    subgraph \"Imported (Blue)\"\n        I[Recording]\n    end\n    subgraph \"Computed (Red)\"\n        C[Analysis]\n    end\n\n    L --&gt; C\n    M --&gt; S\n    S --&gt; I\n    I --&gt; C\n</code></pre> Tier Role Examples Lookup Reference data, parameters Species, analysis methods Manual Human-entered observations Subjects, sessions Imported Automated data acquisition Recordings, images Computed Derived results Analyses, statistics"},{"location":"explanation/relational-workflow-model/#4-directed-acyclic-graph-dag","title":"4. Directed Acyclic Graph (DAG)","text":"<p>The schema forms a DAG that:</p> <ul> <li>Prohibits circular dependencies</li> <li>Ensures valid execution sequences</li> <li>Enables efficient parallel execution</li> <li>Supports resumable computation</li> </ul>"},{"location":"explanation/relational-workflow-model/#the-workflow-normalization-principle","title":"The Workflow Normalization Principle","text":"<p>\"Every table represents an entity type that is created at a specific step in a workflow, and all attributes describe that entity as it exists at that workflow step.\"</p> <p>This principle extends entity normalization with temporal and operational dimensions.</p>"},{"location":"explanation/relational-workflow-model/#why-this-matters","title":"Why This Matters","text":""},{"location":"explanation/relational-workflow-model/#unified-design-and-implementation","title":"Unified Design and Implementation","text":"<p>Unlike the ERM-SQL gap, DataJoint provides unified:</p> <ul> <li>Diagramming \u2014 Schema diagrams reflect actual structure</li> <li>Definition \u2014 Table definitions are executable code</li> <li>Querying \u2014 Operators understand workflow semantics</li> </ul> <p>No translation needed between conceptual design and implementation.</p>"},{"location":"explanation/relational-workflow-model/#temporal-and-operational-awareness","title":"Temporal and Operational Awareness","text":"<p>The model captures the dynamic nature of workflows:</p> <ul> <li>Data processing sequences</li> <li>Computational dependencies</li> <li>Operation ordering</li> </ul>"},{"location":"explanation/relational-workflow-model/#immutability-and-provenance","title":"Immutability and Provenance","text":"<p>Workflow artifacts are immutable once created:</p> <ul> <li>Preserves execution history</li> <li>Maintains data provenance</li> <li>Enables reproducible science</li> </ul> <p>When you delete upstream data, dependent results cascade-delete automatically. To correct errors, you delete, reinsert, and recompute\u2014ensuring every result represents a consistent computation from valid inputs.</p>"},{"location":"explanation/relational-workflow-model/#workflow-integrity","title":"Workflow Integrity","text":"<p>The DAG structure guarantees:</p> <ul> <li>No circular dependencies</li> <li>Valid operation sequences</li> <li>Enforced temporal order</li> <li>Computational validity</li> </ul>"},{"location":"explanation/relational-workflow-model/#query-algebra-with-workflow-semantics","title":"Query Algebra with Workflow Semantics","text":"<p>DataJoint's five operators provide a complete query algebra:</p> Operator Symbol Purpose Restriction <code>&amp;</code> Filter entities Join <code>*</code> Combine from converging paths Projection <code>.proj()</code> Select/compute attributes Aggregation <code>.aggr()</code> Summarize groups Union <code>+</code> Combine parallel branches <p>These operators:</p> <ul> <li>Take entity sets as input, produce entity sets as output</li> <li>Preserve entity integrity</li> <li>Respect declared dependencies (no ambiguous joins)</li> </ul>"},{"location":"explanation/relational-workflow-model/#from-transactions-to-transformations","title":"From Transactions to Transformations","text":"<p>The Relational Workflow Model represents a conceptual shift:</p> Traditional View Workflow View Tables store data Entity sets are workflow steps Rows are records Entities are execution instances Foreign keys enforce consistency Dependencies specify information flow Updates modify state Computations create new states Schemas organize storage Schemas specify pipelines Queries retrieve data Queries trace provenance <p>This makes DataJoint feel less like a traditional database and more like a workflow engine with persistent state\u2014one that maintains computational validity while supporting scientific flexibility.</p>"},{"location":"explanation/relational-workflow-model/#summary","title":"Summary","text":"<p>The Relational Workflow Model:</p> <ol> <li>Extends relational theory (doesn't replace it)</li> <li>Adds temporal and operational semantics</li> <li>Eliminates the design-implementation gap</li> <li>Enables reproducible computational workflows</li> <li>Maintains mathematical rigor</li> </ol> <p>It's not a departure from relational databases\u2014it's their evolution for computational workflows.</p>"},{"location":"explanation/semantic-matching/","title":"Semantic Matching","text":"<p>Semantic matching ensures that attributes are only matched in joins when they share both the same name and the same lineage (origin). This prevents accidental joins on unrelated attributes that happen to share names.</p>"},{"location":"explanation/semantic-matching/#relationship-to-natural-joins","title":"Relationship to Natural Joins","text":"<p>DataJoint's join operator (<code>*</code>) performs a natural join\u2014the standard relational operation that matches tuples on all common attribute names. If you're familiar with SQL's <code>NATURAL JOIN</code> or relational algebra, DataJoint's join works the same way.</p> <p>Semantic matching adds a safety check on top of the natural join. Before performing the join, DataJoint verifies that all common attributes (namesakes) actually represent the same thing by checking their lineage. If two attributes share a name but have different origins, the join is rejected rather than silently producing incorrect results.</p> <p>In other words: semantic matching is a natural join that rejects common pitfalls of joining on unrelated attributes.</p> <p>Two attributes are homologous if they share the same lineage\u2014that is, they trace back to the same original definition. Attributes with the same name are called namesakes. Semantic matching requires that all namesakes be homologous.</p>"},{"location":"explanation/semantic-matching/#all-binary-operators-use-semantic-matching","title":"All Binary Operators Use Semantic Matching","text":"<p>Semantic matching applies to all binary operators that combine two query expressions, not just join:</p> Operator Syntax Semantic Check Join <code>A * B</code> Namesakes must be homologous Restriction <code>A &amp; B</code> Namesakes must be homologous Anti-restriction <code>A - B</code> Namesakes must be homologous Aggregation <code>A.aggr(B, ...)</code> Namesakes must be homologous Extension <code>A.extend(B)</code> Namesakes must be homologous Union <code>A + B</code> Namesakes must be homologous <p>In each case, DataJoint verifies that any common attribute names between the two operands share the same lineage before proceeding with the operation.</p> <p>These concepts were first introduced in Yatsenko et al., 2018<sup>1</sup>.</p>"},{"location":"explanation/semantic-matching/#why-semantic-matching","title":"Why Semantic Matching?","text":"<p>The natural join is elegant and powerful, but it has a well-known weakness: it relies entirely on naming conventions. If two tables happen to have columns with the same name but different meanings, a natural join silently produces a Cartesian product filtered on unrelated values\u2014a subtle bug that can go undetected.</p> <pre><code># Classic natural join pitfall\nclass Student(dj.Manual):\n    definition = \"\"\"\n    id : int64  # student ID\n    ---\n    name : varchar(100)\n    \"\"\"\n\nclass Course(dj.Manual):\n    definition = \"\"\"\n    id : int64  # course ID\n    ---\n    title : varchar(100)\n    \"\"\"\n\n# Natural join would match on 'id', but these are unrelated!\n# Student #5 paired with Course #5 is meaningless.\n</code></pre> <p>DataJoint's semantic matching solves this by tracking the lineage of each attribute\u2014where it was originally defined. Attributes only match if they have the same lineage, ensuring that joins always combine semantically related data.</p>"},{"location":"explanation/semantic-matching/#attribute-lineage","title":"Attribute Lineage","text":"<p>Lineage identifies the origin of an attribute\u2014the dimension where it was first defined. A dimension is an independent axis of variation introduced by a table that defines new primary key attributes. See Schema Dimensions for details.</p> <p>Lineage is represented as a string:</p> <pre><code>schema_name.table_name.attribute_name\n</code></pre>"},{"location":"explanation/semantic-matching/#lineage-assignment-rules","title":"Lineage Assignment Rules","text":"Attribute Type Lineage Value Native primary key <code>this_schema.this_table.attr_name</code> FK-inherited (primary or secondary) Traced to original definition Native secondary <code>None</code> Computed (in projection) <code>None</code>"},{"location":"explanation/semantic-matching/#example","title":"Example","text":"<pre><code>class Session(dj.Manual):         # table: session\n    definition = \"\"\"\n    session_id : int64\n    ---\n    session_date : date\n    \"\"\"\n\nclass Trial(dj.Manual):           # table: trial\n    definition = \"\"\"\n    -&gt; Session\n    trial_num : int32\n    ---\n    stimulus : varchar(100)\n    \"\"\"\n</code></pre> <p>Lineages:</p> <ul> <li><code>Session.session_id</code> \u2192 <code>myschema.session.session_id</code> (native PK)</li> <li><code>Session.session_date</code> \u2192 <code>None</code> (native secondary)</li> <li><code>Trial.session_id</code> \u2192 <code>myschema.session.session_id</code> (inherited via FK)</li> <li><code>Trial.trial_num</code> \u2192 <code>myschema.trial.trial_num</code> (native PK)</li> <li><code>Trial.stimulus</code> \u2192 <code>None</code> (native secondary)</li> </ul> <p>Notice that <code>Trial.session_id</code> has the same lineage as <code>Session.session_id</code> because it was inherited through the foreign key reference. This allows <code>Session * Trial</code> to work correctly\u2014both <code>session_id</code> attributes are homologous.</p>"},{"location":"explanation/semantic-matching/#terminology","title":"Terminology","text":"Term Definition Lineage The origin of an attribute: <code>schema.table.attribute</code> Homologous attributes Attributes with the same lineage Namesake attributes Attributes with the same name Homologous namesakes Same name AND same lineage \u2014 used for join matching Non-homologous namesakes Same name BUT different lineage \u2014 cause join errors"},{"location":"explanation/semantic-matching/#semantic-matching-rules","title":"Semantic Matching Rules","text":"<p>When two expressions are joined, DataJoint checks all namesake attributes (attributes with the same name):</p> Scenario Action Same name, same lineage (both non-null) Match \u2014 attributes are joined Same name, different lineage Error \u2014 non-homologous namesakes Same name, either lineage is null Error \u2014 cannot verify homology Different names No match \u2014 attributes kept separate"},{"location":"explanation/semantic-matching/#when-semantic-matching-fails","title":"When Semantic Matching Fails","text":"<p>If you see an error like:</p> <pre><code>DataJointError: Cannot join on attribute `id`: different lineages\n(university.student.id vs university.course.id).\nUse .proj() to rename one of the attributes.\n</code></pre> <p>This means you're trying to join tables that have a namesake attribute (<code>id</code>) with different lineages. The solutions are:</p> <ol> <li> <p>Rename one attribute using projection:    <pre><code>Student() * Course().proj(course_id='id')\n</code></pre></p> </li> <li> <p>Bypass semantic checking (use with caution):    <pre><code>Student().join(Course(), semantic_check=False)\n</code></pre></p> </li> <li> <p>Use descriptive names in your schema design (best practice):    <pre><code>class Student(dj.Manual):\n    definition = \"\"\"\n    student_id : int64  # not just 'id'\n    ---\n    name : varchar(100)\n    \"\"\"\n</code></pre></p> </li> </ol>"},{"location":"explanation/semantic-matching/#examples","title":"Examples","text":""},{"location":"explanation/semantic-matching/#valid-join-shared-lineage","title":"Valid Join (Shared Lineage)","text":"<pre><code>class Student(dj.Manual):\n    definition = \"\"\"\n    student_id : int64\n    ---\n    name : varchar(100)\n    \"\"\"\n\nclass Enrollment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Student\n    -&gt; Course\n    ---\n    grade : varchar(2)\n    \"\"\"\n\n# Works: student_id has same lineage in both\nStudent() * Enrollment()\n</code></pre>"},{"location":"explanation/semantic-matching/#multi-hop-fk-inheritance","title":"Multi-hop FK Inheritance","text":"<p>Lineage is preserved through multiple levels of foreign key inheritance:</p> <pre><code>class Session(dj.Manual):\n    definition = \"\"\"\n    session_id : int64\n    ---\n    session_date : date\n    \"\"\"\n\nclass Trial(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    trial_num : int32\n    \"\"\"\n\nclass Response(dj.Computed):\n    definition = \"\"\"\n    -&gt; Trial\n    ---\n    response_time : float64\n    \"\"\"\n\n# All work: session_id traces back to Session in all tables\nSession() * Trial()\nSession() * Response()\nTrial() * Response()\n</code></pre>"},{"location":"explanation/semantic-matching/#secondary-fk-attribute","title":"Secondary FK Attribute","text":"<p>Lineage works for secondary (non-primary-key) foreign key attributes too:</p> <pre><code>class Course(dj.Manual):\n    definition = \"\"\"\n    course_id : int unsigned\n    ---\n    title : varchar(100)\n    \"\"\"\n\nclass FavoriteCourse(dj.Manual):\n    definition = \"\"\"\n    student_id : int unsigned\n    ---\n    -&gt; Course\n    \"\"\"\n\nclass RequiredCourse(dj.Manual):\n    definition = \"\"\"\n    major_id : int unsigned\n    ---\n    -&gt; Course\n    \"\"\"\n\n# Works: course_id is secondary in both, but has same lineage\nFavoriteCourse() * RequiredCourse()\n</code></pre>"},{"location":"explanation/semantic-matching/#aliased-foreign-key","title":"Aliased Foreign Key","text":"<p>When you alias a foreign key, the new name gets the same lineage as the original:</p> <pre><code>class Person(dj.Manual):\n    definition = \"\"\"\n    person_id : int unsigned\n    ---\n    full_name : varchar(100)\n    \"\"\"\n\nclass Marriage(dj.Manual):\n    definition = \"\"\"\n    -&gt; Person.proj(husband='person_id')\n    -&gt; Person.proj(wife='person_id')\n    ---\n    marriage_date : date\n    \"\"\"\n\n# husband and wife both have lineage: schema.person.person_id\n# They are homologous (same lineage) but have different names\n</code></pre>"},{"location":"explanation/semantic-matching/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use descriptive attribute names: Prefer <code>student_id</code> over generic <code>id</code></p> </li> <li> <p>Leverage foreign keys: Inherited attributes maintain lineage automatically</p> </li> <li> <p>Rebuild lineage for legacy schemas: Run <code>schema.rebuild_lineage()</code> once</p> </li> <li> <p>Rebuild upstream schemas first: For cross-schema FKs, rebuild parent schemas before child schemas</p> </li> <li> <p>Restart after rebuilding: Restart Python kernel to pick up new lineage information</p> </li> <li> <p>Use <code>semantic_check=False</code> sparingly: Only when you're certain the natural join is correct</p> </li> </ol>"},{"location":"explanation/semantic-matching/#design-rationale","title":"Design Rationale","text":"<p>Semantic matching reflects a core DataJoint principle: schema design should encode meaning. When you create a foreign key reference, you're declaring that two attributes represent the same concept. DataJoint tracks this through lineage, allowing safe joins without relying on naming conventions alone.</p> <p>This is especially valuable in large, collaborative projects where different teams might independently choose similar attribute names for unrelated concepts.</p> <ol> <li> <p>Yatsenko D, Walker EY, Tolias AS (2018). DataJoint: A Simpler Relational Data Model. arXiv:1807.11104 \u21a9</p> </li> </ol>"},{"location":"explanation/type-system/","title":"Type System","text":"<p>DataJoint's type system provides a three-layer architecture that balances database efficiency with Python convenience.</p> <p>New in 2.1</p> <p>PostgreSQL support was added in DataJoint 2.1. Core types work identically on both MySQL and PostgreSQL backends.</p>"},{"location":"explanation/type-system/#three-layer-architecture","title":"Three-Layer Architecture","text":"<pre><code>graph TB\n    subgraph \"Layer 3: Codecs\"\n        blob[\"\u2039blob\u203a\"]\n        blob_at[\"\u2039blob@\u203a\"]\n        attach[\"\u2039attach\u203a\"]\n        attach_at[\"\u2039attach@\u203a\"]\n        npy[\"\u2039npy@\u203a\"]\n        object[\"\u2039object@\u203a\"]\n        filepath[\"\u2039filepath@\u203a\"]\n        hash[\"\u2039hash@\u203a\"]\n        plugin[\"\u2039plugin\u203a\"]\n    end\n    subgraph \"Layer 2: Core Types\"\n        int32\n        float64\n        varchar\n        json\n        bytes\n        uuid\n    end\n    subgraph \"Layer 1: Native Types (MySQL / PostgreSQL)\"\n        INT[\"INT / INTEGER\"]\n        DOUBLE[\"DOUBLE / DOUBLE PRECISION\"]\n        VARCHAR_N[\"VARCHAR\"]\n        JSON_N[\"JSON\"]\n        BYTES_N[\"LONGBLOB / BYTEA\"]\n        UUID_N[\"BINARY(16) / UUID\"]\n    end\n\n    blob --&gt; bytes\n    blob_at --&gt; hash\n    attach --&gt; bytes\n    attach_at --&gt; hash\n    hash --&gt; json\n    npy --&gt; json\n    object --&gt; json\n    filepath --&gt; json\n\n    bytes --&gt; BYTES_N\n    json --&gt; JSON_N\n    int32 --&gt; INT\n    float64 --&gt; DOUBLE\n    varchar --&gt; VARCHAR_N\n    uuid --&gt; UUID_N\n</code></pre> <p>Core types provide portability \u2014 the same table definition works on both MySQL and PostgreSQL. For example, <code>bytes</code> maps to <code>LONGBLOB</code> on MySQL but <code>BYTEA</code> on PostgreSQL; <code>uuid</code> maps to <code>BINARY(16)</code> on MySQL but native <code>UUID</code> on PostgreSQL. Native types can be used directly but sacrifice cross-backend compatibility.</p>"},{"location":"explanation/type-system/#layer-1-native-database-types","title":"Layer 1: Native Database Types","text":"<p>Backend-specific types. Can be used directly at the cost of portability.</p> <pre><code># Native types \u2014 work but not portable\ncolumn : TINYINT UNSIGNED   # MySQL only\ncolumn : MEDIUMBLOB         # MySQL only (use BYTEA on PostgreSQL)\ncolumn : SERIAL             # PostgreSQL only\n</code></pre> MySQL PostgreSQL Portable Alternative <code>LONGBLOB</code> <code>BYTEA</code> <code>bytes</code> <code>BINARY(16)</code> <code>UUID</code> <code>uuid</code> <code>SMALLINT</code> <code>SMALLINT</code> <code>int16</code> <code>DOUBLE</code> <code>DOUBLE PRECISION</code> <code>float64</code>"},{"location":"explanation/type-system/#layer-2-core-datajoint-types","title":"Layer 2: Core DataJoint Types","text":"<p>Standardized, scientist-friendly types that work identically across backends.</p>"},{"location":"explanation/type-system/#numeric-types","title":"Numeric Types","text":"Type Description Range <code>int8</code> 8-bit signed -128 to 127 <code>int16</code> 16-bit signed -32,768 to 32,767 <code>int32</code> 32-bit signed \u00b12 billion <code>int64</code> 64-bit signed \u00b19 quintillion <code>float32</code> 32-bit float ~7 significant digits <code>float64</code> 64-bit float ~15 significant digits <code>decimal(n,f)</code> Fixed-point Exact decimal <p>Unsigned integer types are not provided. Choose a signed type with sufficient range for your data.</p>"},{"location":"explanation/type-system/#string-types","title":"String Types","text":"Type Description <code>char(n)</code> Fixed-length string <code>varchar(n)</code> Variable-length string <code>enum(...)</code> Enumeration of string labels"},{"location":"explanation/type-system/#other-types","title":"Other Types","text":"Type Description <code>bool</code> True/False <code>date</code> Date only <code>datetime</code> Date and time (UTC) <code>json</code> JSON document <code>uuid</code> Universally unique identifier <code>bytes</code> Raw binary"},{"location":"explanation/type-system/#layer-3-codec-types","title":"Layer 3: Codec Types","text":"<p>Codecs provide <code>encode()</code>/<code>decode()</code> semantics for complex Python objects.</p>"},{"location":"explanation/type-system/#syntax","title":"Syntax","text":"<p>Codec types use angle bracket notation:</p> <ul> <li>Angle brackets: <code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>, <code>&lt;object@store&gt;</code></li> <li><code>@</code> indicates object store: <code>&lt;blob@&gt;</code> stores in the object store</li> <li>Store name: <code>&lt;blob@cold&gt;</code> uses named store \"cold\"</li> </ul>"},{"location":"explanation/type-system/#built-in-codecs","title":"Built-in Codecs","text":"Codec Database Object Store Addressing Returns <code>&lt;blob&gt;</code> \u2705 \u2705 <code>&lt;blob@&gt;</code> Hash Python object <code>&lt;attach&gt;</code> \u2705 \u2705 <code>&lt;attach@&gt;</code> Hash Local file path <code>&lt;npy@&gt;</code> \u274c \u2705 Schema NpyRef (lazy) <code>&lt;object@&gt;</code> \u274c \u2705 Schema ObjectRef <code>&lt;hash@&gt;</code> \u274c \u2705 Hash bytes <code>&lt;filepath@&gt;</code> \u274c \u2705 User-managed ObjectRef"},{"location":"explanation/type-system/#plugin-codecs","title":"Plugin Codecs","text":"<p>Additional schema-addressed codecs are available as separately installed packages. This ecosystem is actively expanding\u2014new codecs are added as community needs arise.</p> Package Codec Description Repository <code>dj-zarr-codecs</code> <code>&lt;zarr@&gt;</code> Schema-addressed Zarr arrays with lazy chunked access datajoint/dj-zarr-codecs <code>dj-figpack-codecs</code> <code>&lt;figpack@&gt;</code> Schema-addressed interactive browser visualizations datajoint/dj-figpack-codecs <code>dj-photon-codecs</code> <code>&lt;photon@&gt;</code> Schema-addressed photon imaging data formats datajoint/dj-photon-codecs <p>Installation and discovery:</p> <p>Plugin codecs use Python's entry point mechanism for automatic registration. Install the package and DataJoint discovers the codec automatically:</p> <pre><code>pip install dj-zarr-codecs\n</code></pre> <pre><code>import datajoint as dj\n\n# Codec is available immediately after install\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Recording\n    ---\n    data : &lt;zarr@store&gt;\n    \"\"\"\n</code></pre> <p>Packages declare their codecs in <code>pyproject.toml</code> under the <code>datajoint.codecs</code> entry point group:</p> <pre><code>[project.entry-points.\"datajoint.codecs\"]\nzarr = \"dj_zarr_codecs:ZarrCodec\"\n</code></pre> <p>DataJoint loads these entry points on first use, making third-party codecs indistinguishable from built-ins.</p>"},{"location":"explanation/type-system/#blob-serialized-python-objects","title":"<code>&lt;blob&gt;</code> \u2014 Serialized Python Objects","text":"<p>Stores NumPy arrays, dicts, lists, and other Python objects using DataJoint's custom binary serialization format. The blob type has been in continuous use in DataJoint pipelines for 15+ years and maintains full backward compatibility. It provides an efficient way to serialize complex objects into an opaque binary string.</p> <p>Modern alternatives</p> <p>Schema-addressed codecs introduced in DataJoint 2.0 (<code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>, and plugin codecs) offer modern high-performance accessibility with transparent formats, lazy loading, and browsable storage paths\u2014while maintaining rigorous data integrity and consistency. Consider these for new pipelines where interoperability and direct data access are priorities.</p> <p>Serialization format:</p> <ul> <li>Protocol headers:<ul> <li><code>mYm</code> \u2014 MATLAB-compatible format (see mYm on MATLAB FileExchange and mym on GitHub)</li> <li><code>dj0</code> \u2014 Python-extended format supporting additional types</li> </ul> </li> <li>Optional compression: zlib compression for data &gt; 1KB</li> <li>Type-specific encoding: Each Python type has a specific serialization code</li> <li>Version detection: Protocol header embedded in blob enables format detection</li> </ul> <p>Supported types:</p> <ul> <li>NumPy arrays (numeric, structured, recarrays)</li> <li>Collections (dict, list, tuple, set)</li> <li>Scalars (int, float, bool, complex, str, bytes)</li> <li>Date/time objects (datetime, date, time)</li> <li>UUID, Decimal</li> </ul> <pre><code>class Results(dj.Computed):\n    definition = \"\"\"\n    -&gt; Analysis\n    ---\n    spike_times : &lt;blob&gt;        # In database\n    waveforms : &lt;blob@&gt;         # Object store, default store\n    raw_data : &lt;blob@archive&gt;   # Object store, 'archive' store\n    \"\"\"\n</code></pre> <p>Storage modes:</p> <ul> <li><code>&lt;blob&gt;</code> \u2014 Stored in database as LONGBLOB (up to ~1GB depending on MySQL config)</li> <li><code>&lt;blob@&gt;</code> \u2014 Stored in object store via <code>&lt;hash@&gt;</code> with MD5 deduplication</li> </ul>"},{"location":"explanation/type-system/#attach-file-attachments","title":"<code>&lt;attach&gt;</code> \u2014 File Attachments","text":"<p>Stores files with filename preserved.</p> <pre><code>class Config(dj.Manual):\n    definition = \"\"\"\n    config_id : int\n    ---\n    settings : &lt;attach&gt;         # Small config file in database\n    data_file : &lt;attach@&gt;       # Large file in object store\n    \"\"\"\n</code></pre>"},{"location":"explanation/type-system/#npy-numpy-arrays-as-npy-files","title":"<code>&lt;npy@&gt;</code> \u2014 NumPy Arrays as .npy Files","text":"<p>Schema-addressed storage for NumPy arrays as standard <code>.npy</code> files. Returns <code>NpyRef</code> which provides metadata access (shape, dtype) without downloading.</p> <pre><code>class Recording(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    waveform : &lt;npy@&gt;           # Default store\n    spectrogram : &lt;npy@archive&gt; # Named store\n    \"\"\"\n</code></pre> <p>Lazy access:</p> <pre><code>ref = (Recording &amp; key).fetch1('waveform')\nref.shape   # (1000, 32) \u2014 no download\nref.dtype   # float64 \u2014 no download\n\n# Explicit load\narr = ref.load()\n\n# Transparent numpy integration\nresult = np.mean(ref)  # Downloads automatically\n</code></pre> <p>Key features:</p> <ul> <li>Portable format: Standard <code>.npy</code> readable by NumPy, MATLAB, etc.</li> <li>Lazy loading: Shape/dtype available without I/O</li> <li>Safe bulk fetch: Fetching many rows doesn't download until needed</li> <li>Memory mapping: <code>ref.load(mmap_mode='r')</code> for random access to large arrays</li> </ul>"},{"location":"explanation/type-system/#object-schema-addressed-storage","title":"<code>&lt;object@&gt;</code> \u2014 Schema-Addressed Storage","text":"<p>Schema-addressed storage for files and folders. Path mirrors the database structure: <code>{schema}/{table}/{pk}/{attribute}</code>.</p> <pre><code>class ProcessedData(dj.Computed):\n    definition = \"\"\"\n    -&gt; Recording\n    ---\n    results : &lt;object@&gt;       # Stored at {schema}/{table}/{pk}/results/\n    \"\"\"\n</code></pre> <p>Accepts files, folders, or bytes. Returns <code>ObjectRef</code> for lazy access.</p>"},{"location":"explanation/type-system/#filepathstore-portable-references","title":"<code>&lt;filepath@store&gt;</code> \u2014 Portable References","text":"<p>References to independently-managed files with portable paths.</p> <pre><code>class RawData(dj.Manual):\n    definition = \"\"\"\n    session_id : int\n    ---\n    recording : &lt;filepath@raw&gt;  # Relative to 'raw' store\n    \"\"\"\n</code></pre>"},{"location":"explanation/type-system/#storage-modes","title":"Storage Modes","text":"<p>Object store codecs use one of two addressing schemes:</p> <p>Hash-addressed \u2014 Path derived from content hash (e.g., <code>_hash/ab/cd/abcd1234...</code>). Provides automatic deduplication\u2014identical content stored once. Used by <code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;hash@&gt;</code>.</p> <p>Schema-addressed \u2014 Path mirrors database structure: <code>{schema}/{table}/{pk}/{attribute}</code>. Human-readable, browsable paths that reflect your data organization. No deduplication. Used by <code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>, and plugin codecs (<code>&lt;zarr@&gt;</code>, <code>&lt;figpack@&gt;</code>, <code>&lt;photon@&gt;</code>).</p> Mode Database Object Store Deduplication Use Case Database Data \u2014 \u2014 Small data Hash-addressed Metadata Content hash path \u2705 Automatic Large/repeated data Schema-addressed Metadata Schema-mirrored path \u274c None Complex files, browsable storage"},{"location":"explanation/type-system/#custom-codecs","title":"Custom Codecs","text":"<p>Extend the type system for domain-specific data:</p> <pre><code>class GraphCodec(dj.Codec):\n    \"\"\"Store NetworkX graphs.\"\"\"\n    name = \"graph\"\n\n    def get_dtype(self, is_store):\n        return \"&lt;blob&gt;\"\n\n    def encode(self, graph, *, key=None, store_name=None):\n        return {\n            'nodes': list(graph.nodes()),\n            'edges': list(graph.edges())\n        }\n\n    def decode(self, stored, *, key=None):\n        import networkx as nx\n        G = nx.Graph()\n        G.add_nodes_from(stored['nodes'])\n        G.add_edges_from(stored['edges'])\n        return G\n</code></pre> <p>Usage:</p> <pre><code>class Network(dj.Computed):\n    definition = \"\"\"\n    -&gt; Analysis\n    ---\n    connectivity : &lt;graph&gt;\n    \"\"\"\n</code></pre>"},{"location":"explanation/type-system/#choosing-types","title":"Choosing Types","text":"Data Recommended Type Small scalars Core types (<code>int32</code>, <code>float64</code>) Short strings <code>varchar(n)</code> NumPy arrays (small) <code>&lt;blob&gt;</code> NumPy arrays (large) <code>&lt;npy@&gt;</code> or <code>&lt;blob@&gt;</code> Files to attach <code>&lt;attach&gt;</code> or <code>&lt;attach@&gt;</code> Zarr arrays <code>&lt;zarr@&gt;</code> (plugin) Complex file structures <code>&lt;object@&gt;</code> Interactive visualizations <code>&lt;figpack@&gt;</code> (plugin) File references (in-store) <code>&lt;filepath@store&gt;</code> Custom objects Custom codec"},{"location":"explanation/type-system/#summary","title":"Summary","text":"<ol> <li>Core types for simple data \u2014 <code>int32</code>, <code>varchar</code>, <code>datetime</code></li> <li><code>&lt;blob&gt;</code> for Python objects \u2014 NumPy arrays, dicts</li> <li><code>@</code> suffix for object store \u2014 <code>&lt;blob@&gt;</code>, <code>&lt;object@&gt;</code></li> <li>Custom codecs for domain-specific types</li> </ol>"},{"location":"explanation/type-system/#see-also","title":"See Also","text":"<p>How-to Guides:</p> <ul> <li>Choose a Storage Type \u2014 Decision guide for selecting the right type</li> <li>Configure Object Storage \u2014 Setting up stores for external data</li> <li>Use Object Storage \u2014 Working with <code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;object@&gt;</code></li> <li>Use the npy Codec \u2014 Storing NumPy arrays as <code>.npy</code> files</li> <li>Use Plugin Codecs \u2014 Installing and using third-party codecs</li> <li>Create a Custom Codec \u2014 Building your own codec</li> </ul> <p>Reference:</p> <ul> <li>Type System Specification \u2014 Complete type reference</li> <li>Codec API \u2014 Codec class interface</li> <li>npy Codec Specification \u2014 NpyRef and NpyCodec details</li> <li>Object Store Configuration \u2014 Store settings reference</li> </ul>"},{"location":"explanation/whats-new-2/","title":"What's New in DataJoint 2.0","text":"<p>DataJoint 2.0 is a major release that establishes DataJoint as a mature framework for scientific data pipelines. The version jump from 0.14 to 2.0 reflects the significance of these changes.</p> <p>\ud83d\udcd8 Upgrading from legacy DataJoint (pre-2.0)?</p> <p>This page summarizes new features and concepts. For step-by-step migration instructions, see the Migration Guide.</p>"},{"location":"explanation/whats-new-2/#overview","title":"Overview","text":"<p>DataJoint 2.0 introduces fundamental improvements to type handling, job coordination, and object storage while maintaining compatibility with your existing pipelines during migration. Key themes:</p> <ul> <li>Explicit over implicit: All type conversions are now explicit through the codec system</li> <li>Better distributed computing: Per-table job coordination with improved error handling</li> <li>Object storage integration: Native support for large arrays and files</li> <li>Future-proof architecture: Portable types preparing for PostgreSQL backend support</li> </ul>"},{"location":"explanation/whats-new-2/#breaking-changes-at-a-glance","title":"Breaking Changes at a Glance","text":"<p>If you're upgrading from legacy DataJoint, these changes require code updates:</p> Area Legacy 2.0 Fetch API <code>table.fetch()</code> <code>table.to_dicts()</code> or <code>.to_arrays()</code> Update <code>(table &amp; key)._update('attr', val)</code> <code>table.update1({**key, 'attr': val})</code> Join <code>table1 @ table2</code> <code>table1 * table2</code> (with semantic check) Type syntax <code>longblob</code>, <code>int unsigned</code> <code>&lt;blob&gt;</code>, <code>int64</code> Jobs <code>~jobs</code> table Per-table <code>~~table_name</code> <p>See the Migration Guide for complete upgrade steps.</p>"},{"location":"explanation/whats-new-2/#object-augmented-schema-oas","title":"Object-Augmented Schema (OAS)","text":"<p>DataJoint 2.0 unifies relational tables with object storage into a single coherent system. The relational database stores metadata and references while large objects (arrays, files, Zarr datasets) are stored in object storage\u2014with full referential integrity maintained across both layers.</p> <p>\u2192 Type System Specification</p> <p>Three storage sections:</p> Section Addressing Use Case Internal Row-based (in database) Small objects (&lt; 1 MB) Hash-addressed Content hash Arrays, files (deduplication) Path-addressed Primary key path Zarr, HDF5, streaming access <p>New syntax:</p> <pre><code>definition = \"\"\"\nrecording_id : uuid\n---\nmetadata : &lt;blob&gt;           # Internal storage\nraw_data : &lt;blob@store&gt;     # Hash-addressed object storage\nzarr_array : &lt;object@store&gt; # Path-addressed for Zarr/HDF5\n\"\"\"\n</code></pre>"},{"location":"explanation/whats-new-2/#explicit-type-system","title":"Explicit Type System","text":"<p>Breaking change: DataJoint 2.0 makes all type conversions explicit through a three-tier architecture.</p> <p>\u2192 Type System Specification \u00b7 Codec API Specification</p>"},{"location":"explanation/whats-new-2/#what-changed","title":"What Changed","text":"<p>Legacy DataJoint overloaded MySQL types with implicit conversions: - <code>longblob</code> could be blob serialization OR in-table attachment - <code>attach</code> was implicitly converted to longblob - <code>uuid</code> was used internally for external storage</p> <p>DataJoint 2.0 makes everything explicit:</p> Legacy (Implicit) 2.0 (Explicit) <code>longblob</code> <code>&lt;blob&gt;</code> <code>attach</code> <code>&lt;attach&gt;</code> <code>blob@store</code> <code>&lt;blob@store&gt;</code> <code>int unsigned</code> <code>int64</code>"},{"location":"explanation/whats-new-2/#three-tier-architecture","title":"Three-Tier Architecture","text":"<ol> <li>Native types: MySQL types (<code>INT</code>, <code>VARCHAR</code>, <code>LONGBLOB</code>)</li> <li>Core types: Portable aliases (<code>int32</code>, <code>float64</code>, <code>varchar</code>, <code>uuid</code>, <code>json</code>)</li> <li>Codecs: Serialization for Python objects (<code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>, <code>&lt;object@&gt;</code>)</li> </ol>"},{"location":"explanation/whats-new-2/#custom-codecs","title":"Custom Codecs","text":"<p>Replace legacy AdaptedTypes with the new codec API:</p> <pre><code>class GraphCodec(dj.Codec):\n    name = \"graph\"\n\n    def encode(self, value, **kwargs):\n        return list(value.edges)\n\n    def decode(self, stored, **kwargs):\n        import networkx as nx\n        return nx.Graph(stored)\n</code></pre>"},{"location":"explanation/whats-new-2/#jobs-20","title":"Jobs 2.0","text":"<p>Breaking change: Redesigned job coordination with per-table job management.</p> <p>\u2192 AutoPopulate Specification \u00b7 Job Metadata Specification</p>"},{"location":"explanation/whats-new-2/#what-changed_1","title":"What Changed","text":"Legacy (Schema-level) 2.0 (Per-table) One <code>~jobs</code> table per schema One <code>~~table_name</code> per Computed/Imported table Opaque hashed keys Native primary keys (readable) Statuses: <code>reserved</code>, <code>error</code>, <code>ignore</code> Added: <code>pending</code>, <code>success</code> No priority support Priority column (lower = more urgent)"},{"location":"explanation/whats-new-2/#new-features","title":"New Features","text":"<ul> <li>Automatic refresh: Job queue synchronized with pending work automatically</li> <li>Better coordination: Multiple workers coordinate via database without conflicts</li> <li>Error tracking: Built-in error table (<code>Table.jobs.errors</code>) with full stack traces</li> <li>Priority support: Control computation order with priority values</li> </ul> <pre><code># Distributed mode with coordination\nAnalysis.populate(reserve_jobs=True, processes=4)\n\n# Monitor progress\nAnalysis.jobs.progress()  # {'pending': 10, 'reserved': 2, 'error': 0}\n\n# Handle errors\nAnalysis.jobs.errors.to_dicts()\n\n# Set priorities\nAnalysis.jobs.update({'session_id': 123}, priority=1)  # High priority\n</code></pre>"},{"location":"explanation/whats-new-2/#semantic-matching","title":"Semantic Matching","text":"<p>Breaking change: Query operations now use lineage-based matching by default.</p> <p>\u2192 Semantic Matching Specification</p>"},{"location":"explanation/whats-new-2/#what-changed_2","title":"What Changed","text":"<p>Legacy DataJoint used SQL-style natural joins: attributes matched if they had the same name, regardless of meaning.</p> <p>DataJoint 2.0 validates semantic lineage: Attributes must share common origin through foreign key chains, not just coincidentally matching names.</p> <pre><code># 2.0: Semantic join (default) - validates lineage\nresult = TableA * TableB  # Only matches attributes with shared origin\n\n# Legacy behavior (if needed)\nresult = TableA.join(TableB, semantic_check=False)\n</code></pre> <p>Why this matters: Prevents accidental matches between attributes like <code>session_id</code> that happen to share a name but refer to different entities in different parts of your schema.</p> <p>During migration: If semantic matching fails, it often indicates a malformed join that should be reviewed rather than forced.</p>"},{"location":"explanation/whats-new-2/#configuration-system","title":"Configuration System","text":"<p>A cleaner configuration approach with separation of concerns.</p> <p>\u2192 Configuration Reference</p> <ul> <li><code>datajoint.json</code>: Non-sensitive settings (commit to version control)</li> <li><code>.secrets/</code>: Credentials (never commit)</li> <li>Environment variables: For CI/CD and production</li> </ul> <pre><code>export DJ_HOST=db.example.com\nexport DJ_USER=myuser\nexport DJ_PASS=mypassword\n</code></pre>"},{"location":"explanation/whats-new-2/#objectref-api-new","title":"ObjectRef API (New)","text":"<p>New feature: Path-addressed storage returns <code>ObjectRef</code> handles that support streaming access without downloading entire datasets.</p> <pre><code>ref = (Dataset &amp; key).fetch1('zarr_array')\n\n# Direct fsspec access for Zarr/xarray\nz = zarr.open(ref.fsmap, mode='r')\n\n# Or download locally\nlocal_path = ref.download('/tmp/data')\n\n# Stream chunks without full download\nwith ref.open('rb') as f:\n    chunk = f.read(1024)\n</code></pre> <p>This enables efficient access to large datasets stored in Zarr, HDF5, or custom formats.</p>"},{"location":"explanation/whats-new-2/#deprecated-and-removed","title":"Deprecated and Removed","text":""},{"location":"explanation/whats-new-2/#removed-apis","title":"Removed APIs","text":"<ul> <li><code>.fetch()</code> method: Replaced with <code>.to_dicts()</code>, <code>.to_arrays()</code>, or <code>.to_pandas()</code></li> <li><code>._update()</code> method: Replaced with <code>.update1()</code></li> <li><code>@</code> operator (natural join): Use <code>*</code> with semantic matching or <code>.join(semantic_check=False)</code></li> <li><code>dj.U() * table</code> pattern: Use just <code>table</code> (universal set is implicit)</li> </ul>"},{"location":"explanation/whats-new-2/#deprecated-features","title":"Deprecated Features","text":"<ul> <li>AdaptedTypes: Replaced by codec system (still works but migration recommended)</li> <li>Native type syntax: <code>int unsigned</code> \u2192 <code>int64</code> (warnings on new tables)</li> <li>Legacy external storage (<code>blob@store</code>): Replaced by <code>&lt;blob@store&gt;</code> codec syntax</li> </ul>"},{"location":"explanation/whats-new-2/#legacy-support","title":"Legacy Support","text":"<p>During migration (Phases 1-3), both legacy and 2.0 APIs can coexist: - Legacy clients can still access data - 2.0 clients understand legacy column types - Dual attributes enable cross-testing</p> <p>After finalization (Phase 4+), only 2.0 clients are supported.</p>"},{"location":"explanation/whats-new-2/#license-change","title":"License Change","text":"<p>DataJoint 2.0 is licensed under the Apache License 2.0 (previously LGPL-2.1). This provides:</p> <ul> <li>More permissive for commercial and academic use</li> <li>Clearer patent grant provisions</li> <li>Better compatibility with broader ecosystem</li> </ul>"},{"location":"explanation/whats-new-2/#migration-path","title":"Migration Path","text":"<p>\u2192 Complete Migration Guide</p> <p>Upgrading from DataJoint 0.x is a phased process designed to minimize risk:</p>"},{"location":"explanation/whats-new-2/#phase-1-code-updates-reversible","title":"Phase 1: Code Updates (Reversible)","text":"<ul> <li>Update Python code to 2.0 API patterns (<code>.fetch()</code> \u2192 <code>.to_dicts()</code>, etc.)</li> <li>Update configuration files (<code>dj_local_conf.json</code> \u2192 <code>datajoint.json</code> + <code>.secrets/</code>)</li> <li>No database changes \u2014 legacy clients still work</li> </ul>"},{"location":"explanation/whats-new-2/#phase-2-type-migration-reversible","title":"Phase 2: Type Migration (Reversible)","text":"<ul> <li>Update database column comments to use core types (<code>:int64:</code>, <code>:&lt;blob&gt;:</code>)</li> <li>Rebuild <code>~lineage</code> tables for semantic matching</li> <li>Update Python table definitions</li> <li>Legacy clients still work \u2014 only metadata changed</li> </ul>"},{"location":"explanation/whats-new-2/#phase-3-external-storage-dual-attributes-reversible","title":"Phase 3: External Storage Dual Attributes (Reversible)","text":"<ul> <li>Create <code>*_v2</code> attributes alongside legacy external storage columns</li> <li>Both APIs can access data during transition</li> <li>Enables cross-testing between legacy and 2.0</li> <li>Legacy clients still work</li> </ul>"},{"location":"explanation/whats-new-2/#phase-4-finalize-point-of-no-return","title":"Phase 4: Finalize (Point of No Return)","text":"<ul> <li>Remove legacy external storage columns</li> <li>Drop old <code>~jobs</code> and <code>~external_*</code> tables</li> <li>Legacy clients stop working \u2014 database backup required</li> </ul>"},{"location":"explanation/whats-new-2/#phase-5-adopt-new-features-optional","title":"Phase 5: Adopt New Features (Optional)","text":"<ul> <li>Use new codecs (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>)</li> <li>Leverage Jobs 2.0 features (priority, better errors)</li> <li>Implement custom codecs for domain-specific types</li> </ul>"},{"location":"explanation/whats-new-2/#migration-support","title":"Migration Support","text":"<p>The migration guide includes: - AI agent prompts for automated migration steps - Validation commands to check migration status - Rollback procedures for each phase - Dry-run modes for all database changes</p> <p>Most users complete Phases 1-2 in a single session. Phases 3-4 only apply if you use legacy external storage.</p>"},{"location":"explanation/whats-new-2/#see-also","title":"See Also","text":""},{"location":"explanation/whats-new-2/#migration","title":"Migration","text":"<ul> <li>Migration Guide \u2014 Complete upgrade instructions</li> <li>Configuration \u2014 Setup new configuration system</li> </ul>"},{"location":"explanation/whats-new-2/#core-concepts","title":"Core Concepts","text":"<ul> <li>Type System \u2014 Understand the three-tier type architecture</li> <li>Computation Model \u2014 Jobs 2.0 and AutoPopulate</li> <li>Query Algebra \u2014 Semantic matching and operators</li> </ul>"},{"location":"explanation/whats-new-2/#getting-started","title":"Getting Started","text":"<ul> <li>Installation \u2014 Install DataJoint 2.0</li> <li>Tutorials \u2014 Learn by example</li> </ul>"},{"location":"explanation/whats-new-2/#reference","title":"Reference","text":"<ul> <li>Type System Specification \u2014 Complete type system details</li> <li>Codec API \u2014 Build custom codecs</li> <li>AutoPopulate Specification \u2014 Jobs 2.0 reference</li> </ul>"},{"location":"how-to/","title":"How-To Guides","text":"<p>Practical guides for common tasks.</p> <p>These guides help you accomplish specific tasks with DataJoint. Unlike tutorials, they assume you understand the basics and focus on getting things done.</p>"},{"location":"how-to/#setup","title":"Setup","text":"<ul> <li>Installation \u2014 Installing DataJoint</li> <li>Manage Secrets and Credentials \u2014 Secure configuration management</li> <li>Configure Database Connection \u2014 Connection settings</li> <li>Configure Object Storage \u2014 S3, MinIO, file stores</li> <li>Use the Command-Line Interface \u2014 Interactive REPL</li> </ul>"},{"location":"how-to/#schema-design","title":"Schema Design","text":"<ul> <li>Define Tables \u2014 Table definition syntax</li> <li>Model Relationships \u2014 Foreign key patterns</li> <li>Master-Part Tables \u2014 Compositional data patterns</li> <li>Design Primary Keys \u2014 Key selection strategies</li> <li>Read Schema Diagrams \u2014 Interpret visual diagrams</li> </ul>"},{"location":"how-to/#project-management","title":"Project Management","text":"<ul> <li>Manage a Pipeline Project \u2014 Multi-schema pipelines, team collaboration</li> <li>Deploy to Production \u2014 Production mode, schema prefixes, environment config</li> </ul>"},{"location":"how-to/#data-operations","title":"Data Operations","text":"<ul> <li>Insert Data \u2014 Single rows, batches, transactions</li> <li>Query Data \u2014 Operators, restrictions, projections</li> <li>Fetch Results \u2014 DataFrames, dicts, streaming</li> <li>Delete Data \u2014 Safe deletion with cascades</li> </ul>"},{"location":"how-to/#computation","title":"Computation","text":"<ul> <li>Run Computations \u2014 populate() basics</li> <li>Distributed Computing \u2014 Multi-process, cluster</li> <li>Handle Errors \u2014 Error recovery and job management</li> <li>Monitor Progress \u2014 Dashboards and status</li> </ul>"},{"location":"how-to/#object-storage","title":"Object Storage","text":"<ul> <li>Object Storage Overview \u2014 Navigation guide for all storage docs</li> <li>Choose a Storage Type \u2014 Decision guide for codecs</li> <li>Use Object Storage \u2014 When and how</li> <li>Use the <code>&lt;npy&gt;</code> Codec \u2014 NumPy arrays with lazy loading</li> <li>Use Plugin Codecs \u2014 Install codec packages via entry points</li> <li>Create Custom Codecs \u2014 Domain-specific types</li> <li>Manage Large Data \u2014 Blobs, streaming, efficiency</li> <li>Clean Up Object Storage \u2014 Garbage collection</li> </ul>"},{"location":"how-to/#maintenance","title":"Maintenance","text":"<ul> <li>Migrate to v2.0 \u2014 Upgrading existing pipelines</li> <li>Alter Tables \u2014 Schema evolution</li> <li>Backup and Restore \u2014 Data protection</li> </ul>"},{"location":"how-to/#testing","title":"Testing","text":"<ul> <li>Testing Best Practices \u2014 Integration testing with pytest</li> </ul>"},{"location":"how-to/alter-tables/","title":"Alter Tables","text":"<p>Modify existing table structures for schema evolution.</p>"},{"location":"how-to/alter-tables/#basic-alter","title":"Basic Alter","text":"<p>Sync table definition with code:</p> <pre><code># Update definition in code, then:\nMyTable.alter()\n</code></pre> <p>This compares the current code definition with the database and generates <code>ALTER TABLE</code> statements.</p>"},{"location":"how-to/alter-tables/#what-can-be-altered","title":"What Can Be Altered","text":"Change Supported Add columns Yes Drop columns Yes Modify column types Yes Rename columns Yes Change defaults Yes Update table comment Yes Modify primary key No Add/remove foreign keys No Modify indexes No"},{"location":"how-to/alter-tables/#add-a-column","title":"Add a Column","text":"<pre><code># Original\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species : varchar(32)\n    \"\"\"\n\n# Updated - add column\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species : varchar(32)\n    weight = null : float32      # New column\n    \"\"\"\n\n# Apply change\nSubject.alter()\n</code></pre>"},{"location":"how-to/alter-tables/#drop-a-column","title":"Drop a Column","text":"<p>Remove from definition and alter:</p> <pre><code># Column 'old_field' removed from definition\nSubject.alter()\n</code></pre>"},{"location":"how-to/alter-tables/#modify-column-type","title":"Modify Column Type","text":"<pre><code># Change varchar(32) to varchar(100)\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species : varchar(100)       # Was varchar(32)\n    \"\"\"\n\nSubject.alter()\n</code></pre>"},{"location":"how-to/alter-tables/#rename-a-column","title":"Rename a Column","text":"<p>DataJoint tracks renames via comment metadata:</p> <pre><code># Original: species\n# Renamed to: species_name\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species_name : varchar(32)   # Renamed from 'species'\n    \"\"\"\n\nSubject.alter()\n</code></pre>"},{"location":"how-to/alter-tables/#skip-confirmation","title":"Skip Confirmation","text":"<pre><code># Apply without prompting\nSubject.alter(prompt=False)\n</code></pre>"},{"location":"how-to/alter-tables/#view-pending-changes","title":"View Pending Changes","text":"<p>Check what would change without applying:</p> <pre><code># Show current definition\nprint(Subject.describe())\n\n# Compare with code definition\n# (alter() shows diff before prompting)\n</code></pre>"},{"location":"how-to/alter-tables/#unsupported-changes","title":"Unsupported Changes","text":""},{"location":"how-to/alter-tables/#primary-key-changes","title":"Primary Key Changes","text":"<p>Cannot modify primary key attributes:</p> <pre><code># This will raise NotImplementedError\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    new_id : uuid               # Changed primary key\n    ---\n    species : varchar(32)\n    \"\"\"\n\nSubject.alter()  # Error!\n</code></pre> <p>Workaround: Create new table, migrate data, drop old table.</p>"},{"location":"how-to/alter-tables/#foreign-key-changes","title":"Foreign Key Changes","text":"<p>Cannot add or remove foreign key references:</p> <pre><code># Cannot add new FK via alter()\ndefinition = \"\"\"\nsubject_id : varchar(16)\n---\n-&gt; NewReference              # Cannot add via alter\nspecies : varchar(32)\n\"\"\"\n</code></pre> <p>Workaround: Drop dependent tables, recreate with new structure.</p>"},{"location":"how-to/alter-tables/#index-changes","title":"Index Changes","text":"<p>Cannot modify indexes via alter:</p> <pre><code># Cannot add/remove indexes via alter()\ndefinition = \"\"\"\nsubject_id : varchar(16)\n---\nindex(species)               # Cannot add via alter\nspecies : varchar(32)\n\"\"\"\n</code></pre>"},{"location":"how-to/alter-tables/#migration-pattern","title":"Migration Pattern","text":"<p>For unsupported changes, use this pattern:</p> <pre><code># 1. Create new table with desired structure\n@schema\nclass SubjectNew(dj.Manual):\n    definition = \"\"\"\n    subject_id : uuid            # New primary key type\n    ---\n    species : varchar(32)\n    \"\"\"\n\n# 2. Migrate data\nfor row in Subject().to_dicts():\n    SubjectNew.insert1({\n        'subject_id': uuid.uuid4(),  # Generate new keys\n        'species': row['species']\n    })\n\n# 3. Update dependent tables\n# 4. Drop old table\n# 5. Rename new table (if needed, via SQL)\n</code></pre>"},{"location":"how-to/alter-tables/#add-job-metadata-columns","title":"Add Job Metadata Columns","text":"<p>For tables created before enabling job metadata:</p> <pre><code>from datajoint.migrate import add_job_metadata_columns\n\n# Dry run\nadd_job_metadata_columns(ProcessedData, dry_run=True)\n\n# Apply\nadd_job_metadata_columns(ProcessedData, dry_run=False)\n</code></pre>"},{"location":"how-to/alter-tables/#best-practices","title":"Best Practices","text":""},{"location":"how-to/alter-tables/#plan-schema-carefully","title":"Plan Schema Carefully","text":"<p>Primary keys and foreign keys cannot be changed easily. Design carefully upfront.</p>"},{"location":"how-to/alter-tables/#use-migrations-for-production","title":"Use Migrations for Production","text":"<p>For production systems, use versioned migration scripts:</p> <pre><code># migrations/001_add_weight_column.py\ndef upgrade():\n    Subject.alter(prompt=False)\n\ndef downgrade():\n    # Reverse the change\n    pass\n</code></pre>"},{"location":"how-to/alter-tables/#test-in-development-first","title":"Test in Development First","text":"<p>Always test schema changes on a copy:</p> <pre><code># Clone schema for testing\ntest_schema = dj.Schema('test_' + schema.database)\n</code></pre>"},{"location":"how-to/alter-tables/#see-also","title":"See Also","text":"<ul> <li>Define Tables \u2014 Table definition syntax</li> <li>Migrate to v2.0 \u2014 Version migration</li> </ul>"},{"location":"how-to/backup-restore/","title":"Backup and Restore","text":"<p>Protect your data with proper backup strategies.</p> <p>Tip: DataJoint.com provides automatic backups with point-in-time recovery as part of the managed service.</p>"},{"location":"how-to/backup-restore/#overview","title":"Overview","text":"<p>A complete DataJoint backup includes: 1. Database \u2014 Table structures and relational data 2. Object storage \u2014 Large objects stored externally</p>"},{"location":"how-to/backup-restore/#database-backup","title":"Database Backup","text":""},{"location":"how-to/backup-restore/#using-mysqldump","title":"Using mysqldump","text":"<pre><code># Backup single schema\nmysqldump -h host -u user -p database_name &gt; backup.sql\n\n# Backup multiple schemas\nmysqldump -h host -u user -p --databases schema1 schema2 &gt; backup.sql\n\n# Backup all schemas\nmysqldump -h host -u user -p --all-databases &gt; backup.sql\n</code></pre>"},{"location":"how-to/backup-restore/#include-routines-and-triggers","title":"Include Routines and Triggers","text":"<pre><code>mysqldump -h host -u user -p \\\n    --routines \\\n    --triggers \\\n    database_name &gt; backup.sql\n</code></pre>"},{"location":"how-to/backup-restore/#compressed-backup","title":"Compressed Backup","text":"<pre><code>mysqldump -h host -u user -p database_name | gzip &gt; backup.sql.gz\n</code></pre>"},{"location":"how-to/backup-restore/#database-restore","title":"Database Restore","text":"<pre><code># From SQL file\nmysql -h host -u user -p database_name &lt; backup.sql\n\n# From compressed file\ngunzip &lt; backup.sql.gz | mysql -h host -u user -p database_name\n</code></pre>"},{"location":"how-to/backup-restore/#object-storage-backup","title":"Object Storage Backup","text":""},{"location":"how-to/backup-restore/#filesystem-store","title":"Filesystem Store","text":"<pre><code># Sync to backup location\nrsync -av /data/datajoint-store/ /backup/datajoint-store/\n\n# With compression\ntar -czvf store-backup.tar.gz /data/datajoint-store/\n</code></pre>"},{"location":"how-to/backup-restore/#s3minio-store","title":"S3/MinIO Store","text":"<pre><code># Using AWS CLI\naws s3 sync s3://source-bucket s3://backup-bucket\n\n# Using MinIO client\nmc mirror source/bucket backup/bucket\n</code></pre>"},{"location":"how-to/backup-restore/#backup-script-example","title":"Backup Script Example","text":"<pre><code>#!/bin/bash\n# backup-datajoint.sh\n\nDATE=$(date +%Y%m%d_%H%M%S)\nBACKUP_DIR=/backups/datajoint\n\n# Backup database\nmysqldump -h $DJ_HOST -u $DJ_USER -p$DJ_PASS \\\n    --databases my_schema \\\n    | gzip &gt; $BACKUP_DIR/db_$DATE.sql.gz\n\n# Backup object storage\nrsync -av /data/store/ $BACKUP_DIR/store_$DATE/\n\n# Cleanup old backups (keep 7 days)\nfind $BACKUP_DIR -mtime +7 -delete\n\necho \"Backup completed: $DATE\"\n</code></pre>"},{"location":"how-to/backup-restore/#point-in-time-recovery","title":"Point-in-Time Recovery","text":""},{"location":"how-to/backup-restore/#enable-binary-logging","title":"Enable Binary Logging","text":"<p>In MySQL configuration:</p> <pre><code>[mysqld]\nlog-bin = mysql-bin\nbinlog-format = ROW\nexpire_logs_days = 7\n</code></pre>"},{"location":"how-to/backup-restore/#restore-to-point-in-time","title":"Restore to Point in Time","text":"<pre><code># Restore base backup\nmysql -h host -u user -p &lt; backup.sql\n\n# Apply binary logs up to specific time\nmysqlbinlog --stop-datetime=\"2024-01-15 14:30:00\" \\\n    mysql-bin.000001 mysql-bin.000002 \\\n    | mysql -h host -u user -p\n</code></pre>"},{"location":"how-to/backup-restore/#schema-level-export","title":"Schema-Level Export","text":"<p>Export schema structure without data:</p> <pre><code># Structure only\nmysqldump -h host -u user -p --no-data database_name &gt; schema.sql\n</code></pre>"},{"location":"how-to/backup-restore/#table-level-backup","title":"Table-Level Backup","text":"<p>Backup specific tables:</p> <pre><code>mysqldump -h host -u user -p database_name table1 table2 &gt; tables.sql\n</code></pre>"},{"location":"how-to/backup-restore/#datajoint-specific-considerations","title":"DataJoint-Specific Considerations","text":""},{"location":"how-to/backup-restore/#foreign-key-order","title":"Foreign Key Order","text":"<p>When restoring, tables must be created in dependency order. mysqldump handles this automatically, but manual restoration may require:</p> <pre><code># Disable FK checks during restore\nmysql -h host -u user -p -e \"SET FOREIGN_KEY_CHECKS=0; SOURCE backup.sql; SET FOREIGN_KEY_CHECKS=1;\"\n</code></pre>"},{"location":"how-to/backup-restore/#jobs-tables","title":"Jobs Tables","text":"<p>Jobs tables (<code>~~table_name</code>) are recreated automatically. You can exclude them:</p> <pre><code># Exclude jobs tables from backup\nmysqldump -h host -u user -p database_name \\\n    --ignore-table=database_name.~~table1 \\\n    --ignore-table=database_name.~~table2 \\\n    &gt; backup.sql\n</code></pre>"},{"location":"how-to/backup-restore/#blob-data","title":"Blob Data","text":"<p>Blobs stored internally (in database) are included in mysqldump. External objects need separate backup.</p>"},{"location":"how-to/backup-restore/#verification","title":"Verification","text":""},{"location":"how-to/backup-restore/#verify-database-backup","title":"Verify Database Backup","text":"<pre><code># Check backup file\ngunzip -c backup.sql.gz | head -100\n\n# Restore to test database\nmysql -h host -u user -p test_restore &lt; backup.sql\n</code></pre>"},{"location":"how-to/backup-restore/#verify-object-storage","title":"Verify Object Storage","text":"<pre><code>import datajoint as dj\n\n# Check external objects are accessible\nfor key in MyTable().keys():\n    try:\n        (MyTable &amp; key).fetch1('blob_column')\n    except Exception as e:\n        print(f\"Missing: {key} - {e}\")\n</code></pre>"},{"location":"how-to/backup-restore/#disaster-recovery-plan","title":"Disaster Recovery Plan","text":"<ol> <li>Regular backups: Daily database, continuous object sync</li> <li>Offsite copies: Replicate to different location/cloud</li> <li>Test restores: Monthly restore verification</li> <li>Document procedures: Written runbooks for recovery</li> <li>Monitor backups: Alert on backup failures</li> </ol>"},{"location":"how-to/backup-restore/#see-also","title":"See Also","text":"<ul> <li>Configure Object Storage \u2014 Storage setup</li> <li>Manage Large Data \u2014 Object storage patterns</li> </ul>"},{"location":"how-to/choose-storage-type/","title":"Choose a Storage Type","text":"<p>Select the right storage codec for your data based on size, access patterns, and lifecycle requirements.</p>"},{"location":"how-to/choose-storage-type/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>Start: What type of data are you storing?\n\n\u251c\u2500 Small data (typically &lt; 1-10 MB per row)?\n\u2502  \u251c\u2500 Python objects (dicts, arrays)? \u2192 Use &lt;blob&gt; (in-table)\n\u2502  \u2514\u2500 Files with filename? \u2192 Use &lt;attach&gt; (in-table)\n\u2502\n\u251c\u2500 Externally managed files?\n\u2502  \u2514\u2500 YES \u2192 Use &lt;filepath@&gt; (reference only)\n\u2502  \u2514\u2500 NO  \u2192 Continue...\n\u2502\n\u251c\u2500 Need browsable storage or access by external tools?\n\u2502  \u2514\u2500 YES \u2192 Use &lt;object@&gt; or &lt;npy@&gt; (schema-addressed)\n\u2502  \u2514\u2500 NO  \u2192 Continue...\n\u2502\n\u251c\u2500 Need streaming or partial reads?\n\u2502  \u2514\u2500 YES \u2192 Use &lt;object@&gt; (schema-addressed, Zarr/HDF5)\n\u2502  \u2514\u2500 NO  \u2192 Continue...\n\u2502\n\u251c\u2500 NumPy arrays that benefit from lazy loading?\n\u2502  \u2514\u2500 YES \u2192 Use &lt;npy@&gt; (optimized NumPy storage)\n\u2502  \u2514\u2500 NO  \u2192 Continue...\n\u2502\n\u251c\u2500 Python objects (dicts, arrays)?\n\u2502  \u2514\u2500 YES \u2192 Use &lt;blob@&gt; (hash-addressed)\n\u2502  \u2514\u2500 NO  \u2192 Use &lt;attach@&gt; (files with filename preserved)\n</code></pre>"},{"location":"how-to/choose-storage-type/#storage-types-overview","title":"Storage Types Overview","text":"Codec Location Addressing Python Objects Dedup Best For <code>&lt;blob&gt;</code> In-table (database) Row-based \u2705 Yes No Small Python objects (typically &lt; 1-10 MB) <code>&lt;attach&gt;</code> In-table (database) Row-based \u274c No (file path) No Small files with filename preserved <code>&lt;blob@&gt;</code> Object store Content hash \u2705 Yes Yes Large Python objects (with dedup) <code>&lt;attach@&gt;</code> Object store Content hash \u274c No (file path) Yes Large files with filename preserved <code>&lt;npy@&gt;</code> Object store Schema + key \u2705 Yes (arrays) No NumPy arrays (lazy load, navigable) <code>&lt;object@&gt;</code> Object store Schema + key \u274c No (you manage format) No Zarr, HDF5 (browsable, streaming) <code>&lt;filepath@&gt;</code> Object store User path \u274c No (you manage format) No External file references"},{"location":"how-to/choose-storage-type/#key-usability-python-object-convenience","title":"Key Usability: Python Object Convenience","text":"<p>Major advantage of <code>&lt;blob&gt;</code>, <code>&lt;blob@&gt;</code>, and <code>&lt;npy@&gt;</code>: You work with Python objects directly. No manual serialization, file handling, or IO management.</p> <pre><code># &lt;blob&gt; and &lt;blob@&gt;: Insert Python objects, get Python objects back\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Experiment\n    ---\n    results : &lt;blob@&gt;         # Any Python object: dicts, lists, arrays\n    \"\"\"\n\n    def make(self, key):\n        # Insert nested Python structures directly\n        results = {\n            'accuracy': 0.95,\n            'confusion_matrix': np.array([[10, 2], [1, 15]]),\n            'metadata': {'method': 'SVM', 'params': [1, 2, 3]}\n        }\n        self.insert1({**key, 'results': results})\n\n# Fetch: Get Python object back (no manual unpickling)\ndata = (Analysis &amp; key).fetch1('results')\nprint(data['accuracy'])           # 0.95\nprint(data['confusion_matrix'])   # numpy array\n</code></pre> <pre><code># &lt;npy@&gt;: Insert array-like objects, get array-like objects back\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : uuid\n    ---\n    traces : &lt;npy@&gt;           # NumPy arrays (no manual .npy files)\n    \"\"\"\n\n# Insert: Just pass the array\nRecording.insert1({'recording_id': uuid.uuid4(), 'traces': np.random.randn(1000, 32)})\n\n# Fetch: Get array-like object (NpyRef with lazy loading)\nref = (Recording &amp; key).fetch1('traces')\nprint(ref.shape)              # (1000, 32) - metadata without download\nsubset = ref[:100, :]         # Lazy slicing\n</code></pre> <p>Contrast with <code>&lt;object@&gt;</code> and <code>&lt;filepath@&gt;</code>: You manage the format (Zarr, HDF5, etc.) and handle file IO yourself. More flexible, but requires format knowledge.</p>"},{"location":"how-to/choose-storage-type/#detailed-decision-criteria","title":"Detailed Decision Criteria","text":""},{"location":"how-to/choose-storage-type/#size-and-storage-location","title":"Size and Storage Location","text":"<p>Technical Limits:</p> <ul> <li>MySQL: In-table blobs up to 4 GiB (<code>LONGBLOB</code>)</li> <li>PostgreSQL: In-table blobs unlimited (<code>BYTEA</code>)</li> <li>Object stores: Effectively unlimited (S3, file systems, etc.)</li> </ul> <p>Practical Guidance:</p> <p>The choice between in-table (<code>&lt;blob&gt;</code>) and object storage (<code>&lt;blob@&gt;</code>, <code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>) is a complex decision involving:</p> <ul> <li>Accessibility: How fast do you need to access the data?</li> <li>Cost: Database storage vs object storage pricing</li> <li>Performance: Query speed, backup time, replication overhead</li> </ul> <p>General recommendations:</p> <p>Try to keep in-table blobs under ~1-10 MB, but this depends on your specific use case:</p> <pre><code>@schema\nclass Experiment(dj.Manual):\n    definition = \"\"\"\n    experiment_id : uuid\n    ---\n    metadata : &lt;blob&gt;         # Small: config, parameters (&lt; 1 MB)\n    thumbnail : &lt;blob&gt;        # Medium: preview images (&lt; 10 MB)\n    raw_data : &lt;blob@&gt;        # Large: raw recordings (&gt; 10 MB)\n    \"\"\"\n</code></pre> <p>When to use in-table storage (<code>&lt;blob&gt;</code>): - Fast access needed (no external fetch) - Data frequently queried alongside other columns - Transactional consistency critical - Automatic backup with database important - No object storage configuration available</p> <p>When to use object storage (<code>&lt;blob@&gt;</code>, <code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>): - Data larger than ~10 MB - Infrequent access patterns - Need deduplication (hash-addressed types) - Need browsable structure (schema-addressed types) - Want to separate hot data (DB) from cold data (object store)</p> <p>Examples by size: - &lt; 1 MB: Configuration JSON, metadata, small parameter arrays \u2192 <code>&lt;blob&gt;</code> - 1-10 MB: Thumbnails, processed features, small waveforms \u2192 <code>&lt;blob&gt;</code> or <code>&lt;blob@&gt;</code> depending on access pattern - 10-100 MB: Neural recordings, images, PDFs \u2192 <code>&lt;blob@&gt;</code> or <code>&lt;attach@&gt;</code> - &gt; 100 MB: Zarr arrays, HDF5 datasets, large videos \u2192 <code>&lt;object@&gt;</code> or <code>&lt;npy@&gt;</code></p>"},{"location":"how-to/choose-storage-type/#access-pattern-guidelines","title":"Access Pattern Guidelines","text":"<p>Full Access Every Time</p> <p>Use <code>&lt;blob@&gt;</code> (hash-addressed):</p> <pre><code>class ProcessedImage(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawImage\n    ---\n    processed : &lt;blob@&gt;       # Always load full image\n    \"\"\"\n</code></pre> <p>Typical pattern: <pre><code># Fetch always gets full data\nimg = (ProcessedImage &amp; key).fetch1('processed')\n</code></pre></p> <p>Streaming / Partial Reads</p> <p>Use <code>&lt;object@&gt;</code> (schema-addressed):</p> <pre><code>class ScanVolume(dj.Manual):\n    definition = \"\"\"\n    scan_id : uuid\n    ---\n    volume : &lt;object@&gt;        # Stream chunks as needed\n    \"\"\"\n</code></pre> <p>Typical pattern: <pre><code># Get reference without downloading\nref = (ScanVolume &amp; key).fetch1('volume')\n\n# Stream specific chunks\nimport zarr\nz = zarr.open(ref.fsmap, mode='r')\nslice_data = z[100:200, :, :]  # Fetch only this slice\n</code></pre></p> <p>NumPy Arrays with Lazy Loading</p> <p>Use <code>&lt;npy@&gt;</code> (optimized for NumPy):</p> <pre><code>class NeuralActivity(dj.Computed):\n    definition = \"\"\"\n    -&gt; Recording\n    ---\n    traces : &lt;npy@&gt;           # NumPy array, lazy load\n    \"\"\"\n</code></pre> <p>Typical pattern: <pre><code># Returns NpyRef (lazy)\nref = (NeuralActivity &amp; key).fetch1('traces')\n\n# Access like NumPy array (loads on demand)\nsubset = ref[:100, :]         # Efficient slicing\nshape = ref.shape             # Metadata without loading\n</code></pre></p> <p>Why <code>&lt;npy@&gt;</code> over <code>&lt;blob@&gt;</code> for arrays: - Lazy loading (doesn't load until accessed) - Efficient slicing (can fetch subsets) - Preserves shape/dtype metadata - Native NumPy serialization</p>"},{"location":"how-to/choose-storage-type/#lifecycle-and-management","title":"Lifecycle and Management","text":"<p>DataJoint-Managed (Integrated)</p> <p>Use <code>&lt;blob@&gt;</code>, <code>&lt;npy@&gt;</code>, or <code>&lt;object@&gt;</code>:</p> <pre><code>class ManagedData(dj.Manual):\n    definition = \"\"\"\n    data_id : uuid\n    ---\n    content : &lt;blob@&gt;         # DataJoint manages lifecycle\n    \"\"\"\n</code></pre> <p>DataJoint provides: - \u2705 Automatic cleanup (garbage collection) - \u2705 Transactional integrity (atomic with database) - \u2705 Referential integrity (cascading deletes) - \u2705 Content deduplication (for <code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>)</p> <p>User manages: - \u274c File paths (DataJoint decides) - \u274c Cleanup (automatic) - \u274c Integrity (enforced)</p> <p>User-Managed (References)</p> <p>Use <code>&lt;filepath@&gt;</code>:</p> <pre><code>class ExternalData(dj.Manual):\n    definition = \"\"\"\n    data_id : uuid\n    ---\n    raw_file : &lt;filepath@&gt;    # User manages file\n    \"\"\"\n</code></pre> <p>User provides: - \u2705 File paths (you control organization) - \u2705 File lifecycle (you create/delete) - \u2705 Existing files (reference external data)</p> <p>DataJoint provides: - \u2705 Path validation (file exists on insert) - \u2705 ObjectRef for lazy access - \u274c No garbage collection - \u274c No transaction safety for files - \u274c No deduplication</p> <p>Use when: - Files managed by external systems - Referencing existing data archives - Custom file organization required - Large instrument output directories</p>"},{"location":"how-to/choose-storage-type/#storage-type-comparison","title":"Storage Type Comparison","text":""},{"location":"how-to/choose-storage-type/#in-table-blob","title":"In-Table: <code>&lt;blob&gt;</code>","text":"<p>Storage: Database column (LONGBLOB)</p> <p>Syntax: <pre><code>small_data : &lt;blob&gt;\n</code></pre></p> <p>Characteristics: - \u2705 Fast access (in database) - \u2705 Transactional consistency - \u2705 Automatic backup - \u2705 No store configuration needed - \u2705 Python object convenience: Insert/fetch dicts, lists, arrays directly (no manual IO) - \u2705 Automatic serialization + gzip compression - \u2705 Technical limit: 4 GiB (MySQL), unlimited (PostgreSQL) - \u274c Practical limit: Keep under ~1-10 MB for performance - \u274c No deduplication - \u274c Database bloat for large data</p> <p>Best for: - Configuration JSON (dicts/lists) - Small arrays/matrices - Thumbnails - Nested data structures</p>"},{"location":"how-to/choose-storage-type/#in-table-attach","title":"In-Table: <code>&lt;attach&gt;</code>","text":"<p>Storage: Database column (LONGBLOB)</p> <p>Syntax: <pre><code>config_file : &lt;attach&gt;\n</code></pre></p> <p>Characteristics: - \u2705 Fast access (in database) - \u2705 Transactional consistency - \u2705 Automatic backup - \u2705 No store configuration needed - \u2705 Filename preserved: Original filename stored with content - \u2705 Automatic gzip compression - \u2705 Technical limit: 4 GiB (MySQL), unlimited (PostgreSQL) - \u274c Practical limit: Keep under ~1-10 MB for performance - \u274c No deduplication - \u274c Returns file path (extracts to download directory), not Python object</p> <p>Best for: - Small configuration files - Document attachments (&lt; 10 MB) - Files where original filename matters - When you need the file extracted to disk</p> <p>Difference from <code>&lt;blob&gt;</code>: - <code>&lt;blob&gt;</code>: Stores Python objects (dicts, arrays) \u2192 returns Python object - <code>&lt;attach&gt;</code>: Stores files with filename \u2192 returns local file path</p>"},{"location":"how-to/choose-storage-type/#hash-addressed-blob-or-attach","title":"Hash-Addressed: <code>&lt;blob@&gt;</code> or <code>&lt;attach@&gt;</code>","text":"<p>Storage: Object store at <code>{store}/_hash/{schema}/{hash}</code></p> <p>Syntax: <pre><code>data : &lt;blob@&gt;              # Default store\ndata : &lt;blob@mystore&gt;       # Named store\nfile : &lt;attach@&gt;            # File attachments\n</code></pre></p> <p>Characteristics (both): - \u2705 Content deduplication (identical data stored once) - \u2705 Automatic gzip compression - \u2705 Garbage collection - \u2705 Transaction safety - \u2705 Referential integrity - \u2705 Moderate to large files (1 MB - 100 GB) - \u274c Full download on fetch (no streaming) - \u274c Storage path not browsable (hash-based)</p> <p><code>&lt;blob@&gt;</code> specific: - \u2705 Python object convenience: Insert/fetch dicts, lists, arrays directly (no manual IO) - Returns: Python objects</p> <p><code>&lt;attach@&gt;</code> specific: - \u2705 Filename preserved: Original filename stored with content - Returns: Local file path (extracts to download directory)</p> <p>Best for <code>&lt;blob@&gt;</code>: - Large Python objects (NumPy arrays, dicts) - Processed results (nested structures) - Any Python data with duplicates</p> <p>Best for <code>&lt;attach@&gt;</code>: - PDF/document files - Images, videos - Files where original filename/format matters</p> <p>Key difference: - <code>&lt;blob@&gt;</code>: Python objects in, Python objects out (no file handling) - <code>&lt;attach@&gt;</code>: Files in, file paths out (preserves filename)</p>"},{"location":"how-to/choose-storage-type/#schema-addressed-npy-or-object","title":"Schema-Addressed: <code>&lt;npy@&gt;</code> or <code>&lt;object@&gt;</code>","text":"<p>Storage: Object store at <code>{store}/_schema/{schema}/{table}/{key}/{field}.{token}.ext</code></p> <p>Syntax: <pre><code>array : &lt;npy@&gt;              # NumPy arrays\ndataset : &lt;object@&gt;         # Zarr, HDF5, custom\n</code></pre></p> <p>Characteristics: - \u2705 Streaming access (no full download) - \u2705 Partial reads (fetch chunks) - \u2705 Browsable paths (organized by key) - \u2705 Accessible by external tools (not just DataJoint) - \u2705 Very large files (100 MB - TB+) - \u2705 Multi-file datasets (e.g., Zarr directory structures) - \u274c No deduplication - \u274c One file per field per row</p> <p>Key advantages: - Schema-addressed storage is browsable - can be navigated and accessed by external tools (Zarr viewers, HDF5 utilities, direct filesystem access), not just through DataJoint - <code>&lt;npy@&gt;</code> provides array convenience - insert/fetch array-like objects directly (no manual .npy file handling) - <code>&lt;object@&gt;</code> provides flexibility - you manage the format (Zarr, HDF5, custom), DataJoint provides storage and references</p> <p>Best for: - <code>&lt;npy@&gt;</code>: NumPy arrays with lazy loading (no manual IO) - <code>&lt;object@&gt;</code>: Zarr arrays, HDF5 datasets, custom formats (you manage format) - Large video files - Multi-file experimental outputs - Data that needs to be accessed by non-DataJoint tools</p> <p>Difference <code>&lt;npy@&gt;</code> vs <code>&lt;object@&gt;</code>: - <code>&lt;npy@&gt;</code>: Insert/fetch array-like objects (like <code>&lt;blob&gt;</code> but lazy) - no manual .npy handling - <code>&lt;object@&gt;</code>: You manage format and IO (Zarr, HDF5, custom) - more flexible but requires format knowledge</p>"},{"location":"how-to/choose-storage-type/#filepath-references-filepath","title":"Filepath References: <code>&lt;filepath@&gt;</code>","text":"<p>Storage: User-managed paths in object store</p> <p>Syntax: <pre><code>raw_data : &lt;filepath@&gt;      # User-managed file\n</code></pre></p> <p>Characteristics: - \u2705 Reference existing files - \u2705 User controls paths - \u2705 External system compatibility - \u2705 Custom organization - \u274c No lifecycle management - \u274c No garbage collection - \u274c No transaction safety - \u274c No deduplication - \u274c Must avoid <code>_hash/</code> and <code>_schema/</code> prefixes</p> <p>Best for: - Large instrument data directories - Externally managed archives - Legacy data integration - Custom file organization requirements</p>"},{"location":"how-to/choose-storage-type/#common-scenarios","title":"Common Scenarios","text":""},{"location":"how-to/choose-storage-type/#scenario-1-image-processing-pipeline","title":"Scenario 1: Image Processing Pipeline","text":"<pre><code>@schema\nclass RawImage(dj.Manual):\n    \"\"\"Imported from microscope\"\"\"\n    definition = \"\"\"\n    image_id : uuid\n    ---\n    raw_file : &lt;filepath@acquisition&gt;    # Reference microscope output\n    \"\"\"\n\n@schema\nclass CalibratedImage(dj.Computed):\n    \"\"\"Calibrated, moderate size\"\"\"\n    definition = \"\"\"\n    -&gt; RawImage\n    ---\n    calibrated : &lt;blob@&gt;                 # 5 MB processed image\n    \"\"\"\n\n@schema\nclass Thumbnail(dj.Computed):\n    \"\"\"Preview for dashboard\"\"\"\n    definition = \"\"\"\n    -&gt; CalibratedImage\n    ---\n    preview : &lt;blob&gt;                     # 100 KB thumbnail, in-table\n    \"\"\"\n</code></pre> <p>Rationale: - <code>&lt;filepath@&gt;</code>: Reference existing microscope files (large, externally managed) - <code>&lt;blob@&gt;</code>: Processed images (moderate size, deduplicated if reprocessed) - <code>&lt;blob&gt;</code>: Thumbnails (tiny, fast access for UI)</p>"},{"location":"how-to/choose-storage-type/#scenario-2-electrophysiology-recording","title":"Scenario 2: Electrophysiology Recording","text":"<pre><code>@schema\nclass RecordingSession(dj.Manual):\n    \"\"\"Recording metadata\"\"\"\n    definition = \"\"\"\n    session_id : uuid\n    ---\n    config : &lt;blob&gt;                      # 50 KB parameters, in-table\n    \"\"\"\n\n@schema\nclass ContinuousData(dj.Imported):\n    \"\"\"Raw voltage traces\"\"\"\n    definition = \"\"\"\n    -&gt; RecordingSession\n    ---\n    raw_voltage : &lt;object@raw&gt;           # 10 GB Zarr array, streaming\n    \"\"\"\n\n@schema\nclass SpikeWaveforms(dj.Computed):\n    \"\"\"Extracted spike shapes\"\"\"\n    definition = \"\"\"\n    -&gt; ContinuousData\n    unit_id : int64\n    ---\n    waveforms : &lt;npy@&gt;                   # 20 MB array, lazy load\n    \"\"\"\n\n@schema\nclass UnitStats(dj.Computed):\n    \"\"\"Summary statistics\"\"\"\n    definition = \"\"\"\n    -&gt; SpikeWaveforms\n    ---\n    stats : &lt;blob&gt;                       # 10 KB stats dict, in-table\n    \"\"\"\n</code></pre> <p>Rationale: - <code>&lt;blob&gt;</code>: Config and stats (small metadata, fast access) - <code>&lt;object@&gt;</code>: Raw voltage (huge, stream for spike detection) - <code>&lt;npy@&gt;</code>: Waveforms (moderate arrays, load for clustering)</p>"},{"location":"how-to/choose-storage-type/#scenario-3-calcium-imaging-analysis","title":"Scenario 3: Calcium Imaging Analysis","text":"<pre><code>@schema\nclass Movie(dj.Manual):\n    \"\"\"Raw calcium imaging movie\"\"\"\n    definition = \"\"\"\n    movie_id : uuid\n    ---\n    frames : &lt;object@movies&gt;             # 2 GB TIFF stack, streaming\n    \"\"\"\n\n@schema\nclass SegmentedCells(dj.Computed):\n    \"\"\"Cell masks\"\"\"\n    definition = \"\"\"\n    -&gt; Movie\n    ---\n    masks : &lt;npy@&gt;                       # 50 MB mask array, lazy load\n    \"\"\"\n\n@schema\nclass FluorescenceTraces(dj.Computed):\n    \"\"\"Extracted time series\"\"\"\n    definition = \"\"\"\n    -&gt; SegmentedCells\n    cell_id : int64\n    ---\n    trace : &lt;blob@&gt;                      # 500 KB per cell, deduplicated\n    \"\"\"\n\n@schema\nclass TraceSummary(dj.Computed):\n    \"\"\"Event detection results\"\"\"\n    definition = \"\"\"\n    -&gt; FluorescenceTraces\n    ---\n    events : &lt;blob&gt;                      # 5 KB event times, in-table\n    \"\"\"\n</code></pre> <p>Rationale: - <code>&lt;object@&gt;</code>: Movies (huge, stream for segmentation) - <code>&lt;npy@&gt;</code>: Masks (moderate, load for trace extraction) - <code>&lt;blob@&gt;</code>: Traces (per-cell, many rows, deduplication helps) - <code>&lt;blob&gt;</code>: Event summaries (tiny, fast query results)</p>"},{"location":"how-to/choose-storage-type/#configuration-examples","title":"Configuration Examples","text":""},{"location":"how-to/choose-storage-type/#single-store-development","title":"Single Store (Development)","text":"<pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/my-project\"\n    }\n  }\n}\n</code></pre> <p>All <code>@</code> codecs use this store: - <code>&lt;blob@&gt;</code> \u2192 <code>/data/my-project/_hash/{schema}/{hash}</code> - <code>&lt;npy@&gt;</code> \u2192 <code>/data/my-project/_schema/{schema}/{table}/{key}/</code></p>"},{"location":"how-to/choose-storage-type/#multiple-stores-production","title":"Multiple Stores (Production)","text":"<pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"filepath_default\": \"acquisition\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/processed\"\n    },\n    \"acquisition\": {\n      \"protocol\": \"file\",\n      \"location\": \"/mnt/microscope\"\n    },\n    \"archive\": {\n      \"protocol\": \"s3\",\n      \"bucket\": \"long-term-storage\",\n      \"location\": \"lab-data/archive\"\n    }\n  }\n}\n</code></pre> <p>Usage in table definitions: <pre><code>raw : &lt;filepath@&gt;               # Uses filepath_default (acquisition)\nprocessed : &lt;blob@&gt;             # Uses default (main)\nbackup : &lt;blob@archive&gt;         # Uses named store (archive)\n</code></pre></p>"},{"location":"how-to/choose-storage-type/#performance-considerations","title":"Performance Considerations","text":""},{"location":"how-to/choose-storage-type/#read-performance","title":"Read Performance","text":"Codec Random Access Streaming Latency <code>&lt;blob&gt;</code> \u26a1 Excellent N/A &lt;1ms <code>&lt;blob@&gt;</code> \u2705 Good \u274c No ~100ms <code>&lt;npy@&gt;</code> \u2705 Good (lazy) \u2705 Yes ~100ms + chunk time <code>&lt;object@&gt;</code> \u2705 Excellent \u2705 Yes ~100ms + chunk time <code>&lt;filepath@&gt;</code> \u2705 Good \u2705 Yes ~100ms + network"},{"location":"how-to/choose-storage-type/#write-performance","title":"Write Performance","text":"Codec Insert Speed Transaction Safe Deduplication <code>&lt;blob&gt;</code> \u26a1 Fastest \u2705 Yes \u274c No <code>&lt;blob@&gt;</code> \u2705 Fast \u2705 Yes \u2705 Yes <code>&lt;npy@&gt;</code> \u2705 Fast \u2705 Yes \u274c No <code>&lt;object@&gt;</code> \u2705 Fast \u2705 Yes \u274c No <code>&lt;filepath@&gt;</code> \u26a1 Fastest \u26a0\ufe0f Path only \u274c No"},{"location":"how-to/choose-storage-type/#storage-efficiency","title":"Storage Efficiency","text":"Codec Deduplication Compression Overhead <code>&lt;blob&gt;</code> \u274c No \u2705 gzip (automatic) Low <code>&lt;blob@&gt;</code> \u2705 Yes \u2705 gzip (automatic) Medium <code>&lt;npy@&gt;</code> \u274c No \u26a0\ufe0f Format-specific Low <code>&lt;object@&gt;</code> \u274c No \u26a0\ufe0f Format-specific Low <code>&lt;filepath@&gt;</code> \u274c No User-managed Minimal"},{"location":"how-to/choose-storage-type/#migration-between-storage-types","title":"Migration Between Storage Types","text":""},{"location":"how-to/choose-storage-type/#in-table-object-store","title":"In-Table \u2192 Object Store","text":"<pre><code># Add new column with object storage\n@schema\nclass MyTable(dj.Manual):\n    definition = \"\"\"\n    id : int\n    ---\n    data_old : &lt;blob&gt;          # Legacy in-table\n    data_new : &lt;blob@&gt;         # New object storage\n    \"\"\"\n\n# Migrate data\nfor key in MyTable.fetch('KEY'):\n    old_data = (MyTable &amp; key).fetch1('data_old')\n    (MyTable &amp; key).update1({**key, 'data_new': old_data})\n\n# After verification, drop old column via alter()\n</code></pre>"},{"location":"how-to/choose-storage-type/#hash-addressed-schema-addressed","title":"Hash-Addressed \u2192 Schema-Addressed","text":"<pre><code># For large files that need streaming\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : uuid\n    ---\n    data_blob : &lt;blob@&gt;        # Old: full download\n    data_stream : &lt;object@&gt;    # New: streaming access\n    \"\"\"\n\n# Convert and store as Zarr\nimport zarr\nfor key in Recording.fetch('KEY'):\n    data = (Recording &amp; key).fetch1('data_blob')\n\n    # Create Zarr array\n    ref = (Recording &amp; key).create_object_ref('data_stream', '.zarr')\n    z = zarr.open(ref.fsmap, mode='w', shape=data.shape, dtype=data.dtype)\n    z[:] = data\n\n    # Update row\n    (Recording &amp; key).update1({**key, 'data_stream': ref})\n</code></pre>"},{"location":"how-to/choose-storage-type/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/choose-storage-type/#datajointerror-store-not-configured","title":"\"DataJointError: Store not configured\"","text":"<p>Problem: Using <code>@</code> without store configuration</p> <p>Solution: <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/storage\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"how-to/choose-storage-type/#valueerror-path-conflicts-with-reserved-section","title":"\"ValueError: Path conflicts with reserved section\"","text":"<p>Problem: <code>&lt;filepath@&gt;</code> path uses <code>_hash/</code> or <code>_schema/</code></p> <p>Solution: Use different path: <pre><code># Bad\ntable.insert1({'id': 1, 'file': '_hash/mydata.bin'})  # Error!\n\n# Good\ntable.insert1({'id': 1, 'file': 'raw/mydata.bin'})    # OK\n</code></pre></p>"},{"location":"how-to/choose-storage-type/#data-not-deduplicated","title":"Data not deduplicated","text":"<p>Problem: Using <code>&lt;npy@&gt;</code> or <code>&lt;object@&gt;</code> expecting deduplication</p> <p>Solution: Use <code>&lt;blob@&gt;</code> for deduplication: <pre><code># No deduplication\ndata : &lt;npy@&gt;\n\n# With deduplication\ndata : &lt;blob@&gt;\n</code></pre></p>"},{"location":"how-to/choose-storage-type/#out-of-memory-loading-large-array","title":"Out of memory loading large array","text":"<p>Problem: Using <code>&lt;blob@&gt;</code> for huge files</p> <p>Solution: Use <code>&lt;object@&gt;</code> or <code>&lt;npy@&gt;</code> for streaming: <pre><code># Bad: loads 10 GB into memory\nlarge_data : &lt;blob@&gt;\n\n# Good: streaming access\nlarge_data : &lt;object@&gt;\n</code></pre></p>"},{"location":"how-to/choose-storage-type/#see-also","title":"See Also","text":"<ul> <li>Use Object Storage \u2014 How to use codecs in practice</li> <li>Configure Object Storage \u2014 Store configuration</li> <li>Type System \u2014 Complete type system overview</li> <li>Type System Specification \u2014 Technical details</li> <li>NPY Codec Specification \u2014 NumPy array storage</li> </ul>"},{"location":"how-to/configure-database/","title":"Configure Database Connection","text":"<p>Set up your DataJoint database connection.</p> <p>Tip: DataJoint.com handles database configuration automatically with fully managed infrastructure and support.</p>"},{"location":"how-to/configure-database/#configuration-structure","title":"Configuration Structure","text":"<p>DataJoint separates configuration into two parts:</p> <ol> <li><code>datajoint.json</code> \u2014 Non-sensitive settings (checked into version control)</li> <li><code>.secrets/</code> directory \u2014 Credentials and secrets (never committed)</li> </ol>"},{"location":"how-to/configure-database/#project-configuration-datajointjson","title":"Project Configuration (<code>datajoint.json</code>)","text":"<p>Create <code>datajoint.json</code> in your project root for non-sensitive settings:</p> <pre><code>{\n  \"database.host\": \"db.example.com\",\n  \"database.port\": 3306,\n  \"database.use_tls\": true,\n  \"safemode\": true\n}\n</code></pre> <p>This file should be committed to version control.</p>"},{"location":"how-to/configure-database/#secrets-directory-secrets","title":"Secrets Directory (<code>.secrets/</code>)","text":"<p>Store credentials in <code>.secrets/datajoint.json</code>:</p> <pre><code>{\n  \"database.user\": \"myuser\",\n  \"database.password\": \"mypassword\"\n}\n</code></pre> <p>Important: Add <code>.secrets/</code> to your <code>.gitignore</code>:</p> <pre><code>.secrets/\n</code></pre>"},{"location":"how-to/configure-database/#environment-variables","title":"Environment Variables","text":"<p>For CI/CD and production, use environment variables:</p> <pre><code>export DJ_HOST=db.example.com\nexport DJ_USER=myuser\nexport DJ_PASS=mypassword\n</code></pre> <p>Environment variables take precedence over config files.</p>"},{"location":"how-to/configure-database/#configuration-settings","title":"Configuration Settings","text":"Setting Environment Default Description <code>database.host</code> <code>DJ_HOST</code> <code>localhost</code> Database server hostname <code>database.port</code> <code>DJ_PORT</code> Auto Database server port (3306 for MySQL, 5432 for PostgreSQL) <code>database.user</code> <code>DJ_USER</code> \u2014 Database username <code>database.password</code> <code>DJ_PASS</code> \u2014 Database password <code>database.backend</code> <code>DJ_BACKEND</code> <code>mysql</code> Database backend: <code>mysql</code> or <code>postgresql</code> <code>database.use_tls</code> <code>DJ_TLS</code> <code>True</code> Use TLS encryption <code>database.reconnect</code> \u2014 <code>True</code> Auto-reconnect on timeout <code>safemode</code> \u2014 <code>True</code> Prompt before destructive operations"},{"location":"how-to/configure-database/#test-connection","title":"Test Connection","text":"<pre><code>import datajoint as dj\n\n# Connects using configured credentials\nconn = dj.conn()\nprint(f\"Connected to {conn.host}\")\n</code></pre>"},{"location":"how-to/configure-database/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>For scripts, you can set configuration programmatically:</p> <pre><code>import datajoint as dj\n\ndj.config['database.host'] = 'localhost'\n# Credentials from environment or secrets file\n</code></pre>"},{"location":"how-to/configure-database/#temporary-override","title":"Temporary Override","text":"<pre><code>with dj.config.override(database={'host': 'test-server'}):\n    # Uses test-server for this block only\n    conn = dj.conn()\n</code></pre>"},{"location":"how-to/configure-database/#configuration-precedence","title":"Configuration Precedence","text":"<ol> <li>Programmatic settings (highest priority)</li> <li>Environment variables</li> <li><code>.secrets/datajoint.json</code></li> <li><code>datajoint.json</code></li> <li>Default values (lowest priority)</li> </ol>"},{"location":"how-to/configure-database/#tls-configuration","title":"TLS Configuration","text":"<p>For production, always use TLS:</p> <pre><code>{\n  \"database.use_tls\": true\n}\n</code></pre> <p>For local development without TLS:</p> <pre><code>{\n  \"database.use_tls\": false\n}\n</code></pre>"},{"location":"how-to/configure-database/#postgresql-backend","title":"PostgreSQL Backend","text":"<p>New in 2.1</p> <p>PostgreSQL is now supported as an alternative database backend.</p> <p>DataJoint supports both MySQL and PostgreSQL backends. To use PostgreSQL:</p>"},{"location":"how-to/configure-database/#configuration-file","title":"Configuration File","text":"<pre><code>{\n  \"database\": {\n    \"host\": \"localhost\",\n    \"backend\": \"postgresql\"\n  }\n}\n</code></pre> <p>The port defaults to <code>5432</code> when <code>backend</code> is set to <code>postgresql</code>.</p>"},{"location":"how-to/configure-database/#environment-variable","title":"Environment Variable","text":"<pre><code>export DJ_BACKEND=postgresql\nexport DJ_HOST=localhost\nexport DJ_USER=postgres\nexport DJ_PASS=password\n</code></pre>"},{"location":"how-to/configure-database/#programmatic-configuration_1","title":"Programmatic Configuration","text":"<pre><code>import datajoint as dj\n\ndj.config['database.backend'] = 'postgresql'\ndj.config['database.host'] = 'localhost'\n</code></pre>"},{"location":"how-to/configure-database/#docker-compose-for-local-development","title":"Docker Compose for Local Development","text":"<pre><code>services:\n  postgres:\n    image: postgres:15\n    environment:\n      - POSTGRES_PASSWORD=password\n      - POSTGRES_USER=postgres\n      - POSTGRES_DB=test\n    ports:\n      - \"5432:5432\"\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n      timeout: 30s\n      retries: 5\n</code></pre>"},{"location":"how-to/configure-database/#backend-compatibility","title":"Backend Compatibility","text":"<p>DataJoint's core types and query operators work identically on both backends. Table definitions, queries, and computations are portable between MySQL and PostgreSQL without code changes.</p>"},{"location":"how-to/configure-database/#connection-lifecycle","title":"Connection Lifecycle","text":""},{"location":"how-to/configure-database/#persistent-connection-default","title":"Persistent Connection (Default)","text":"<p>DataJoint uses a persistent singleton connection by default:</p> <pre><code>import datajoint as dj\n\n# First call establishes connection\nconn = dj.conn()\n\n# Subsequent calls return the same connection\nconn2 = dj.conn()  # Same as conn\n\n# Reset to create a new connection\nconn3 = dj.conn(reset=True)  # New connection\n</code></pre> <p>This is ideal for interactive sessions and notebooks.</p>"},{"location":"how-to/configure-database/#context-manager-explicit-cleanup","title":"Context Manager (Explicit Cleanup)","text":"<p>For serverless environments (AWS Lambda, Cloud Functions) or when you need explicit connection lifecycle control, use the context manager:</p> <pre><code>import datajoint as dj\n\nwith dj.Connection(host, user, password) as conn:\n    schema = dj.Schema('my_schema', connection=conn)\n    MyTable().insert(data)\n# Connection automatically closed when exiting the block\n</code></pre> <p>The connection closes automatically even if an exception occurs:</p> <pre><code>try:\n    with dj.Connection(**creds) as conn:\n        schema = dj.Schema('my_schema', connection=conn)\n        MyTable().insert(data)\n        raise SomeError()\nexcept SomeError:\n    pass\n# Connection is still closed properly\n</code></pre>"},{"location":"how-to/configure-database/#manual-close","title":"Manual Close","text":"<p>You can also close a connection explicitly:</p> <pre><code>conn = dj.conn()\n# ... do work ...\nconn.close()\n</code></pre>"},{"location":"how-to/configure-storage/","title":"Configure Object Stores","text":"<p>Set up S3, MinIO, or filesystem storage for DataJoint's Object-Augmented Schema (OAS).</p> <p>Tip: DataJoint.com provides pre-configured object stores integrated with your database\u2014no setup required.</p>"},{"location":"how-to/configure-storage/#overview","title":"Overview","text":"<p>DataJoint's Object-Augmented Schema (OAS) integrates relational tables with object storage as a single coherent system. Large data objects (arrays, files, Zarr datasets) are stored in file systems or cloud storage while maintaining full referential integrity with the relational database.</p> <p>Storage models:</p> <ul> <li>Hash-addressed and schema-addressed storage are integrated into the OAS. DataJoint manages paths, lifecycle, integrity, garbage collection, transaction safety, and deduplication.</li> <li>Filepath storage stores only path strings. DataJoint provides no lifecycle management, garbage collection, transaction safety, or deduplication. Users control file creation, organization, and lifecycle.</li> </ul> <p>Storage is configured per-project using named stores. Each store can be used for:</p> <ul> <li>Hash-addressed storage (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>) \u2014 content-addressed with deduplication using <code>_hash/</code> section</li> <li>Schema-addressed storage (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>) \u2014 key-based paths with streaming access using <code>_schema/</code> section</li> <li>Filepath storage (<code>&lt;filepath@&gt;</code>) \u2014 user-managed paths anywhere in the store except <code>_hash/</code> and <code>_schema/</code> (reserved for DataJoint)</li> </ul> <p>Multiple stores can be configured for different data types or storage tiers. One store is designated as the default.</p>"},{"location":"how-to/configure-storage/#configuration-methods","title":"Configuration Methods","text":"<p>DataJoint loads configuration in priority order:</p> <ol> <li>Environment variables (highest priority)</li> <li>Secrets directory (<code>.secrets/</code>)</li> <li>Config file (<code>datajoint.json</code>)</li> <li>Defaults (lowest priority)</li> </ol>"},{"location":"how-to/configure-storage/#single-store-configuration","title":"Single Store Configuration","text":""},{"location":"how-to/configure-storage/#file-system-store","title":"File System Store","text":"<p>For local or network-mounted storage:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/my-project/production\"\n    }\n  }\n}\n</code></pre> <p>Paths will be:</p> <ul> <li>Hash: <code>/data/my-project/production/_hash/{schema}/{hash}</code></li> <li>Schema: <code>/data/my-project/production/_schema/{schema}/{table}/{key}/</code></li> </ul>"},{"location":"how-to/configure-storage/#s3-store","title":"S3 Store","text":"<p>For Amazon S3 or S3-compatible storage:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"my-bucket\",\n      \"location\": \"my-project/production\",\n      \"secure\": true\n    }\n  }\n}\n</code></pre> <p>Store credentials separately in <code>.secrets/</code>:</p> <pre><code>.secrets/\n\u251c\u2500\u2500 stores.main.access_key\n\u2514\u2500\u2500 stores.main.secret_key\n</code></pre> <p>Paths will be:</p> <ul> <li>Hash: <code>s3://my-bucket/my-project/production/_hash/{schema}/{hash}</code></li> <li>Schema: <code>s3://my-bucket/my-project/production/_schema/{schema}/{table}/{key}/</code></li> </ul>"},{"location":"how-to/configure-storage/#minio-store","title":"MinIO Store","text":"<p>MinIO uses the S3 protocol with a custom endpoint:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"minio.example.com:9000\",\n      \"bucket\": \"datajoint\",\n      \"location\": \"lab-data\",\n      \"secure\": false\n    }\n  }\n}\n</code></pre>"},{"location":"how-to/configure-storage/#multiple-stores-configuration","title":"Multiple Stores Configuration","text":"<p>Define multiple stores for different data types or storage tiers:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/my-project/main\",\n      \"partition_pattern\": \"subject_id/session_date\"\n    },\n    \"raw\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/my-project/raw\",\n      \"subfolding\": [2, 2]\n    },\n    \"archive\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"archive-bucket\",\n      \"location\": \"my-project/long-term\"\n    }\n  }\n}\n</code></pre> <p>Store credentials in <code>.secrets/</code>:</p> <pre><code>.secrets/\n\u251c\u2500\u2500 stores.archive.access_key\n\u2514\u2500\u2500 stores.archive.secret_key\n</code></pre> <p>Use named stores in table definitions:</p> <pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : uuid\n    ---\n    raw_data : &lt;blob@raw&gt;         # Hash: _hash/{schema}/{hash}\n    zarr_scan : &lt;object@raw&gt;      # Schema: _schema/{schema}/{table}/{key}/\n    summary : &lt;blob@&gt;             # Uses default store (main)\n    old_data : &lt;blob@archive&gt;     # Archive store, hash-addressed\n    \"\"\"\n</code></pre> <p>Notice that <code>&lt;blob@raw&gt;</code> and <code>&lt;object@raw&gt;</code> both use the \"raw\" store, just different <code>_hash</code> and <code>_schema</code> sections.</p> <p>Example paths with partitioning:</p> <p>For a Recording with <code>subject_id=042</code>, <code>session_date=2024-01-15</code> in the main store: <pre><code>/data/my-project/main/_schema/subject_id=042/session_date=2024-01-15/experiment/Recording/recording_id=uuid-value/zarr_scan.x8f2a9b1.zarr\n</code></pre></p> <p>Without those attributes, it follows normal structure: <pre><code>/data/my-project/main/_schema/experiment/Recording/recording_id=uuid-value/zarr_scan.x8f2a9b1.zarr\n</code></pre></p>"},{"location":"how-to/configure-storage/#verify-configuration","title":"Verify Configuration","text":"<pre><code>import datajoint as dj\n\n# Check default store\nspec = dj.config.get_store_spec()  # Uses stores.default\nprint(spec)\n\n# Check named store\nspec = dj.config.get_store_spec(\"archive\")\nprint(spec)\n\n# List all configured stores\nprint(dj.config.stores.keys())\n</code></pre>"},{"location":"how-to/configure-storage/#configuration-options","title":"Configuration Options","text":"Option Required Description <code>stores.default</code> Yes Name of the default store <code>stores.&lt;name&gt;.protocol</code> Yes <code>file</code>, <code>s3</code>, <code>gcs</code>, or <code>azure</code> <code>stores.&lt;name&gt;.location</code> Yes Base path or prefix (includes project context) <code>stores.&lt;name&gt;.bucket</code> S3/GCS Bucket name <code>stores.&lt;name&gt;.endpoint</code> S3 S3 endpoint URL <code>stores.&lt;name&gt;.secure</code> No Use HTTPS (default: true) <code>stores.&lt;name&gt;.access_key</code> S3 Access key ID (store in <code>.secrets/</code>) <code>stores.&lt;name&gt;.secret_key</code> S3 Secret access key (store in <code>.secrets/</code>) <code>stores.&lt;name&gt;.subfolding</code> No Hash-addressed hierarchy: <code>[2, 2]</code> for 2-level nesting (default: no subfolding) <code>stores.&lt;name&gt;.partition_pattern</code> No Schema-addressed path partitioning: <code>\"subject_id/session_date\"</code> (default: no partitioning) <code>stores.&lt;name&gt;.token_length</code> No Random token length for schema-addressed filenames (default: <code>8</code>)"},{"location":"how-to/configure-storage/#subfolding-hash-addressed-storage-only","title":"Subfolding (Hash-Addressed Storage Only)","text":"<p>Hash-addressed storage (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>) stores content using a Base32-encoded hash as the filename. By default, all files are stored in a flat directory structure:</p> <pre><code>_hash/{schema}/abcdefghijklmnopqrstuvwxyz\n</code></pre> <p>Some filesystems perform poorly with large directories (thousands of files). Subfolding creates a directory hierarchy to distribute files:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/store\",\n      \"project_name\": \"my_project\",\n      \"subfolding\": [2, 2]\n    }\n  }\n}\n</code></pre> <p>With <code>[2, 2]</code> subfolding, hash-addressed paths become:</p> <pre><code>_hash/{schema}/ab/cd/abcdefghijklmnopqrstuvwxyz\n</code></pre> <p>Schema-addressed storage (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>) does not use subfolding\u2014it uses key-based paths:</p> <pre><code>{location}/_schema/{partition}/{schema}/{table}/{key}/{field_name}.{token}.{ext}\n</code></pre>"},{"location":"how-to/configure-storage/#filesystem-recommendations","title":"Filesystem Recommendations","text":"Filesystem Subfolding Needed Notes ext3 Yes Limited directory indexing FAT32/exFAT Yes Linear directory scans NFS Yes Network latency amplifies directory lookups CIFS/SMB Yes Windows network shares ext4 No HTree indexing handles large directories XFS No B+ tree directories scale well ZFS No Efficient directory handling Btrfs No B-tree based S3/MinIO No Object storage uses hash-based lookups GCS No Object storage Azure Blob No Object storage <p>Recommendation: Use <code>[2, 2]</code> for network-mounted filesystems and legacy systems. Modern local filesystems and cloud object storage work well without subfolding.</p>"},{"location":"how-to/configure-storage/#url-representation","title":"URL Representation","text":"<p>DataJoint uses consistent URL representation for all storage backends internally. This means:</p> <ul> <li>Local filesystem paths are represented as <code>file://</code> URLs</li> <li>S3 paths use <code>s3://bucket/path</code></li> <li>GCS paths use <code>gs://bucket/path</code></li> <li>Azure paths use <code>az://container/path</code></li> </ul> <p>You can use either format when specifying paths:</p> <pre><code># Both are equivalent for local files\n\"/data/myfile.dat\"\n\"file:///data/myfile.dat\"\n</code></pre> <p>This unified approach enables:</p> <ul> <li>Consistent internal handling across all storage types</li> <li>Seamless switching between local and cloud storage</li> <li>Integration with fsspec for streaming access</li> </ul>"},{"location":"how-to/configure-storage/#customizing-storage-sections","title":"Customizing Storage Sections","text":"<p>Each store is divided into sections for different storage types. By default, DataJoint uses <code>_hash/</code> for hash-addressed storage and <code>_schema/</code> for schema-addressed storage. You can customize the path prefix for each section using the <code>*_prefix</code> configuration parameters to map DataJoint to existing storage layouts:</p> <pre><code>{\n  \"stores\": {\n    \"legacy\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/existing_storage\",\n      \"hash_prefix\": \"content_addressed\",\n      \"schema_prefix\": \"structured_data\",\n      \"filepath_prefix\": \"raw_files\"\n    }\n  }\n}\n</code></pre> <p>Requirements:</p> <ul> <li>Sections must be mutually exclusive (path prefixes cannot nest)</li> <li>The <code>hash_prefix</code> and <code>schema_prefix</code> sections are reserved for DataJoint-managed storage</li> <li>The <code>filepath_prefix</code> is optional (<code>null</code> = unrestricted, or set a required prefix)</li> </ul> <p>Example with hierarchical layout:</p> <pre><code>{\n  \"stores\": {\n    \"organized\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"neuroscience-data\",\n      \"location\": \"lab-project-2024\",\n      \"hash_prefix\": \"managed/blobs\",      // Path prefix for hash section\n      \"schema_prefix\": \"managed/arrays\",    // Path prefix for schema section\n      \"filepath_prefix\": \"imported\"         // Path prefix for filepath section\n    }\n  }\n}\n</code></pre> <p>Storage section paths become:</p> <ul> <li>Hash: <code>s3://neuroscience-data/lab-project-2024/managed/blobs/{schema}/{hash}</code></li> <li>Schema: <code>s3://neuroscience-data/lab-project-2024/managed/arrays/{schema}/{table}/{key}/</code></li> <li>Filepath: <code>s3://neuroscience-data/lab-project-2024/imported/{user_path}</code></li> </ul>"},{"location":"how-to/configure-storage/#reserved-sections-and-filepath-storage","title":"Reserved Sections and Filepath Storage","text":"<p>DataJoint reserves sections within each store for managed storage. These sections are defined by prefix configuration parameters:</p> <ul> <li>Hash-addressed section (configured via <code>hash_prefix</code>, default: <code>_hash/</code>) \u2014 Content-addressed storage for <code>&lt;blob@&gt;</code> and <code>&lt;attach@&gt;</code> with deduplication</li> <li>Schema-addressed section (configured via <code>schema_prefix</code>, default: <code>_schema/</code>) \u2014 Key-based storage for <code>&lt;object@&gt;</code> and <code>&lt;npy@&gt;</code> with streaming access</li> </ul>"},{"location":"how-to/configure-storage/#user-managed-filepath-storage","title":"User-Managed Filepath Storage","text":"<p>The <code>&lt;filepath@&gt;</code> codec stores paths to files that you manage. DataJoint does not manage lifecycle (no garbage collection), integrity (no transaction safety), or deduplication for filepath storage. You can reference existing files or create new ones\u2014DataJoint simply stores the path string. Files can be anywhere in the store except the reserved sections:</p> <pre><code>@schema\nclass RawData(dj.Manual):\n    definition = \"\"\"\n    session_id : int\n    ---\n    recording : &lt;filepath@acquisition&gt;  # User-managed file path\n    \"\"\"\n\n# Valid paths (user-managed)\ntable.insert1({'session_id': 1, 'recording': 'subject01/session001/data.bin'})  # Existing or new file\ntable.insert1({'session_id': 2, 'recording': 'raw/experiment_2024/data.nwb'})  # Existing or new file\n\n# Invalid paths (reserved for DataJoint - will raise ValueError)\n# These use the default prefixes (_hash and _schema)\ntable.insert1({'session_id': 3, 'recording': '_hash/abc123...'})      # Error!\ntable.insert1({'session_id': 4, 'recording': '_schema/myschema/...'}) # Error!\n\n# If you configured custom prefixes like \"content_addressed\", those would also be blocked\n# table.insert1({'session_id': 5, 'recording': 'content_addressed/file.dat'})  # Error!\n</code></pre> <p>Key characteristics of <code>&lt;filepath@&gt;</code>:</p> <ul> <li>Stores path string only (DataJoint does not manage the files)</li> <li>No lifecycle management: no garbage collection, no transaction safety, no deduplication</li> <li>User controls file creation, organization, and deletion</li> <li>Can reference existing files or create new ones</li> <li>Returns ObjectRef for lazy access on fetch</li> <li>Validates file exists on insert</li> <li>Cannot use reserved sections (configured by <code>hash_prefix</code> and <code>schema_prefix</code>)</li> <li>Can be restricted to specific prefix using <code>filepath_prefix</code> configuration</li> </ul>"},{"location":"how-to/configure-storage/#see-also","title":"See Also","text":"<ul> <li>Use Object Storage \u2014 When and how to use object storage</li> <li>Manage Large Data \u2014 Working with blobs and objects</li> </ul>"},{"location":"how-to/create-custom-codec/","title":"Create Custom Codecs","text":"<p>Define domain-specific types for seamless storage and retrieval.</p>"},{"location":"how-to/create-custom-codec/#overview","title":"Overview","text":"<p>Codecs transform Python objects for storage. Create custom codecs for:</p> <ul> <li>Domain-specific data types (graphs, images, alignments)</li> <li>Specialized serialization formats</li> <li>Integration with external libraries</li> </ul>"},{"location":"how-to/create-custom-codec/#basic-codec-structure","title":"Basic Codec Structure","text":"<pre><code>import datajoint as dj\n\nclass GraphCodec(dj.Codec):\n    \"\"\"Store NetworkX graphs.\"\"\"\n\n    name = \"graph\"  # Used as &lt;graph&gt; in definitions\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"  # Delegate to blob for serialization\n\n    def encode(self, value, *, key=None, store_name=None):\n        import networkx as nx\n        assert isinstance(value, nx.Graph)\n        return list(value.edges)\n\n    def decode(self, stored, *, key=None):\n        import networkx as nx\n        return nx.Graph(stored)\n</code></pre>"},{"location":"how-to/create-custom-codec/#use-in-table-definition","title":"Use in Table Definition","text":"<pre><code>@schema\nclass Connectivity(dj.Manual):\n    definition = \"\"\"\n    conn_id : int\n    ---\n    network : &lt;graph&gt;           # Uses GraphCodec\n    network_large : &lt;graph@&gt;    # In-store\n    \"\"\"\n</code></pre>"},{"location":"how-to/create-custom-codec/#required-methods","title":"Required Methods","text":""},{"location":"how-to/create-custom-codec/#get_dtypeis_store","title":"<code>get_dtype(is_store)</code>","text":"<p>Return the storage type:</p> <ul> <li><code>is_store=False</code>: Inline storage (in database column)</li> <li><code>is_store=True</code>: Object store (with <code>@</code> modifier)</li> </ul> <pre><code>def get_dtype(self, is_store: bool) -&gt; str:\n    if is_store:\n        return \"&lt;blob@&gt;\"  # Blob in object storage\n    return \"bytes\"        # Inline database blob\n</code></pre> <p>Common return values:</p> <ul> <li><code>\"bytes\"</code> \u2014 Binary in database</li> <li><code>\"json\"</code> \u2014 JSON in database</li> <li><code>\"&lt;blob&gt;\"</code> \u2014 Chain to blob codec (in-table storage)</li> <li><code>\"&lt;blob@&gt;\"</code> \u2014 Blob in object storage</li> </ul>"},{"location":"how-to/create-custom-codec/#encodevalue-keynone-store_namenone","title":"<code>encode(value, *, key=None, store_name=None)</code>","text":"<p>Convert Python object to storable format:</p> <pre><code>def encode(self, value, *, key=None, store_name=None):\n    # value: Python object to store\n    # key: Primary key dict (for path construction)\n    # store_name: Target store name\n    return serialized_representation\n</code></pre>"},{"location":"how-to/create-custom-codec/#decodestored-keynone","title":"<code>decode(stored, *, key=None)</code>","text":"<p>Reconstruct Python object:</p> <pre><code>def decode(self, stored, *, key=None):\n    # stored: Data from storage\n    # key: Primary key dict\n    return python_object\n</code></pre>"},{"location":"how-to/create-custom-codec/#optional-validation","title":"Optional: Validation","text":"<p>Override <code>validate()</code> for type checking:</p> <pre><code>def validate(self, value):\n    import networkx as nx\n    if not isinstance(value, nx.Graph):\n        raise TypeError(f\"Expected nx.Graph, got {type(value).__name__}\")\n</code></pre>"},{"location":"how-to/create-custom-codec/#codec-chaining","title":"Codec Chaining","text":"<p>Codecs can delegate to other codecs:</p> <pre><code>class ImageCodec(dj.Codec):\n    name = \"image\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"  # Chain to blob codec\n\n    def encode(self, value, *, key=None, store_name=None):\n        # Convert PIL Image to numpy array\n        # Blob codec handles numpy serialization\n        return np.array(value)\n\n    def decode(self, stored, *, key=None):\n        from PIL import Image\n        return Image.fromarray(stored)\n</code></pre>"},{"location":"how-to/create-custom-codec/#store-only-codecs","title":"Store-Only Codecs","text":"<p>Some codecs require object storage (@ modifier):</p> <pre><code>class ZarrCodec(dj.Codec):\n    name = \"zarr\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        if not is_store:\n            raise DataJointError(\"&lt;zarr&gt; requires @ (store only)\")\n        return \"&lt;object@&gt;\"  # Schema-addressed storage\n\n    def encode(self, path, *, key=None, store_name=None):\n        return path  # Path to zarr directory\n\n    def decode(self, stored, *, key=None):\n        return stored  # Returns ObjectRef for lazy access\n</code></pre> <p>For custom file formats, consider inheriting from <code>SchemaCodec</code>:</p> <pre><code>class ParquetCodec(dj.SchemaCodec):\n    \"\"\"Store DataFrames as Parquet files.\"\"\"\n    name = \"parquet\"\n\n    # get_dtype inherited: requires @, returns \"json\"\n\n    def encode(self, df, *, key=None, store_name=None):\n        schema, table, field, pk = self._extract_context(key)\n        path, _ = self._build_path(schema, table, field, pk, ext=\".parquet\")\n        backend = self._get_backend(store_name)\n        # ... upload parquet file\n        return {\"path\": path, \"store\": store_name, \"shape\": list(df.shape)}\n\n    def decode(self, stored, *, key=None):\n        return ParquetRef(stored, self._get_backend(stored.get(\"store\")))\n</code></pre>"},{"location":"how-to/create-custom-codec/#auto-registration","title":"Auto-Registration","text":"<p>Codecs register automatically when defined:</p> <pre><code>class MyCodec(dj.Codec):\n    name = \"mytype\"  # Registers as &lt;mytype&gt;\n    ...\n\n# Now usable in table definitions:\n# my_attr : &lt;mytype&gt;\n</code></pre> <p>Skip registration for abstract bases:</p> <pre><code>class BaseCodec(dj.Codec, register=False):\n    # Abstract base, not registered\n    pass\n</code></pre>"},{"location":"how-to/create-custom-codec/#complete-example","title":"Complete Example","text":"<pre><code>import datajoint as dj\nimport SimpleITK as sitk\nimport numpy as np\n\nclass MedicalImageCodec(dj.Codec):\n    \"\"\"Store SimpleITK medical images with metadata.\"\"\"\n\n    name = \"medimage\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob@&gt;\" if is_store else \"&lt;blob&gt;\"\n\n    def encode(self, image, *, key=None, store_name=None):\n        return {\n            'array': sitk.GetArrayFromImage(image),\n            'spacing': image.GetSpacing(),\n            'origin': image.GetOrigin(),\n            'direction': image.GetDirection(),\n        }\n\n    def decode(self, stored, *, key=None):\n        image = sitk.GetImageFromArray(stored['array'])\n        image.SetSpacing(stored['spacing'])\n        image.SetOrigin(stored['origin'])\n        image.SetDirection(stored['direction'])\n        return image\n\n    def validate(self, value):\n        if not isinstance(value, sitk.Image):\n            raise TypeError(f\"Expected sitk.Image, got {type(value).__name__}\")\n\n\n@schema\nclass Scan(dj.Manual):\n    definition = \"\"\"\n    scan_id : uuid\n    ---\n    ct_image : &lt;medimage@&gt;      # CT scan with metadata\n    \"\"\"\n</code></pre>"},{"location":"how-to/create-custom-codec/#see-also","title":"See Also","text":"<ul> <li>Use Object Storage \u2014 Storage patterns</li> <li>Manage Large Data \u2014 Working with large objects</li> </ul>"},{"location":"how-to/define-tables/","title":"Define Tables","text":"<p>Create DataJoint table classes with proper definitions.</p>"},{"location":"how-to/define-tables/#create-a-schema","title":"Create a Schema","text":"<pre><code>import datajoint as dj\n\nschema = dj.Schema('my_schema')  # Creates schema in database if it doesn't exist\n</code></pre> <p>The <code>Schema</code> object connects to the database and creates the schema (database) if it doesn't already exist.</p>"},{"location":"how-to/define-tables/#basic-table-structure","title":"Basic Table Structure","text":"<pre><code>@schema\nclass MyTable(dj.Manual):\n    definition = \"\"\"\n    # Table comment (optional)\n    primary_attr : type      # attribute comment\n    ---\n    secondary_attr : type    # attribute comment\n    optional_attr = null : type\n    \"\"\"\n</code></pre>"},{"location":"how-to/define-tables/#table-types","title":"Table Types","text":"Type Base Class Purpose Manual <code>dj.Manual</code> User-entered data Lookup <code>dj.Lookup</code> Reference data with <code>contents</code> Imported <code>dj.Imported</code> Data from external sources Computed <code>dj.Computed</code> Derived data Part <code>dj.Part</code> Child of master table"},{"location":"how-to/define-tables/#primary-key-above-","title":"Primary Key (Above <code>---</code>)","text":"<pre><code>definition = \"\"\"\nsubject_id : varchar(16)      # Subject identifier\nsession_idx : int32          # Session number\n---\n...\n\"\"\"\n</code></pre> <p>Primary key attributes:</p> <ul> <li>Cannot be NULL</li> <li>Must be unique together</li> <li>Cannot be changed after insertion</li> </ul>"},{"location":"how-to/define-tables/#secondary-attributes-below-","title":"Secondary Attributes (Below <code>---</code>)","text":"<pre><code>definition = \"\"\"\n...\n---\nsession_date : date           # Required attribute\nnotes = '' : varchar(1000)    # Optional with default\nscore = null : float32        # Nullable attribute\n\"\"\"\n</code></pre>"},{"location":"how-to/define-tables/#default-values-and-nullable-attributes","title":"Default Values and Nullable Attributes","text":"<p>Default values are specified with <code>= value</code> before the type:</p> <pre><code>definition = \"\"\"\nsubject_id : varchar(16)\n---\nweight = null : float32           # Nullable (default is NULL)\nnotes = '' : varchar(1000)        # Default empty string\nis_active = 1 : bool              # Default true\ncreated = CURRENT_TIMESTAMP : timestamp\n\"\"\"\n</code></pre> <p>Key rules:</p> <ul> <li>The only way to make an attribute nullable is <code>= null</code></li> <li>Attributes without defaults are required (NOT NULL)</li> <li>Primary key attributes cannot be nullable</li> <li>Primary key attributes cannot have static defaults</li> </ul> <p>Timestamp defaults:</p> <p>Primary keys can use time-dependent defaults like <code>CURRENT_TIMESTAMP</code>:</p> <pre><code>definition = \"\"\"\ncreated_at = CURRENT_TIMESTAMP : timestamp(6)   # Microsecond precision\n---\ndata : &lt;blob&gt;\n\"\"\"\n</code></pre> <p>Timestamp precision options:</p> <ul> <li><code>timestamp</code> or <code>datetime</code> \u2014 Second precision</li> <li><code>timestamp(3)</code> or <code>datetime(3)</code> \u2014 Millisecond precision</li> <li><code>timestamp(6)</code> or <code>datetime(6)</code> \u2014 Microsecond precision</li> </ul>"},{"location":"how-to/define-tables/#auto-increment-not-recommended","title":"Auto-Increment (Not Recommended)","text":"<p>DataJoint core types do not support <code>AUTO_INCREMENT</code>. This is intentional\u2014explicit key values enforce entity integrity and prevent silent creation of duplicate records.</p> <p>Use <code>uuid</code> or natural keys instead:</p> <pre><code>definition = \"\"\"\nrecording_id : uuid              # Globally unique, client-generated\n---\n...\n\"\"\"\n</code></pre> <p>If you must use auto-increment, native MySQL types allow it (with a warning):</p> <pre><code>definition = \"\"\"\nrecord_id : int unsigned auto_increment    # Native type\n---\n...\n\"\"\"\n</code></pre> <p>See Design Primary Keys for detailed guidance on key selection and why DataJoint avoids auto-increment.</p>"},{"location":"how-to/define-tables/#core-datajoint-types","title":"Core DataJoint Types","text":"Type Description <code>bool</code> Boolean (true/false) <code>int8</code>, <code>int16</code>, <code>int32</code>, <code>int64</code> Signed integers <code>float32</code>, <code>float64</code> Floating point <code>decimal(m,n)</code> Fixed precision decimal <code>varchar(n)</code> Variable-length string <code>char(n)</code> Fixed-length string <code>date</code> Date (YYYY-MM-DD) <code>datetime</code> Date and time <code>datetime(3)</code> With millisecond precision <code>datetime(6)</code> With microsecond precision <code>uuid</code> UUID type <code>enum('a', 'b', 'c')</code> Enumerated values <code>json</code> JSON data <code>bytes</code> Raw binary data"},{"location":"how-to/define-tables/#built-in-codecs","title":"Built-in Codecs","text":"<p>Codecs serialize Python objects to database storage. Use angle brackets for codec types:</p> Codec Description <code>&lt;blob&gt;</code> Serialized Python objects (NumPy arrays, etc.) stored in database <code>&lt;blob@store&gt;</code> Serialized objects in object storage <code>&lt;attach&gt;</code> File attachments in database <code>&lt;attach@store&gt;</code> File attachments in object storage <code>&lt;object@store&gt;</code> Files/folders via ObjectRef (schema-addressed, supports Zarr/HDF5) <p>Example:</p> <pre><code>definition = \"\"\"\nrecording_id : uuid\n---\nneural_data : &lt;blob@raw&gt;      # NumPy array in 'raw' store\nconfig_file : &lt;attach&gt;        # Attached file in database\nparameters : json             # JSON data (core type, no brackets)\n\"\"\"\n</code></pre>"},{"location":"how-to/define-tables/#native-database-types","title":"Native Database Types","text":"<p>You can also use native MySQL/MariaDB types directly when needed:</p> <pre><code>definition = \"\"\"\nrecord_id : int unsigned      # Native MySQL type\ndata : mediumblob             # For larger binary data\ndescription : text            # Unlimited text\n\"\"\"\n</code></pre> <p>Native types are flagged with a warning at declaration time but are allowed. Core DataJoint types (like <code>int32</code>, <code>float64</code>) are portable and recommended for most use cases. Native database types provide access to database-specific features when needed.</p>"},{"location":"how-to/define-tables/#foreign-keys","title":"Foreign Keys","text":"<pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject                # References Subject table\n    session_idx : int32\n    ---\n    session_date : date\n    \"\"\"\n</code></pre> <p>The <code>-&gt;</code> inherits primary key attributes from the referenced table.</p>"},{"location":"how-to/define-tables/#foreign-key-modifiers","title":"Foreign Key Modifiers","text":"<p>Use modifiers in brackets to change foreign key behavior:</p> Modifier Effect <code>[nullable]</code> Makes FK attributes nullable (optional relationship) <code>[unique]</code> Creates UNIQUE INDEX on FK attributes (one-to-one) <code>[nullable, unique]</code> Both: optional one-to-one relationship <pre><code>@schema\nclass SessionAnnotation(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    -&gt; [nullable] Experimenter     # Optional: experimenter may be unknown\n    -&gt; [unique] Protocol           # Unique: each protocol used at most once\n    -&gt; [nullable, unique] Reviewer # Optional &amp; unique\n    \"\"\"\n</code></pre> <p>Placement rules:</p> <ul> <li><code>[nullable]</code> only allowed in secondary position (below <code>---</code>)</li> <li><code>[unique]</code> allowed in both primary and secondary positions</li> <li>Primary key FKs cannot be nullable</li> </ul> <p>Nullable unique behavior:</p> <p>Multiple rows can have NULL in a <code>[nullable, unique]</code> FK because SQL's UNIQUE constraint does not consider NULLs equal. This enables optional one-to-one relationships.</p>"},{"location":"how-to/define-tables/#lookup-tables-with-contents","title":"Lookup Tables with Contents","text":"<pre><code>@schema\nclass TaskType(dj.Lookup):\n    definition = \"\"\"\n    task_type : varchar(32)\n    ---\n    description : varchar(200)\n    \"\"\"\n    contents = [\n        {'task_type': 'detection', 'description': 'Detect target stimulus'},\n        {'task_type': 'discrimination', 'description': 'Distinguish between stimuli'},\n    ]\n</code></pre>"},{"location":"how-to/define-tables/#part-tables","title":"Part Tables","text":"<pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int32\n    ---\n    session_date : date\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32\n        ---\n        outcome : enum('hit', 'miss')\n        reaction_time : float32\n        \"\"\"\n</code></pre>"},{"location":"how-to/define-tables/#computed-tables","title":"Computed Tables","text":"<pre><code>@schema\nclass SessionStats(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    n_trials : int64\n    hit_rate : float32\n    \"\"\"\n\n    def make(self, key):\n        trials = (Session.Trial &amp; key).to_dicts()\n        self.insert1({\n            **key,\n            'n_trials': len(trials),\n            'hit_rate': sum(t['outcome'] == 'hit' for t in trials) / len(trials)\n        })\n</code></pre>"},{"location":"how-to/define-tables/#indexes","title":"Indexes","text":"<p>Declare indexes at the end of the definition, after all attributes:</p> <pre><code>definition = \"\"\"\nsubject_id : varchar(16)\nsession_idx : int32\n---\nsession_date : date\nexperimenter : varchar(50)\nindex (session_date)              # Index for faster queries\nindex (experimenter)              # Another index\nunique index (external_id)        # Unique constraint\n\"\"\"\n</code></pre>"},{"location":"how-to/define-tables/#declaring-tables","title":"Declaring Tables","text":"<p>Tables are declared in the database when the <code>@schema</code> decorator applies to the class:</p> <pre><code>@schema  # Table is declared here\nclass Session(dj.Manual):\n    definition = \"\"\"\n    session_id : int32\n    ---\n    session_date : date\n    \"\"\"\n</code></pre> <p>The decorator reads the <code>definition</code> string, parses it, and creates the corresponding table in the database if it doesn't exist.</p>"},{"location":"how-to/define-tables/#dropping-tables-and-schemas","title":"Dropping Tables and Schemas","text":"<p>During prototyping (before data are populated), you can drop and recreate tables:</p> <pre><code># Drop a single table\nSession.drop()\n\n# Drop entire schema (all tables)\nschema.drop()\n</code></pre> <p>Warning: These operations permanently delete data. Use only during development.</p>"},{"location":"how-to/define-tables/#view-table-definition","title":"View Table Definition","text":"<pre><code># Show SQL definition\nprint(Session().describe())\n\n# Show heading\nprint(Session().heading)\n</code></pre>"},{"location":"how-to/define-tables/#see-also","title":"See Also","text":"<ul> <li>Master-Part Tables \u2014 Working with part tables</li> <li>Model Relationships \u2014 Foreign key patterns</li> <li>Design Primary Keys \u2014 Key selection strategies</li> <li>Insert Data \u2014 Adding data to tables</li> </ul>"},{"location":"how-to/delete-data/","title":"Delete Data","text":"<p>Remove data safely with proper cascade handling.</p>"},{"location":"how-to/delete-data/#basic-delete","title":"Basic Delete","text":"<p>Delete rows matching a restriction:</p> <pre><code># Delete specific subject\n(Subject &amp; {'subject_id': 'M001'}).delete()\n\n# Delete with condition\n(Session &amp; \"session_date &lt; '2024-01-01'\").delete()\n</code></pre>"},{"location":"how-to/delete-data/#cascade-behavior","title":"Cascade Behavior","text":"<p>Deleting a row automatically cascades to all dependent tables:</p> <pre><code># Deletes subject AND all their sessions AND all trials\n(Subject &amp; {'subject_id': 'M001'}).delete()\n</code></pre> <p>This maintains referential integrity\u2014no orphaned records remain.</p>"},{"location":"how-to/delete-data/#confirmation-prompt","title":"Confirmation Prompt","text":"<p>The <code>prompt</code> parameter controls confirmation behavior:</p> <pre><code># Uses dj.config['safemode'] setting (default behavior)\n(Subject &amp; key).delete()\n\n# Explicitly skip confirmation\n(Subject &amp; key).delete(prompt=False)\n\n# Explicitly require confirmation\n(Subject &amp; key).delete(prompt=True)\n</code></pre> <p>When prompted, you'll see what will be deleted:</p> <pre><code>About to delete:\n  1 rows from `lab`.`subject`\n  5 rows from `lab`.`session`\n  127 rows from `lab`.`trial`\n\nProceed? [yes, No]:\n</code></pre>"},{"location":"how-to/delete-data/#safe-mode-configuration","title":"Safe Mode Configuration","text":"<p>Control the default prompting behavior:</p> <pre><code>import datajoint as dj\n\n# Check current setting\nprint(dj.config['safemode'])\n\n# Disable prompts globally (use with caution)\ndj.config['safemode'] = False\n\n# Re-enable prompts\ndj.config['safemode'] = True\n</code></pre> <p>Or temporarily override:</p> <pre><code>with dj.config.override(safemode=False):\n    (Subject &amp; restriction).delete()\n</code></pre>"},{"location":"how-to/delete-data/#transaction-handling","title":"Transaction Handling","text":"<p>Deletes are atomic\u2014all cascading deletes succeed or none do:</p> <pre><code># All-or-nothing delete (default)\n(Subject &amp; restriction).delete(transaction=True)\n</code></pre> <p>Within an existing transaction:</p> <pre><code>with dj.conn().transaction:\n    (Table1 &amp; key1).delete(transaction=False)\n    (Table2 &amp; key2).delete(transaction=False)\n    Table3.insert(rows)\n</code></pre>"},{"location":"how-to/delete-data/#part-tables","title":"Part Tables","text":"<p>Part tables cannot be deleted directly by default (master-part integrity):</p> <pre><code># This raises an error\nSession.Trial.delete()  # DataJointError\n\n# Delete from master instead (cascades to parts)\n(Session &amp; key).delete()\n</code></pre> <p>Use <code>part_integrity</code> to control this behavior:</p> <pre><code># Allow direct deletion (breaks master-part integrity)\n(Session.Trial &amp; key).delete(part_integrity=\"ignore\")\n\n# Delete parts AND cascade up to delete master\n(Session.Trial &amp; key).delete(part_integrity=\"cascade\")\n</code></pre> Policy Behavior <code>\"enforce\"</code> (default) Error if parts deleted without masters <code>\"ignore\"</code> Allow deleting parts without masters <code>\"cascade\"</code> Also delete masters when parts are deleted"},{"location":"how-to/delete-data/#quick-delete","title":"Quick Delete","text":"<p>Delete without cascade (fails if dependent rows exist):</p> <pre><code># Only works if no dependent tables have matching rows\n(Subject &amp; key).delete_quick()\n</code></pre>"},{"location":"how-to/delete-data/#delete-patterns","title":"Delete Patterns","text":""},{"location":"how-to/delete-data/#by-primary-key","title":"By Primary Key","text":"<pre><code>(Session &amp; {'subject_id': 'M001', 'session_idx': 1}).delete()\n</code></pre>"},{"location":"how-to/delete-data/#by-condition","title":"By Condition","text":"<pre><code>(Trial &amp; \"outcome = 'miss'\").delete()\n</code></pre>"},{"location":"how-to/delete-data/#by-join","title":"By Join","text":"<pre><code># Delete trials from sessions before 2024\nold_sessions = Session &amp; \"session_date &lt; '2024-01-01'\"\n(Trial &amp; old_sessions).delete()\n</code></pre>"},{"location":"how-to/delete-data/#all-rows","title":"All Rows","text":"<pre><code># Delete everything in table (and dependents)\nMyTable.delete()\n</code></pre>"},{"location":"how-to/delete-data/#the-recomputation-pattern","title":"The Recomputation Pattern","text":"<p>When source data needs correction, use delete \u2192 insert \u2192 populate:</p> <pre><code>key = {'subject_id': 'M001', 'session_idx': 1}\n\n# 1. Delete cascades to computed tables\n(Session &amp; key).delete(prompt=False)\n\n# 2. Reinsert with corrected data\nwith dj.conn().transaction:\n    Session.insert1({**key, 'session_date': '2024-01-08', 'duration': 40.0})\n    Session.Trial.insert(corrected_trials)\n\n# 3. Recompute derived data\nProcessedData.populate()\n</code></pre> <p>This ensures all derived data remains consistent with source data.</p>"},{"location":"how-to/delete-data/#return-value","title":"Return Value","text":"<p><code>delete()</code> returns the count of deleted rows from the primary table:</p> <pre><code>count = (Subject &amp; restriction).delete(prompt=False)\nprint(f\"Deleted {count} subjects\")\n</code></pre>"},{"location":"how-to/delete-data/#see-also","title":"See Also","text":"<ul> <li>Master-Part Tables \u2014 Compositional data patterns</li> <li>Model Relationships \u2014 Foreign key patterns</li> <li>Insert Data \u2014 Adding data to tables</li> <li>Run Computations \u2014 Recomputing after changes</li> </ul>"},{"location":"how-to/deploy-production/","title":"Deploy to Production","text":"<p>Configure DataJoint for production environments with controlled schema changes and project isolation.</p>"},{"location":"how-to/deploy-production/#overview","title":"Overview","text":"<p>Development and production environments have different requirements:</p> Concern Development Production Schema changes Automatic table creation Controlled, explicit changes only Naming Ad-hoc schema names Consistent project prefixes Configuration Local settings Environment-based <p>DataJoint 2.0 provides settings to enforce production discipline.</p>"},{"location":"how-to/deploy-production/#prevent-automatic-table-creation","title":"Prevent Automatic Table Creation","text":"<p>By default, DataJoint creates tables automatically when you first access them. This is convenient during development but dangerous in production\u2014a typo or code bug could create unintended tables.</p>"},{"location":"how-to/deploy-production/#enable-production-mode","title":"Enable Production Mode","text":"<p>Set <code>create_tables=False</code> to prevent automatic table creation:</p> <pre><code>import datajoint as dj\n\n# Production mode: no automatic table creation\ndj.config.database.create_tables = False\n</code></pre> <p>Or via environment variable:</p> <pre><code>export DJ_CREATE_TABLES=false\n</code></pre> <p>Or in <code>datajoint.json</code>:</p> <pre><code>{\n  \"database\": {\n    \"create_tables\": false\n  }\n}\n</code></pre>"},{"location":"how-to/deploy-production/#what-changes","title":"What Changes","text":"<p>With <code>create_tables=False</code>:</p> Action Development (True) Production (False) Access existing table Works Works Access missing table Creates it Raises error Explicit <code>Schema(create_tables=True)</code> Creates Creates (override)"},{"location":"how-to/deploy-production/#example-production-safety","title":"Example: Production Safety","text":"<pre><code>import datajoint as dj\n\ndj.config.database.create_tables = False\nschema = dj.Schema('myproject_ephys')\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int\n    ---\n    path : varchar(255)\n    \"\"\"\n\n# If table doesn't exist in database:\nRecording()  # Raises DataJointError: Table not found\n</code></pre>"},{"location":"how-to/deploy-production/#override-for-migrations","title":"Override for Migrations","text":"<p>When you need to create tables during a controlled migration:</p> <pre><code># Explicit override for this schema only\nschema = dj.Schema('myproject_ephys', create_tables=True)\n\n@schema\nclass NewTable(dj.Manual):\n    definition = \"\"\"...\"\"\"\n\nNewTable()  # Creates the table\n</code></pre>"},{"location":"how-to/deploy-production/#use-database-prefixes","title":"Use Database Prefixes","text":"<p>When multiple projects share a database server, use prefixes to avoid naming collisions and organize schemas.</p>"},{"location":"how-to/deploy-production/#configure-database-prefix","title":"Configure Database Prefix","text":"<pre><code>import datajoint as dj\n\ndj.config.database.database_prefix = 'myproject_'\n</code></pre> <p>Or via environment variable:</p> <pre><code>export DJ_DATABASE_PREFIX=myproject_\n</code></pre> <p>Or in <code>datajoint.json</code>:</p> <pre><code>{\n  \"database\": {\n    \"database_prefix\": \"myproject_\"\n  }\n}\n</code></pre>"},{"location":"how-to/deploy-production/#apply-prefix-to-schemas","title":"Apply Prefix to Schemas","text":"<p>Use the prefix when creating schemas:</p> <pre><code>import datajoint as dj\n\nprefix = dj.config.database.database_prefix  # 'myproject_'\n\n# Schema names include prefix\nsubject_schema = dj.Schema(prefix + 'subject')   # myproject_subject\nsession_schema = dj.Schema(prefix + 'session')   # myproject_session\nephys_schema = dj.Schema(prefix + 'ephys')       # myproject_ephys\n</code></pre>"},{"location":"how-to/deploy-production/#benefits","title":"Benefits","text":"<ul> <li>Isolation: Multiple projects coexist without conflicts</li> <li>Visibility: Easy to identify which schemas belong to which project</li> <li>Permissions: Grant access by prefix pattern (<code>myproject_*</code>)</li> <li>Cleanup: Drop all project schemas by prefix</li> </ul>"},{"location":"how-to/deploy-production/#database-permissions-by-prefix","title":"Database Permissions by Prefix","text":"<pre><code>-- Grant access to all schemas with prefix\nGRANT ALL PRIVILEGES ON `myproject_%`.* TO 'developer'@'10.0.0.%';\n\n-- Read-only access to another project\nGRANT SELECT ON `otherproject_%`.* TO 'developer'@'10.0.0.%';\n</code></pre> <p>Restrict Host Access</p> <p>Avoid using <code>'%'</code> for the host in production GRANT statements\u2014this allows connections from any IP address. Use specific IP addresses or subnet patterns like <code>'10.0.0.%'</code> to limit access to your internal network.</p>"},{"location":"how-to/deploy-production/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>Use different configurations for development, staging, and production.</p>"},{"location":"how-to/deploy-production/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>DataJoint loads settings in priority order:</p> <ol> <li>Environment variables (highest priority)</li> <li>Secrets directory (<code>.secrets/</code>)</li> <li>Config file (<code>datajoint.json</code>)</li> <li>Defaults (lowest priority)</li> </ol>"},{"location":"how-to/deploy-production/#development-setup","title":"Development Setup","text":"<p>datajoint.json (committed): <pre><code>{\n  \"database\": {\n    \"host\": \"localhost\",\n    \"create_tables\": true\n  }\n}\n</code></pre></p> <p>.secrets/database.user: <pre><code>dev_user\n</code></pre></p>"},{"location":"how-to/deploy-production/#production-setup","title":"Production Setup","text":"<p>Override via environment:</p> <pre><code># Production database\nexport DJ_HOST=prod-db.example.com\nexport DJ_USER=prod_user\nexport DJ_PASS=prod_password\n\n# Production mode\nexport DJ_CREATE_TABLES=false\nexport DJ_DATABASE_PREFIX=myproject_\n\n# Disable interactive prompts\nexport DJ_SAFEMODE=false\n</code></pre>"},{"location":"how-to/deploy-production/#dockerkubernetes-example","title":"Docker/Kubernetes Example","text":"<p>DataJoint automatically loads credentials from <code>/run/secrets/datajoint/</code> when that directory exists (standard Docker/Kubernetes secrets mount point).</p> <pre><code># docker-compose.yaml\nservices:\n  worker:\n    image: my-pipeline:latest\n    environment:\n      - DJ_HOST=db.example.com\n      - DJ_CREATE_TABLES=false\n      - DJ_DATABASE_PREFIX=prod_\n    volumes:\n      # Mount secrets directory\n      - type: bind\n        source: ./secrets\n        target: /run/secrets/datajoint\n        read_only: true\n</code></pre> <p>Create the secrets directory with credential files:</p> <pre><code>mkdir -p secrets\necho \"prod_user\" &gt; secrets/database.user\necho \"prod_password\" &gt; secrets/database.password\nchmod 600 secrets/*\n</code></pre> <p>For Kubernetes, use a Secret mounted to <code>/run/secrets/datajoint/</code>:</p> <pre><code># kubernetes deployment\nspec:\n  containers:\n    - name: worker\n      volumeMounts:\n        - name: dj-secrets\n          mountPath: /run/secrets/datajoint\n          readOnly: true\n  volumes:\n    - name: dj-secrets\n      secret:\n        secretName: datajoint-credentials\n</code></pre>"},{"location":"how-to/deploy-production/#complete-production-configuration","title":"Complete Production Configuration","text":""},{"location":"how-to/deploy-production/#datajointjson-committed","title":"datajoint.json (committed)","text":"<pre><code>{\n  \"database\": {\n    \"host\": \"localhost\",\n    \"port\": 3306\n  },\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"my-org-data\",\n      \"location\": \"myproject\"\n    }\n  }\n}\n</code></pre>"},{"location":"how-to/deploy-production/#production-environment-variables","title":"Production Environment Variables","text":"<pre><code># Database\nexport DJ_HOST=prod-mysql.example.com\nexport DJ_USER=prod_service\nexport DJ_PASS=&lt;from-secret-manager&gt;\n\n# Production behavior\nexport DJ_CREATE_TABLES=false\nexport DJ_DATABASE_PREFIX=prod_\nexport DJ_SAFEMODE=false\n\n# Logging\nexport DJ_LOG_LEVEL=WARNING\n</code></pre>"},{"location":"how-to/deploy-production/#verification-script","title":"Verification Script","text":"<pre><code>#!/usr/bin/env python\n\"\"\"Verify production configuration before deployment.\"\"\"\nimport datajoint as dj\n\ndef verify_production_config():\n    \"\"\"Check that production settings are correctly applied.\"\"\"\n    errors = []\n\n    # Check create_tables is disabled\n    if dj.config.database.create_tables:\n        errors.append(\"create_tables should be False in production\")\n\n    # Check database prefix is set\n    if not dj.config.database.database_prefix:\n        errors.append(\"database_prefix should be set in production\")\n\n    # Check not pointing to localhost\n    if dj.config.database.host == 'localhost':\n        errors.append(\"database.host is localhost - expected production host\")\n\n    if errors:\n        for e in errors:\n            print(f\"ERROR: {e}\")\n        return False\n\n    print(\"Production configuration verified\")\n    return True\n\nif __name__ == '__main__':\n    import sys\n    sys.exit(0 if verify_production_config() else 1)\n</code></pre>"},{"location":"how-to/deploy-production/#summary","title":"Summary","text":"Setting Development Production <code>database.create_tables</code> <code>true</code> <code>false</code> <code>database.database_prefix</code> <code>\"\"</code> or <code>dev_</code> <code>prod_</code> <code>safemode</code> <code>true</code> <code>false</code> (automated) <code>loglevel</code> <code>DEBUG</code> <code>WARNING</code>"},{"location":"how-to/deploy-production/#see-also","title":"See Also","text":"<ul> <li>Manage Pipeline Project \u2014 Project organization</li> <li>Configuration Reference \u2014 All settings</li> <li>Manage Secrets \u2014 Credential management</li> </ul>"},{"location":"how-to/design-primary-keys/","title":"Design Primary Keys","text":"<p>Choose effective primary keys for your tables.</p>"},{"location":"how-to/design-primary-keys/#primary-key-principles","title":"Primary Key Principles","text":"<p>Primary key attributes:</p> <ul> <li>Uniquely identify each entity</li> <li>Cannot be NULL</li> <li>Cannot be changed after insertion</li> <li>Are inherited by dependent tables via foreign keys</li> </ul>"},{"location":"how-to/design-primary-keys/#natural-keys","title":"Natural Keys","text":"<p>Use meaningful identifiers when they exist:</p> <pre><code>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)    # Lab-assigned ID like 'M001'\n    ---\n    species : varchar(32)\n    \"\"\"\n</code></pre> <p>Good candidates: - Lab-assigned IDs - Standard identifiers (NCBI accession, DOI) - Meaningful codes with enforced uniqueness</p>"},{"location":"how-to/design-primary-keys/#composite-keys","title":"Composite Keys","text":"<p>Combine attributes when a single attribute isn't unique:</p> <pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int32        # Session number within subject\n    ---\n    session_date : date\n    \"\"\"\n</code></pre> <p>The primary key is <code>(subject_id, session_idx)</code>.</p>"},{"location":"how-to/design-primary-keys/#surrogate-keys","title":"Surrogate Keys","text":"<p>Use UUIDs when natural keys don't exist:</p> <pre><code>@schema\nclass Experiment(dj.Manual):\n    definition = \"\"\"\n    experiment_id : uuid\n    ---\n    description : varchar(500)\n    \"\"\"\n</code></pre> <p>Generate UUIDs:</p> <pre><code>import uuid\n\nExperiment.insert1({\n    'experiment_id': uuid.uuid4(),\n    'description': 'Pilot study'\n})\n</code></pre>"},{"location":"how-to/design-primary-keys/#why-datajoint-avoids-auto-increment","title":"Why DataJoint Avoids Auto-Increment","text":"<p>DataJoint discourages <code>auto_increment</code> for primary keys:</p> <ol> <li> <p>Encourages lazy design \u2014 Users treat it as \"row number\" rather than thinking about what uniquely identifies the entity in their domain.</p> </li> <li> <p>Incompatible with composite keys \u2014 DataJoint schemas routinely use composite keys like <code>(subject_id, session_idx, trial_idx)</code>. MySQL allows only one auto_increment column per table, and it must be first in the key.</p> </li> <li> <p>Breaks reproducibility \u2014 Auto_increment values depend on insertion order. Rebuilding a pipeline produces different IDs.</p> </li> <li> <p>No client-server handshake \u2014 The client discovers the ID only after insertion, complicating error handling and concurrent access.</p> </li> <li> <p>Meaningless foreign keys \u2014 Downstream tables inherit opaque integers rather than traceable lineage.</p> </li> </ol> <p>Instead, use: - Natural keys that identify entities in your domain - UUIDs when no natural identifier exists - Composite keys combining foreign keys with sequence numbers</p>"},{"location":"how-to/design-primary-keys/#foreign-keys-in-primary-key","title":"Foreign Keys in Primary Key","text":"<p>Foreign keys above the <code>---</code> become part of the primary key:</p> <pre><code>@schema\nclass Trial(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session                  # In primary key\n    trial_idx : int32          # In primary key\n    ---\n    -&gt; Stimulus                  # NOT in primary key\n    outcome : enum('hit', 'miss')\n    \"\"\"\n</code></pre>"},{"location":"how-to/design-primary-keys/#key-design-guidelines","title":"Key Design Guidelines","text":""},{"location":"how-to/design-primary-keys/#keep-keys-small","title":"Keep Keys Small","text":"<p>Prefer <code>int32</code> over <code>int64</code> when the range allows:</p> <pre><code># Good: Appropriate size\nsession_idx : int32     # Max 65,535 sessions per subject\n\n# Avoid: Unnecessarily large\nsession_idx : int64      # Wastes space, slower joins\n</code></pre>"},{"location":"how-to/design-primary-keys/#use-fixed-width-for-joins","title":"Use Fixed-Width for Joins","text":"<p>Fixed-width types join faster:</p> <pre><code># Good: Fixed width\nsubject_id : char(8)\n\n# Acceptable: Variable width\nsubject_id : varchar(16)\n</code></pre>"},{"location":"how-to/design-primary-keys/#avoid-dates-as-primary-keys","title":"Avoid Dates as Primary Keys","text":"<p>Dates alone rarely guarantee uniqueness:</p> <pre><code># Bad: Date might not be unique\nsession_date : date\n---\n...\n\n# Good: Add a sequence number\n-&gt; Subject\nsession_idx : int32\n---\nsession_date : date\n</code></pre>"},{"location":"how-to/design-primary-keys/#avoid-computed-values","title":"Avoid Computed Values","text":"<p>Primary keys should be stable inputs, not derived:</p> <pre><code># Bad: Derived from other data\nhash_id : varchar(64)  # MD5 of some content\n\n# Good: Assigned identifier\nrecording_id : uuid\n</code></pre>"},{"location":"how-to/design-primary-keys/#migration-considerations","title":"Migration Considerations","text":"<p>Once a table has data, primary keys cannot be changed. Plan carefully:</p> <pre><code># Consider future needs\n@schema\nclass Scan(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    scan_idx : int16           # Might need int32 for high-throughput\n    ---\n    ...\n    \"\"\"\n</code></pre>"},{"location":"how-to/design-primary-keys/#see-also","title":"See Also","text":"<ul> <li>Define Tables \u2014 Table definition syntax</li> <li>Model Relationships \u2014 Foreign key patterns</li> </ul>"},{"location":"how-to/distributed-computing/","title":"Distributed Computing","text":"<p>Run computations across multiple workers with job coordination.</p>"},{"location":"how-to/distributed-computing/#enable-distributed-mode","title":"Enable Distributed Mode","text":"<p>Use <code>reserve_jobs=True</code> to enable job coordination:</p> <pre><code># Single worker (default)\nProcessedData.populate()\n\n# Distributed mode with job reservation\nProcessedData.populate(reserve_jobs=True)\n</code></pre>"},{"location":"how-to/distributed-computing/#how-it-works","title":"How It Works","text":"<p>With <code>reserve_jobs=True</code>: 1. Worker checks the jobs table for pending work 2. Atomically reserves a job before processing 3. Other workers see the job as reserved and skip it 4. On completion, job is marked success (or error)</p>"},{"location":"how-to/distributed-computing/#multi-process-on-single-machine","title":"Multi-Process on Single Machine","text":"<pre><code># Use multiple processes\nProcessedData.populate(reserve_jobs=True, processes=4)\n</code></pre> <p>Each process:</p> <ul> <li>Opens its own database connection</li> <li>Reserves jobs independently</li> <li>Processes in parallel</li> </ul>"},{"location":"how-to/distributed-computing/#multi-machine-cluster","title":"Multi-Machine Cluster","text":"<p>Run the same script on multiple machines:</p> <pre><code># worker_script.py - run on each machine\nimport datajoint as dj\nfrom my_pipeline import ProcessedData\n\n# Each worker reserves and processes different jobs\nProcessedData.populate(\n    reserve_jobs=True,\n    display_progress=True,\n    suppress_errors=True\n)\n</code></pre> <p>Workers automatically coordinate through the jobs table.</p>"},{"location":"how-to/distributed-computing/#job-table","title":"Job Table","text":"<p>Each auto-populated table has a jobs table (<code>~~table_name</code>):</p> <pre><code># View job status\nProcessedData.jobs\n\n# Filter by status\nProcessedData.jobs.pending\nProcessedData.jobs.reserved\nProcessedData.jobs.errors\nProcessedData.jobs.completed\n</code></pre>"},{"location":"how-to/distributed-computing/#job-statuses","title":"Job Statuses","text":"Status Description <code>pending</code> Queued, ready to process <code>reserved</code> Being processed by a worker <code>success</code> Completed successfully <code>error</code> Failed with error <code>ignore</code> Marked to skip"},{"location":"how-to/distributed-computing/#refresh-job-queue","title":"Refresh Job Queue","text":"<p>Sync the job queue with current key_source:</p> <pre><code># Add new pending jobs, remove stale ones\nresult = ProcessedData.jobs.refresh()\nprint(f\"Added: {result['added']}, Removed: {result['removed']}\")\n</code></pre>"},{"location":"how-to/distributed-computing/#priority-scheduling","title":"Priority Scheduling","text":"<p>Control processing order with priorities:</p> <pre><code># Refresh with specific priority\nProcessedData.jobs.refresh(priority=1)  # Lower = more urgent\n\n# Process only high-priority jobs\nProcessedData.populate(reserve_jobs=True, priority=3)\n</code></pre>"},{"location":"how-to/distributed-computing/#error-recovery","title":"Error Recovery","text":"<p>Handle failed jobs:</p> <pre><code># View errors\nerrors = ProcessedData.jobs.errors\nfor job in errors.to_dicts():\n    print(f\"Key: {job}, Error: {job['error_message']}\")\n\n# Clear errors to retry\nerrors.delete()\nProcessedData.populate(reserve_jobs=True)\n</code></pre>"},{"location":"how-to/distributed-computing/#orphan-detection","title":"Orphan Detection","text":"<p>Jobs from crashed workers are automatically recovered:</p> <pre><code># Refresh with orphan timeout (seconds)\nProcessedData.jobs.refresh(orphan_timeout=3600)\n</code></pre> <p>Reserved jobs older than the timeout are reset to pending.</p>"},{"location":"how-to/distributed-computing/#configuration","title":"Configuration","text":"<pre><code>import datajoint as dj\n\n# Auto-refresh on populate (default: True)\ndj.config.jobs.auto_refresh = True\n\n# Keep completed job records (default: False)\ndj.config.jobs.keep_completed = True\n\n# Stale job timeout in seconds (default: 3600)\ndj.config.jobs.stale_timeout = 3600\n\n# Default job priority (default: 5)\ndj.config.jobs.default_priority = 5\n\n# Track code version (default: None)\ndj.config.jobs.version_method = \"git\"\n</code></pre>"},{"location":"how-to/distributed-computing/#populate-options","title":"Populate Options","text":"Option Default Description <code>reserve_jobs</code> <code>False</code> Enable job coordination <code>processes</code> <code>1</code> Number of worker processes <code>max_calls</code> <code>None</code> Limit jobs per run <code>display_progress</code> <code>False</code> Show progress bar <code>suppress_errors</code> <code>False</code> Continue on errors <code>priority</code> <code>None</code> Filter by priority <code>refresh</code> <code>None</code> Force refresh before run"},{"location":"how-to/distributed-computing/#example-cluster-setup","title":"Example: Cluster Setup","text":"<pre><code># config.py - shared configuration\nimport datajoint as dj\n\ndj.config.jobs.auto_refresh = True\ndj.config.jobs.keep_completed = True\ndj.config.jobs.version_method = \"git\"\n\n# worker.py - run on each node\nfrom config import *\nfrom my_pipeline import ProcessedData\n\nwhile True:\n    result = ProcessedData.populate(\n        reserve_jobs=True,\n        max_calls=100,\n        suppress_errors=True,\n        display_progress=True\n    )\n    if result['success_count'] == 0:\n        break  # No more work\n</code></pre>"},{"location":"how-to/distributed-computing/#see-also","title":"See Also","text":"<ul> <li>Run Computations \u2014 Basic populate usage</li> <li>Handle Errors \u2014 Error recovery patterns</li> <li>Monitor Progress \u2014 Tracking job status</li> </ul>"},{"location":"how-to/fetch-results/","title":"Fetch Results","text":"<p>Retrieve query results in various formats.</p>"},{"location":"how-to/fetch-results/#list-of-dictionaries","title":"List of Dictionaries","text":"<pre><code>rows = Subject.to_dicts()\n# [{'subject_id': 'M001', 'species': 'Mus musculus', ...}, ...]\n\nfor row in rows:\n    print(row['subject_id'], row['species'])\n</code></pre>"},{"location":"how-to/fetch-results/#pandas-dataframe","title":"pandas DataFrame","text":"<pre><code>df = Subject.to_pandas()\n# Primary key becomes the index\n\n# With multi-column primary key\ndf = Session.to_pandas()\n# MultiIndex on (subject_id, session_idx)\n</code></pre>"},{"location":"how-to/fetch-results/#numpy-arrays","title":"NumPy Arrays","text":"<pre><code># Structured array (all columns)\narr = Subject.to_arrays()\n\n# Specific columns as separate arrays\nspecies, weights = Subject.to_arrays('species', 'weight')\n</code></pre>"},{"location":"how-to/fetch-results/#primary-keys-only","title":"Primary Keys Only","text":"<pre><code>keys = Session.keys()\n# [{'subject_id': 'M001', 'session_idx': 1}, ...]\n\nfor key in keys:\n    process(Session &amp; key)\n</code></pre>"},{"location":"how-to/fetch-results/#single-row","title":"Single Row","text":"<pre><code># As dictionary (raises if not exactly 1 row)\nrow = (Subject &amp; {'subject_id': 'M001'}).fetch1()\n\n# Specific attributes\nspecies, weight = (Subject &amp; {'subject_id': 'M001'}).fetch1('species', 'weight')\n</code></pre>"},{"location":"how-to/fetch-results/#ordering-and-limiting","title":"Ordering and Limiting","text":"<pre><code># Sort by single attribute\nSubject.to_dicts(order_by='weight DESC')\n\n# Sort by multiple attributes\nSession.to_dicts(order_by=['session_date DESC', 'duration'])\n\n# Sort by primary key\nSubject.to_dicts(order_by='KEY')\n\n# Limit rows\nSubject.to_dicts(limit=10)\n\n# Pagination\nSubject.to_dicts(order_by='KEY', limit=10, offset=20)\n</code></pre>"},{"location":"how-to/fetch-results/#streaming-lazy-iteration","title":"Streaming (Lazy Iteration)","text":"<pre><code># Memory-efficient iteration\nfor row in Subject:\n    process(row)\n    if done:\n        break  # Early termination\n</code></pre>"},{"location":"how-to/fetch-results/#polars-dataframe","title":"polars DataFrame","text":"<pre><code># Requires: pip install datajoint[polars]\ndf = Subject.to_polars()\n</code></pre>"},{"location":"how-to/fetch-results/#pyarrow-table","title":"PyArrow Table","text":"<pre><code># Requires: pip install datajoint[arrow]\ntable = Subject.to_arrow()\n</code></pre>"},{"location":"how-to/fetch-results/#method-summary","title":"Method Summary","text":"Method Returns Use Case <code>to_dicts()</code> <code>list[dict]</code> JSON, iteration <code>to_pandas()</code> <code>DataFrame</code> Data analysis <code>to_polars()</code> <code>polars.DataFrame</code> Fast analysis <code>to_arrow()</code> <code>pyarrow.Table</code> Interop <code>to_arrays()</code> <code>np.ndarray</code> Numeric computation <code>to_arrays('a', 'b')</code> <code>tuple[array, ...]</code> Specific columns <code>keys()</code> <code>list[dict]</code> Primary keys <code>fetch1()</code> <code>dict</code> Single row <code>for row in table:</code> Iterator Streaming"},{"location":"how-to/fetch-results/#common-parameters","title":"Common Parameters","text":"<p>All output methods accept:</p> Parameter Description <code>order_by</code> Sort by column(s): <code>'name'</code>, <code>'name DESC'</code>, <code>['a', 'b DESC']</code>, <code>'KEY'</code> <code>limit</code> Maximum rows to return <code>offset</code> Rows to skip"},{"location":"how-to/fetch-results/#see-also","title":"See Also","text":"<ul> <li>Query Data \u2014 Building queries</li> <li>Fetch API Specification \u2014 Complete reference</li> </ul>"},{"location":"how-to/garbage-collection/","title":"Clean Up Object Storage","text":"<p>Remove orphaned data from object storage after deleting database rows.</p>"},{"location":"how-to/garbage-collection/#why-garbage-collection","title":"Why Garbage Collection?","text":"<p>When you delete rows from tables with in-store types (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>), the database records are removed but the stored objects remain. This is by design:</p> <ul> <li>Hash-addressed storage (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>) uses deduplication\u2014the   same content may be referenced by multiple rows</li> <li>Schema-addressed storage (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>) stores each row's data   at a unique path, but immediate deletion could cause issues with concurrent   operations</li> </ul> <p>Run garbage collection periodically to reclaim storage space.</p>"},{"location":"how-to/garbage-collection/#basic-usage","title":"Basic Usage","text":"<pre><code>import datajoint as dj\n\n# Scan for orphaned items (dry run)\nstats = dj.gc.scan(schema1, schema2)\nprint(dj.gc.format_stats(stats))\n\n# Remove orphaned items\nstats = dj.gc.collect(schema1, schema2, dry_run=False)\nprint(dj.gc.format_stats(stats))\n</code></pre>"},{"location":"how-to/garbage-collection/#scan-before-collecting","title":"Scan Before Collecting","text":"<p>Always scan first to see what would be deleted:</p> <pre><code># Check what's orphaned\nstats = dj.gc.scan(my_schema)\n\nprint(f\"Hash-addressed orphaned: {stats['hash_orphaned']}\")\nprint(f\"Schema paths orphaned: {stats['schema_paths_orphaned']}\")\nprint(f\"Total bytes: {stats['orphaned_bytes'] / 1e6:.1f} MB\")\n</code></pre>"},{"location":"how-to/garbage-collection/#dry-run-mode","title":"Dry Run Mode","text":"<p>The default <code>dry_run=True</code> reports what would be deleted without deleting:</p> <pre><code># Safe: shows what would be deleted\nstats = dj.gc.collect(my_schema, dry_run=True)\nprint(dj.gc.format_stats(stats))\n\n# After review, actually delete\nstats = dj.gc.collect(my_schema, dry_run=False)\n</code></pre>"},{"location":"how-to/garbage-collection/#multiple-schemas","title":"Multiple Schemas","text":"<p>If your data spans multiple schemas, scan all of them together:</p> <pre><code># Important: include ALL schemas that might share storage\nstats = dj.gc.collect(\n    schema_raw,\n    schema_processed,\n    schema_analysis,\n    dry_run=False\n)\n</code></pre> <p>Per-schema deduplication</p> <p>Hash-addressed storage is deduplicated within each schema. Different schemas have independent storage, so you only need to scan schemas that share the same database.</p>"},{"location":"how-to/garbage-collection/#named-stores","title":"Named Stores","text":"<p>If you use multiple named stores, specify which to clean:</p> <pre><code># Clean specific store\nstats = dj.gc.collect(my_schema, store_name='archive', dry_run=False)\n\n# Or clean default store\nstats = dj.gc.collect(my_schema, dry_run=False)  # uses default store\n</code></pre>"},{"location":"how-to/garbage-collection/#verbose-mode","title":"Verbose Mode","text":"<p>See detailed progress:</p> <pre><code>stats = dj.gc.collect(\n    my_schema,\n    dry_run=False,\n    verbose=True  # logs each deletion\n)\n</code></pre>"},{"location":"how-to/garbage-collection/#understanding-the-statistics","title":"Understanding the Statistics","text":"<pre><code>stats = dj.gc.scan(my_schema)\n\n# Hash-addressed storage (&lt;blob@&gt;, &lt;attach@&gt;, &lt;hash@&gt;)\nstats['hash_referenced']      # Items still in database\nstats['hash_stored']          # Items in storage\nstats['hash_orphaned']        # Unreferenced (can be deleted)\nstats['hash_orphaned_bytes']  # Size of orphaned items\n\n# Schema-addressed storage (&lt;object@&gt;, &lt;npy@&gt;)\nstats['schema_paths_referenced']  # Paths still in database\nstats['schema_paths_stored']      # Paths in storage\nstats['schema_paths_orphaned']    # Unreferenced paths\nstats['schema_paths_orphaned_bytes']\n\n# Totals\nstats['referenced']   # Total referenced items\nstats['stored']       # Total stored items\nstats['orphaned']     # Total orphaned items\nstats['orphaned_bytes']\n</code></pre>"},{"location":"how-to/garbage-collection/#scheduled-collection","title":"Scheduled Collection","text":"<p>Run GC periodically in production:</p> <pre><code># In a cron job or scheduled task\nimport datajoint as dj\nfrom myproject import schema1, schema2, schema3\n\nstats = dj.gc.collect(\n    schema1, schema2, schema3,\n    dry_run=False,\n    verbose=True\n)\n\nif stats['errors'] &gt; 0:\n    logging.warning(f\"GC completed with {stats['errors']} errors\")\nelse:\n    logging.info(f\"GC freed {stats['bytes_freed'] / 1e6:.1f} MB\")\n</code></pre>"},{"location":"how-to/garbage-collection/#how-storage-addressing-works","title":"How Storage Addressing Works","text":"<p>DataJoint uses two storage patterns:</p>"},{"location":"how-to/garbage-collection/#hash-addressed-blob-attach-hash","title":"Hash-Addressed (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;hash@&gt;</code>)","text":"<pre><code>_hash/\n  {schema}/\n    ab/\n      cd/\n        abcdefghij...  # Content identified by Base32-encoded MD5 hash\n</code></pre> <ul> <li>Duplicate content shares storage within each schema</li> <li>Paths are stored in metadata\u2014safe from config changes</li> <li>Cannot delete until no rows reference the content</li> <li>GC compares stored paths against filesystem</li> </ul>"},{"location":"how-to/garbage-collection/#schema-addressed-object-npy","title":"Schema-Addressed (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>)","text":"<pre><code>myschema/\n  mytable/\n    primary_key_values/\n      attribute_name/\n        data.zarr/\n        data.npy\n</code></pre> <ul> <li>Each row has unique path based on schema structure</li> <li>Paths mirror database organization</li> <li>GC removes paths not referenced by any row</li> </ul>"},{"location":"how-to/garbage-collection/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/garbage-collection/#at-least-one-schema-must-be-provided","title":"\"At least one schema must be provided\"","text":"<pre><code># Wrong\ndj.gc.scan()\n\n# Right\ndj.gc.scan(my_schema)\n</code></pre>"},{"location":"how-to/garbage-collection/#storage-not-decreasing","title":"Storage not decreasing","text":"<p>Check that you're scanning all schemas:</p> <pre><code># List all schemas that use this store\n# Make sure to include them all in the scan\n</code></pre>"},{"location":"how-to/garbage-collection/#permission-errors","title":"Permission errors","text":"<p>Ensure your storage credentials allow deletion:</p> <pre><code># Check store configuration\nspec = dj.config.get_object_store_spec('mystore')\n# Verify write/delete permissions\n</code></pre>"},{"location":"how-to/garbage-collection/#see-also","title":"See Also","text":"<ul> <li>Manage Large Data \u2014 Storage patterns and streaming</li> <li>Configure Object Storage \u2014 Storage setup</li> <li>Delete Data \u2014 Row deletion with cascades</li> </ul>"},{"location":"how-to/handle-errors/","title":"Handle Errors","text":"<p>Manage computation errors and recover failed jobs.</p>"},{"location":"how-to/handle-errors/#suppress-errors-during-populate","title":"Suppress Errors During Populate","text":"<p>Continue processing despite individual failures:</p> <pre><code># Stop on first error (default)\nProcessedData.populate()\n\n# Log errors but continue\nProcessedData.populate(suppress_errors=True)\n</code></pre>"},{"location":"how-to/handle-errors/#view-failed-jobs","title":"View Failed Jobs","text":"<p>Check the jobs table for errors:</p> <pre><code># All error jobs\nProcessedData.jobs.errors\n\n# View error details\nfor job in ProcessedData.jobs.errors.to_dicts():\n    print(f\"Key: {job}\")\n    print(f\"Message: {job['error_message']}\")\n</code></pre>"},{"location":"how-to/handle-errors/#get-full-stack-trace","title":"Get Full Stack Trace","text":"<p>Error stack traces are stored in the jobs table:</p> <pre><code>job = (ProcessedData.jobs.errors &amp; key).fetch1()\nprint(job['error_stack'])\n</code></pre>"},{"location":"how-to/handle-errors/#retry-failed-jobs","title":"Retry Failed Jobs","text":"<p>Clear error status and rerun:</p> <pre><code># Delete error records to retry\nProcessedData.jobs.errors.delete()\n\n# Reprocess\nProcessedData.populate(reserve_jobs=True)\n</code></pre>"},{"location":"how-to/handle-errors/#retry-specific-jobs","title":"Retry Specific Jobs","text":"<p>Target specific failed jobs:</p> <pre><code># Clear one error\n(ProcessedData.jobs &amp; key &amp; \"status='error'\").delete()\n\n# Retry just that key\nProcessedData.populate(key, reserve_jobs=True)\n</code></pre>"},{"location":"how-to/handle-errors/#ignore-problematic-jobs","title":"Ignore Problematic Jobs","text":"<p>Mark jobs to skip permanently:</p> <pre><code># Mark job as ignored\nProcessedData.jobs.ignore(key)\n\n# View ignored jobs\nProcessedData.jobs.ignored\n</code></pre>"},{"location":"how-to/handle-errors/#error-handling-in-make","title":"Error Handling in make()","text":"<p>Handle expected errors gracefully:</p> <pre><code>@schema\nclass ProcessedData(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    result : float64\n    \"\"\"\n\n    def make(self, key):\n        try:\n            data = (RawData &amp; key).fetch1('data')\n            result = risky_computation(data)\n        except ValueError as e:\n            # Log and skip this key\n            logger.warning(f\"Skipping {key}: {e}\")\n            return  # Don't insert, job remains pending\n\n        self.insert1({**key, 'result': result})\n</code></pre>"},{"location":"how-to/handle-errors/#transaction-rollback","title":"Transaction Rollback","text":"<p>Failed <code>make()</code> calls automatically rollback:</p> <pre><code>def make(self, key):\n    # These inserts are in a transaction\n    self.insert1({**key, 'result': value1})\n    PartTable.insert(parts)\n\n    # If this raises, all inserts are rolled back\n    validate_result(key)\n</code></pre>"},{"location":"how-to/handle-errors/#return-exception-objects","title":"Return Exception Objects","text":"<p>Get exception objects for programmatic handling:</p> <pre><code>result = ProcessedData.populate(\n    suppress_errors=True,\n    return_exception_objects=True\n)\n\nfor key, exception in result['error_list']:\n    if isinstance(exception, TimeoutError):\n        # Handle timeout differently\n        schedule_for_later(key)\n</code></pre>"},{"location":"how-to/handle-errors/#monitor-error-rate","title":"Monitor Error Rate","text":"<p>Track errors over time:</p> <pre><code>progress = ProcessedData.jobs.progress()\nprint(f\"Pending: {progress.get('pending', 0)}\")\nprint(f\"Errors: {progress.get('error', 0)}\")\nprint(f\"Success: {progress.get('success', 0)}\")\n\nerror_rate = progress.get('error', 0) / sum(progress.values())\nprint(f\"Error rate: {error_rate:.1%}\")\n</code></pre>"},{"location":"how-to/handle-errors/#common-error-patterns","title":"Common Error Patterns","text":""},{"location":"how-to/handle-errors/#data-quality-issues","title":"Data Quality Issues","text":"<pre><code>def make(self, key):\n    data = (RawData &amp; key).fetch1('data')\n\n    if not validate_data(data):\n        raise DataJointError(f\"Invalid data for {key}\")\n\n    # Process valid data\n    self.insert1({**key, 'result': process(data)})\n</code></pre>"},{"location":"how-to/handle-errors/#resource-constraints","title":"Resource Constraints","text":"<pre><code>def make(self, key):\n    try:\n        result = memory_intensive_computation(key)\n    except MemoryError:\n        # Clear caches and retry once\n        gc.collect()\n        result = memory_intensive_computation(key)\n\n    self.insert1({**key, 'result': result})\n</code></pre>"},{"location":"how-to/handle-errors/#external-service-failures","title":"External Service Failures","text":"<pre><code>def make(self, key):\n    for attempt in range(3):\n        try:\n            data = fetch_from_external_api(key)\n            break\n        except ConnectionError:\n            if attempt == 2:\n                raise\n            time.sleep(2 ** attempt)  # Exponential backoff\n\n    self.insert1({**key, 'result': process(data)})\n</code></pre>"},{"location":"how-to/handle-errors/#see-also","title":"See Also","text":"<ul> <li>Run Computations \u2014 Basic populate usage</li> <li>Distributed Computing \u2014 Multi-worker error handling</li> <li>Monitor Progress \u2014 Tracking job status</li> </ul>"},{"location":"how-to/insert-data/","title":"Insert Data","text":"<p>Add data to DataJoint tables.</p>"},{"location":"how-to/insert-data/#single-row","title":"Single Row","text":"<pre><code>Subject.insert1({\n    'subject_id': 'M001',\n    'species': 'Mus musculus',\n    'date_of_birth': '2026-01-15',\n    'sex': 'M'\n})\n</code></pre>"},{"location":"how-to/insert-data/#multiple-rows","title":"Multiple Rows","text":"<pre><code>Subject.insert([\n    {'subject_id': 'M001', 'species': 'Mus musculus', 'date_of_birth': '2026-01-15', 'sex': 'M'},\n    {'subject_id': 'M002', 'species': 'Mus musculus', 'date_of_birth': '2026-02-01', 'sex': 'F'},\n    {'subject_id': 'M003', 'species': 'Mus musculus', 'date_of_birth': '2026-02-15', 'sex': 'M'},\n])\n</code></pre>"},{"location":"how-to/insert-data/#from-pandas-dataframe","title":"From pandas DataFrame","text":"<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'subject_id': ['M004', 'M005'],\n    'species': ['Mus musculus', 'Mus musculus'],\n    'date_of_birth': ['2026-03-01', '2026-03-15'],\n    'sex': ['F', 'M']\n})\n\nSubject.insert(df)\n</code></pre>"},{"location":"how-to/insert-data/#handle-duplicates","title":"Handle Duplicates","text":"<pre><code># Skip rows with existing primary keys\nSubject.insert(rows, skip_duplicates=True)\n\n# Replace existing rows (use sparingly\u2014breaks immutability)\nSubject.insert(rows, replace=True)\n</code></pre>"},{"location":"how-to/insert-data/#ignore-extra-fields","title":"Ignore Extra Fields","text":"<pre><code># Ignore fields not in the table definition\nSubject.insert(rows, ignore_extra_fields=True)\n</code></pre>"},{"location":"how-to/insert-data/#master-part-tables","title":"Master-Part Tables","text":"<p>Use a transaction to maintain compositional integrity:</p> <pre><code>with dj.conn().transaction:\n    Session.insert1({\n        'subject_id': 'M001',\n        'session_idx': 1,\n        'session_date': '2026-01-20'\n    })\n    Session.Trial.insert([\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 1, 'outcome': 'hit', 'reaction_time': 0.35},\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 2, 'outcome': 'miss', 'reaction_time': 0.82},\n    ])\n</code></pre>"},{"location":"how-to/insert-data/#insert-from-query","title":"Insert from Query","text":"<pre><code># Copy data from another table or query result\nNewTable.insert(OldTable &amp; 'condition')\n\n# With projection\nNewTable.insert(OldTable.proj('attr1', 'attr2', new_name='old_name'))\n</code></pre>"},{"location":"how-to/insert-data/#validate-before-insert","title":"Validate Before Insert","text":"<pre><code>result = Subject.validate(rows)\n\nif result:\n    Subject.insert(rows)\nelse:\n    print(\"Validation errors:\")\n    for error in result.errors:\n        print(f\"  {error}\")\n</code></pre>"},{"location":"how-to/insert-data/#insert-with-blobs","title":"Insert with Blobs","text":"<pre><code>import numpy as np\n\ndata = np.random.randn(100, 100)\n\nImageData.insert1({\n    'image_id': 1,\n    'pixel_data': data  # Automatically serialized\n})\n</code></pre>"},{"location":"how-to/insert-data/#insert-options-summary","title":"Insert Options Summary","text":"Option Default Description <code>skip_duplicates</code> <code>False</code> Skip rows with existing keys <code>replace</code> <code>False</code> Replace existing rows <code>ignore_extra_fields</code> <code>False</code> Ignore unknown fields"},{"location":"how-to/insert-data/#best-practices","title":"Best Practices","text":""},{"location":"how-to/insert-data/#batch-inserts-for-performance","title":"Batch inserts for performance","text":"<pre><code># Good: Single insert call\nSubject.insert(all_rows)\n\n# Slow: Loop of insert1 calls\nfor row in all_rows:\n    Subject.insert1(row)\n</code></pre>"},{"location":"how-to/insert-data/#use-transactions-for-related-inserts","title":"Use transactions for related inserts","text":"<pre><code>with dj.conn().transaction:\n    Parent.insert1(parent_row)\n    Child.insert(child_rows)\n</code></pre>"},{"location":"how-to/insert-data/#validate-before-bulk-inserts","title":"Validate before bulk inserts","text":"<pre><code>if Subject.validate(rows):\n    Subject.insert(rows)\n</code></pre>"},{"location":"how-to/insert-data/#see-also","title":"See Also","text":"<ul> <li>Master-Part Tables \u2014 Atomic insertion of master and parts</li> <li>Define Tables \u2014 Table definition syntax</li> <li>Delete Data \u2014 Removing data from tables</li> </ul>"},{"location":"how-to/installation/","title":"Installation","text":"<p>Install DataJoint Python and set up your environment.</p>"},{"location":"how-to/installation/#install-datajoint-20","title":"Install DataJoint 2.0","text":"<pre><code>pip install datajoint\n</code></pre> <p>With optional dependencies:</p> <pre><code># For diagram visualization (matplotlib, ipython)\npip install datajoint[viz]\n\n# For polars DataFrame support\npip install datajoint[polars]\n\n# For cloud storage backends\npip install datajoint[s3]    # AWS S3\npip install datajoint[gcs]   # Google Cloud Storage\npip install datajoint[azure] # Azure Blob Storage\n</code></pre> <p>Upgrading from 0.14.x?</p> <p>See the Migration Guide for breaking changes and upgrade instructions. Legacy documentation for 0.14.x is available at datajoint.github.io.</p>"},{"location":"how-to/installation/#verify-installation","title":"Verify Installation","text":"<p>Check your installed version:</p> <pre><code>import datajoint as dj\nprint(dj.__version__)\n</code></pre> <p>Expected output for this documentation: - <code>2.0.0</code> or higher \u2014 You're ready to follow this documentation - <code>0.14.x</code> or lower \u2014 You have the stable version, use legacy docs instead</p>"},{"location":"how-to/installation/#if-you-have-an-older-version","title":"If You Have an Older Version","text":"Your Situation Action Installed 0.14.x, want to upgrade <code>pip install --upgrade datajoint</code> Have existing 0.14.x pipeline to upgrade Follow Migration Guide"},{"location":"how-to/installation/#database-server","title":"Database Server","text":"<p>DataJoint requires a MySQL-compatible database server:</p>"},{"location":"how-to/installation/#local-development-docker","title":"Local Development (Docker)","text":"<pre><code>docker run -d \\\n  --name datajoint-db \\\n  -p 3306:3306 \\\n  -e MYSQL_ROOT_PASSWORD=simple \\\n  mysql:8.0\n</code></pre>"},{"location":"how-to/installation/#datajointcom-recommended","title":"DataJoint.com (Recommended)","text":"<p>DataJoint.com provides fully managed infrastructure for scientific data pipelines\u2014cloud or on-premises\u2014with comprehensive support, automatic backups, object storage, and team collaboration features.</p>"},{"location":"how-to/installation/#self-managed-cloud-databases","title":"Self-Managed Cloud Databases","text":"<ul> <li>Amazon RDS \u2014 MySQL or Aurora</li> <li>Google Cloud SQL \u2014 MySQL</li> <li>Azure Database \u2014 MySQL</li> </ul> <p>See Configure Database Connection for connection setup.</p>"},{"location":"how-to/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>MySQL 8.0+ or MariaDB 10.6+</li> <li>Network access to database server</li> </ul>"},{"location":"how-to/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/installation/#pymysql-connection-errors","title":"<code>pymysql</code> connection errors","text":"<pre><code>pip install pymysql --force-reinstall\n</code></pre>"},{"location":"how-to/installation/#ssltls-connection-issues","title":"SSL/TLS connection issues","text":"<p>Set <code>use_tls=False</code> for local development:</p> <pre><code>dj.config['database.use_tls'] = False\n</code></pre>"},{"location":"how-to/installation/#permission-denied","title":"Permission denied","text":"<p>Ensure your database user has appropriate privileges:</p> <pre><code>GRANT ALL PRIVILEGES ON `your_schema%`.* TO 'username'@'%';\n</code></pre>"},{"location":"how-to/manage-large-data/","title":"Manage Large Data","text":"<p>Work effectively with blobs and object storage.</p>"},{"location":"how-to/manage-large-data/#choose-the-right-storage","title":"Choose the Right Storage","text":"Data Size Recommended Syntax &lt; 1 MB Database <code>&lt;blob&gt;</code> 1 MB - 1 GB Hash-addressed <code>&lt;blob@&gt;</code> &gt; 1 GB Schema-addressed <code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>"},{"location":"how-to/manage-large-data/#streaming-large-results","title":"Streaming Large Results","text":"<p>Avoid loading everything into memory:</p> <pre><code># Bad: loads all data at once\nall_data = LargeTable().to_arrays('big_column')\n\n# Good: stream rows lazily (single cursor, one row at a time)\nfor row in LargeTable():\n    process(row['big_column'])\n\n# Good: batch by ID range\nkeys = LargeTable().keys()\nbatch_size = 100\nfor i in range(0, len(keys), batch_size):\n    batch_keys = keys[i:i + batch_size]\n    data = (LargeTable() &amp; batch_keys).to_arrays('big_column')\n    process(data)\n</code></pre>"},{"location":"how-to/manage-large-data/#lazy-loading-with-objectref","title":"Lazy Loading with ObjectRef","text":"<p><code>&lt;object@&gt;</code> and <code>&lt;filepath@&gt;</code> return lazy references:</p> <pre><code># Returns ObjectRef, not the actual data\nref = (Dataset &amp; key).fetch1('large_file')\n\n# Stream without full download\nwith ref.open('rb') as f:\n    # Process in chunks\n    while chunk := f.read(1024 * 1024):\n        process(chunk)\n\n# Or download when needed\nlocal_path = ref.download('/tmp/working')\n</code></pre>"},{"location":"how-to/manage-large-data/#selective-fetching","title":"Selective Fetching","text":"<p>Fetch only what you need:</p> <pre><code># Bad: fetches all columns including blobs\nrow = MyTable.fetch1()\n\n# Good: fetch only metadata\nmetadata = (MyTable &amp; key).fetch1('name', 'date', 'status')\n\n# Then fetch blob only if needed\nif needs_processing(metadata):\n    data = (MyTable &amp; key).fetch1('large_data')\n</code></pre>"},{"location":"how-to/manage-large-data/#projection-for-efficiency","title":"Projection for Efficiency","text":"<p>Exclude large columns from joins:</p> <pre><code># Slow: joins include blob columns\nresult = Table1 * Table2\n\n# Fast: project away blobs before join\nresult = Table1.proj('id', 'name') * Table2.proj('id', 'status')\n</code></pre>"},{"location":"how-to/manage-large-data/#batch-inserts","title":"Batch Inserts","text":"<p>Insert large data efficiently:</p> <pre><code># Good: single transaction for related data\nwith dj.conn().transaction:\n    for item in large_batch:\n        MyTable.insert1(item)\n</code></pre>"},{"location":"how-to/manage-large-data/#content-deduplication","title":"Content Deduplication","text":"<p><code>&lt;blob@&gt;</code> and <code>&lt;attach@&gt;</code> automatically deduplicate within each schema:</p> <pre><code># Same array inserted twice\ndata = np.random.randn(1000, 1000)\nTable.insert1({'id': 1, 'data': data})\nTable.insert1({'id': 2, 'data': data})  # References same storage\n\n# Only one copy exists in object storage (per schema)\n</code></pre> <p>Deduplication is per-schema\u2014identical content in different schemas is stored separately. This enables independent garbage collection per schema.</p>"},{"location":"how-to/manage-large-data/#storage-cleanup","title":"Storage Cleanup","text":"<p>Object storage items are not automatically deleted with rows. Run garbage collection periodically:</p> <pre><code>import datajoint as dj\n\n# Objects are NOT automatically deleted with rows\n(MyTable &amp; old_data).delete()\n\n# Scan for orphaned items\nstats = dj.gc.scan(my_schema)\nprint(dj.gc.format_stats(stats))\n\n# Remove orphaned items\nstats = dj.gc.collect(my_schema, dry_run=False)\n</code></pre> <p>See Clean Up Object Storage for details.</p>"},{"location":"how-to/manage-large-data/#monitor-storage-usage","title":"Monitor Storage Usage","text":"<p>Check object store size:</p> <pre><code># Get store configuration\nspec = dj.config.get_object_store_spec()\n\n# For S3/MinIO, use boto3 or similar\n# For filesystem, use standard tools\n</code></pre>"},{"location":"how-to/manage-large-data/#compression","title":"Compression","text":"<p>Blobs are compressed by default:</p> <pre><code># Compression happens automatically in &lt;blob&gt;\nlarge_array = np.zeros((10000, 10000))  # Compresses well\nsparse_data = np.random.randn(10000, 10000)  # Less compression\n</code></pre>"},{"location":"how-to/manage-large-data/#memory-management","title":"Memory Management","text":"<p>For very large computations:</p> <pre><code>def make(self, key):\n    # Process in chunks\n    for chunk_idx in range(n_chunks):\n        chunk_data = load_chunk(key, chunk_idx)\n        result = process(chunk_data)\n        save_partial_result(key, chunk_idx, result)\n        del chunk_data  # Free memory\n\n    # Combine results\n    final = combine_results(key)\n    self.insert1({**key, 'result': final})\n</code></pre>"},{"location":"how-to/manage-large-data/#external-tools-for-very-large-data","title":"External Tools for Very Large Data","text":"<p>For datasets too large for DataJoint:</p> <pre><code>@schema\nclass LargeDataset(dj.Manual):\n    definition = \"\"\"\n    dataset_id : uuid\n    ---\n    zarr_path : &lt;filepath@&gt;     # Reference to external Zarr\n    \"\"\"\n\n# Store path reference, process with specialized tools\nimport zarr\nstore = zarr.open(local_zarr_path)\n# ... process with Zarr/Dask ...\n\nLargeDataset.insert1({\n    'dataset_id': uuid.uuid4(),\n    'zarr_path': local_zarr_path\n})\n</code></pre>"},{"location":"how-to/manage-large-data/#see-also","title":"See Also","text":"<ul> <li>Use Object Storage \u2014 Storage patterns</li> <li>Configure Object Storage \u2014 Storage setup</li> <li>Create Custom Codecs \u2014 Domain-specific types</li> </ul>"},{"location":"how-to/manage-pipeline-project/","title":"Manage a Pipeline Project","text":"<p>Organize multi-schema pipelines for team collaboration.</p>"},{"location":"how-to/manage-pipeline-project/#overview","title":"Overview","text":"<p>A production DataJoint pipeline typically involves:</p> <ul> <li>Multiple schemas \u2014 Organized by experimental modality or processing stage</li> <li>Team of users \u2014 With different roles and access levels</li> <li>Shared infrastructure \u2014 Database server, object storage, code repository</li> <li>Coordination \u2014 Between code, database, and storage permissions</li> </ul> <p>This guide covers practical project organization. For conceptual background on pipeline architecture and the DAG structure, see Data Pipelines.</p> <p>For a fully managed solution, request a DataJoint Platform account.</p>"},{"location":"how-to/manage-pipeline-project/#project-structure","title":"Project Structure","text":"<p>Use a modern Python project layout with source code under <code>src/</code>:</p> <pre><code>my_pipeline/\n\u251c\u2500\u2500 datajoint.json          # Shared settings (committed)\n\u251c\u2500\u2500 .secrets/               # Local credentials (gitignored)\n\u2502   \u251c\u2500\u2500 database.password\n\u2502   \u2514\u2500\u2500 storage.credentials\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 pyproject.toml          # Package metadata and dependencies\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_pipeline/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 subject.py      # subject schema\n\u2502       \u251c\u2500\u2500 session.py      # session schema\n\u2502       \u251c\u2500\u2500 ephys.py        # ephys schema\n\u2502       \u251c\u2500\u2500 imaging.py      # imaging schema\n\u2502       \u251c\u2500\u2500 analysis.py     # analysis schema\n\u2502       \u2514\u2500\u2500 utils/\n\u2502           \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2514\u2500\u2500 test_ephys.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#one-module-per-schema","title":"One Module Per Schema","text":"<p>Each module defines and binds to its schema:</p> <pre><code># src/my_pipeline/ephys.py\nimport datajoint as dj\nfrom . import session  # Import dependency\n\nschema = dj.Schema('ephys')\n\n@schema\nclass Probe(dj.Lookup):\n    definition = \"\"\"\n    probe_type : varchar(32)\n    ---\n    num_channels : int32\n    \"\"\"\n\n@schema\nclass Recording(dj.Imported):\n    definition = \"\"\"\n    -&gt; session.Session\n    -&gt; Probe\n    ---\n    recording_path : varchar(255)\n    \"\"\"\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#import-dependencies-mirror-foreign-keys","title":"Import Dependencies Mirror Foreign Keys","text":"<p>Module imports reflect the schema DAG:</p> <pre><code># analysis.py depends on both ephys and imaging\nfrom . import ephys\nfrom . import imaging\n\nschema = dj.Schema('analysis')\n\n@schema\nclass MultiModalAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; ephys.Recording\n    -&gt; imaging.Scan\n    ---\n    correlation : float64\n    \"\"\"\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#repository-configuration","title":"Repository Configuration","text":""},{"location":"how-to/manage-pipeline-project/#shared-settings","title":"Shared Settings","text":"<p>Store non-secret configuration in <code>datajoint.json</code> at the project root:</p> <p>datajoint.json (committed): <pre><code>{\n  \"database\": {\n    \"host\": \"db.example.com\",\n    \"port\": 3306\n  },\n  \"stores\": {\n    \"main\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.example.com\",\n      \"bucket\": \"my-org-data\",\n      \"location\": \"my_pipeline\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"how-to/manage-pipeline-project/#credentials-management","title":"Credentials Management","text":"<p>Credentials are stored locally and never committed:</p> <p>Option 1: <code>.secrets/</code> directory <pre><code>.secrets/\n\u251c\u2500\u2500 database.user\n\u251c\u2500\u2500 database.password\n\u251c\u2500\u2500 storage.access_key\n\u2514\u2500\u2500 storage.secret_key\n</code></pre></p> <p>Option 2: Environment variables <pre><code>export DJ_USER=alice\nexport DJ_PASS=alice_password\nexport DJ_STORES__MAIN__ACCESS_KEY=...\nexport DJ_STORES__MAIN__SECRET_KEY=...\n</code></pre></p>"},{"location":"how-to/manage-pipeline-project/#essential-gitignore","title":"Essential <code>.gitignore</code>","text":"<pre><code># Credentials\n.secrets/\n\n# Python\n__pycache__/\n*.pyc\n*.egg-info/\ndist/\nbuild/\n\n# Environment\n.env\n.venv/\n\n# IDE\n.idea/\n.vscode/\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#pyprojecttoml-example","title":"<code>pyproject.toml</code> Example","text":"<pre><code>[project]\nname = \"my-pipeline\"\nversion = \"1.0.0\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [\n    \"datajoint&gt;=2.0\",\n    \"numpy\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest\", \"pytest-cov\"]\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#database-access-control","title":"Database Access Control","text":""},{"location":"how-to/manage-pipeline-project/#the-complexity","title":"The Complexity","text":"<p>Multi-user database access requires:</p> <ol> <li>User accounts \u2014 Individual credentials per team member</li> <li>Schema permissions \u2014 Which users can access which schemas</li> <li>Operation permissions \u2014 SELECT, INSERT, UPDATE, DELETE, CREATE, DROP</li> <li>Role hierarchy \u2014 Admin, developer, analyst, viewer</li> <li>Audit trail \u2014 Who modified what and when</li> </ol>"},{"location":"how-to/manage-pipeline-project/#basic-mysql-grants","title":"Basic MySQL Grants","text":"<pre><code>-- Create user\nCREATE USER 'alice'@'%' IDENTIFIED BY 'password';\n\n-- Grant read-only on specific schema\nGRANT SELECT ON ephys.* TO 'alice'@'%';\n\n-- Grant read-write on specific schema\nGRANT SELECT, INSERT, UPDATE, DELETE ON analysis.* TO 'alice'@'%';\n\n-- Grant full access (developers)\nGRANT ALL PRIVILEGES ON my_pipeline_*.* TO 'bob'@'%';\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#role-based-access-patterns","title":"Role-Based Access Patterns","text":"Role Permissions Typical Use Viewer SELECT Browse data, run queries Analyst SELECT, INSERT on analysis Add analysis results Operator SELECT, INSERT, DELETE on data schemas Run pipeline Developer ALL on development schemas Schema changes Admin ALL + GRANT User management"},{"location":"how-to/manage-pipeline-project/#considerations","title":"Considerations","text":"<ul> <li>Users need SELECT on parent schemas to INSERT into child schemas (FK validation)</li> <li>Cascading deletes require DELETE on all dependent schemas</li> <li>Schema creation requires CREATE privilege</li> <li>Coordinating permissions across many schemas becomes complex</li> </ul>"},{"location":"how-to/manage-pipeline-project/#object-storage-access-control","title":"Object Storage Access Control","text":""},{"location":"how-to/manage-pipeline-project/#the-complexity_1","title":"The Complexity","text":"<p>Object storage permissions must align with database permissions:</p> <ol> <li>Bucket/prefix policies \u2014 Map to schema access</li> <li>Read vs write \u2014 Match SELECT vs INSERT/UPDATE</li> <li>Credential distribution \u2014 Per-user or shared service accounts</li> <li>Cross-schema objects \u2014 When computed tables reference multiple inputs</li> </ol>"},{"location":"how-to/manage-pipeline-project/#hierarchical-storage-structure","title":"Hierarchical Storage Structure","text":"<p>A DataJoint project creates a structured storage pattern:</p> <pre><code>\ud83d\udcc1 project_name/\n\u251c\u2500\u2500 \ud83d\udcc1 schema_name1/\n\u251c\u2500\u2500 \ud83d\udcc1 schema_name2/\n\u251c\u2500\u2500 \ud83d\udcc1 schema_name3/\n\u2502   \u251c\u2500\u2500 objects/\n\u2502   \u2502   \u2514\u2500\u2500 table1/\n\u2502   \u2502       \u2514\u2500\u2500 key1-value1/\n\u2502   \u2514\u2500\u2500 fields/\n\u2502       \u2514\u2500\u2500 table1-field1/\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#s3minio-policy-example","title":"S3/MinIO Policy Example","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\"],\n      \"Resource\": \"arn:aws:s3:::my-lab-data/datajoint/ephys/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\n      \"Resource\": \"arn:aws:s3:::my-lab-data/datajoint/analysis/*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#considerations_1","title":"Considerations","text":"<ul> <li>Object paths include schema name: <code>{project}/{schema}/{table}/...</code></li> <li>Users need read access to fetch blobs from upstream schemas</li> <li>Content-addressed storage (<code>&lt;blob@&gt;</code>) shares objects across tables</li> <li>Garbage collection requires coordinated delete permissions</li> </ul>"},{"location":"how-to/manage-pipeline-project/#pipeline-initialization","title":"Pipeline Initialization","text":""},{"location":"how-to/manage-pipeline-project/#schema-creation-order","title":"Schema Creation Order","text":"<p>Initialize schemas in dependency order:</p> <pre><code># src/my_pipeline/__init__.py\nfrom . import subject   # No dependencies\nfrom . import session   # Depends on subject\nfrom . import ephys     # Depends on session\nfrom . import imaging   # Depends on session\nfrom . import analysis  # Depends on ephys, imaging\n\ndef initialize():\n    \"\"\"Create all schemas in dependency order.\"\"\"\n    # Schemas are created when modules are imported\n    # and tables are first accessed\n    subject.Subject()\n    session.Session()\n    ephys.Recording()\n    imaging.Scan()\n    analysis.MultiModalAnalysis()\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#version-coordination","title":"Version Coordination","text":"<p>Track schema versions with your code:</p> <pre><code># src/my_pipeline/version.py\n__version__ = \"1.2.0\"\n\nSCHEMA_VERSIONS = {\n    'subject': '1.0.0',\n    'session': '1.1.0',\n    'ephys': '1.2.0',\n    'imaging': '1.2.0',\n    'analysis': '1.2.0',\n}\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#team-workflows","title":"Team Workflows","text":""},{"location":"how-to/manage-pipeline-project/#development-vs-production","title":"Development vs Production","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Development   \u2502     \u2502   Production    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dev_subject     \u2502     \u2502 subject         \u2502\n\u2502 dev_session     \u2502     \u2502 session         \u2502\n\u2502 dev_ephys       \u2502     \u2502 ephys           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                       \u2502\n        \u2502    Schema promotion   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#branching-strategy","title":"Branching Strategy","text":"<pre><code>main \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n       \u2502              \u2502\n       \u2502 feature/     \u2502 hotfix/\n       \u25bc              \u25bc\n    ephys-v2      fix-recording\n       \u2502              \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u25b6 main\n</code></pre>"},{"location":"how-to/manage-pipeline-project/#summary-of-complexities","title":"Summary of Complexities","text":"<p>Managing a team pipeline requires coordinating:</p> Component Challenges Code Module dependencies, version control, deployment Database User accounts, schema permissions, role hierarchy Object Storage Bucket policies, credential distribution, path alignment Compute Worker deployment, job distribution, resource allocation Monitoring Progress tracking, error alerting, audit logging <p>These challenges grow with team size and pipeline complexity. The DataJoint Platform provides integrated management for all these concerns.</p>"},{"location":"how-to/manage-pipeline-project/#see-also","title":"See Also","text":"<ul> <li>Deploy to Production \u2014 Production mode and environment configuration</li> <li>Data Pipelines \u2014 Conceptual overview and architecture</li> <li>Configure Object Storage \u2014 Storage setup</li> <li>Distributed Computing \u2014 Multi-worker pipelines</li> <li>Model Relationships \u2014 Foreign key patterns</li> </ul>"},{"location":"how-to/manage-secrets/","title":"Manage Secrets and Credentials","text":"<p>Secure configuration management for database credentials, storage access keys, and other sensitive settings.</p>"},{"location":"how-to/manage-secrets/#overview","title":"Overview","text":"<p>DataJoint separates configuration into sensitive and non-sensitive components:</p> Component Location Purpose Version Control Non-sensitive <code>datajoint.json</code> Project settings, defaults \u2705 Commit to git Sensitive <code>.secrets/</code> directory Credentials, API keys \u274c Never commit Dynamic Environment variables CI/CD, production \u26a0\ufe0f Context-dependent"},{"location":"how-to/manage-secrets/#configuration-priority","title":"Configuration Priority","text":"<p>DataJoint loads configuration in this priority order (highest to lowest):</p> <ol> <li>Programmatic settings \u2014 <code>dj.config['key'] = value</code></li> <li>Environment variables \u2014 <code>DJ_HOST</code>, <code>DJ_USER</code>, etc.</li> <li>Secrets directory \u2014 <code>.secrets/datajoint.json</code>, <code>.secrets/stores.*</code></li> <li>Project configuration \u2014 <code>datajoint.json</code></li> <li>Default values \u2014 Built-in defaults</li> </ol> <p>Higher priority sources override lower ones.</p>"},{"location":"how-to/manage-secrets/#secrets-directory-structure","title":"<code>.secrets/</code> Directory Structure","text":"<p>Create a <code>.secrets/</code> directory in your project root:</p> <pre><code>project/\n\u251c\u2500\u2500 datajoint.json               # Non-sensitive settings (commit)\n\u251c\u2500\u2500 .gitignore                   # Must include .secrets/\n\u251c\u2500\u2500 .secrets/\n\u2502   \u251c\u2500\u2500 datajoint.json           # Database credentials\n\u2502   \u251c\u2500\u2500 stores.main.access_key   # S3/cloud storage credentials\n\u2502   \u251c\u2500\u2500 stores.main.secret_key\n\u2502   \u251c\u2500\u2500 stores.archive.access_key\n\u2502   \u2514\u2500\u2500 stores.archive.secret_key\n\u2514\u2500\u2500 ...\n</code></pre> <p>Critical: Add <code>.secrets/</code> to <code>.gitignore</code>:</p> <pre><code># .gitignore\n.secrets/\n</code></pre>"},{"location":"how-to/manage-secrets/#database-credentials","title":"Database Credentials","text":""},{"location":"how-to/manage-secrets/#option-1-secrets-directory-recommended-for-development","title":"Option 1: Secrets Directory (Recommended for Development)","text":"<p>Create <code>.secrets/datajoint.json</code>:</p> <pre><code>{\n  \"database.user\": \"myuser\",\n  \"database.password\": \"mypassword\"\n}\n</code></pre> <p>Non-sensitive database settings go in <code>datajoint.json</code>:</p> <pre><code>{\n  \"database.host\": \"db.example.com\",\n  \"database.port\": 3306,\n  \"database.use_tls\": true,\n  \"safemode\": true\n}\n</code></pre>"},{"location":"how-to/manage-secrets/#option-2-environment-variables-recommended-for-production","title":"Option 2: Environment Variables (Recommended for Production)","text":"<p>For CI/CD and production environments:</p> <pre><code>export DJ_HOST=db.example.com\nexport DJ_USER=myuser\nexport DJ_PASS=mypassword\nexport DJ_PORT=3306\nexport DJ_TLS=true\n</code></pre>"},{"location":"how-to/manage-secrets/#option-3-programmatic-configuration","title":"Option 3: Programmatic Configuration","text":"<p>For scripts and applications:</p> <pre><code>import datajoint as dj\n\ndj.config['database.host'] = 'localhost'\ndj.config['database.user'] = 'myuser'\ndj.config['database.password'] = 'mypassword'\n</code></pre> <p>Security note: Only use this when credentials come from secure sources (environment, vault, secrets manager).</p>"},{"location":"how-to/manage-secrets/#object-storage-credentials","title":"Object Storage Credentials","text":""},{"location":"how-to/manage-secrets/#file-storage-no-credentials","title":"File Storage (No Credentials)","text":"<p>Local or network-mounted file systems don't require credentials:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/my-project\"\n    }\n  }\n}\n</code></pre>"},{"location":"how-to/manage-secrets/#s3minio-storage-with-credentials","title":"S3/MinIO Storage (With Credentials)","text":""},{"location":"how-to/manage-secrets/#config-in-datajointjson-non-sensitive","title":"Config in <code>datajoint.json</code> (non-sensitive):","text":"<pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"my-bucket\",\n      \"location\": \"my-project/data\"\n    },\n    \"archive\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"archive-bucket\",\n      \"location\": \"my-project/archive\"\n    }\n  }\n}\n</code></pre>"},{"location":"how-to/manage-secrets/#credentials-in-secrets-directory","title":"Credentials in <code>.secrets/</code> directory:","text":"<p>Create separate files for each store's credentials:</p> <pre><code>.secrets/stores.main.access_key\n.secrets/stores.main.secret_key\n.secrets/stores.archive.access_key\n.secrets/stores.archive.secret_key\n</code></pre> <p>File format: Plain text, one credential per file:</p> <pre><code># .secrets/stores.main.access_key\nAKIAIOSFODNN7EXAMPLE\n\n# .secrets/stores.main.secret_key\nwJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre>"},{"location":"how-to/manage-secrets/#alternative-environment-variables","title":"Alternative: Environment Variables","text":"<p>For cloud deployments:</p> <pre><code>export DJ_STORES_MAIN_ACCESS_KEY=AKIAIOSFODNN7EXAMPLE\nexport DJ_STORES_MAIN_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n</code></pre>"},{"location":"how-to/manage-secrets/#environment-variable-reference","title":"Environment Variable Reference","text":""},{"location":"how-to/manage-secrets/#database-connections","title":"Database Connections","text":"Setting Environment Variable Description <code>database.host</code> <code>DJ_HOST</code> Database hostname <code>database.port</code> <code>DJ_PORT</code> Database port (default: 3306) <code>database.user</code> <code>DJ_USER</code> Database username <code>database.password</code> <code>DJ_PASS</code> Database password <code>database.use_tls</code> <code>DJ_TLS</code> Use TLS encryption (true/false)"},{"location":"how-to/manage-secrets/#object-stores","title":"Object Stores","text":"Pattern Example Description <code>DJ_STORES_&lt;NAME&gt;_ACCESS_KEY</code> <code>DJ_STORES_MAIN_ACCESS_KEY</code> S3 access key ID <code>DJ_STORES_&lt;NAME&gt;_SECRET_KEY</code> <code>DJ_STORES_MAIN_SECRET_KEY</code> S3 secret access key <p>Note: <code>&lt;NAME&gt;</code> is the uppercase store name with <code>_</code> replacing special characters.</p>"},{"location":"how-to/manage-secrets/#security-best-practices","title":"Security Best Practices","text":""},{"location":"how-to/manage-secrets/#development-environment","title":"Development Environment","text":"<pre><code># 1. Initialize secrets directory\nmkdir -p .secrets\nchmod 700 .secrets  # Owner-only access\n\n# 2. Create .gitignore\necho \".secrets/\" &gt;&gt; .gitignore\n\n# 3. Store credentials in .secrets/\ncat &gt; .secrets/datajoint.json &lt;&lt;EOF\n{\n  \"database.user\": \"dev_user\",\n  \"database.password\": \"dev_password\"\n}\nEOF\n\n# 4. Set restrictive permissions\nchmod 600 .secrets/datajoint.json\n</code></pre>"},{"location":"how-to/manage-secrets/#production-environment","title":"Production Environment","text":"<pre><code># Use environment variables from secure sources\nexport DJ_USER=$(vault read -field=username secret/datajoint/db)\nexport DJ_PASS=$(vault read -field=password secret/datajoint/db)\nexport DJ_STORES_MAIN_ACCESS_KEY=$(vault read -field=access_key secret/datajoint/s3)\nexport DJ_STORES_MAIN_SECRET_KEY=$(vault read -field=secret_key secret/datajoint/s3)\n</code></pre>"},{"location":"how-to/manage-secrets/#cicd-environment","title":"CI/CD Environment","text":"<p>Use your CI system's secrets management:</p> <p>GitHub Actions: <pre><code>env:\n  DJ_HOST: ${{ secrets.DJ_HOST }}\n  DJ_USER: ${{ secrets.DJ_USER }}\n  DJ_PASS: ${{ secrets.DJ_PASS }}\n</code></pre></p> <p>GitLab CI: <pre><code>variables:\n  DJ_HOST: $DJ_HOST\n  DJ_USER: $DJ_USER\n  DJ_PASS: $DJ_PASS\n</code></pre></p>"},{"location":"how-to/manage-secrets/#docker-containers","title":"Docker Containers","text":"<p>Pass as environment variables:</p> <pre><code>docker run -e DJ_HOST=db.example.com \\\n           -e DJ_USER=myuser \\\n           -e DJ_PASS=mypassword \\\n           my-datajoint-app\n</code></pre> <p>Or mount secrets as read-only volume:</p> <pre><code>docker run -v $(pwd)/.secrets:/app/.secrets:ro \\\n           my-datajoint-app\n</code></pre> <p>Docker Compose:</p> <pre><code>services:\n  app:\n    image: my-datajoint-app\n    env_file: .env\n    volumes:\n      - ./.secrets:/app/.secrets:ro\n</code></pre>"},{"location":"how-to/manage-secrets/#cloud-platforms","title":"Cloud Platforms","text":"<p>AWS Lambda: <pre><code>import os\nimport datajoint as dj\n\ndj.config['database.user'] = os.environ['DJ_USER']\ndj.config['database.password'] = os.environ['DJ_PASS']\n</code></pre></p> <p>Google Cloud Functions: <pre><code>from google.cloud import secretmanager\n\nclient = secretmanager.SecretManagerServiceClient()\nname = f\"projects/{project_id}/secrets/dj-password/versions/latest\"\nresponse = client.access_secret_version(request={\"name\": name})\ndj.config['database.password'] = response.payload.data.decode(\"UTF-8\")\n</code></pre></p> <p>Azure Functions: <pre><code>import os\nimport datajoint as dj\n\ndj.config['database.user'] = os.environ['DJ_USER']\ndj.config['database.password'] = os.environ['DJ_PASS']\n</code></pre></p>"},{"location":"how-to/manage-secrets/#common-patterns","title":"Common Patterns","text":""},{"location":"how-to/manage-secrets/#local-development-with-secrets-directory","title":"Local Development with Secrets Directory","text":"<pre><code>import datajoint as dj\n\n# Config loaded automatically from:\n# 1. datajoint.json (project settings)\n# 2. .secrets/datajoint.json (credentials)\nconn = dj.conn()\n</code></pre>"},{"location":"how-to/manage-secrets/#production-with-environment-variables","title":"Production with Environment Variables","text":"<pre><code>import os\nimport datajoint as dj\n\n# All configuration from environment\n# No files needed\nconn = dj.conn()\n</code></pre>"},{"location":"how-to/manage-secrets/#multi-environment-setup","title":"Multi-Environment Setup","text":"<p>Structure:</p> <pre><code>project/\n\u251c\u2500\u2500 datajoint.json               # Shared settings\n\u251c\u2500\u2500 .secrets/\n\u2502   \u251c\u2500\u2500 datajoint.dev.json      # Development credentials\n\u2502   \u251c\u2500\u2500 datajoint.staging.json  # Staging credentials\n\u2502   \u2514\u2500\u2500 datajoint.prod.json     # Production credentials (if needed)\n\u2514\u2500\u2500 ...\n</code></pre> <p>Load by environment:</p> <pre><code>import os\nimport datajoint as dj\n\nenv = os.getenv('ENV', 'dev')\nsecrets_path = f'.secrets/datajoint.{env}.json'\n\n# Load environment-specific secrets\nimport json\nwith open(secrets_path) as f:\n    secrets = json.load(f)\n    for key, value in secrets.items():\n        dj.config[key] = value\n\nconn = dj.conn()\n</code></pre>"},{"location":"how-to/manage-secrets/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/manage-secrets/#secrets-not-loading","title":"Secrets Not Loading","text":"<p>Check configuration priority:</p> <pre><code>import datajoint as dj\n\n# View current configuration\nprint(dj.config)\n\n# Check specific setting\nprint(dj.config['database.user'])\n\n# See where value came from\nprint(dj.config._config_sources)  # Not a real attribute, just conceptual\n</code></pre>"},{"location":"how-to/manage-secrets/#permission-errors","title":"Permission Errors","text":"<pre><code># Fix .secrets/ permissions\nchmod 700 .secrets\nchmod 600 .secrets/*\n</code></pre>"},{"location":"how-to/manage-secrets/#environment-variables-not-taking-effect","title":"Environment Variables Not Taking Effect","text":"<pre><code>import os\nimport datajoint as dj\n\n# Check environment variables are set\nprint(os.getenv('DJ_USER'))\nprint(os.getenv('DJ_PASS'))\n\n# Force reload configuration\ndj.config.clear()\nconn = dj.conn(reset=True)\n</code></pre>"},{"location":"how-to/manage-secrets/#accidentally-committed-secrets","title":"Accidentally Committed Secrets","text":"<p>Immediate actions:</p> <ol> <li>Rotate credentials immediately</li> <li>Remove from git history:</li> </ol> <pre><code># Remove file from history\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch .secrets/datajoint.json\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\n# Force push (coordinate with team!)\ngit push origin --force --all\n</code></pre> <ol> <li>Verify removal:</li> </ol> <pre><code>git log --all --full-history -- .secrets/datajoint.json\n</code></pre>"},{"location":"how-to/manage-secrets/#configuration-templates","title":"Configuration Templates","text":""},{"location":"how-to/manage-secrets/#minimal-development-setup","title":"Minimal Development Setup","text":"<pre><code>// datajoint.json\n{\n  \"database.host\": \"localhost\",\n  \"safemode\": true\n}\n</code></pre> <pre><code>// .secrets/datajoint.json\n{\n  \"database.user\": \"root\",\n  \"database.password\": \"simple\"\n}\n</code></pre>"},{"location":"how-to/manage-secrets/#production-with-s3-storage","title":"Production with S3 Storage","text":"<pre><code>// datajoint.json\n{\n  \"database.host\": \"db.example.com\",\n  \"database.port\": 3306,\n  \"database.use_tls\": true,\n  \"safemode\": false,\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"my-bucket\",\n      \"location\": \"my-project\"\n    }\n  }\n}\n</code></pre> <pre><code>// .secrets/ directory\n.secrets/datajoint.json                 # Database credentials\n.secrets/stores.main.access_key         # S3 access key\n.secrets/stores.main.secret_key         # S3 secret key\n</code></pre>"},{"location":"how-to/manage-secrets/#see-also","title":"See Also","text":"<ul> <li>Configure Database Connection \u2014 Database-specific configuration</li> <li>Configure Object Stores \u2014 Storage-specific configuration</li> <li>Configuration Reference \u2014 Complete configuration options</li> </ul>"},{"location":"how-to/master-part/","title":"Master-Part Tables","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\n\nschema = dj.Schema('howto_masterpart')\nschema.drop(prompt=False)\nschema = dj.Schema('howto_masterpart')\n</pre> import datajoint as dj  schema = dj.Schema('howto_masterpart') schema.drop(prompt=False) schema = dj.Schema('howto_masterpart') <pre>[2026-01-27 15:28:07] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species : varchar(32)\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int16\n    ---\n    session_date : date\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32\n        ---\n        stimulus : varchar(32)\n        response : varchar(32)\n        reaction_time : float32\n        \"\"\"\n\ndj.Diagram(schema)\n</pre> @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : varchar(16)     ---     species : varchar(32)     \"\"\"  @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_idx : int16     ---     session_date : date     \"\"\"      class Trial(dj.Part):         definition = \"\"\"         -&gt; master         trial_idx : int32         ---         stimulus : varchar(32)         response : varchar(32)         reaction_time : float32         \"\"\"  dj.Diagram(schema) Out[2]: <p>The <code>-&gt; master</code> syntax:</p> <ul> <li>Inherits the master's primary key</li> <li>Creates a foreign key constraint</li> <li>Enforces that parts cannot exist without their master</li> </ul> <p>SQL naming: Part tables use double underscore in the database: <code>session__trial</code></p> In\u00a0[3]: Copied! <pre># Insert subject first\nSubject.insert1({'subject_id': 'M001', 'species': 'Mus musculus'})\n\n# Insert session and trials atomically\nwith dj.conn().transaction:\n    Session.insert1({\n        'subject_id': 'M001',\n        'session_idx': 1,\n        'session_date': '2026-01-20'\n    })\n    Session.Trial.insert([\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 1, \n         'stimulus': 'A', 'response': 'left', 'reaction_time': 0.35},\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 2, \n         'stimulus': 'B', 'response': 'right', 'reaction_time': 0.42},\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 3, \n         'stimulus': 'A', 'response': 'left', 'reaction_time': 0.28},\n    ])\n\nSession.Trial()\n</pre> # Insert subject first Subject.insert1({'subject_id': 'M001', 'species': 'Mus musculus'})  # Insert session and trials atomically with dj.conn().transaction:     Session.insert1({         'subject_id': 'M001',         'session_idx': 1,         'session_date': '2026-01-20'     })     Session.Trial.insert([         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 1,           'stimulus': 'A', 'response': 'left', 'reaction_time': 0.35},         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 2,           'stimulus': 'B', 'response': 'right', 'reaction_time': 0.42},         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 3,           'stimulus': 'A', 'response': 'left', 'reaction_time': 0.28},     ])  Session.Trial() Out[3]: None <p>subject_id</p> None <p>session_idx</p> None <p>trial_idx</p> <p>stimulus</p> <p>response</p> <p>reaction_time</p> M001 1 1 A left 0.35M001 1 2 B right 0.42M001 1 3 A left 0.28 <p>Total: 3</p> <p>The transaction ensures both inserts succeed or neither does. If inserting trials fails, the session is rolled back.</p> In\u00a0[4]: Copied! <pre># All trials\nSession.Trial()\n</pre> # All trials Session.Trial() Out[4]: None <p>subject_id</p> None <p>session_idx</p> None <p>trial_idx</p> <p>stimulus</p> <p>response</p> <p>reaction_time</p> M001 1 1 A left 0.35M001 1 2 B right 0.42M001 1 3 A left 0.28 <p>Total: 3</p> In\u00a0[5]: Copied! <pre># Join master and parts to get session info with each trial\nSession * Session.Trial\n</pre> # Join master and parts to get session info with each trial Session * Session.Trial Out[5]: <p>subject_id</p> None <p>session_idx</p> None <p>trial_idx</p> <p>stimulus</p> <p>response</p> <p>reaction_time</p> <p>session_date</p> M001 1 1 A left 0.35 2026-01-20M001 1 2 B right 0.42 2026-01-20M001 1 3 A left 0.28 2026-01-20 <p>Total: 3</p> In\u00a0[6]: Copied! <pre># Aggregate: count trials and compute mean reaction time per session\nSession.aggr(\n    Session.Trial,\n    n_trials='count(trial_idx)',\n    mean_rt='avg(reaction_time)'\n)\n</pre> # Aggregate: count trials and compute mean reaction time per session Session.aggr(     Session.Trial,     n_trials='count(trial_idx)',     mean_rt='avg(reaction_time)' ) Out[6]: <p>subject_id</p> None <p>session_idx</p> <p>n_trials</p> calculated attribute <p>mean_rt</p> calculated attribute M001 1 3 0.3499999940395355 <p>Total: 1</p> In\u00a0[7]: Copied! <pre>@schema\nclass SessionAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    n_trials : int32\n    mean_rt : float32\n    \"\"\"\n\n    class TrialScore(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Session.Trial\n        ---\n        score : float32\n        \"\"\"\n\n    def make(self, key):\n        trials = (Session.Trial &amp; key).to_dicts()\n        \n        # Insert master\n        self.insert1({\n            **key,\n            'n_trials': len(trials),\n            'mean_rt': sum(t['reaction_time'] for t in trials) / len(trials)\n        })\n        \n        # Insert parts\n        self.TrialScore.insert([\n            {**key, 'trial_idx': t['trial_idx'], 'score': 1.0 / t['reaction_time']}\n            for t in trials\n        ])\n\nSessionAnalysis.populate()\nSessionAnalysis.TrialScore()\n</pre> @schema class SessionAnalysis(dj.Computed):     definition = \"\"\"     -&gt; Session     ---     n_trials : int32     mean_rt : float32     \"\"\"      class TrialScore(dj.Part):         definition = \"\"\"         -&gt; master         -&gt; Session.Trial         ---         score : float32         \"\"\"      def make(self, key):         trials = (Session.Trial &amp; key).to_dicts()                  # Insert master         self.insert1({             **key,             'n_trials': len(trials),             'mean_rt': sum(t['reaction_time'] for t in trials) / len(trials)         })                  # Insert parts         self.TrialScore.insert([             {**key, 'trial_idx': t['trial_idx'], 'score': 1.0 / t['reaction_time']}             for t in trials         ])  SessionAnalysis.populate() SessionAnalysis.TrialScore() Out[7]: None <p>subject_id</p> None <p>session_idx</p> None <p>trial_idx</p> None <p>score</p> M001 1 1 2.857143M001 1 2 2.3809524M001 1 3 3.5714285 <p>Total: 3</p> In\u00a0[8]: Copied! <pre># This deletes the session AND all its trials AND computed results\n(Session &amp; {'subject_id': 'M001', 'session_idx': 1}).delete(prompt=False)\n\n# Verify parts are gone\nlen(Session.Trial())\n</pre> # This deletes the session AND all its trials AND computed results (Session &amp; {'subject_id': 'M001', 'session_idx': 1}).delete(prompt=False)  # Verify parts are gone len(Session.Trial()) <pre>[2026-01-27 15:28:08] Deleting 3 rows from \"howto_masterpart\".\"__session_analysis__trial_score\"\n</pre> <pre>[2026-01-27 15:28:08] Deleting 3 rows from \"howto_masterpart\".\"session__trial\"\n</pre> <pre>[2026-01-27 15:28:08] Deleting 1 rows from \"howto_masterpart\".\"__session_analysis\"\n</pre> <pre>[2026-01-27 15:28:08] Deleting 1 rows from \"howto_masterpart\".\"session\"\n</pre> Out[8]: <pre>0</pre> <p>Direct deletion from parts is blocked by default. Use <code>part_integrity</code> to control this behavior. See Delete Data for details.</p> In\u00a0[9]: Copied! <pre>schema.drop(prompt=False)\n</pre> schema.drop(prompt=False)"},{"location":"how-to/master-part/#master-part-tables","title":"Master-Part Tables\u00b6","text":"<p>Work with compositional data where a master entity contains multiple detail records.</p>"},{"location":"how-to/master-part/#when-to-use-part-tables","title":"When to Use Part Tables\u00b6","text":"<p>Part tables model compositional relationships where:</p> <ul> <li>Parts cannot exist without their master</li> <li>Parts are typically queried in the context of their master</li> <li>The master-parts unit should be inserted and deleted together</li> </ul> <p>Good use cases:</p> Master Part Why Session Trial Trials only exist within sessions Probe Electrode Electrodes are physical parts of probes Video Frame Frames are components of videos Segmentation Cell Detected cells belong to one segmentation <p>Use regular foreign keys instead when:</p> <ul> <li>Entities have independent meaning (e.g., subjects across sessions)</li> <li>Entities need to be queried without the parent context</li> <li>The relationship is a loose association rather than strict composition</li> </ul>"},{"location":"how-to/master-part/#define-master-and-part-tables","title":"Define Master and Part Tables\u00b6","text":"<p>Part tables are nested classes inheriting from <code>dj.Part</code>. Use <code>-&gt; master</code> to reference the enclosing table:</p>"},{"location":"how-to/master-part/#insert-master-and-parts","title":"Insert Master and Parts\u00b6","text":"<p>Masters must exist before their parts. Use a transaction for atomicity:</p>"},{"location":"how-to/master-part/#query-and-aggregate-parts","title":"Query and Aggregate Parts\u00b6","text":"<p>Access parts through the master class:</p>"},{"location":"how-to/master-part/#computed-tables-with-parts","title":"Computed Tables with Parts\u00b6","text":"<p>Computed tables can have parts. In <code>make()</code>, insert the master first, then its parts:</p>"},{"location":"how-to/master-part/#delete-patterns","title":"Delete Patterns\u00b6","text":"<p>Deleting from the master cascades to its parts:</p>"},{"location":"how-to/master-part/#see-also","title":"See Also\u00b6","text":"<ul> <li>Define Tables \u2014 Table definition syntax</li> <li>Insert Data \u2014 Insertion patterns</li> <li>Delete Data \u2014 Deletion and part_integrity options</li> <li>Model Relationships \u2014 Foreign key patterns</li> <li>Master-Part Specification \u2014 Full reference</li> </ul>"},{"location":"how-to/migrate-to-v20/","title":"Migrate to DataJoint 2.0","text":"<p>Upgrade existing pipelines from legacy DataJoint (pre-2.0) to DataJoint 2.0.</p> <p>This guide is optimized for AI coding assistants. Point your AI agent at this document and it will execute the migration with your oversight.</p> <p>Temporary module</p> <p>The <code>datajoint.migrate</code> module is provided temporarily to assist with migration. It will be deprecated in DataJoint 2.1 and removed in 2.2. Complete your migration while on 2.0.</p>"},{"location":"how-to/migrate-to-v20/#requirements","title":"Requirements","text":""},{"location":"how-to/migrate-to-v20/#system-requirements","title":"System Requirements","text":"Component Legacy (pre-2.0) DataJoint 2.0 Python 3.8+ 3.10+ MySQL 5.7+ 8.0+ Character encoding (varies) UTF-8 (utf8mb4) Collation (varies) utf8mb4_bin <p>Action required: Upgrade your Python environment and MySQL server before installing DataJoint 2.0.</p> <p>Character encoding and collation: DataJoint 2.0 standardizes on UTF-8 encoding with binary collation (case-sensitive comparisons). This is configured server-wide and is assumed by DataJoint:</p> <ul> <li>MySQL: <code>utf8mb4</code> character set with <code>utf8mb4_bin</code> collation</li> <li>PostgreSQL (future): <code>UTF8</code> encoding with <code>C</code> collation</li> </ul> <p>Like timezone handling, encoding is infrastructure configuration, not part of the data model. Ensure your MySQL server is configured with these defaults before migration.</p>"},{"location":"how-to/migrate-to-v20/#license-change","title":"License Change","text":"<p>DataJoint 2.0 is licensed under Apache 2.0 (previously LGPL-2.1).</p> <ul> <li>More permissive for commercial and academic use</li> <li>Compatible with broader ecosystem of tools</li> <li>Clearer patent grant provisions</li> </ul> <p>No action required\u2014the new license is more permissive.</p>"},{"location":"how-to/migrate-to-v20/#future-backend-support","title":"Future Backend Support","text":"<p>DataJoint 2.0 introduces portable type aliases (<code>int64</code>, <code>float64</code>, etc.) that prepare the codebase for PostgreSQL backend compatibility in a future release. Migration to core types ensures your schemas will work seamlessly when Postgres support is available.</p> <p>String quoting in restrictions: MySQL and PostgreSQL handle quotes differently. MySQL allows both single and double quotes for string literals, but PostgreSQL interprets double quotes as identifier (column) references. For PostgreSQL compatibility, replace double quotes with single quotes inside SQL restriction strings:</p> <pre><code># Before (MySQL only)\nTable &amp; 'name = \"Alice\"'\nTable &amp; 'date &gt; \"2024-01-01\"'\n\n# After (PostgreSQL compatible)\nTable &amp; \"name = 'Alice'\"\nTable &amp; \"date &gt; '2024-01-01'\"\n</code></pre> <p>See Database Backends Specification for details.</p>"},{"location":"how-to/migrate-to-v20/#before-you-start-testing-recommendation","title":"Before You Start: Testing Recommendation","text":"<p>\u26a1 Want AI agents to automate Phases I-II for you?</p> <p>Create unit and integration tests for your pipeline against a QA database before starting migration. This enables AI agents to perform most migration work automatically, reducing manual effort by 50-80%.</p> <p>\u2192 See Recommendation: Create Tests Before Migration for details.</p> <p>Why this matters:</p> <ul> <li>With tests: Agents migrate code \u2192 run tests \u2192 fix failures \u2192 verify automatically</li> <li>Without tests: Manual verification at every step, higher risk, more time</li> </ul> <p>Tests provide immediate ROI during migration and ongoing value for development.</p>"},{"location":"how-to/migrate-to-v20/#whats-new-in-20","title":"What's New in 2.0","text":""},{"location":"how-to/migrate-to-v20/#3-tier-column-type-system","title":"3-Tier Column Type System","text":"<p>DataJoint 2.0 introduces a unified type system with three tiers:</p> Tier Description Examples Migration Native Raw MySQL types <code>int unsigned</code>, <code>tinyint</code> Auto-converted to core types Core Standardized portable types <code>int64</code>, <code>float64</code>, <code>varchar(100)</code>, <code>json</code> Phase I Codec Serialization to blob or storage <code>&lt;blob&gt;</code>, <code>&lt;blob@store&gt;</code>, <code>&lt;npy@&gt;</code> Phase I-III <p>Learn more: Type System Concept \u00b7 Type System Reference</p>"},{"location":"how-to/migrate-to-v20/#codecs","title":"Codecs","text":"<p>DataJoint 2.0 makes serialization explicit with codecs. In pre-2.0, <code>longblob</code> automatically serialized Python objects; in 2.0, you explicitly choose <code>&lt;blob&gt;</code>.</p>"},{"location":"how-to/migrate-to-v20/#migration-legacy-20","title":"Migration: Legacy \u2192 2.0","text":"pre-2.0 (Implicit) 2.0 (Explicit) Storage Migration <code>longblob</code> <code>&lt;blob&gt;</code> In-table Phase I code, Phase III data <code>mediumblob</code> <code>&lt;blob&gt;</code> In-table Phase I code, Phase III data <code>attach</code> <code>&lt;attach&gt;</code> In-table Phase I code, Phase III data <code>blob@store</code> <code>&lt;blob@store&gt;</code> In-store (hash) Phase I code, Phase III data <code>attach@store</code> <code>&lt;attach@store&gt;</code> In-store (hash) Phase I code, Phase III data <code>filepath@store</code> <code>&lt;filepath@store&gt;</code> In-store (filepath) Phase I code, Phase III data"},{"location":"how-to/migrate-to-v20/#new-in-20-schema-addressed-storage","title":"New in 2.0: Schema-Addressed Storage","text":"<p>These codecs are NEW\u2014there's no legacy equivalent to migrate:</p> Codec Description Storage Adoption <code>&lt;npy@store&gt;</code> NumPy arrays with lazy loading In-store (schema) Phase IV (optional) <code>&lt;object@store&gt;</code> Zarr, HDF5, custom formats In-store (schema) Phase IV (optional) <p>Key principles:</p> <ul> <li>All legacy codec conversions happen in Phase I (code) and Phase III (data)</li> <li>New codecs (<code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>) are adopted in Phase IV for new features or enhanced workflows</li> <li>Schema-addressed storage organizes data by table structure\u2014no migration needed, just new functionality</li> </ul> <p>Learn more: Codec API Reference \u00b7 Custom Codecs</p>"},{"location":"how-to/migrate-to-v20/#column-comment-format-critical-for-blob-migration","title":"Column Comment Format (Critical for Blob Migration)","text":"<p>DataJoint 2.0 stores type information in the SQL column comment using a <code>:type:</code> prefix format:</p> <pre><code>-- 2.0 column comment format\nCOMMENT ':&lt;type&gt;:user comment'\n\n-- Examples\nCOMMENT ':int64:subject identifier'\nCOMMENT ':&lt;blob&gt;:serialized neural data'\nCOMMENT ':&lt;blob@store&gt;:large array in object storage'\n</code></pre> <p>Why this matters for blob columns:</p> <p>In pre-2.0, <code>longblob</code> columns automatically deserialized Python objects using DataJoint's binary serialization format. DataJoint 2.0 identifies blob columns by checking for <code>:&lt;blob&gt;:</code> in the column comment. Without this marker, blob columns are treated as raw binary data and will NOT be deserialized.</p> Column Comment DataJoint 2.0 Behavior <code>:&lt;blob&gt;:neural data</code> \u2713 Deserializes to Python/NumPy objects <code>neural data</code> (no marker) \u2717 Returns raw bytes (no deserialization) <p>Migration requirement: Existing blob columns need their comments updated to include the <code>:&lt;blob&gt;:</code> prefix. This is a metadata-only change\u2014the actual blob data format is unchanged.</p>"},{"location":"how-to/migrate-to-v20/#checking-migration-status","title":"Checking Migration Status","text":"<pre><code>from datajoint.migrate import check_migration_status\n\nstatus = check_migration_status(schema)\nprint(f\"Blob columns: {status['total_blob_columns']}\")\nprint(f\"  Migrated: {status['migrated']}\")\nprint(f\"  Pending: {status['pending']}\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#migrating-blob-column-comments","title":"Migrating Blob Column Comments","text":"<p>Use <code>migrate_columns()</code> to add type markers to all columns (integers, floats, and blobs):</p> <pre><code>from datajoint.migrate import migrate_columns\n\n# Preview changes (dry run)\nresult = migrate_columns(schema, dry_run=True)\nprint(f\"Would migrate {len(result['sql_statements'])} columns\")\nfor sql in result['sql_statements']:\n    print(f\"  {sql}\")\n\n# Apply changes\nresult = migrate_columns(schema, dry_run=False)\nprint(f\"Migrated {result['columns_migrated']} columns\")\n</code></pre> <p>Or use <code>migrate_blob_columns()</code> to migrate only blob columns:</p> <pre><code>from datajoint.migrate import migrate_blob_columns\n\n# Preview\nresult = migrate_blob_columns(schema, dry_run=True)\nprint(f\"Would migrate {result['needs_migration']} blob columns\")\n\n# Apply\nresult = migrate_blob_columns(schema, dry_run=False)\nprint(f\"Migrated {result['migrated']} blob columns\")\n</code></pre> <p>What the migration does:</p> <pre><code>-- Before migration\nALTER TABLE `schema`.`table`\n  MODIFY COLUMN `data` longblob COMMENT 'neural recording';\n\n-- After migration\nALTER TABLE `schema`.`table`\n  MODIFY COLUMN `data` longblob COMMENT ':&lt;blob&gt;:neural recording';\n</code></pre> <p>The data itself is unchanged\u2014only the comment metadata is updated.</p>"},{"location":"how-to/migrate-to-v20/#unified-stores-configuration","title":"Unified Stores Configuration","text":"<p>DataJoint 2.0 replaces <code>external.*</code> with unified <code>stores.*</code> configuration:</p> <p>pre-2.0 (legacy): <pre><code>{\n  \"external\": {\n    \"protocol\": \"file\",\n    \"location\": \"/data/external\"\n  }\n}\n</code></pre></p> <p>2.0 (unified stores): <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/stores\"\n    }\n  }\n}\n</code></pre></p> <p>Learn more: Configuration Reference \u00b7 Configure Object Storage</p>"},{"location":"how-to/migrate-to-v20/#query-api-changes","title":"Query API Changes","text":"pre-2.0 2.0 Phase <code>table.fetch()</code> <code>table.to_arrays()</code> or <code>table.to_dicts()</code> I <code>table.fetch(..., format=\"frame\")</code> <code>table.to_pandas(...)</code> I <code>table.fetch1()</code> <code>table.fetch1()</code> (unchanged) \u2014 <code>table.fetch1('KEY')</code> <code>table.keys()</code> I <code>(table &amp; key)._update('attr', val)</code> <code>table.update1({**key, 'attr': val})</code> I <code>table1 @ table2</code> <code>table1 * table2</code> (natural join with semantic checks) I <code>a.join(b, left=True)</code> Consider <code>a.extend(b)</code> I <code>dj.U('attr') &amp; table</code> Unchanged (correct pattern) \u2014 <code>dj.U('attr') * table</code> <code>table</code> (was a hack to change primary key) I <code>dj.ERD(schema)</code> <code>dj.Diagram(schema)</code> I <code>table.insert([(1, 'a'), (2, 'b')])</code> Must use dicts/DataFrames (no positional tuples) I <p>Note: The <code>fetch()</code> method remains available in DataJoint 2.0 with a deprecation warning. Your existing code will work immediately\u2014<code>fetch()</code> automatically delegates to the appropriate 2.0 method (<code>to_arrays()</code>, <code>to_dicts()</code>, or <code>to_pandas()</code>). You can migrate incrementally as time permits.</p> <p>Learn more: Fetch API Reference \u00b7 Query Operators Reference \u00b7 Semantic Matching</p>"},{"location":"how-to/migrate-to-v20/#migration-overview","title":"Migration Overview","text":"Phase Goal Code Changes Schema/Store Changes Production Impact I Branch &amp; code migration All API updates, type syntax, all codecs (in-table and in-store) Empty <code>_v2</code> schemas + test stores None II Test compatibility \u2014 Populate <code>_v2</code> schemas with sample data, test equivalence None III Migrate production data \u2014 Multiple migration options Varies IV Adopt new features Optional enhancements Optional Running on 2.0 <p>Key principles:</p> <ul> <li>Phase I implements ALL code changes including in-store codecs (using test stores)</li> <li>Production runs on pre-2.0 undisturbed through Phase II</li> <li>Phase III is data migration only\u2014the code is already complete</li> </ul> <p>Timeline:</p> <ul> <li>Phase I: ~1-4 hours (with AI assistance)</li> <li>Phase II: ~1-2 days</li> <li>Phase III: ~1-7 days (depends on data size and option chosen)</li> <li>Phase IV: Ongoing feature adoption</li> </ul>"},{"location":"how-to/migrate-to-v20/#recommendation-create-tests-before-migration","title":"Recommendation: Create Tests Before Migration","text":"<p>Highly recommended for automated, agent-driven migration.</p> <p>If you create unit and integration tests for your pipeline before starting Phase I, AI coding agents can perform most of the migration work automatically, substantially reducing manual effort.</p>"},{"location":"how-to/migrate-to-v20/#why-tests-enable-automated-migration","title":"Why Tests Enable Automated Migration","text":"<p>With tests:</p> <ol> <li> <p>Phase I automation - Agent can:</p> <ul> <li>Migrate code to 2.0 API</li> <li>Run tests to verify correctness</li> <li>Fix failures iteratively</li> <li>Complete migration with high confidence</li> </ul> </li> <li> <p>Phase II automation - Agent can:</p> <ul> <li>Populate <code>_v2</code> schemas with test data</li> <li>Run tests against both legacy and v2 pipelines</li> <li>Verify equivalence automatically</li> <li>Generate validation reports</li> </ul> </li> <li> <p>Phase III guidance - Agent can:</p> <ul> <li>Run tests after data migration</li> <li>Catch issues immediately</li> <li>Guide production cutover with confidence</li> </ul> </li> </ol> <p>Without tests:</p> <ul> <li>Manual verification at each step</li> <li>Higher risk of missed issues</li> <li>More time-intensive validation</li> <li>Uncertainty about correctness</li> </ul>"},{"location":"how-to/migrate-to-v20/#what-tests-to-create","title":"What Tests to Create","text":"<p>Create tests against a QA database and object store (separate from production):</p> <p>Unit tests:</p> <ul> <li>Table definitions compile correctly</li> <li>Schema relationships are valid</li> <li>Populate methods work for individual tables</li> <li>Query patterns return expected results</li> </ul> <p>Integration tests:</p> <ul> <li>End-to-end pipeline execution</li> <li>Data flows through computed tables correctly</li> <li>External file references work (if using <code>&lt;filepath@&gt;</code>)</li> <li>Object storage operations work (if using in-store codecs)</li> </ul> <p>Example test structure:</p> <pre><code># tests/test_tables.py\nimport pytest\nimport datajoint as dj\nfrom my_pipeline import Mouse, Session, Neuron\n\n@pytest.fixture\ndef test_schema():\n    \"\"\"Use QA database for testing.\"\"\"\n    dj.config['database.host'] = 'qa-db.example.com'\n    schema = dj.schema('test_pipeline')\n    yield schema\n    schema.drop()  # Cleanup after test\n\ndef test_mouse_insert(test_schema):\n    \"\"\"Test manual table insertion.\"\"\"\n    Mouse.insert1({'mouse_id': 0, 'dob': '2024-01-01', 'sex': 'M'})\n    assert len(Mouse()) == 1\n\ndef test_session_populate(test_schema):\n    \"\"\"Test session insertion and relationships.\"\"\"\n    Mouse.insert1({'mouse_id': 0, 'dob': '2024-01-01', 'sex': 'M'})\n    Session.insert1({\n        'mouse_id': 0,\n        'session_date': '2024-06-01',\n        'experimenter': 'Alice'\n    })\n    assert len(Session() &amp; 'mouse_id=0') == 1\n\ndef test_neuron_computation(test_schema):\n    \"\"\"Test computed table populate.\"\"\"\n    # Insert upstream data\n    Mouse.insert1({'mouse_id': 0, 'dob': '2024-01-01', 'sex': 'M'})\n    Session.insert1({\n        'mouse_id': 0,\n        'session_date': '2024-06-01',\n        'experimenter': 'Alice'\n    })\n\n    # Populate computed table\n    Neuron.populate()\n\n    # Verify results\n    assert len(Neuron()) &gt; 0\n\ndef test_query_patterns(test_schema):\n    \"\"\"Test common query patterns.\"\"\"\n    # Setup data\n    Mouse.insert1({'mouse_id': 0, 'dob': '2024-01-01', 'sex': 'M'})\n\n    # Test fetch\n    mice = Mouse.fetch(as_dict=True)\n    assert len(mice) == 1\n\n    # Test restriction\n    male_mice = Mouse &amp; \"sex='M'\"\n    assert len(male_mice) == 1\n</code></pre> <p>Integration test example:</p> <pre><code># tests/test_pipeline.py\ndef test_full_pipeline(test_schema):\n    \"\"\"Test complete pipeline execution.\"\"\"\n    # 1. Insert manual data\n    Mouse.insert([\n        {'mouse_id': 0, 'dob': '2024-01-01', 'sex': 'M'},\n        {'mouse_id': 1, 'dob': '2024-01-15', 'sex': 'F'},\n    ])\n\n    Session.insert([\n        {'mouse_id': 0, 'session_date': '2024-06-01', 'experimenter': 'Alice'},\n        {'mouse_id': 1, 'session_date': '2024-06-03', 'experimenter': 'Bob'},\n    ])\n\n    # 2. Populate computed tables\n    Neuron.populate()\n    Analysis.populate()\n\n    # 3. Verify data flows correctly\n    assert len(Mouse()) == 2\n    assert len(Session()) == 2\n    assert len(Neuron()) &gt; 0\n    assert len(Analysis()) &gt; 0\n\n    # 4. Test queries work\n    alice_sessions = Session &amp; \"experimenter='Alice'\"\n    assert len(alice_sessions) == 1\n</code></pre>"},{"location":"how-to/migrate-to-v20/#how-to-use-tests-with-ai-agents","title":"How to Use Tests with AI Agents","text":"<p>Once tests are created, an AI agent can:</p> <pre><code># Agent workflow for Phase I\n1. git checkout -b pre/v2.0\n2. Update schema declarations to _v2\n3. Convert table definitions to 2.0 syntax\n4. Convert API calls (fetch \u2192 to_dicts, etc.)\n5. Run: pytest tests/\n6. Fix any failures iteratively\n7. Repeat 5-6 until all tests pass\n8. Phase I complete automatically!\n\n# Agent workflow for Phase II\n1. Populate _v2 schemas with test data\n2. Run tests against _v2 schemas\n3. Compare with legacy results\n4. Generate validation report\n5. Phase II complete automatically!\n</code></pre>"},{"location":"how-to/migrate-to-v20/#investment-vs-return","title":"Investment vs. Return","text":"<p>Time investment:</p> <ul> <li>Creating tests: ~1-3 days</li> <li>QA database setup: ~1-2 hours</li> </ul> <p>Time saved:</p> <ul> <li>Phase I: ~50-75% reduction (mostly automated)</li> <li>Phase II: ~80% reduction (fully automated validation)</li> <li>Phase III: Higher confidence, faster debugging</li> </ul> <p>Net benefit: Tests pay for themselves during migration and provide ongoing value for future development.</p>"},{"location":"how-to/migrate-to-v20/#when-to-skip-tests","title":"When to Skip Tests","text":"<p>Skip test creation if:</p> <ul> <li>Pipeline is very simple (few tables, no computation)</li> <li>One-time migration with no ongoing development</li> <li>Team has extensive manual testing procedures already</li> <li>Time pressure requires starting migration immediately</li> </ul> <p>Note: Even minimal tests (just table insertion and populate) provide significant value for automated migration.</p>"},{"location":"how-to/migrate-to-v20/#phase-i-branch-and-code-migration","title":"Phase I: Branch and Code Migration","text":"<p>Goal: Implement complete 2.0 API in code using test schemas and test stores.</p> <p>End state:</p> <ul> <li>All Python code uses 2.0 API patterns (fetch, types, codecs)</li> <li>All codecs implemented (in-table <code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code> AND in-store <code>&lt;blob@&gt;</code>, legacy only)</li> <li>Code points to <code>schema_v2</code> databases (empty) and test object stores</li> <li>Production continues on main branch with pre-2.0 undisturbed</li> </ul> <p>What's NOT migrated yet: Production data and production stores (Phase III)</p>"},{"location":"how-to/migrate-to-v20/#step-1-pin-legacy-datajoint-on-main-branch","title":"Step 1: Pin Legacy DataJoint on Main Branch","text":"<p>Ensure production code stays on pre-2.0:</p> <pre><code>git checkout main\n\n# Pin legacy version in requirements\necho \"datajoint&lt;2.0.0\" &gt; requirements.txt\n\ngit add requirements.txt\ngit commit -m \"chore: pin legacy datajoint for production\"\ngit push origin main\n</code></pre> <p>Why: This prevents accidental upgrades to 2.0 in production.</p>"},{"location":"how-to/migrate-to-v20/#step-2-create-migration-branch","title":"Step 2: Create Migration Branch","text":"<pre><code># Create feature branch\ngit checkout -b pre/v2.0\n\n# Install DataJoint 2.0\npip install --upgrade pip\npip install \"datajoint&gt;=2.0.0\"\n\n# Update requirements\necho \"datajoint&gt;=2.0.0\" &gt; requirements.txt\n\ngit add requirements.txt\ngit commit -m \"chore: upgrade to datajoint 2.0\"\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-3-update-schema-declarations","title":"Step 3: Update Schema Declarations","text":"<p>Critical early step: Update all <code>dj.schema()</code> calls to use <code>_v2</code> suffix for parallel testing and validation.</p> <p>Why do this first:</p> <ul> <li>Creates parallel schemas alongside production (e.g., <code>my_pipeline_v2</code>)</li> <li>Allows testing 2.0 code without affecting production schemas</li> <li>Enables side-by-side validation in Phase II</li> <li>Production schemas remain untouched on <code>main</code> branch</li> </ul>"},{"location":"how-to/migrate-to-v20/#find-all-schema-declarations","title":"Find All Schema Declarations","text":"<pre><code># Find all schema() calls in your codebase\ngrep -rn \"dj.schema\\|dj.Schema\" --include=\"*.py\" .\n\n# Example output:\n# pipeline/session.py:5:schema = dj.schema('my_pipeline')\n# pipeline/analysis.py:8:schema = dj.schema('my_pipeline')\n# pipeline/ephys.py:3:schema = dj.schema('ephys_pipeline')\n</code></pre>"},{"location":"how-to/migrate-to-v20/#update-schema-names","title":"Update Schema Names","text":"<p>For each schema declaration, add <code>_v2</code> suffix:</p> <pre><code># BEFORE (production, on main branch)\nschema = dj.schema('my_pipeline')\n\n# AFTER (testing, on pre/v2.0 branch)\nschema = dj.schema('my_pipeline_v2')\n</code></pre> <p>Multiple schemas example:</p> <pre><code># BEFORE\nsession_schema = dj.schema('sessions')\nanalysis_schema = dj.schema('analysis')\nephys_schema = dj.schema('ephys')\n\n# AFTER\nsession_schema = dj.schema('sessions_v2')\nanalysis_schema = dj.schema('analysis_v2')\nephys_schema = dj.schema('ephys_v2')\n</code></pre>"},{"location":"how-to/migrate-to-v20/#ai-agent-prompt-update-schema-declarations","title":"AI Agent Prompt: Update Schema Declarations","text":"<p>\ud83e\udd16 AI Agent Prompt: Phase I - Update Schema Declarations with _v2 Suffix</p> <pre><code>You are updating DataJoint schema declarations for 2.0 migration testing.\n\nTASK: Add _v2 suffix to all dj.schema() calls for parallel testing.\n\nCONTEXT:\n- Branch: pre/v2.0 (just created)\n- Production schemas on main branch remain unchanged\n- _v2 schemas will be empty until table definitions are converted\n- This enables side-by-side testing without affecting production\n\nSTEPS:\n\n1. Find all schema declarations:\n   grep -rn \"dj.schema\\|dj.Schema\" --include=\"*.py\" .\n\n2. For EACH schema declaration, add _v2 suffix:\n   OLD: schema = dj.schema('my_pipeline')\n   NEW: schema = dj.schema('my_pipeline_v2')\n\n3. Preserve all other arguments:\n   OLD: schema = dj.schema('sessions', locals())\n   NEW: schema = dj.schema('sessions_v2', locals())\n\n   OLD: schema = dj.schema('analysis', create_schema=True)\n   NEW: schema = dj.schema('analysis_v2', create_schema=True)\n\n4. Update any string references to schema names:\n   OLD: conn.query(\"USE my_pipeline\")\n   NEW: conn.query(\"USE my_pipeline_v2\")\n\n   OLD: if schema_name == 'my_pipeline':\n   NEW: if schema_name == 'my_pipeline_v2':\n\nNAMING CONVENTION:\n\n- my_pipeline \u2192 my_pipeline_v2\n- sessions \u2192 sessions_v2\n- ephys_pipeline \u2192 ephys_pipeline_v2\n- lab.mouse \u2192 lab.mouse_v2\n\nVERIFICATION:\n\nAfter updating, verify:\n- All dj.schema() calls have _v2 suffix\n- No hard-coded schema names without _v2 suffix\n- No duplicate schema names (each should be unique)\n\nCOMMIT:\n\ngit add -A\ngit commit -m \"feat(phase-i): add _v2 suffix to all schema declarations\n\n- Update all dj.schema() calls to use _v2 suffix\n- Enables parallel testing without affecting production schemas\n- Production schemas on main branch remain unchanged\n\nSchemas updated:\n- my_pipeline \u2192 my_pipeline_v2\n- [list other schemas...]\"\n</code></pre>"},{"location":"how-to/migrate-to-v20/#verify-schema-name-changes","title":"Verify Schema Name Changes","text":"<pre><code>import datajoint as dj\n\n# Test connection (should work before any tables created)\nconn = dj.conn()\nprint(\"\u2713 Connected to database\")\n\n# At this point, _v2 schemas don't exist yet\n# They will be created in Step 5 when table definitions are applied\n</code></pre>"},{"location":"how-to/migrate-to-v20/#commit-schema-declaration-changes","title":"Commit Schema Declaration Changes","text":"<pre><code>git add -A\ngit commit -m \"feat(phase-i): add _v2 suffix to all schema declarations\n\n- Update all dj.schema() calls to use _v2 suffix\n- Enables parallel testing without affecting production schemas\n- Next: configure stores and convert table definitions\"\n</code></pre> <p>Next steps:</p> <ul> <li>Step 4: Configure object stores (if applicable)</li> <li>Step 5: Convert table definitions to 2.0 syntax</li> <li>When table definitions are applied, <code>_v2</code> schemas will be created</li> </ul>"},{"location":"how-to/migrate-to-v20/#step-4-configure-datajoint-20","title":"Step 4: Configure DataJoint 2.0","text":"<p>Create new configuration files for 2.0.</p> <p>Note: Schema declarations already updated in Step 3 with <code>_v2</code> suffix. Now configure database connection and stores.</p>"},{"location":"how-to/migrate-to-v20/#background-configuration-changes","title":"Background: Configuration Changes","text":"<p>DataJoint 2.0 uses:</p> <ul> <li><code>.secrets/datajoint.json</code> for credentials (gitignored)</li> <li><code>datajoint.json</code> for non-sensitive settings (checked in)</li> <li><code>stores.*</code> instead of <code>external.*</code></li> </ul> <p>Learn more: Configuration Reference</p>"},{"location":"how-to/migrate-to-v20/#create-configuration-files","title":"Create Configuration Files","text":"<pre><code># Create .secrets directory\nmkdir -p .secrets\necho \".secrets/\" &gt;&gt; .gitignore\n\n# Create template\npython -c \"import datajoint as dj; dj.config.save_template()\"\n</code></pre> <p>Edit <code>.secrets/datajoint.json</code>: <pre><code>{\n  \"database.host\": \"your-database-host\",\n  \"database.user\": \"your-username\",\n  \"database.password\": \"your-password\"\n}\n</code></pre></p> <p>Edit <code>datajoint.json</code>: <pre><code>{\n  \"loglevel\": \"INFO\",\n  \"safemode\": true,\n  \"display.limit\": 12,\n  \"display.width\": 100,\n  \"display.show_tuple_count\": true\n}\n</code></pre></p>"},{"location":"how-to/migrate-to-v20/#verify-connection","title":"Verify Connection","text":"<pre><code>import datajoint as dj\n\n# Test connection\nconn = dj.conn()\nprint(f\"Connected to {conn.conn_info['host']}\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-5-configure-test-object-stores-if-applicable","title":"Step 5: Configure Test Object Stores (If Applicable)","text":"<p>Skip this step if: Your legacy pipeline uses only in-table storage (<code>longblob</code>, <code>mediumblob</code>, <code>blob</code>, <code>attach</code>). You can skip to Step 6.</p> <p>Configure test stores if: Your legacy pipeline uses pre-2.0 in-store formats:</p> <ul> <li><code>blob@store</code> (hash-addressed blobs in object store)</li> <li><code>attach@store</code> (hash-addressed attachments in object store)</li> <li><code>filepath@store</code> (filepath references to external files)</li> </ul> <p>Note: <code>&lt;npy@&gt;</code> and <code>&lt;object@&gt;</code> are NEW in 2.0 (schema-addressed storage). They have no legacy equivalent and don't need migration. Adopt them in Phase IV for new features.</p>"},{"location":"how-to/migrate-to-v20/#background-pre-20-implicit-vs-20-explicit-codecs","title":"Background: pre-2.0 Implicit vs 2.0 Explicit Codecs","text":"<p>pre-2.0 implicit serialization:</p> <ul> <li><code>longblob</code> \u2192 automatic Python object serialization (pickle)</li> <li><code>mediumblob</code> \u2192 automatic Python object serialization (pickle)</li> <li><code>blob</code> \u2192 automatic Python object serialization (pickle)</li> <li>No explicit codec choice - serialization was built-in</li> </ul> <p>2.0 explicit codecs:</p> <ul> <li><code>&lt;blob&gt;</code> \u2192 explicit Python object serialization (same behavior, now explicit)</li> <li><code>&lt;attach&gt;</code> \u2192 explicit file attachment (was separate feature)</li> <li>Legacy in-store formats converted to explicit <code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;filepath@&gt;</code> syntax</li> </ul>"},{"location":"how-to/migrate-to-v20/#background-unified-stores","title":"Background: Unified Stores","text":"<p>2.0 uses unified stores configuration:</p> <ul> <li>Single <code>stores.*</code> config for all storage types (hash-addressed + schema-addressed + filepath)</li> <li>Named stores with <code>default</code> pointer</li> <li>Supports multiple stores with different backends</li> </ul> <p>Learn more: Configure Object Storage \u00b7 Object Store Configuration Spec</p>"},{"location":"how-to/migrate-to-v20/#configure-test-stores","title":"Configure Test Stores","text":"<p>Edit <code>datajoint.json</code> to use test directories: <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/v2_test_stores/main\"\n    }\n  }\n}\n</code></pre></p> <p>Note: Use separate test locations (e.g., <code>/data/v2_test_stores/</code>) to avoid conflicts with production stores.</p> <p>For multiple test stores: <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"filepath_default\": \"raw_data\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/v2_test_stores/main\"\n    },\n    \"raw_data\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/v2_test_stores/raw\"\n    }\n  }\n}\n</code></pre></p> <p>For cloud storage (using test bucket/prefix): <pre><code>{\n  \"stores\": {\n    \"default\": \"s3_store\",\n    \"s3_store\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"my-datajoint-test-bucket\",\n      \"location\": \"v2-test\"\n    }\n  }\n}\n</code></pre></p> <p>Store credentials in <code>.secrets/stores.s3_store.access_key</code> and <code>.secrets/stores.s3_store.secret_key</code>: <pre><code>echo \"YOUR_ACCESS_KEY\" &gt; .secrets/stores.s3_store.access_key\necho \"YOUR_SECRET_KEY\" &gt; .secrets/stores.s3_store.secret_key\n</code></pre></p>"},{"location":"how-to/migrate-to-v20/#ai-agent-prompt-configure-and-test-stores","title":"AI Agent Prompt: Configure and Test Stores","text":"<p>\ud83e\udd16 AI Agent Prompt: Phase I - Configure Test Stores and Verify Codecs</p> <pre><code>You are configuring test object stores for DataJoint 2.0 migration.\n\nTASK: Set up test stores and verify all in-store codecs work correctly\nbefore migrating production data.\n\nCONTEXT:\n- Phase I uses TEST stores (separate from production)\n- Testing verifies codecs work with legacy schema structure\n- File organization must match expectations\n- Production data migration happens in Phase III\n\nSTEPS:\n\n1. Configure test stores in datajoint.json:\n   - Use test locations (e.g., /data/v2_test_stores/)\n   - For cloud: use test bucket or prefix (e.g., \"v2-test\")\n   - Configure hash_prefix, schema_prefix, filepath_prefix if needed\n\n2. Store credentials in .secrets/ directory:\n   - Create .secrets/stores.&lt;name&gt;.access_key (S3/GCS/Azure)\n   - Create .secrets/stores.&lt;name&gt;.secret_key (S3)\n   - Verify .secrets/ is gitignored\n\n3. Test ALL in-store codecs from legacy schema:\n   - &lt;blob@store&gt; (hash-addressed blob storage)\n   - &lt;attach@store&gt; (hash-addressed attachments)\n   - &lt;filepath@store&gt; (filepath references)\n\n4. Create test table with all three codecs:\n   ```python\n   @schema\n   class StoreTest(dj.Manual):\n       definition = \"\"\"\n       test_id : int\n       ---\n       blob_data : &lt;blob@&gt;\n       attach_data : &lt;attach@&gt;\n       filepath_data : &lt;filepath@&gt;\n       \"\"\"\n   ```\n\n5. Insert test data and verify:\n   - Insert sample data for each codec\n   - Fetch data back successfully\n   - Verify files appear at expected paths\n\n6. Understand hash-addressed storage structure:\n   {location}/{hash_prefix}/{schema_name}/{hash}[.ext]\n\n   With subfolding [2, 2]:\n   {location}/{hash_prefix}/{schema_name}/{h1}{h2}/{h3}{h4}/{hash}[.ext]\n\n   Properties:\n   - Immutable (content-addressed)\n   - Deduplicated (same content \u2192 same path)\n   - Integrity (hash validates content)\n\n7. Verify file organization meets expectations:\n   - Check files exist at {location}/{hash_prefix}/{schema}/\n   - Verify subfolding structure if configured\n   - Confirm filepath references work correctly\n\n8. Clean up test:\n   - Delete test data\n   - Drop test schema\n   - Verify no errors during cleanup\n\nHASH-ADDRESSED STORAGE:\n\nUnderstanding the hash-addressed section is critical for migration:\n\n- Path format: {location}/{hash_prefix}/{schema}/{hash}\n- Hash computed from serialized content (Blake2b)\n- Hash encoded as base32 (lowercase, no padding)\n- Subfolding splits hash into directory levels\n- Same content always produces same path (deduplication)\n\nExample with hash_prefix=\"_hash\", subfolding=[2,2]:\n  /data/store/_hash/my_schema/ab/cd/abcdef123456...\n\nLearn more: Object Store Configuration Spec\n(../reference/specs/object-store-configuration.md#hash-addressed-storage)\n\nVERIFICATION:\n\n- [ ] Test stores configured in datajoint.json\n- [ ] Credentials stored in .secrets/ (not committed)\n- [ ] Connection to test stores successful\n- [ ] &lt;blob@&gt; codec tested and working\n- [ ] &lt;attach@&gt; codec tested and working\n- [ ] &lt;filepath@&gt; codec tested and working\n- [ ] Files appear at expected locations\n- [ ] Hash-addressed structure understood\n- [ ] Test cleanup successful\n\nREPORT:\n\nTest results for store configuration:\n- Store names: [list configured stores]\n- Store protocol: [file/s3/gcs/azure]\n- Store location: [test path/bucket]\n- Hash prefix: [configured value]\n- Codecs tested: [blob@, attach@, filepath@]\n- Files verified at: [example paths]\n- Issues found: [any errors or unexpected behavior]\n\nCOMMIT MESSAGE:\n\"feat(phase-i): configure test stores and verify codecs\n\n- Configure test stores with [protocol] at [location]\n- Store credentials in .secrets/ directory\n- Test all in-store codecs: blob@, attach@, filepath@\n- Verify hash-addressed file organization\n- Confirm codecs work with legacy schema structure\n\nTest stores ready for table definition conversion.\"\n</code></pre>"},{"location":"how-to/migrate-to-v20/#test-in-store-codecs","title":"Test In-Store Codecs","text":"<p>After configuring test stores, verify that in-store codecs work correctly and understand the file organization.</p> <p>Create test table with all in-store codecs:</p> <pre><code>import datajoint as dj\nimport numpy as np\n\n# Create test schema\nschema = dj.schema('test_stores_v2')\n\n@schema\nclass StoreTest(dj.Manual):\n    definition = \"\"\"\n    test_id : int\n    ---\n    blob_data : &lt;blob@&gt;          # Hash-addressed blob\n    attach_data : &lt;attach@&gt;      # Hash-addressed attachment\n    filepath_data : &lt;filepath@&gt;  # Filepath reference\n    \"\"\"\n\n# Test data\ntest_blob = {'key': 'value', 'array': [1, 2, 3]}\ntest_attach = {'metadata': 'test attachment'}\n\n# For filepath, create test file first\nimport tempfile\nimport os\ntemp_dir = tempfile.gettempdir()\ntest_file_path = 'test_data/sample.txt'\nfull_path = os.path.join(\n    dj.config['stores']['default']['location'],\n    test_file_path\n)\nos.makedirs(os.path.dirname(full_path), exist_ok=True)\nwith open(full_path, 'w') as f:\n    f.write('test content')\n\n# Insert test data\nStoreTest.insert1({\n    'test_id': 1,\n    'blob_data': test_blob,\n    'attach_data': test_attach,\n    'filepath_data': test_file_path\n})\n\nprint(\"\u2713 Test data inserted successfully\")\n</code></pre> <p>Verify file organization:</p> <pre><code># Fetch and verify\nresult = (StoreTest &amp; {'test_id': 1}).fetch1()\nprint(f\"\u2713 blob_data: {result['blob_data']}\")\nprint(f\"\u2713 attach_data: {result['attach_data']}\")\nprint(f\"\u2713 filepath_data: {result['filepath_data']}\")\n\n# Inspect hash-addressed file organization\nstore_spec = dj.config.get_store_spec()\nhash_prefix = store_spec.get('hash_prefix', '_hash')\nlocation = store_spec['location']\n\nprint(f\"\\nStore organization:\")\nprint(f\"  Location: {location}\")\nprint(f\"  Hash prefix: {hash_prefix}/\")\nprint(f\"  Expected structure: {hash_prefix}/{{schema}}/{{hash}}\")\nprint(f\"\\nVerify files exist at:\")\nprint(f\"  {location}/{hash_prefix}/test_stores_v2/\")\n</code></pre> <p>Review hash-addressed storage structure:</p> <p>Hash-addressed storage (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>) uses content-based paths:</p> <pre><code>{location}/{hash_prefix}/{schema_name}/{hash}[.ext]\n</code></pre> <p>With subfolding enabled (e.g., <code>[2, 2]</code>):</p> <pre><code>{location}/{hash_prefix}/{schema_name}/{h1}{h2}/{h3}{h4}/{hash}[.ext]\n</code></pre> <p>Properties:</p> <ul> <li>Immutable: Content defines path, cannot be changed</li> <li>Deduplicated: Identical content stored once</li> <li>Integrity: Hash validates content on retrieval</li> </ul> <p>Learn more: [Object Store Configuration \u2014 Hash-Addressed Storage] (../reference/specs/object-store-configuration.md#hash-addressed-storage)</p> <p>Cleanup test:</p> <pre><code># Remove test data\n(StoreTest &amp; {'test_id': 1}).delete()\nschema.drop()\nprint(\"\u2713 Test cleanup complete\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-6-convert-table-definitions","title":"Step 6: Convert Table Definitions","text":"<p>Update table definitions in topological order (tables before their dependents).</p> <p>Note: Schema declarations already updated to <code>_v2</code> suffix in Step 3.</p>"},{"location":"how-to/migrate-to-v20/#background-type-syntax-changes","title":"Background: Type Syntax Changes","text":"<p>Convert ALL types and codecs in Phase I:</p> <p>Integer and Float Types:</p> pre-2.0 2.0 Category <code>int unsigned</code> <code>int64</code> Core type <code>int</code> <code>int32</code> Core type <code>smallint unsigned</code> <code>int32</code> Core type <code>tinyint unsigned</code> <code>int16</code> Core type <code>bigint unsigned</code> <code>int64</code> Core type <code>float</code> <code>float32</code> Core type <code>double</code> <code>float64</code> Core type <p>String, Date, and Structured Types:</p> pre-2.0 2.0 Notes <code>varchar(N)</code>, <code>char(N)</code> Unchanged Core types <code>date</code> Unchanged Core type <code>enum('a', 'b')</code> Unchanged Core type <code>bool</code>, <code>boolean</code> <code>bool</code> Core type (MySQL stores as tinyint(1)) <code>datetime</code> <code>datetime</code> Core type; UTC standard in 2.0 <code>timestamp</code> <code>datetime</code> Ask user: Review timezone convention, convert to UTC datetime <code>json</code> <code>json</code> Core type (was available but underdocumented) <code>uuid</code> <code>uuid</code> Core type (widely used in legacy) <code>text</code> <code>varchar(N)</code> or keep as native Native type: Consider migrating to <code>varchar(n)</code> <code>time</code> <code>datetime</code> or keep as native Native type: Consider using <code>datetime</code> <code>tinyint(1)</code> <code>bool</code> or <code>int16</code> Ask user: was this boolean or small integer? <p>Codecs:</p> pre-2.0 2.0 Category <code>longblob</code> <code>&lt;blob&gt;</code> Codec (in-table) <code>attach</code> <code>&lt;attach&gt;</code> Codec (in-table) <code>blob@store</code> <code>&lt;blob@store&gt;</code> Codec (in-store) <code>attach@store</code> <code>&lt;attach@store&gt;</code> Codec (in-store) <code>filepath@store</code> <code>&lt;filepath@store&gt;</code> Codec (in-store) <p>Important Notes:</p> <ul> <li>Core vs Native Types: DataJoint 2.0 distinguishes core types (portable, standardized) from native types (backend-specific). Core types are preferred. Native types like <code>text</code> and <code>time</code> are allowed but discouraged\u2014they may generate warnings and lack portability guarantees.</li> </ul> <ul> <li>Datetime/Timestamp: DataJoint 2.0 adopts UTC as the standard for all datetime storage. The database stores UTC; timezones are handled by application front-ends and client APIs. For <code>timestamp</code> columns, review your existing timezone convention\u2014you may need data conversion. We recommend adopting UTC throughout your pipeline and converting <code>timestamp</code> to <code>datetime</code>.</li> </ul> <ul> <li>Bool: Legacy DataJoint supported <code>bool</code> and <code>boolean</code> types (MySQL stores as <code>tinyint(1)</code>). Keep as <code>bool</code> in 2.0. Only explicit <code>tinyint(1)</code> declarations need review:<ul> <li>If used for boolean semantics (yes/no, active/inactive) \u2192 <code>bool</code></li> <li>If used for small integers (counts, indices 0-255) \u2192 <code>int16</code></li> </ul> </li> </ul> <ul> <li>Text Type: <code>text</code> is a native MySQL type, not a core type. Consider migrating to <code>varchar(n)</code> with appropriate length. If your text truly needs unlimited length, you can keep <code>text</code> as a native type (will generate a warning).</li> </ul> <ul> <li>Time Type: <code>time</code> is a native MySQL type with no core equivalent. We recommend migrating to <code>datetime</code> (which can represent both date and time components). If you only need time-of-day without date, you can keep <code>time</code> as a native type (will generate a warning).</li> </ul> <ul> <li>JSON: Core type that was available in pre-2.0 but underdocumented. Many users serialized JSON into blobs. If you have custom JSON serialization in blobs, you can migrate to native <code>json</code> type (optional).</li> </ul> <ul> <li>Enum: Core type\u2014no changes needed.</li> </ul> <ul> <li>In-store codecs: Code is converted in Phase I using test stores. Production data migration happens in Phase III.</li> </ul> <p>Learn more: Type System Reference \u00b7 Definition Syntax</p>"},{"location":"how-to/migrate-to-v20/#ai-agent-prompt-convert-table-definitions","title":"AI Agent Prompt: Convert Table Definitions","text":"<p>Use this prompt with your AI coding assistant:</p> <p>\ud83e\udd16 AI Agent Prompt: Phase I - Table Definition Conversion</p> <pre><code>You are converting DataJoint pre-2.0 table definitions to 2.0 syntax.\n\nTASK: Update all table definitions in this repository to DataJoint 2.0 type syntax.\n\nCONTEXT:\n\n- We are on branch: pre/v2.0\n- Production (main branch) remains on pre-2.0\n- Schema declarations ALREADY updated with _v2 suffix (Step 3)\n- Now converting table definitions to match\n- Schemas will be created empty when definitions are applied\n\nSCOPE - PHASE I:\n\n1. Convert ALL type syntax to 2.0 core types\n2. Convert ALL legacy codecs (in-table AND in-store)\n   - In-table: longblob \u2192 &lt;blob&gt;, mediumblob \u2192 &lt;blob&gt;, attach \u2192 &lt;attach&gt;\n   - In-store (legacy only): blob@store \u2192 &lt;blob@store&gt;, attach@store \u2192 &lt;attach@store&gt;, filepath@store \u2192 &lt;filepath@store&gt;\n3. Code will use TEST stores configured in datajoint.json\n4. Do NOT add new 2.0 codecs (&lt;npy@&gt;, &lt;object@&gt;) - these are for Phase IV adoption\n5. Production data migration happens in Phase III (code is complete after Phase I)\n\nTYPE CONVERSIONS:\n\nCore Types (Integer and Float):\n  int unsigned \u2192 int64\n  int \u2192 int32\n  smallint unsigned \u2192 int32\n  smallint \u2192 int16\n  tinyint unsigned \u2192 int16\n  tinyint \u2192 int8\n  bigint unsigned \u2192 int64\n  bigint \u2192 int64\n  float \u2192 float32\n  double \u2192 float64\n  decimal(M,D) \u2192 decimal(M,D)  # unchanged\n\nCore Types (String and Date):\n  varchar(N) \u2192 varchar(N)  # unchanged (core type)\n  char(N) \u2192 char(N)  # unchanged (core type)\n  date \u2192 date  # unchanged (core type)\n  enum('a', 'b') \u2192 enum('a', 'b')  # unchanged (core type)\n  bool \u2192 bool  # unchanged (core type, MySQL stores as tinyint(1))\n  boolean \u2192 bool  # unchanged (core type, MySQL stores as tinyint(1))\n  datetime \u2192 datetime  # unchanged (core type)\n\nCore Types (Structured Data):\n  json \u2192 json  # unchanged (core type, was available but underdocumented in pre-2.0)\n  uuid \u2192 uuid  # unchanged (core type, widely used in pre-2.0)\n\nNative Types (Discouraged but Allowed):\n  text \u2192 Consider varchar(N) with appropriate length, or keep as native type\n  time \u2192 Consider datetime (can represent date+time), or keep as native type\n\nSpecial Cases - REQUIRE USER REVIEW:\n\n  tinyint(1) \u2192 ASK USER: bool or int16?\n    Note: Legacy DataJoint had bool/boolean types. Only explicit tinyint(1) needs review.\n    - Boolean semantics (yes/no, active/inactive) \u2192 bool\n    - Small integer (counts, indices 0-255) \u2192 int16\n    Example:\n      is_active : tinyint(1)  # Boolean semantics \u2192 bool\n      priority : tinyint(1)   # 0-10 scale \u2192 int16\n      has_data : bool         # Already bool \u2192 keep as bool\n\n  timestamp \u2192 ASK USER about timezone convention, then convert to datetime\n    Example:\n      created_at : timestamp  # pre-2.0 (UNKNOWN timezone convention)\n      created_at : datetime   # 2.0 (UTC standard)\n\nIMPORTANT - Datetime and Timestamp Conversion:\n\nDataJoint 2.0 adopts UTC as the standard for all datetime storage (no timezone information).\nThe database stores UTC; timezones are handled by application front-ends and client APIs.\n\nConversion rules:\n\n- datetime \u2192 Keep as datetime (assume UTC, core type)\n- timestamp \u2192 ASK USER about timezone convention, then convert to datetime\n- date \u2192 Keep as date (core type)\n- time \u2192 ASK USER: recommend datetime (core type) or keep as time (native type)\n\nFor EACH timestamp column, ASK THE USER:\n\n1. \"What timezone convention was used for [column_name]?\"\n   - UTC (no conversion needed)\n   - Server local time (requires conversion to UTC)\n   - Application local time (requires conversion to UTC)\n   - Mixed/unknown (requires data audit)\n\n2. \"Does this use MySQL's auto-update behavior (ON UPDATE CURRENT_TIMESTAMP)?\"\n   - If yes, may need to update table schema\n   - If no, application controls the value\n\n3. After clarifying, recommend:\n   - Convert type: timestamp \u2192 datetime\n   - If not already UTC: Add data conversion script to Phase III\n   - Update application code to store UTC times\n   - Handle timezone display in application front-ends and client APIs\n\nExample conversation:\n  AI: \"I found timestamp column 'session_time'. What timezone was used?\"\n  User: \"Server time (US/Eastern)\"\n  AI: \"I recommend converting to UTC. I'll convert the type to datetime and add a\n       data conversion step in Phase III to convert US/Eastern times to UTC.\"\n\nExample:\n  # pre-2.0\n  session_time : timestamp    # Was storing US/Eastern\n  event_time : timestamp      # Already UTC\n\n  # 2.0 (after user confirmation)\n  session_time : datetime     # Converted to UTC in Phase III\n  event_time : datetime       # No data conversion needed\n\nIMPORTANT - Bool Type:\n\nLegacy DataJoint already supported bool and boolean types (MySQL stores as tinyint(1)).\n\nConversion rules:\n\n- bool \u2192 Keep as bool (no change)\n- boolean \u2192 Keep as bool (no change)\n- tinyint(1) \u2192 ASK USER: was this boolean or small integer?\n\nOnly explicit tinyint(1) declarations need review because:\n\n- Legacy had bool/boolean for true/false values\n- Some users explicitly used tinyint(1) for small integers (0-255)\n\nExample:\n  # pre-2.0\n  is_active : bool           # Already bool \u2192 no change\n  enabled : boolean          # Already boolean \u2192 bool\n  is_valid : tinyint(1)      # ASK: Boolean semantics? \u2192 bool\n  n_retries : tinyint(1)     # ASK: Small integer? \u2192 int16\n\n  # 2.0\n  is_active : bool           # Unchanged\n  enabled : bool             # boolean \u2192 bool\n  is_valid : bool            # Boolean semantics\n  n_retries : int16          # Small integer\n\nIMPORTANT - Enum Types:\n\nenum is a core type\u2014no changes required.\n\nExample:\n  sex : enum('M', 'F', 'U')  # No change needed\n\nIMPORTANT - JSON Type:\n\njson is a core type that was available in pre-2.0 but underdocumented. Many users\nserialized JSON into blobs. If you have custom JSON serialization in blobs, you can\nmigrate to native json type (optional migration, not required).\n\nExample:\n  # Optional: migrate blob with JSON to native json\n  config : longblob  # Contains serialized JSON\n  config : json      # Core JSON type (optional improvement)\n\nIMPORTANT - Native Types (text and time):\n\ntext and time are NATIVE MySQL types, NOT core types. They are allowed but discouraged.\n\nFor text:\n\n- ASK USER: What is the maximum expected length?\n- Recommend migrating to varchar(n) with appropriate length (core type)\n- Or keep as text (native type, will generate warning)\n\nFor time:\n\n- ASK USER: Is this time-of-day only, or is date also relevant?\n- Recommend migrating to datetime (core type, can represent date+time)\n- Or keep as time (native type, will generate warning)\n\nExample:\n  # pre-2.0\n  description : text           # Native type\n  session_start : time         # Native type (time-of-day)\n\n  # 2.0 (recommended)\n  description : varchar(1000)  # Core type (after asking user about max length)\n  session_start : datetime     # Core type (if date is also relevant)\n\n  # 2.0 (alternative - keep native)\n  description : text           # Native type (if truly unlimited length needed)\n  session_start : time         # Native type (if only time-of-day needed)\n\nIn-Table Codecs:\n  longblob \u2192 &lt;blob&gt;\n  attach \u2192 &lt;attach&gt;\n\nIn-Store Codecs (LEGACY formats only - convert these):\n  blob@store \u2192 &lt;blob@store&gt;  # Add angle brackets\n  attach@store \u2192 &lt;attach@store&gt;  # Add angle brackets\n  filepath@store \u2192 &lt;filepath@store&gt;  # Add angle brackets\n\nIMPORTANT - Do NOT use these during migration (NEW in 2.0):\n  &lt;npy@store&gt;  # Schema-addressed storage - NEW feature\n  &lt;object@store&gt;  # Schema-addressed storage - NEW feature\n  # These have NO legacy equivalent\n  # Adopt in Phase IV AFTER migration is complete\n  # Do NOT convert existing attributes to these codecs\n\nSCHEMA DECLARATIONS:\n  OLD: schema = dj.schema('my_pipeline')\n  NEW: schema = dj.schema('my_pipeline_v2')\n\nPROCESS:\n1. Identify all Python files with DataJoint schemas\n2. For each schema:\n   a. Update schema declaration (add _v2 suffix)\n   b. Create schema on database (empty for now)\n3. For each table definition in TOPOLOGICAL ORDER:\n   a. Convert ALL type syntax (core types + all codecs)\n   b. Verify syntax is valid\n4. Test that all tables can be declared (run file to create tables)\n5. Verify in-store codecs work with test stores\n\nVERIFICATION:\n\n- All schema declarations use _v2 suffix\n- All native types converted to core types\n- All codecs converted (in-table AND in-store)\n- Test stores configured and accessible\n- No syntax errors\n- All tables create successfully (empty)\n\nEXAMPLE CONVERSION:\n\n# pre-2.0\nschema = dj.schema('neuroscience_pipeline')\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int unsigned\n    ---\n    sampling_rate : float\n    signal : blob@raw  # pre-2.0 in-store syntax\n    waveforms : blob@raw  # pre-2.0 in-store syntax\n    metadata : longblob  # pre-2.0 in-table\n    \"\"\"\n\n# 2.0 (Phase I with test stores)\nschema = dj.schema('neuroscience_pipeline_v2')\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int64\n    ---\n    sampling_rate : float32\n    signal : &lt;blob@raw&gt;  # Converted: blob@raw \u2192 &lt;blob@raw&gt;\n    waveforms : &lt;blob@raw&gt;  # Converted: blob@raw \u2192 &lt;blob@raw&gt;\n    metadata : &lt;blob&gt;  # Converted: longblob \u2192 &lt;blob&gt;\n    \"\"\"\n\n# Phase I: Only convert existing legacy formats\n# Do NOT add new codecs like &lt;npy@&gt; during migration\n\n# If you want to adopt &lt;npy@&gt; later (Phase IV), that's a separate step:\n# - After migration is complete\n# - For new features or performance improvements\n# - Not required for migration\n\nREPORT:\n\n- Schemas converted: [list with _v2 suffix]\n- Tables converted: [count by schema]\n- Type conversions: [count by type]\n- Codecs converted:\n  - In-table: [count of &lt;blob&gt;, &lt;attach&gt;]\n  - In-store: [count of &lt;blob@&gt;, &lt;npy@&gt;, &lt;filepath@&gt;]\n- Tables created successfully: [list]\n- Test stores configured: [list store names]\n\nCOMMIT MESSAGE FORMAT:\n\"feat(phase-i): convert table definitions to 2.0 syntax\n\n- Update schema declarations to *_v2\n- Convert native types to core types (int64, float64, etc.)\n- Convert all codecs (in-table + in-store)\n- Configure test stores for development/testing\n\nTables converted: X\nCodecs converted: Y (in-table: Z, in-store: W)\"\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-7-convert-query-and-insert-code","title":"Step 7: Convert Query and Insert Code","text":"<p>Update all DataJoint API calls to 2.0 patterns.</p>"},{"location":"how-to/migrate-to-v20/#background-api-changes","title":"Background: API Changes","text":"<p>Fetch API:</p> <p>Note: <code>fetch()</code> remains available with a deprecation warning and works immediately. Convert to new methods when convenient for cleaner, more explicit code.</p> <ul> <li><code>fetch()</code> \u2192 <code>to_arrays()</code> (recarray-like) or <code>to_dicts()</code> (list of dicts)</li> <li><code>fetch(..., format=\"frame\")</code> \u2192 <code>to_pandas()</code> (pandas DataFrame)</li> <li><code>fetch('attr1', 'attr2')</code> \u2192 <code>to_arrays('attr1', 'attr2')</code> (returns tuple)</li> <li><code>fetch1()</code> \u2192 unchanged (still returns dict for single row)</li> </ul> <p>Update Method:</p> <ul> <li><code>(table &amp; key)._update('attr', val)</code> \u2192 <code>table.update1({**key, 'attr': val})</code></li> </ul> <p>Join Operators:</p> <ul> <li><code>table1 @ table2</code> \u2192 <code>table1 * table2</code> (natural join with semantic checks enabled)</li> <li><code>a.join(b, left=True)</code> \u2192 Consider <code>a.extend(b)</code></li> </ul> <p>Universal Set:</p> <ul> <li><code>dj.U('attr') &amp; table</code> \u2192 Unchanged (correct pattern for projecting attributes)</li> <li><code>dj.U('attr') * table</code> \u2192 <code>table</code> (was a hack to change primary key)</li> </ul> <p>Visualization:</p> <ul> <li><code>dj.ERD(schema)</code> \u2192 <code>dj.Diagram(schema)</code> (ERD deprecated)</li> </ul> <p>Learn more: Fetch API Reference \u00b7 Query Operators</p>"},{"location":"how-to/migrate-to-v20/#ai-agent-prompt-convert-query-and-insert-code","title":"AI Agent Prompt: Convert Query and Insert Code","text":"<p>\ud83e\udd16 AI Agent Prompt: Phase I - Query and Insert Code Conversion</p> <pre><code>You are converting DataJoint pre-2.0 query and insert code to 2.0 API.\n\nTASK: Update all query, fetch, and insert code to use DataJoint 2.0 API\npatterns.\n\nLEARN MORE: See Fetch API Reference (../reference/specs/fetch-api.md),\nQuery Operators (../reference/operators.md), and Semantic Matching\n(../reference/specs/semantic-matching.md).\n\nCONTEXT:\n- Branch: pre/v2.0\n- Schema declarations already updated to _v2 suffix\n- Table definitions already converted\n- Production code on main branch unchanged\n\nAPI CONVERSIONS:\n\n1. Fetch API (recommended conversion - fetch() still works with deprecation warning):\n\n   NOTE: fetch() remains available in 2.0 and automatically delegates to the\n   new methods. Existing code works immediately. Convert when convenient.\n\n   OLD: data = table.fetch()\n   NEW: data = table.to_arrays()  # recarray-like\n   # OR: keep as fetch() - works with deprecation warning\n\n   OLD: data = table.fetch(as_dict=True)\n   NEW: data = table.to_dicts()  # list of dicts\n\n   OLD: data = table.fetch(format=\"frame\")\n   NEW: data = table.to_pandas()  # pandas DataFrame\n\n   OLD: data = table.fetch('attr1', 'attr2')\n   NEW: data = table.to_arrays('attr1', 'attr2')  # returns tuple\n\n   OLD: row = table.fetch1()\n   NEW: row = table.fetch1()  # UNCHANGED\n\n   OLD: keys = table.fetch1('KEY')\n   NEW: keys = table.keys()  # Returns list of dicts with primary key values\n\n   OLD: keys, a, b = table.fetch(\"KEY\", \"a\", \"b\")\n   NEW: a, b = table.to_arrays('a', 'b', include_key=True)\n   # Returns tuple with keys included\n\n2. Update Method (always convert):\n   OLD: (table &amp; key)._update('attr', value)\n   NEW: table.update1({**key, 'attr': value})\n\n3. Join Operator (always convert):\n   OLD: result = table1 @ table2\n   NEW: result = table1 * table2  # Natural join WITH semantic checks\n\n   IMPORTANT: The @ operator bypassed semantic checks. The * operator\n   enables semantic checks by default. If semantic checks fail,\n   INVESTIGATE\u2014this may reveal errors in your schema or data.\n\n   For left joins:\n   OLD: result = a.join(b, left=True)\n   NEW: result = a.extend(b)  # Consider using extend for left joins\n\n4. Universal Set (CHECK - distinguish correct from hack):\n   CORRECT (unchanged):\n   result = dj.U('attr') &amp; table  # Projects specific attributes, unchanged\n\n   HACK (always refactor):\n   OLD: result = dj.U('attr') * table  # Was hack to change primary key\n   NEW: result = table  # Simply use table directly\n\n   Note: The * operator with dj.U() was a hack. Replace with just table.\n\n5. Insert (CHANGED - requires named keys):\n   OLD: table.insert([(1, 'Alice'), (2, 'Bob')])  # Positional tuples\n   NEW: table.insert([{'id': 1, 'name': 'Alice'},\n                      {'id': 2, 'name': 'Bob'}])  # Dicts\n\n   DataJoint 2.0 requires named key-value mappings for insert:\n   - Dicts (most common)\n   - DataFrames\n   - Other DataJoint queries\n\n   Positional tuples/lists are NO LONGER SUPPORTED.\n\n6. Delete (unchanged):\n   (table &amp; key).delete()  # unchanged\n   (table &amp; restriction).delete()  # unchanged\n\n7. String Quoting in Restrictions (PostgreSQL compatibility):\n   Replace double quotes with single quotes for string literals in SQL restrictions.\n\n   MySQL allows both quote styles, but PostgreSQL interprets double quotes as\n   identifier (column) references, causing errors.\n\n   OLD: Table &amp; 'name = \"Alice\"'\n   OLD: Table &amp; 'date &gt; \"2024-01-01\"'\n   OLD: Table &amp; 'strain = \"C57BL/6\"'\n\n   NEW: Table &amp; \"name = 'Alice'\"\n   NEW: Table &amp; \"date &gt; '2024-01-01'\"\n   NEW: Table &amp; \"strain = 'C57BL/6'\"\n\n   Note: Dictionary restrictions handle quoting automatically but only support\n   equality comparisons. For range comparisons (&gt;, &lt;, LIKE, etc.), use string\n   restrictions with single-quoted values.\n\nPROCESS:\n1. Find all Python files with DataJoint code\n2. For each file:\n   a. Search for fetch patterns\n   b. Replace with 2.0 equivalents\n   c. Search for update patterns\n   d. Replace with update1()\n   e. Search for @ operator (replace with * for natural join)\n   f. Search for .join(x, left=True) patterns (consider .extend(x))\n   g. Search for dj.U() * patterns (replace with just table)\n   h. Verify dj.U() &amp; patterns remain unchanged\n   i. Search for string restrictions with double-quoted values\n   j. Replace double quotes with single quotes inside SQL strings\n3. Run syntax checks\n4. Run existing tests if available\n5. If semantic checks fail after @ \u2192 * conversion, investigate schema/data\n\nVERIFICATION:\n\n- .fetch() calls either converted OR intentionally kept (works with deprecation warning)\n- No .fetch1('KEY') calls remaining (replaced with .keys())\n- No ._update() calls remaining\n- No @ operator between tables\n- dj.U() * patterns replaced with just table\n- dj.U() &amp; patterns remain unchanged\n- No double-quoted string literals in SQL restrictions\n- All tests pass (if available)\n- Semantic check failures investigated and resolved\n\nCOMMON PATTERNS:\n\nPattern 1: Fetch all as dicts\nOLD: sessions = Session.fetch(as_dict=True)\nNEW: sessions = Session.to_dicts()\n\nPattern 2: Fetch specific attributes\nOLD: mouse_ids, dobs = Mouse.fetch('mouse_id', 'dob')\nNEW: mouse_ids, dobs = Mouse.to_arrays('mouse_id', 'dob')\n\nPattern 3: Fetch as pandas DataFrame\nOLD: df = Mouse.fetch(format=\"frame\")\nNEW: df = Mouse.to_pandas()\n\nPattern 4: Fetch single row\nOLD: row = (Mouse &amp; key).fetch1()  # unchanged\nNEW: row = (Mouse &amp; key).fetch1()  # unchanged\n\nPattern 5: Update attribute\nOLD: (Session &amp; key)._update('experimenter', 'Alice')\nNEW: Session.update1({**key, 'experimenter': 'Alice'})\n\nPattern 6: Fetch primary keys\nOLD: keys = Mouse.fetch1('KEY')\nNEW: keys = Mouse.keys()\n\nPattern 7: Fetch with keys included\nOLD: keys, weights, ages = Mouse.fetch(\"KEY\", \"weight\", \"age\")\nNEW: weights, ages = Mouse.to_arrays('weight', 'age', include_key=True)\n\nPattern 8: Natural join (now WITH semantic checks)\nOLD: result = Neuron @ Session\nNEW: result = Neuron * Session\n# Semantic checks enabled\u2014may reveal schema errors\n\nPattern 9: Left join\nOLD: result = Session.join(Experiment, left=True)\nNEW: result = Session.extend(Experiment)  # Consider using extend\n\nPattern 10: Universal set (distinguish correct from hack)\nCORRECT (unchanged):\nOLD: all_dates = dj.U('session_date') &amp; Session\nNEW: all_dates = dj.U('session_date') &amp; Session  # Unchanged, correct\n\nHACK (always replace):\nOLD: result = dj.U('new_pk') * Session  # Hack to change primary key\nNEW: result = Session  # Simply use table directly\n\nREPORT:\n\n- Files modified: [list]\n- fetch() \u2192 to_arrays/to_dicts: [count]\n- fetch(..., format=\"frame\") \u2192 to_pandas(): [count]\n- fetch1('KEY') \u2192 keys(): [count]\n- _update() \u2192 update1(): [count]\n- @ \u2192 * (natural join): [count]\n- .join(x, left=True) \u2192 .extend(x): [count]\n- dj.U() * table \u2192 table: [count]\n- dj.U() &amp; table patterns (unchanged): [count]\n- dj.ERD() \u2192 dj.Diagram(): [count]\n- Semantic check failures: [count and resolution]\n- Tests passed: [yes/no]\n\nCOMMIT MESSAGE FORMAT:\n\"feat(phase-i): convert query and insert code to 2.0 API\n\n- Replace fetch() with to_arrays()/to_dicts()/to_pandas()\n- Replace fetch1('KEY') with keys()\n- Replace _update() with update1()\n- Replace @ operator with * (enables semantic checks)\n- Replace .join(x, left=True) with .extend(x)\n- Replace dj.ERD() with dj.Diagram()\n- Replace dj.U() * table with just table (was hack)\n- Keep dj.U() &amp; table patterns unchanged (correct)\n- Investigate and resolve semantic check failures\n\nAPI conversions: X fetch, Y update, Z join\"\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-8-update-populate-methods","title":"Step 8: Update Populate Methods","text":"<p><code>make()</code> methods in Computed and Imported tables use the same API patterns covered in Steps 6-7.</p> <p>Apply the following conversions to all <code>make()</code> methods:</p> <ol> <li> <p>Fetch API conversions (from Step 7)</p> <ul> <li><code>fetch()</code> \u2192 <code>to_arrays()</code> or <code>to_dicts()</code></li> <li><code>fetch(..., format=\"frame\")</code> \u2192 <code>to_pandas()</code></li> <li><code>fetch1('KEY')</code> \u2192 <code>keys()</code></li> <li>All other fetch patterns</li> </ul> </li> <li> <p>Join conversions (from Step 7)</p> <ul> <li><code>@</code> \u2192 <code>*</code> (enables semantic checks)</li> <li><code>a.join(b, left=True)</code> \u2192 <code>a.extend(b)</code></li> <li><code>dj.U() * table</code> \u2192 <code>table</code> (was a hack)</li> </ul> </li> <li> <p>Insert conversions (NEW REQUIREMENT)</p> <ul> <li>Positional tuples NO LONGER SUPPORTED</li> <li>Must use named key-value mappings:    <pre><code># OLD (no longer works)\nself.insert1((key['id'], computed_value, timestamp))\n\n# NEW (required)\nself.insert1({\n    **key,\n    'computed_value': computed_value,\n    'timestamp': timestamp\n})\n</code></pre></li> </ul> </li> </ol> <p>Note: Since these are the same conversions from Step 7, you can apply them in a single pass. The only additional consideration is ensuring insert statements use dicts.</p>"},{"location":"how-to/migrate-to-v20/#step-9-verify-phase-i-complete","title":"Step 9: Verify Phase I Complete","text":""},{"location":"how-to/migrate-to-v20/#checklist","title":"Checklist","text":"<ul> <li> <code>pre/v2.0</code> branch created</li> <li> DataJoint 2.0 installed (<code>pip list | grep datajoint</code>)</li> <li> Configuration files created (<code>.secrets/</code>, <code>datajoint.json</code>)</li> <li> Test stores configured (if using in-store codecs)</li> <li> In-store codecs tested (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;filepath@&gt;</code>)</li> <li> Hash-addressed file organization verified and understood</li> <li> All schema declarations use <code>_v2</code> suffix</li> <li> All table definitions use 2.0 type syntax</li> <li> All in-table codecs converted (<code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>)</li> <li> All in-store codecs converted (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;filepath@&gt;</code>)</li> <li> All <code>fetch()</code> calls converted OR intentionally kept (works with deprecation warning)</li> <li> All <code>fetch(..., format=\"frame\")</code> converted to <code>to_pandas()</code></li> <li> All <code>fetch1('KEY')</code> converted to <code>keys()</code></li> <li> All <code>._update()</code> calls converted</li> <li> All <code>@</code> operators converted to <code>*</code></li> <li> All <code>dj.U() * table</code> patterns replaced with just <code>table</code> (was a hack)</li> <li> All <code>dj.U() &amp; table</code> patterns verified as unchanged (correct)</li> <li> All <code>dj.ERD()</code> calls converted to <code>dj.Diagram()</code></li> <li> All populate methods updated</li> <li> No syntax errors</li> <li> All <code>_v2</code> schemas created (empty)</li> </ul>"},{"location":"how-to/migrate-to-v20/#test-schema-creation","title":"Test Schema Creation","text":"<pre><code># Run your main module to create all tables\nimport your_pipeline_v2\n\n# Verify schemas exist\nimport datajoint as dj\nconn = dj.conn()\n\nschemas = conn.query(\"SHOW DATABASES LIKE '%_v2'\").fetchall()\nprint(f\"Created {len(schemas)} _v2 schemas:\")\nfor schema in schemas:\n    print(f\"  - {schema[0]}\")\n\n# Verify tables created\nfor schema_name in [s[0] for s in schemas]:\n    tables = conn.query(\n        f\"SELECT COUNT(*) FROM information_schema.TABLES \"\n        f\"WHERE TABLE_SCHEMA='{schema_name}'\"\n    ).fetchone()[0]\n    print(f\"{schema_name}: {tables} tables\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#commit-phase-i","title":"Commit Phase I","text":"<pre><code># Review all changes\ngit status\ngit diff\n\n# Commit\ngit add .\ngit commit -m \"feat: complete Phase I migration to DataJoint 2.0\n\nSummary:\n- Created _v2 schemas (empty)\n- Converted all table definitions to 2.0 syntax\n- Converted all query/insert code to 2.0 API\n- Converted all populate methods\n- Configured test stores for in-store codecs\n- Production data migration deferred to Phase III\n\nSchemas: X\nTables: Y\nCode files: Z\"\n\ngit push origin pre/v2.0\n</code></pre> <p>\u2705 Phase I Complete!</p> <p>You now have:</p> <ul> <li>2.0-compatible code on <code>pre/v2.0</code> branch</li> <li>Empty <code>_v2</code> schemas ready for testing</li> <li>Production still running on <code>main</code> branch with pre-2.0</li> </ul> <p>Next: Phase II - Test with sample data</p>"},{"location":"how-to/migrate-to-v20/#phase-ii-test-compatibility-and-equivalence","title":"Phase II: Test Compatibility and Equivalence","text":"<p>Goal: Validate that the 2.0 pipeline produces equivalent results to the legacy pipeline.</p> <p>End state:</p> <ul> <li>2.0 pipeline runs correctly with sample data in <code>_v2</code> schemas and test stores</li> <li>Results are equivalent to running legacy pipeline on same data</li> <li>Confidence that migration is correct before touching production</li> <li>Production still untouched</li> </ul> <p>Key principle: Test with identical data in both legacy and v2 schemas to verify equivalence.</p>"},{"location":"how-to/migrate-to-v20/#step-1-run-your-regular-workflow","title":"Step 1: Run Your Regular Workflow","text":"<p>Use your existing data entry and populate processes on the <code>_v2</code> schemas:</p> <pre><code># Import your v2 pipeline\nfrom your_pipeline_v2 import schema  # Points to my_pipeline_v2\n\n# Follow your normal workflow:\n# 1. Insert test data into manual tables (same process as usual)\n# 2. Run populate on computed/imported tables (same process as usual)\n# 3. Run any queries or analysis scripts (using 2.0 API)\n\n# Example (adapt to your pipeline):\n# YourManualTable.insert([...])  # Your usual insert process\n# YourComputedTable.populate(display_progress=True)  # Your usual populate\n</code></pre> <p>Key points:</p> <ul> <li>Use a representative subset of data (not full production dataset)</li> <li>Follow your existing workflow - don't create artificial examples</li> <li>Populate computed tables using your normal populate process</li> <li>Run any existing analysis or query scripts you have</li> <li>Test that everything works with the 2.0 API</li> </ul>"},{"location":"how-to/migrate-to-v20/#step-2-compare-with-legacy-schema-equivalence-testing","title":"Step 2: Compare with Legacy Schema (Equivalence Testing)","text":"<p>Critical: Run identical data through both legacy and v2 pipelines to verify equivalence.</p>"},{"location":"how-to/migrate-to-v20/#option-a-side-by-side-comparison","title":"Option A: Side-by-Side Comparison","text":"<pre><code># compare_legacy_v2.py\nimport datajoint as dj\nimport numpy as np\n\n# Import both legacy and v2 modules\nimport your_pipeline as legacy  # pre-2.0 on main branch (checkout to test)\nimport your_pipeline_v2 as v2  # 2.0 on pre/v2.0 branch\n\ndef compare_results():\n    \"\"\"Compare query results between legacy and v2.\"\"\"\n\n    # Insert same data into both schemas\n    test_data = [\n        {'mouse_id': 0, 'dob': '2024-01-01', 'sex': 'M'},\n        {'mouse_id': 1, 'dob': '2024-01-15', 'sex': 'F'},\n    ]\n\n    legacy.Mouse.insert(test_data, skip_duplicates=True)\n    v2.Mouse.insert(test_data, skip_duplicates=True)\n\n    # Compare query results\n    legacy_mice = legacy.Mouse.fetch(as_dict=True)  # pre-2.0 syntax\n    v2_mice = v2.Mouse.to_dicts()  # 2.0 syntax\n\n    assert len(legacy_mice) == len(v2_mice), \"Row count mismatch!\"\n\n    # Compare values (excluding fetch-specific artifacts)\n    for leg, v2_row in zip(legacy_mice, v2_mice):\n        for key in leg.keys():\n            if leg[key] != v2_row[key]:\n                print(f\"MISMATCH: {key}: {leg[key]} != {v2_row[key]}\")\n                return False\n\n    print(\"\u2713 Query results are equivalent!\")\n    return True\n\ndef compare_populate():\n    \"\"\"Compare populate results.\"\"\"\n\n    # Populate both\n    legacy.Neuron.populate(display_progress=True)\n    v2.Neuron.populate(display_progress=True)\n\n    # Compare counts\n    legacy_count = len(legacy.Neuron())\n    v2_count = len(v2.Neuron())\n\n    assert legacy_count == v2_count, f\"Count mismatch: {legacy_count} != {v2_count}\"\n\n    print(f\"\u2713 Populate generated same number of rows: {v2_count}\")\n\n    # Compare computed values (if numeric)\n    for key in (legacy.Neuron &amp; 'neuron_id=0').keys():\n        leg_val = (legacy.Neuron &amp; key).fetch1('activity')\n        v2_val = (v2.Neuron &amp; key).fetch1('activity')\n\n        if isinstance(leg_val, np.ndarray):\n            assert np.allclose(leg_val, v2_val, rtol=1e-9), \"Array values differ!\"\n        else:\n            assert leg_val == v2_val, f\"Value mismatch: {leg_val} != {v2_val}\"\n\n    print(\"\u2713 Populate results are equivalent!\")\n    return True\n\nif __name__ == '__main__':\n    print(\"Comparing legacy and v2 pipelines...\")\n    compare_results()\n    compare_populate()\n    print(\"\\n\u2713 All equivalence tests passed!\")\n</code></pre> <p>Run comparison:</p> <pre><code>python compare_legacy_v2.py\n</code></pre>"},{"location":"how-to/migrate-to-v20/#option-b-data-copy-and-validation","title":"Option B: Data Copy and Validation","text":"<p>If you can't easily import both modules:</p> <ol> <li>Copy sample data from production to both legacy test schema and <code>_v2</code> schema</li> <li>Run populate on both</li> <li>Use helper to compare:</li> </ol> <pre><code>from datajoint.migrate import compare_query_results\n\n# Compare table contents\nresult = compare_query_results(\n    prod_schema='my_pipeline',\n    test_schema='my_pipeline_v2',\n    table='neuron',\n    tolerance=1e-6,\n)\n\nif result['match']:\n    print(f\"\u2713 {result['row_count']} rows match\")\nelse:\n    print(f\"\u2717 Discrepancies found:\")\n    for disc in result['discrepancies']:\n        print(f\"  {disc}\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-3-run-existing-tests","title":"Step 3: Run Existing Tests","text":"<p>If you have a test suite:</p> <pre><code># Run tests against _v2 schemas\npytest tests/ -v\n\n# Or specific test modules\npytest tests/test_queries.py -v\npytest tests/test_populate.py -v\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-4-document-test-results","title":"Step 4: Document Test Results","text":"<p>Document your testing process and results:</p> <p>What to document:</p> <ul> <li>Date of testing</li> <li>Test data used (subset, size, representative samples)</li> <li>Tables tested and row counts</li> <li>Populate results (did computed tables generate expected rows?)</li> <li>Equivalence test results (if comparing with legacy)</li> <li>Any issues found and how they were resolved</li> <li>Test suite results (if you have automated tests)</li> </ul> <p>Purpose: Creates a record of validation for your team and future reference. Useful when planning production migration in Phase III.</p> <p>\u2705 Phase II Complete!</p> <p>You now have:</p> <ul> <li>Validated 2.0 pipeline with sample data</li> <li>Confidence in code migration</li> <li>Test report documenting success</li> <li>Ready to migrate production data</li> </ul> <p>Next: Phase III - Migrate production data</p>"},{"location":"how-to/migrate-to-v20/#phase-iii-migrate-production-data","title":"Phase III: Migrate Production Data","text":"<p>Goal: Migrate production data and configure production stores. Code is complete from Phase I.</p> <p>End state:</p> <ul> <li>Production data migrated to <code>_v2</code> schemas</li> <li>Production stores configured (replacing test stores)</li> <li>In-store metadata updated (UUID \u2192 JSON)</li> <li>Ready to switch production to 2.0</li> </ul> <p>Key principle: All code changes were completed in Phase I. This phase is DATA migration only.</p> <p>Prerequisites:</p> <ul> <li>Phase I complete (all code migrated)</li> <li>Phase II complete (equivalence validated)</li> <li>Production backup created</li> <li>Production workloads quiesced</li> </ul> <p>Options:</p> <ul> <li>Option A: Copy data, rename schemas (recommended - safest)</li> <li>Option B: In-place migration (for very large databases)</li> <li>Option C: Gradual migration with legacy compatibility</li> </ul> <p>Choose the option that best fits your needs.</p>"},{"location":"how-to/migrate-to-v20/#option-a-copy-data-and-rename-schemas-recommended","title":"Option A: Copy Data and Rename Schemas (Recommended)","text":"<p>Best for: Most pipelines, especially &lt; 1 TB</p> <p>Advantages:</p> <ul> <li>Safe - production unchanged until final step</li> <li>Easy rollback</li> <li>Can practice multiple times</li> </ul> <p>Process:</p>"},{"location":"how-to/migrate-to-v20/#0-configure-production-stores","title":"0. Configure Production Stores","text":"<p>Update <code>datajoint.json</code> to point to production stores (not test stores):</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/production_stores/main\"  # Production location\n    }\n  }\n}\n</code></pre> <p>For in-store data migration: You can either:</p> <ul> <li>Keep files in place (recommended): Point to existing pre-2.0 store locations</li> <li>Copy to new location: Configure new production stores and copy files</li> </ul> <p>Commit this change: <pre><code>git add datajoint.json\ngit commit -m \"config: update stores to production locations\"\n</code></pre></p>"},{"location":"how-to/migrate-to-v20/#1-backup-production","title":"1. Backup Production","text":"<pre><code># Full backup\nmysqldump --all-databases &gt; backup_$(date +%Y%m%d).sql\n\n# Or schema-specific\nmysqldump my_pipeline &gt; my_pipeline_backup_$(date +%Y%m%d).sql\n</code></pre>"},{"location":"how-to/migrate-to-v20/#2-copy-manual-table-data","title":"2. Copy Manual Table Data","text":"<pre><code>from datajoint.migrate import copy_table_data\n\n# Copy each manual table\ntables = ['mouse', 'session', 'experimenter']  # Your manual tables\n\nfor table in tables:\n    result = copy_table_data(\n        source_schema='my_pipeline',\n        dest_schema='my_pipeline_v2',\n        table=table,\n    )\n    print(f\"{table}: copied {result['rows_copied']} rows\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#3-populate-computed-tables","title":"3. Populate Computed Tables","text":"<pre><code>from your_pipeline_v2 import Neuron, Analysis\n\n# Populate using 2.0 code\nNeuron.populate(display_progress=True)\nAnalysis.populate(display_progress=True)\n</code></pre>"},{"location":"how-to/migrate-to-v20/#4-migrate-in-store-metadata","title":"4. Migrate In-Store Metadata","text":"<p>Important: Your code already handles in-store codecs (converted in Phase I). This step just updates metadata format.</p> <p>If you have tables using <code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, or <code>&lt;filepath@&gt;</code> codecs, migrate the storage metadata from legacy BINARY(16) UUID format to 2.0 JSON format:</p> <pre><code>from datajoint.migrate import migrate_external_pointers_v2\n\n# Update metadata format (UUID \u2192 JSON)\n# This does NOT move files\u2014just updates database pointers\nresult = migrate_external_pointers_v2(\n    schema='my_pipeline_v2',\n    table='recording',\n    attribute='signal',\n    source_store='raw',  # Legacy pre-2.0 store name\n    dest_store='raw',  # 2.0 store name (from datajoint.json)\n    copy_files=False,  # Keep files in place (recommended)\n)\n\nprint(f\"Migrated {result['rows_migrated']} pointers\")\n</code></pre> <p>What this does:</p> <ul> <li>Reads legacy BINARY(16) UUID pointers from <code>~external_*</code> hidden tables</li> <li>Creates new JSON metadata with file path, store name, hash</li> <li>Writes JSON to the <code>&lt;blob@store&gt;</code> column (code written in Phase I)</li> <li>Does NOT copy files (unless <code>copy_files=True</code>)</li> </ul> <p>Result: Files stay in place, but 2.0 code can now access them via the new codec system.</p>"},{"location":"how-to/migrate-to-v20/#5-validate-data-integrity","title":"5. Validate Data Integrity","text":"<pre><code>from datajoint.migrate import compare_query_results\n\n# Compare production vs _v2\ntables_to_check = ['mouse', 'session', 'neuron', 'analysis']\n\nall_match = True\nfor table in tables_to_check:\n    result = compare_query_results(\n        prod_schema='my_pipeline',\n        test_schema='my_pipeline_v2',\n        table=table,\n        tolerance=1e-6,\n    )\n\n    if result['match']:\n        print(f\"\u2713 {table}: {result['row_count']} rows match\")\n    else:\n        print(f\"\u2717 {table}: discrepancies found\")\n        for disc in result['discrepancies'][:5]:\n            print(f\"    {disc}\")\n        all_match = False\n\nif all_match:\n    print(\"\\n\u2713 All tables validated! Ready for cutover.\")\nelse:\n    print(\"\\n\u2717 Fix discrepancies before proceeding.\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#6-schedule-cutover","title":"6. Schedule Cutover","text":"<p>Pre-cutover checklist:</p> <ul> <li> Full backup verified</li> <li> All data copied</li> <li> All computed tables populated</li> <li> Validation passed</li> <li> Team notified</li> <li> Maintenance window scheduled</li> <li> All pre-2.0 clients stopped</li> </ul> <p>Execute cutover:</p> <pre><code>-- Rename production \u2192 old\nRENAME TABLE `my_pipeline` TO `my_pipeline_old`;\n\n-- Rename _v2 \u2192 production\nRENAME TABLE `my_pipeline_v2` TO `my_pipeline`;\n</code></pre> <p>Update code:</p> <pre><code># On pre/v2.0 branch, update schema names back\nsed -i '' 's/_v2//g' your_pipeline/*.py\n\ngit add .\ngit commit -m \"chore: remove _v2 suffix for production\"\n\n# Merge to main\ngit checkout main\ngit merge pre/v2.0\ngit push origin main\n\n# Deploy updated code\n</code></pre>"},{"location":"how-to/migrate-to-v20/#7-verify-production","title":"7. Verify Production","text":"<pre><code># Test production after cutover\nfrom your_pipeline import schema, Mouse, Neuron\n\nprint(f\"Mice: {len(Mouse())}\")\nprint(f\"Neurons: {len(Neuron())}\")\n\n# Run a populate\nNeuron.populate(limit=5, display_progress=True)\n</code></pre>"},{"location":"how-to/migrate-to-v20/#8-cleanup-after-1-2-weeks","title":"8. Cleanup (After 1-2 Weeks)","text":"<pre><code>-- After confirming production stable\nDROP DATABASE `my_pipeline_old`;\n</code></pre>"},{"location":"how-to/migrate-to-v20/#option-b-in-place-migration","title":"Option B: In-Place Migration","text":"<p>Best for: Very large databases (&gt; 1 TB) where copying is impractical</p> <p>Warning: Modifies production schema directly. Test thoroughly first!</p>"},{"location":"how-to/migrate-to-v20/#step-1-backup-production","title":"Step 1: Backup Production","text":"<pre><code>from datajoint.migrate import backup_schema\n\nresult = backup_schema('my_pipeline', 'my_pipeline_backup_20260114')\nprint(f\"Backed up {result['tables_backed_up']} tables, {result['rows_backed_up']} rows\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-2-add-type-markers-to-column-comments","title":"Step 2: Add Type Markers to Column Comments","text":"<p>This is the critical step for blob deserialization. Without <code>:&lt;blob&gt;:</code> markers, blob columns return raw bytes instead of deserialized Python objects.</p> <pre><code>from datajoint.migrate import migrate_columns, check_migration_status\nimport datajoint as dj\n\nschema = dj.Schema('my_pipeline')\n\n# Check current status\nstatus = check_migration_status(schema)\nprint(f\"Blob columns needing migration: {status['pending']}\")\n\n# Preview changes\nresult = migrate_columns(schema, dry_run=True)\nprint(f\"Would update {len(result['sql_statements'])} columns:\")\nfor sql in result['sql_statements'][:5]:  # Show first 5\n    print(f\"  {sql}\")\n\n# Apply changes (updates column comments only, no data changes)\nresult = migrate_columns(schema, dry_run=False)\nprint(f\"Migrated {result['columns_migrated']} columns\")\n</code></pre> <p>What this does: Adds <code>:&lt;type&gt;:</code> prefix to column comments:</p> <ul> <li><code>longblob</code> \u2192 <code>COMMENT ':&lt;blob&gt;:...'</code></li> <li><code>int unsigned</code> \u2192 <code>COMMENT ':uint32:...'</code></li> <li>etc.</li> </ul>"},{"location":"how-to/migrate-to-v20/#step-3-rebuild-lineage-table","title":"Step 3: Rebuild Lineage Table","text":"<pre><code>from datajoint.migrate import rebuild_lineage\n\nresult = rebuild_lineage(schema, dry_run=False)\nprint(f\"Rebuilt lineage: {result['lineage_entries']} entries\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-4-migrate-external-storage-if-applicable","title":"Step 4: Migrate External Storage (if applicable)","text":"<p>If you use <code>blob@store</code>, <code>attach@store</code>, or <code>filepath@store</code>:</p> <pre><code>from datajoint.migrate import migrate_external, migrate_filepath\n\n# Preview external blob/attach migration\nresult = migrate_external(schema, dry_run=True)\nprint(f\"Found {result['columns_found']} external columns\")\n\n# Apply migration (adds _v2 columns with JSON metadata)\nresult = migrate_external(schema, dry_run=False)\nprint(f\"Migrated {result['rows_migrated']} rows\")\n\n# Similarly for filepath columns\nresult = migrate_filepath(schema, dry_run=False)\n\n# After verification, finalize (rename columns)\nresult = migrate_external(schema, finalize=True)\nresult = migrate_filepath(schema, finalize=True)\n</code></pre>"},{"location":"how-to/migrate-to-v20/#step-5-verify-migration","title":"Step 5: Verify Migration","text":"<pre><code>from datajoint.migrate import verify_schema_v20\n\nresult = verify_schema_v20('my_pipeline')\nif result['compatible']:\n    print(\"\u2713 Schema fully migrated to 2.0\")\nelse:\n    print(\"Issues found:\")\n    for issue in result['issues']:\n        print(f\"  - {issue}\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#option-c-gradual-migration-with-legacy-compatibility","title":"Option C: Gradual Migration with Legacy Compatibility","text":"<p>Best for: Pipelines that must support both pre-2.0 and 2.0 clients simultaneously</p> <p>Strategy: Create dual columns for in-store codecs</p>"},{"location":"how-to/migrate-to-v20/#1-add-_v2-columns","title":"1. Add <code>_v2</code> Columns","text":"<p>For each in-store attribute, add a corresponding <code>_v2</code> column:</p> <pre><code>-- Add _v2 column for in-store codec\nALTER TABLE `my_pipeline`.`recording`\n  ADD COLUMN `signal_v2` JSON COMMENT ':&lt;blob@raw&gt;:signal data';\n</code></pre>"},{"location":"how-to/migrate-to-v20/#2-populate-_v2-columns","title":"2. Populate <code>_v2</code> Columns","text":"<pre><code>from datajoint.migrate import populate_v2_columns\n\nresult = populate_v2_columns(\n    schema='my_pipeline',\n    table='recording',\n    attribute='signal',\n    v2_attribute='signal_v2',\n    source_store='raw',\n    dest_store='raw',\n)\n\nprint(f\"Populated {result['rows']} _v2 columns\")\n</code></pre>"},{"location":"how-to/migrate-to-v20/#3-update-code-to-use-_v2-columns","title":"3. Update Code to Use <code>_v2</code> Columns","text":"<pre><code># Update table definition\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int64\n    ---\n    signal : blob@raw  # Legacy (pre-2.0 clients)\n    signal_v2 : &lt;blob@raw&gt;  # 2.0 clients\n    \"\"\"\n</code></pre> <p>Both APIs work:</p> <ul> <li>pre-2.0 clients use <code>signal</code></li> <li>2.0 clients use <code>signal_v2</code></li> </ul>"},{"location":"how-to/migrate-to-v20/#4-final-cutover","title":"4. Final Cutover","text":"<p>Once all clients upgraded to 2.0:</p> <pre><code>-- Drop legacy column\nALTER TABLE `my_pipeline`.`recording`\n  DROP COLUMN `signal`;\n\n-- Rename _v2 to original name\nALTER TABLE `my_pipeline`.`recording`\n  CHANGE COLUMN `signal_v2` `signal` JSON;\n</code></pre>"},{"location":"how-to/migrate-to-v20/#phase-iv-adopt-new-features","title":"Phase IV: Adopt New Features","text":"<p>After successful migration, adopt DataJoint 2.0 features incrementally based on your needs. Migration is complete - these are optional enhancements.</p>"},{"location":"how-to/migrate-to-v20/#new-features-overview","title":"New Features Overview","text":"<p>Schema-addressed storage (<code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>) - Lazy-loading arrays with fsspec integration - Hierarchical organization by primary key - Mutable objects with streaming access - See: Object Storage Tutorial</p> <p>Semantic matching - Lineage-based join validation (enabled by default with <code>*</code> operator) - Catches errors from incompatible data combinations - See: Semantic Matching Spec</p> <p>Jobs 2.0 - Per-table job tracking (<code>~~table_name</code>) - Priority-based populate (with <code>reserve_jobs=True</code>) - Improved distributed computing coordination - See: Distributed Computing Tutorial</p> <p>Custom codecs - Domain-specific data types - Extensible type system - See: Custom Codecs Tutorial</p>"},{"location":"how-to/migrate-to-v20/#learning-path","title":"Learning Path","text":"<p>Start here:</p> <ol> <li>Object Storage Tutorial -    Learn <code>&lt;npy@&gt;</code> and <code>&lt;object@&gt;</code> for large arrays</li> <li>Distributed Computing Tutorial -    Jobs 2.0 with priority-based populate</li> <li>Custom Codecs Tutorial -    Create domain-specific types</li> </ol> <p>Reference documentation:</p> <ul> <li>Object Store Configuration</li> <li>NPY Codec Spec</li> <li>Codec API</li> <li>Semantic Matching Spec</li> <li>AutoPopulate Spec</li> </ul> <p>Adopt features incrementally:</p> <ul> <li>Start with one table using <code>&lt;npy@&gt;</code> for large arrays</li> <li>Test performance and workflow improvements</li> <li>Expand to other tables as needed</li> <li>No need to adopt all features at once</li> </ul>"},{"location":"how-to/migrate-to-v20/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/migrate-to-v20/#import-errors","title":"Import Errors","text":"<p>Issue: Module not found after migration</p> <p>Solution: <pre><code># Ensure all imports use datajoint namespace\nimport datajoint as dj\nfrom datajoint import schema, Manual, Computed\n</code></pre></p>"},{"location":"how-to/migrate-to-v20/#schema-not-found","title":"Schema Not Found","text":"<p>Issue: <code>Database 'schema_v2' doesn't exist</code></p> <p>Solution: <pre><code># Ensure schema declared and created\nschema = dj.schema('schema_v2')\nschema.spawn_missing_classes()\n</code></pre></p>"},{"location":"how-to/migrate-to-v20/#type-syntax-errors","title":"Type Syntax Errors","text":"<p>Issue: <code>Invalid type: 'int unsigned'</code></p> <p>Solution: Update to core types <pre><code># Wrong\ndefinition = \"\"\"\nid : int unsigned\n\"\"\"\n\n# Correct\ndefinition = \"\"\"\nid : int64\n\"\"\"\n</code></pre></p>"},{"location":"how-to/migrate-to-v20/#external-storage-not-found","title":"External Storage Not Found","text":"<p>Issue: Can't access external data after migration</p> <p>Solution: <pre><code># Ensure stores configured\ndj.config['stores.default'] = 'main'\ndj.config['stores.main.location'] = '/data/stores'\n\n# Verify\nfrom datajoint.settings import get_store_spec\nprint(get_store_spec('main'))\n</code></pre></p>"},{"location":"how-to/migrate-to-v20/#summary","title":"Summary","text":"<p>Phase I: Branch and code migration (~1-4 hours with AI) - Create <code>pre/v2.0</code> branch - Update all code to 2.0 API - Create empty <code>_v2</code> schemas</p> <p>Phase II: Test with sample data (~1-2 days) - Insert test data - Validate functionality - Test new features</p> <p>Phase III: Migrate production data (~1-7 days) - Choose migration option - Copy or migrate data - Validate integrity - Execute cutover</p> <p>Phase IV: Adopt new features (ongoing) - Object storage - Semantic matching - Custom codecs - Jobs 2.0</p> <p>Total timeline: ~1-2 weeks for most pipelines</p>"},{"location":"how-to/migrate-to-v20/#see-also","title":"See Also","text":"<p>Core Documentation:</p> <ul> <li>Type System Concept</li> <li>Configuration Reference</li> <li>Definition Syntax</li> <li>Fetch API Reference</li> </ul> <p>Tutorials:</p> <ul> <li>Object Storage</li> <li>Custom Codecs</li> <li>Distributed Computing</li> </ul> <p>Specifications:</p> <ul> <li>Type System Spec</li> <li>Codec API Spec</li> <li>Object Store Configuration</li> <li>Semantic Matching</li> </ul>"},{"location":"how-to/model-relationships/","title":"Model Relationships","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\n\nschema = dj.Schema('howto_relationships')\nschema.drop(prompt=False)\nschema = dj.Schema('howto_relationships')\n</pre> import datajoint as dj  schema = dj.Schema('howto_relationships') schema.drop(prompt=False) schema = dj.Schema('howto_relationships') <pre>[2026-01-27 15:28:11] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species : varchar(32)\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int32\n    ---\n    session_date : date\n    \"\"\"\n\ndj.Diagram(Subject) + dj.Diagram(Session)\n</pre> @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : varchar(16)     ---     species : varchar(32)     \"\"\"  @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_idx : int32     ---     session_date : date     \"\"\"  dj.Diagram(Subject) + dj.Diagram(Session) Out[2]: <p>The <code>-&gt;</code> syntax:</p> <ul> <li>Inherits all primary key attributes from the referenced table</li> <li>Creates a foreign key constraint</li> <li>Establishes dependency for cascading deletes</li> <li>Defines workflow order (parent must exist before child)</li> </ul> In\u00a0[3]: Copied! <pre>@schema\nclass Trial(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session              # Part of primary key\n    trial_idx : int32      # Additional PK attribute\n    ---\n    outcome : varchar(20)\n    \"\"\"\n\ndj.Diagram(Session) + dj.Diagram(Trial)\n</pre> @schema class Trial(dj.Manual):     definition = \"\"\"     -&gt; Session              # Part of primary key     trial_idx : int32      # Additional PK attribute     ---     outcome : varchar(20)     \"\"\"  dj.Diagram(Session) + dj.Diagram(Trial) Out[3]: <p>Thin solid line = containment. Trials are identified within their session. Trial #1 for Session A is different from Trial #1 for Session B.</p> <p>Notice <code>Trial</code> is underlined \u2014 it introduces a new dimension (<code>trial_idx</code>). A dimension is an independent axis of variation in your data, introduced by a table that defines new primary key attributes.</p> In\u00a0[4]: Copied! <pre>@schema\nclass Equipment(dj.Lookup):\n    definition = \"\"\"\n    equipment_id : varchar(16)\n    ---\n    equipment_name : varchar(60)\n    \"\"\"\n    contents = [\n        {'equipment_id': 'rig1', 'equipment_name': 'Main Recording Rig'},\n        {'equipment_id': 'rig2', 'equipment_name': 'Backup Rig'},\n    ]\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : uuid         # Independent identity\n    ---\n    -&gt; Equipment                # Reference, not part of identity\n    duration : float32\n    \"\"\"\n\ndj.Diagram(Equipment) + dj.Diagram(Recording)\n</pre> @schema class Equipment(dj.Lookup):     definition = \"\"\"     equipment_id : varchar(16)     ---     equipment_name : varchar(60)     \"\"\"     contents = [         {'equipment_id': 'rig1', 'equipment_name': 'Main Recording Rig'},         {'equipment_id': 'rig2', 'equipment_name': 'Backup Rig'},     ]  @schema class Recording(dj.Manual):     definition = \"\"\"     recording_id : uuid         # Independent identity     ---     -&gt; Equipment                # Reference, not part of identity     duration : float32     \"\"\"  dj.Diagram(Equipment) + dj.Diagram(Recording) Out[4]: <p>Dashed line = reference. Recordings have their own global identity independent of equipment.</p> <p>Both tables are underlined \u2014 each introduces its own dimension.</p> In\u00a0[5]: Copied! <pre>@schema\nclass SubjectDetails(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject              # Entire primary key\n    ---\n    weight : float32\n    notes : varchar(1000)\n    \"\"\"\n\ndj.Diagram(Subject) + dj.Diagram(SubjectDetails)\n</pre> @schema class SubjectDetails(dj.Manual):     definition = \"\"\"     -&gt; Subject              # Entire primary key     ---     weight : float32     notes : varchar(1000)     \"\"\"  dj.Diagram(Subject) + dj.Diagram(SubjectDetails) Out[5]: <p>Thick solid line = extension. Each subject has at most one details record. The tables share identity.</p> <p>Notice <code>SubjectDetails</code> is not underlined \u2014 it doesn't introduce a new dimension.</p> In\u00a0[6]: Copied! <pre>@schema\nclass Stimulus(dj.Lookup):\n    definition = \"\"\"\n    stimulus_type : varchar(32)\n    \"\"\"\n    contents = [{'stimulus_type': 'visual'}, {'stimulus_type': 'auditory'}]\n\n@schema\nclass TrialStimulus(dj.Manual):\n    definition = \"\"\"\n    -&gt; Trial\n    ---\n    -&gt; [nullable] Stimulus   # Some trials have no stimulus\n    \"\"\"\n\ndj.Diagram(Trial) + dj.Diagram(Stimulus) + dj.Diagram(TrialStimulus)\n</pre> @schema class Stimulus(dj.Lookup):     definition = \"\"\"     stimulus_type : varchar(32)     \"\"\"     contents = [{'stimulus_type': 'visual'}, {'stimulus_type': 'auditory'}]  @schema class TrialStimulus(dj.Manual):     definition = \"\"\"     -&gt; Trial     ---     -&gt; [nullable] Stimulus   # Some trials have no stimulus     \"\"\"  dj.Diagram(Trial) + dj.Diagram(Stimulus) + dj.Diagram(TrialStimulus) Out[6]: <p>Only secondary foreign keys (below <code>---</code>) can be nullable.</p> <p>Note: The <code>[nullable]</code> modifier is NOT visible in diagrams \u2014 check the table definition.</p> In\u00a0[7]: Copied! <pre>@schema\nclass Employee(dj.Manual):\n    definition = \"\"\"\n    employee_id : int64\n    ---\n    name : varchar(60)\n    \"\"\"\n\n@schema\nclass ParkingSpot(dj.Manual):\n    definition = \"\"\"\n    spot_id : int64\n    ---\n    -&gt; [unique] Employee     # Each employee has at most one spot\n    location : varchar(30)\n    \"\"\"\n\ndj.Diagram(Employee) + dj.Diagram(ParkingSpot)\n</pre> @schema class Employee(dj.Manual):     definition = \"\"\"     employee_id : int64     ---     name : varchar(60)     \"\"\"  @schema class ParkingSpot(dj.Manual):     definition = \"\"\"     spot_id : int64     ---     -&gt; [unique] Employee     # Each employee has at most one spot     location : varchar(30)     \"\"\"  dj.Diagram(Employee) + dj.Diagram(ParkingSpot) Out[7]: <p>Note: The <code>[unique]</code> modifier is NOT visible in diagrams \u2014 the line is still dashed. Check the table definition to see the constraint.</p> In\u00a0[8]: Copied! <pre>@schema\nclass Protocol(dj.Lookup):\n    definition = \"\"\"\n    protocol_id : varchar(16)\n    ---\n    protocol_name : varchar(100)\n    \"\"\"\n    contents = [\n        {'protocol_id': 'iacuc_01', 'protocol_name': 'Mouse Protocol'},\n        {'protocol_id': 'iacuc_02', 'protocol_name': 'Rat Protocol'},\n    ]\n\n@schema\nclass Assignment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    -&gt; Protocol\n    ---\n    assigned_date : date\n    \"\"\"\n\ndj.Diagram(Subject) + dj.Diagram(Protocol) + dj.Diagram(Assignment)\n</pre> @schema class Protocol(dj.Lookup):     definition = \"\"\"     protocol_id : varchar(16)     ---     protocol_name : varchar(100)     \"\"\"     contents = [         {'protocol_id': 'iacuc_01', 'protocol_name': 'Mouse Protocol'},         {'protocol_id': 'iacuc_02', 'protocol_name': 'Rat Protocol'},     ]  @schema class Assignment(dj.Manual):     definition = \"\"\"     -&gt; Subject     -&gt; Protocol     ---     assigned_date : date     \"\"\"  dj.Diagram(Subject) + dj.Diagram(Protocol) + dj.Diagram(Assignment) Out[8]: <p>Two thin solid lines converge into <code>Assignment</code>. Each subject-protocol combination appears at most once.</p> <p>Notice <code>Assignment</code> is not underlined \u2014 it doesn't introduce a new dimension, just combines existing ones.</p> In\u00a0[9]: Copied! <pre># Already defined: Subject -&gt; Session -&gt; Trial\n# Show the full hierarchy\ndj.Diagram(Subject) + dj.Diagram(Session) + dj.Diagram(Trial)\n</pre> # Already defined: Subject -&gt; Session -&gt; Trial # Show the full hierarchy dj.Diagram(Subject) + dj.Diagram(Session) + dj.Diagram(Trial) Out[9]: <p>Primary keys cascade: Trial's key is <code>(subject_id, session_idx, trial_idx)</code>.</p> <p>All three tables are underlined \u2014 each introduces a dimension.</p> In\u00a0[10]: Copied! <pre>@schema\nclass Scan(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    scan_idx : int32\n    ---\n    depth : float32\n    \"\"\"\n\n    class ROI(dj.Part):\n        definition = \"\"\"\n        -&gt; master               # References Scan's primary key\n        roi_idx : int32        # Additional dimension\n        ---\n        x : float32\n        y : float32\n        \"\"\"\n\ndj.Diagram(Session) + dj.Diagram(Scan) + dj.Diagram(Scan.ROI)\n</pre> @schema class Scan(dj.Manual):     definition = \"\"\"     -&gt; Session     scan_idx : int32     ---     depth : float32     \"\"\"      class ROI(dj.Part):         definition = \"\"\"         -&gt; master               # References Scan's primary key         roi_idx : int32        # Additional dimension         ---         x : float32         y : float32         \"\"\"  dj.Diagram(Session) + dj.Diagram(Scan) + dj.Diagram(Scan.ROI) Out[10]: <p><code>-&gt; master</code> is the standard way to declare the foreign key to the enclosing table. It references the master's primary key.</p> <p>Notice:</p> <ul> <li><code>Scan</code> is underlined (introduces <code>scan_idx</code>)</li> <li><code>Scan.ROI</code> is underlined (introduces <code>roi_idx</code>) \u2014 Part tables CAN introduce dimensions</li> </ul> In\u00a0[11]: Copied! <pre>@schema\nclass Comparison(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session.proj(session_a='session_idx')\n    -&gt; Session.proj(session_b='session_idx')\n    ---\n    similarity : float32\n    \"\"\"\n\ndj.Diagram(Session) + dj.Diagram(Comparison)\n</pre> @schema class Comparison(dj.Manual):     definition = \"\"\"     -&gt; Session.proj(session_a='session_idx')     -&gt; Session.proj(session_b='session_idx')     ---     similarity : float32     \"\"\"  dj.Diagram(Session) + dj.Diagram(Comparison) Out[11]: <p>Orange dots indicate renamed foreign keys. Hover over them to see the projection expression.</p> <p>This creates attributes <code>session_a</code> and <code>session_b</code>, both referencing <code>Session.session_idx</code>.</p> In\u00a0[12]: Copied! <pre>@schema\nclass TrialAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Trial\n    ---\n    score : float64\n    \"\"\"\n    \n    def make(self, key):\n        self.insert1({**key, 'score': 0.95})\n\ndj.Diagram(Trial) + dj.Diagram(TrialAnalysis)\n</pre> @schema class TrialAnalysis(dj.Computed):     definition = \"\"\"     -&gt; Trial     ---     score : float64     \"\"\"          def make(self, key):         self.insert1({**key, 'score': 0.95})  dj.Diagram(Trial) + dj.Diagram(TrialAnalysis) Out[12]: <p>Thick solid line to a red (Computed) table that is not underlined.</p> <p>Computed tables never introduce dimensions \u2014 their primary key is entirely inherited from dependencies.</p> In\u00a0[13]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[13]: In\u00a0[14]: Copied! <pre>schema.drop(prompt=False)\n</pre> schema.drop(prompt=False)"},{"location":"how-to/model-relationships/#model-relationships","title":"Model Relationships\u00b6","text":"<p>Define foreign key relationships between tables. This guide shows how different foreign key placements create different relationship types, with actual schema diagrams.</p>"},{"location":"how-to/model-relationships/#basic-foreign-key","title":"Basic Foreign Key\u00b6","text":"<p>Reference another table with <code>-&gt;</code>:</p>"},{"location":"how-to/model-relationships/#foreign-key-placement","title":"Foreign Key Placement\u00b6","text":"<p>Where you place a foreign key determines the relationship type:</p> Placement Relationship Diagram Line Entire primary key One-to-one extension Thick solid Part of primary key One-to-many containment Thin solid Secondary attribute One-to-many reference Dashed"},{"location":"how-to/model-relationships/#one-to-many-containment","title":"One-to-Many: Containment\u00b6","text":"<p>Foreign key as part of the primary key (above <code>---</code>):</p>"},{"location":"how-to/model-relationships/#one-to-many-reference","title":"One-to-Many: Reference\u00b6","text":"<p>Foreign key as secondary attribute (below <code>---</code>):</p>"},{"location":"how-to/model-relationships/#one-to-one-extension","title":"One-to-One: Extension\u00b6","text":"<p>Foreign key is the entire primary key:</p>"},{"location":"how-to/model-relationships/#optional-nullable-foreign-keys","title":"Optional (Nullable) Foreign Keys\u00b6","text":"<p>Make a reference optional with <code>[nullable]</code>:</p>"},{"location":"how-to/model-relationships/#unique-foreign-keys","title":"Unique Foreign Keys\u00b6","text":"<p>Enforce one-to-one on a secondary FK with <code>[unique]</code>:</p>"},{"location":"how-to/model-relationships/#nullable-unique-foreign-keys","title":"Nullable Unique Foreign Keys\u00b6","text":"<p>Combine <code>[nullable, unique]</code> for optional one-to-one relationships:</p> <pre>-&gt; [nullable, unique] ParentTable\n</pre> <p>Multiple rows can have NULL values because SQL's UNIQUE constraint does not consider NULLs as equal to each other. This is useful when:</p> <ul> <li>Not every child needs to reference a parent</li> <li>But each parent can be referenced by at most one child</li> </ul>"},{"location":"how-to/model-relationships/#many-to-many","title":"Many-to-Many\u00b6","text":"<p>Use an association table with composite primary key:</p>"},{"location":"how-to/model-relationships/#hierarchies","title":"Hierarchies\u00b6","text":"<p>Cascading one-to-many relationships create tree structures:</p>"},{"location":"how-to/model-relationships/#part-tables","title":"Part Tables\u00b6","text":"<p>Part tables use the <code>-&gt; master</code> alias to reference their enclosing table:</p>"},{"location":"how-to/model-relationships/#renamed-foreign-keys","title":"Renamed Foreign Keys\u00b6","text":"<p>Reference the same table multiple times with <code>.proj()</code> to rename attributes:</p>"},{"location":"how-to/model-relationships/#computed-dependencies","title":"Computed Dependencies\u00b6","text":"<p>Computed tables inherit keys from their dependencies:</p>"},{"location":"how-to/model-relationships/#full-schema-view","title":"Full Schema View\u00b6","text":""},{"location":"how-to/model-relationships/#schema-as-dag","title":"Schema as DAG\u00b6","text":"<p>DataJoint schemas form a directed acyclic graph (DAG). Foreign keys:</p> <ul> <li>Define data relationships</li> <li>Prescribe workflow execution order</li> <li>Enable cascading deletes</li> </ul> <p>There are no cyclic dependencies\u2014parent tables must always be populated before their children.</p>"},{"location":"how-to/model-relationships/#summary","title":"Summary\u00b6","text":"Pattern Declaration Line Style Dimensions One-to-one FK is entire PK Thick solid No new dimension One-to-many (contain) FK + other attrs in PK Thin solid Usually new dimension One-to-many (ref) FK in secondary Dashed Independent dimensions Many-to-many Two FKs in PK Two thin solid No new dimension Part table <code>-&gt; master</code> Thin solid May introduce dimension"},{"location":"how-to/model-relationships/#see-also","title":"See Also\u00b6","text":"<ul> <li>Define Tables \u2014 Table definition syntax</li> <li>Design Primary Keys \u2014 Key selection strategies</li> <li>Read Diagrams \u2014 Diagram notation reference</li> <li>Delete Data \u2014 Cascade behavior</li> <li>Master-Part Tables \u2014 Compositional data patterns</li> </ul>"},{"location":"how-to/monitor-progress/","title":"Monitor Progress","text":"<p>Track computation progress and job status.</p>"},{"location":"how-to/monitor-progress/#progress-display","title":"Progress Display","text":"<p>Show progress bar during populate:</p> <pre><code>ProcessedData.populate(display_progress=True)\n</code></pre>"},{"location":"how-to/monitor-progress/#check-remaining-work","title":"Check Remaining Work","text":"<p>Count entries left to compute:</p> <pre><code># What's left to compute\nremaining = ProcessedData.key_source - ProcessedData\nprint(f\"{len(remaining)} entries remaining\")\n</code></pre>"},{"location":"how-to/monitor-progress/#job-status-summary","title":"Job Status Summary","text":"<p>Get counts by status:</p> <pre><code>progress = ProcessedData.jobs.progress()\n# {'pending': 100, 'reserved': 5, 'error': 3, 'success': 892}\n\nfor status, count in progress.items():\n    print(f\"{status}: {count}\")\n</code></pre>"},{"location":"how-to/monitor-progress/#filter-jobs-by-status","title":"Filter Jobs by Status","text":"<p>Access jobs by their current status:</p> <pre><code># Pending jobs (waiting to run)\nProcessedData.jobs.pending\n\n# Currently running\nProcessedData.jobs.reserved\n\n# Failed jobs\nProcessedData.jobs.errors\n\n# Completed jobs (if keep_completed=True)\nProcessedData.jobs.completed\n\n# Skipped jobs\nProcessedData.jobs.ignored\n</code></pre>"},{"location":"how-to/monitor-progress/#view-job-details","title":"View Job Details","text":"<p>Inspect specific jobs:</p> <pre><code># All jobs for a key\n(ProcessedData.jobs &amp; key).fetch1()\n\n# Recent errors\nProcessedData.jobs.errors.to_dicts(\n    order_by='completed_time DESC',\n    limit=10\n)\n</code></pre>"},{"location":"how-to/monitor-progress/#worker-information","title":"Worker Information","text":"<p>See which workers are processing:</p> <pre><code>for job in ProcessedData.jobs.reserved.to_dicts():\n    print(f\"Key: {job}\")\n    print(f\"Host: {job['host']}\")\n    print(f\"PID: {job['pid']}\")\n    print(f\"Started: {job['reserved_time']}\")\n</code></pre>"},{"location":"how-to/monitor-progress/#computation-timing","title":"Computation Timing","text":"<p>Track how long jobs take:</p> <pre><code># Average duration of completed jobs\ncompleted = ProcessedData.jobs.completed.to_arrays('duration')\nprint(f\"Average: {np.mean(completed):.1f}s\")\nprint(f\"Median: {np.median(completed):.1f}s\")\n</code></pre>"},{"location":"how-to/monitor-progress/#enable-job-metadata","title":"Enable Job Metadata","text":"<p>Store timing info in computed tables:</p> <pre><code>import datajoint as dj\n\ndj.config.jobs.add_job_metadata = True\ndj.config.jobs.keep_completed = True\n</code></pre> <p>This adds hidden attributes to computed tables:</p> <ul> <li><code>_job_start_time</code> \u2014 When computation began</li> <li><code>_job_duration</code> \u2014 How long it took</li> <li><code>_job_version</code> \u2014 Code version (if configured)</li> </ul>"},{"location":"how-to/monitor-progress/#simple-progress-script","title":"Simple Progress Script","text":"<pre><code>import time\nfrom my_pipeline import ProcessedData\n\nwhile True:\n    remaining, total = ProcessedData.progress()\n\n    print(f\"\\rProgress: {total - remaining}/{total} ({(total - remaining) / total:.0%})\", end='')\n\n    if remaining == 0:\n        print(\"\\nDone!\")\n        break\n\n    time.sleep(10)\n</code></pre> <p>For distributed mode with job tracking:</p> <pre><code>import time\nfrom my_pipeline import ProcessedData\n\nwhile True:\n    status = ProcessedData.jobs.progress()\n\n    print(f\"\\rPending: {status.get('pending', 0)} | \"\n          f\"Running: {status.get('reserved', 0)} | \"\n          f\"Done: {status.get('success', 0)} | \"\n          f\"Errors: {status.get('error', 0)}\", end='')\n\n    if status.get('pending', 0) == 0 and status.get('reserved', 0) == 0:\n        print(\"\\nDone!\")\n        break\n\n    time.sleep(10)\n</code></pre>"},{"location":"how-to/monitor-progress/#pipeline-wide-status","title":"Pipeline-Wide Status","text":"<p>Check multiple tables:</p> <pre><code>tables = [RawData, ProcessedData, Analysis]\n\nfor table in tables:\n    total = len(table.key_source)\n    done = len(table())\n    print(f\"{table.__name__}: {done}/{total} ({done/total:.0%})\")\n</code></pre>"},{"location":"how-to/monitor-progress/#see-also","title":"See Also","text":"<ul> <li>Run Computations \u2014 Basic populate usage</li> <li>Distributed Computing \u2014 Multi-worker setup</li> <li>Handle Errors \u2014 Error recovery</li> </ul>"},{"location":"how-to/object-storage-overview/","title":"Object Storage Overview","text":"<p>Navigate DataJoint's object storage documentation to find what you need.</p>"},{"location":"how-to/object-storage-overview/#quick-navigation-by-task","title":"Quick Navigation by Task","text":"<p>I want to...</p> Task Guide Est. Time \u2705 Decide which storage type to use Choose a Storage Type 5-10 min \u2705 Set up S3, MinIO, or file storage Configure Object Storage 10-15 min \u2705 Store and retrieve large data Use Object Storage 15-20 min \u2705 Work with NumPy arrays efficiently Use NPY Codec 10 min \u2705 Create domain-specific types Create Custom Codec 30-45 min \u2705 Optimize storage performance Manage Large Data 20 min \u2705 Clean up unused data Garbage Collection 10 min"},{"location":"how-to/object-storage-overview/#conceptual-understanding","title":"Conceptual Understanding","text":"<p>Why does DataJoint have object storage?</p> <p>Traditional databases excel at structured, relational data but struggle with large arrays, files, and streaming data. DataJoint's Object-Augmented Schema (OAS) unifies relational tables with object storage into a single coherent system:</p> <ul> <li>Relational database: Metadata, keys, relationships (structured data &lt; 1 MB)</li> <li>Object storage: Arrays, files, datasets (large data &gt; 1 MB)</li> <li>Full referential integrity: Maintained across both layers</li> </ul> <p>Read: Object-Augmented Schemas for conceptual overview.</p>"},{"location":"how-to/object-storage-overview/#three-storage-modes","title":"Three Storage Modes","text":""},{"location":"how-to/object-storage-overview/#in-table-storage-blob","title":"In-Table Storage (<code>&lt;blob&gt;</code>)","text":"<p>What: Data stored directly in database column When: Small objects &lt; 1 MB (JSON, thumbnails, small arrays) Why: Fast access, transactional consistency, no store setup</p> <pre><code>metadata : &lt;blob&gt;         # Stored in MySQL\n</code></pre> <p>Guide: Use Object Storage</p>"},{"location":"how-to/object-storage-overview/#object-store-integrated","title":"Object Store (Integrated)","text":"<p>What: DataJoint-managed storage in S3, file systems, or cloud storage When: Large data (arrays, files, datasets) needing lifecycle management Why: Deduplication, garbage collection, transaction safety, referential integrity</p> <p>Two addressing schemes:</p>"},{"location":"how-to/object-storage-overview/#hash-addressed-blob-attach","title":"Hash-Addressed (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>)","text":"<ul> <li>Content-based paths (MD5 hash)</li> <li>Automatic deduplication</li> <li>Best for: Write-once data, attachments</li> </ul> <pre><code>waveform : &lt;blob@&gt;        # Hash: _hash/{schema}/{hash}\ndocument : &lt;attach@&gt;      # Hash: _hash/{schema}/{hash}\n</code></pre>"},{"location":"how-to/object-storage-overview/#schema-addressed-npy-object","title":"Schema-Addressed (<code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>)","text":"<ul> <li>Key-based paths (browsable)</li> <li>Streaming access, partial reads</li> <li>Best for: Zarr, HDF5, large arrays</li> </ul> <pre><code>traces : &lt;npy@&gt;           # Schema: _schema/{schema}/{table}/{key}/\nvolume : &lt;object@&gt;        # Schema: _schema/{schema}/{table}/{key}/\n</code></pre> <p>Guides: - Choose a Storage Type \u2014 Decision criteria - Use Object Storage \u2014 How to use codecs</p>"},{"location":"how-to/object-storage-overview/#filepath-references-filepath","title":"Filepath References (<code>&lt;filepath@&gt;</code>)","text":"<p>What: User-managed file paths (DataJoint stores path string only) When: Existing data archives, externally managed files Why: No file lifecycle management, no deduplication, user controls paths</p> <pre><code>raw_data : &lt;filepath@&gt;    # User-managed path\n</code></pre> <p>Guide: Use Object Storage</p>"},{"location":"how-to/object-storage-overview/#documentation-by-level","title":"Documentation by Level","text":""},{"location":"how-to/object-storage-overview/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Choose a Storage Type \u2014 Start here</p> <ul> <li>Quick decision tree (5 minutes)</li> <li>Size guidelines (&lt; 1 MB, 1-100 MB, &gt; 100 MB)</li> <li>Access pattern considerations</li> <li>Lifecycle management options</li> </ul> </li> <li> <p>Configure Object Storage \u2014 Setup</p> <ul> <li>File system, S3, MinIO configuration</li> <li>Single vs multiple stores</li> <li>Credentials management</li> <li>Store verification</li> </ul> </li> <li> <p>Use Object Storage \u2014 Basic usage</p> <ul> <li>Insert/fetch patterns</li> <li>In-table vs object store</li> <li>Addressing schemes (hash vs schema)</li> <li>ObjectRef for lazy access</li> </ul> </li> </ol>"},{"location":"how-to/object-storage-overview/#intermediate","title":"Intermediate","text":"<ol> <li> <p>Use NPY Codec \u2014 NumPy arrays</p> <ul> <li>Lazy loading (doesn't load until accessed)</li> <li>Efficient slicing (fetch subsets)</li> <li>Shape/dtype metadata</li> <li>When to use <code>&lt;npy@&gt;</code> vs <code>&lt;blob@&gt;</code></li> </ul> </li> <li> <p>Manage Large Data \u2014 Optimization</p> <ul> <li>Storage tiers (hot/warm/cold)</li> <li>Compression strategies</li> <li>Batch operations</li> <li>Performance tuning</li> </ul> </li> <li> <p>Garbage Collection \u2014 Cleanup</p> <ul> <li>Automatic cleanup for integrated storage</li> <li>Manual cleanup for filepath references</li> <li>Orphan detection</li> <li>Recovery procedures</li> </ul> </li> </ol>"},{"location":"how-to/object-storage-overview/#advanced","title":"Advanced","text":"<ol> <li>Create Custom Codec \u2014 Extensibility<ul> <li>Domain-specific types</li> <li>Codec API (encode/decode)</li> <li>HashCodec vs SchemaCodec patterns</li> <li>Integration with existing formats</li> </ul> </li> </ol>"},{"location":"how-to/object-storage-overview/#technical-reference","title":"Technical Reference","text":"<p>For implementation details and specifications:</p>"},{"location":"how-to/object-storage-overview/#specifications","title":"Specifications","text":"<ul> <li>Type System Spec \u2014 Three-layer architecture</li> <li>Codec API Spec \u2014 Custom codec interface</li> <li>NPY Codec Spec \u2014 NumPy array storage</li> <li>Object Store Configuration Spec \u2014 Store config details</li> </ul>"},{"location":"how-to/object-storage-overview/#explanations","title":"Explanations","text":"<ul> <li>Type System \u2014 Conceptual overview</li> <li>Data Pipelines (OAS section) \u2014 Why OAS exists</li> <li>Custom Codecs \u2014 Design patterns</li> </ul>"},{"location":"how-to/object-storage-overview/#common-workflows","title":"Common Workflows","text":""},{"location":"how-to/object-storage-overview/#workflow-1-adding-object-storage-to-existing-pipeline","title":"Workflow 1: Adding Object Storage to Existing Pipeline","text":"<ol> <li>Configure Object Storage \u2014 Set up store</li> <li>Choose a Storage Type \u2014 Select codec</li> <li>Update table definitions with <code>@</code> modifier</li> <li>Use Object Storage \u2014 Insert/fetch patterns</li> </ol> <p>Estimate: 30-60 minutes</p>"},{"location":"how-to/object-storage-overview/#workflow-2-migrating-from-in-table-to-object-store","title":"Workflow 2: Migrating from In-Table to Object Store","text":"<ol> <li>Choose a Storage Type \u2014 Determine new codec</li> <li>Add new column with object storage codec</li> <li>Migrate data (see Use Object Storage)</li> <li>Verify data integrity</li> <li>Drop old column (see Alter Tables)</li> </ol> <p>Estimate: 1-2 hours for small datasets</p>"},{"location":"how-to/object-storage-overview/#workflow-3-working-with-very-large-arrays-1-gb","title":"Workflow 3: Working with Very Large Arrays (&gt; 1 GB)","text":"<ol> <li>Use <code>&lt;object@&gt;</code> or <code>&lt;npy@&gt;</code> (not <code>&lt;blob@&gt;</code>)</li> <li>Configure Object Storage \u2014 Ensure adequate storage</li> <li>For Zarr: Store as <code>&lt;object@&gt;</code> with <code>.zarr</code> extension</li> <li>For streaming: Use <code>ObjectRef.fsmap</code> (see Use Object Storage)</li> </ol> <p>Key advantage: No need to download full dataset into memory</p>"},{"location":"how-to/object-storage-overview/#workflow-4-building-custom-domain-types","title":"Workflow 4: Building Custom Domain Types","text":"<ol> <li>Read Custom Codecs \u2014 Understand patterns</li> <li>Create Custom Codec \u2014 Implementation guide</li> <li>Codec API Spec \u2014 Technical reference</li> <li>Test with small dataset</li> <li>Deploy to production</li> </ol> <p>Estimate: 2-4 hours for simple codecs</p>"},{"location":"how-to/object-storage-overview/#decision-trees","title":"Decision Trees","text":""},{"location":"how-to/object-storage-overview/#which-storage-mode","title":"\"Which storage mode?\"","text":"<pre><code>Is data &lt; 1 MB per row?\n\u251c\u2500 YES \u2192 &lt;blob&gt; (in-table)\n\u2514\u2500 NO  \u2192 Continue...\n\nIs data managed externally?\n\u251c\u2500 YES \u2192 &lt;filepath@&gt; (user-managed reference)\n\u2514\u2500 NO  \u2192 Continue...\n\nNeed streaming or partial reads?\n\u251c\u2500 YES \u2192 &lt;object@&gt; or &lt;npy@&gt; (schema-addressed)\n\u2514\u2500 NO  \u2192 &lt;blob@&gt; (hash-addressed, full download)\n</code></pre> <p>Full guide: Choose a Storage Type</p>"},{"location":"how-to/object-storage-overview/#which-codec-for-object-storage","title":"\"Which codec for object storage?\"","text":"<pre><code>NumPy arrays that benefit from lazy loading?\n\u251c\u2500 YES \u2192 &lt;npy@&gt;\n\u2514\u2500 NO  \u2192 Continue...\n\nLarge files (&gt; 100 MB) needing streaming?\n\u251c\u2500 YES \u2192 &lt;object@&gt;\n\u2514\u2500 NO  \u2192 Continue...\n\nWrite-once data with potential duplicates?\n\u251c\u2500 YES \u2192 &lt;blob@&gt; (deduplication via hashing)\n\u2514\u2500 NO  \u2192 &lt;blob@&gt; or &lt;object@&gt; (choose based on access pattern)\n</code></pre> <p>Full guide: Choose a Storage Type</p>"},{"location":"how-to/object-storage-overview/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/object-storage-overview/#common-issues","title":"Common Issues","text":"Problem Likely Cause Solution Guide \"Store not configured\" Missing stores config Configure Object Storage Out of memory loading array Using <code>&lt;blob@&gt;</code> for huge data Choose a Storage Type \u2192 Use <code>&lt;object@&gt;</code> Slow fetches Wrong codec choice Manage Large Data Data not deduplicated Using wrong codec Choose a Storage Type Path conflicts with reserved <code>&lt;filepath@&gt;</code> using <code>_hash/</code> or <code>_schema/</code> Use Object Storage Missing files after delete Expected behavior for integrated storage Garbage Collection"},{"location":"how-to/object-storage-overview/#getting-help","title":"Getting Help","text":"<ul> <li>Check FAQ for common questions</li> <li>Search GitHub Discussions</li> <li>Review specification for exact behavior</li> </ul>"},{"location":"how-to/object-storage-overview/#see-also","title":"See Also","text":""},{"location":"how-to/object-storage-overview/#related-concepts","title":"Related Concepts","text":"<ul> <li>Type System \u2014 Three-layer type architecture</li> <li>Data Pipelines \u2014 Object-Augmented Schemas</li> </ul>"},{"location":"how-to/object-storage-overview/#related-how-tos","title":"Related How-Tos","text":"<ul> <li>Manage Secrets \u2014 Credentials for S3/cloud storage</li> <li>Define Tables \u2014 Table definition syntax</li> <li>Insert Data \u2014 Data insertion patterns</li> </ul>"},{"location":"how-to/object-storage-overview/#related-tutorials","title":"Related Tutorials","text":"<ul> <li>Object Storage Tutorial \u2014 Hands-on learning</li> <li>Custom Codecs Tutorial \u2014 Build your own codec</li> </ul>"},{"location":"how-to/query-data/","title":"Query Data","text":"<p>Filter, join, and transform data with DataJoint operators.</p>"},{"location":"how-to/query-data/#restriction","title":"Restriction (<code>&amp;</code>)","text":"<p>Filter rows that match a condition:</p> <pre><code># String condition\nSession &amp; \"session_date &gt; '2026-01-01'\"\nSession &amp; \"duration BETWEEN 30 AND 60\"\n\n# Dictionary (exact match)\nSession &amp; {'subject_id': 'M001'}\nSession &amp; {'subject_id': 'M001', 'session_idx': 1}\n\n# Query expression\nSession &amp; Subject                    # Sessions for subjects in Subject\nSession &amp; (Subject &amp; \"sex = 'M'\")    # Sessions for male subjects\n\n# List (OR)\nSession &amp; [{'subject_id': 'M001'}, {'subject_id': 'M002'}]\n</code></pre>"},{"location":"how-to/query-data/#top-n-rows-djtop","title":"Top N Rows (<code>dj.Top</code>)","text":"<p>Limit results with optional ordering:</p> <pre><code># First 10 by primary key\nSession &amp; dj.Top(10)\n\n# Top 10 by date (descending)\nSession &amp; dj.Top(10, 'session_date DESC')\n\n# Pagination: skip 20, take 10\nSession &amp; dj.Top(10, 'session_date DESC', offset=20)\n\n# All rows ordered\nSession &amp; dj.Top(None, 'session_date DESC')\n</code></pre> <p>Use <code>\"KEY\"</code> for primary key ordering, <code>\"KEY DESC\"</code> for reverse:</p> <pre><code>Session &amp; dj.Top(10, 'KEY DESC')  # Last 10 by primary key\n</code></pre>"},{"location":"how-to/query-data/#anti-restriction-","title":"Anti-Restriction (<code>-</code>)","text":"<p>Filter rows that do NOT match:</p> <pre><code>Subject - Session          # Subjects without sessions\nSession - {'subject_id': 'M001'}\n</code></pre>"},{"location":"how-to/query-data/#projection-proj","title":"Projection (<code>.proj()</code>)","text":"<p>Select, rename, or compute attributes:</p> <pre><code># Primary key only\nSubject.proj()\n\n# Specific attributes\nSubject.proj('species', 'sex')\n\n# All attributes\nSubject.proj(...)\n\n# All except some\nSubject.proj(..., '-notes')\n\n# Rename\nSubject.proj(animal_species='species')\n\n# Computed\nSubject.proj(weight_kg='weight / 1000')\n</code></pre>"},{"location":"how-to/query-data/#join","title":"Join (<code>*</code>)","text":"<p>Combine tables on matching attributes:</p> <pre><code>Subject * Session\nSubject * Session * Experimenter\n\n# Restrict then join\n(Subject &amp; \"sex = 'M'\") * Session\n</code></pre>"},{"location":"how-to/query-data/#aggregation-aggr","title":"Aggregation (<code>.aggr()</code>)","text":"<p>Group and summarize:</p> <pre><code># Count trials per session\nSession.aggr(Session.Trial, n_trials='count(trial_idx)')\n\n# Multiple aggregates\nSession.aggr(\n    Session.Trial,\n    n_trials='count(trial_idx)',\n    avg_rt='avg(reaction_time)',\n    min_rt='min(reaction_time)'\n)\n\n# Exclude sessions without trials\nSession.aggr(Session.Trial, n='count(trial_idx)', exclude_nonmatching=True)\n</code></pre>"},{"location":"how-to/query-data/#universal-set-dju","title":"Universal Set (<code>dj.U()</code>)","text":"<p>Group by arbitrary attributes:</p> <pre><code># Unique values\ndj.U('species') &amp; Subject\n\n# Group by non-primary-key attribute\ndj.U('session_date').aggr(Session, n='count(session_idx)')\n\n# Global aggregation (one row)\ndj.U().aggr(Session, total='count(*)')\n</code></pre>"},{"location":"how-to/query-data/#extension-extend","title":"Extension (<code>.extend()</code>)","text":"<p>Add attributes without losing rows:</p> <pre><code># Add experimenter info, keep all sessions\nSession.extend(Experimenter)\n</code></pre>"},{"location":"how-to/query-data/#chain-operations","title":"Chain Operations","text":"<pre><code>result = (\n    Subject\n    &amp; \"sex = 'M'\"\n    * Session\n    &amp; \"duration &gt; 30\"\n).proj('species', 'session_date', 'duration')\n</code></pre>"},{"location":"how-to/query-data/#operator-precedence","title":"Operator Precedence","text":"Priority Operator Operation Highest <code>*</code> Join <code>+</code>, <code>-</code> Union, Anti-restriction Lowest <code>&amp;</code> Restriction <p>Use parentheses for clarity:</p> <pre><code>(Subject &amp; condition) * Session    # Restrict then join\nSubject * (Session &amp; condition)    # Join then restrict\n</code></pre>"},{"location":"how-to/query-data/#view-query","title":"View Query","text":"<pre><code># See generated SQL\nprint((Subject &amp; condition).make_sql())\n\n# Count rows without fetching\nlen(Subject &amp; condition)\n</code></pre>"},{"location":"how-to/query-data/#see-also","title":"See Also","text":"<ul> <li>Operators Reference \u2014 Complete operator documentation</li> <li>Fetch Results \u2014 Retrieving query results</li> </ul>"},{"location":"how-to/read-diagrams/","title":"Read Schema Diagrams","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\n\nschema = dj.Schema('howto_diagrams')\nschema.drop(prompt=False)\nschema = dj.Schema('howto_diagrams')\n</pre> import datajoint as dj  schema = dj.Schema('howto_diagrams') schema.drop(prompt=False) schema = dj.Schema('howto_diagrams') <pre>[2026-01-27 15:28:16] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Customer(dj.Manual):\n    definition = \"\"\"\n    customer_id : int64\n    ---\n    name : varchar(60)\n    \"\"\"\n\n@schema\nclass CustomerPreferences(dj.Manual):\n    definition = \"\"\"\n    -&gt; Customer               # FK is entire PK\n    ---\n    theme : varchar(20)\n    notifications : bool\n    \"\"\"\n\ndj.Diagram(Customer) + dj.Diagram(CustomerPreferences)\n</pre> @schema class Customer(dj.Manual):     definition = \"\"\"     customer_id : int64     ---     name : varchar(60)     \"\"\"  @schema class CustomerPreferences(dj.Manual):     definition = \"\"\"     -&gt; Customer               # FK is entire PK     ---     theme : varchar(20)     notifications : bool     \"\"\"  dj.Diagram(Customer) + dj.Diagram(CustomerPreferences) Out[2]: <p>Equivalent ER Diagram:</p> <p>DataJoint vs ER: The thick solid line immediately shows this is one-to-one. In ER notation, you must read the crow's foot symbols (<code>||--o|</code>).</p> <p>Note: <code>CustomerPreferences</code> is not underlined \u2014 it exists in the Customer dimension space.</p> In\u00a0[3]: Copied! <pre>@schema\nclass Account(dj.Manual):\n    definition = \"\"\"\n    -&gt; Customer               # Part of PK\n    account_num : int32      # Additional PK field\n    ---\n    balance : decimal(10,2)\n    \"\"\"\n\ndj.Diagram(Customer) + dj.Diagram(Account)\n</pre> @schema class Account(dj.Manual):     definition = \"\"\"     -&gt; Customer               # Part of PK     account_num : int32      # Additional PK field     ---     balance : decimal(10,2)     \"\"\"  dj.Diagram(Customer) + dj.Diagram(Account) Out[3]: <p>Equivalent ER Diagram:</p> <p>DataJoint vs ER: The thin solid line shows containment \u2014 accounts belong to customers. In ER, you see <code>||--o{</code> (one-to-many).</p> <p>Note: <code>Account</code> is underlined \u2014 it introduces the Account dimension.</p> In\u00a0[4]: Copied! <pre>@schema\nclass Department(dj.Manual):\n    definition = \"\"\"\n    dept_id : int32\n    ---\n    dept_name : varchar(60)\n    \"\"\"\n\n@schema\nclass Employee(dj.Manual):\n    definition = \"\"\"\n    employee_id : int64      # Own independent PK\n    ---\n    -&gt; Department             # Secondary attribute\n    employee_name : varchar(60)\n    \"\"\"\n\ndj.Diagram(Department) + dj.Diagram(Employee)\n</pre> @schema class Department(dj.Manual):     definition = \"\"\"     dept_id : int32     ---     dept_name : varchar(60)     \"\"\"  @schema class Employee(dj.Manual):     definition = \"\"\"     employee_id : int64      # Own independent PK     ---     -&gt; Department             # Secondary attribute     employee_name : varchar(60)     \"\"\"  dj.Diagram(Department) + dj.Diagram(Employee) Out[4]: <p>Equivalent ER Diagram:</p> <p>DataJoint vs ER: Both show one-to-many, but DataJoint's dashed line tells you immediately that Employee has independent identity. In ER, you must examine whether the FK is part of the PK.</p> <p>Note: Both tables are underlined \u2014 each introduces its own dimension.</p> In\u00a0[5]: Copied! <pre>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)  # NEW dimension\n    ---\n    species : varchar(50)\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject                # Inherits subject_id\n    session_idx : int32      # NEW dimension\n    ---\n    session_date : date\n    \"\"\"\n\n@schema \nclass SessionQC(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session                # Inherits both, adds nothing\n    ---\n    passed : bool\n    \"\"\"\n    def make(self, key):\n        self.insert1({**key, 'passed': True})\n\ndj.Diagram(schema)\n</pre> @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : varchar(16)  # NEW dimension     ---     species : varchar(50)     \"\"\"  @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject                # Inherits subject_id     session_idx : int32      # NEW dimension     ---     session_date : date     \"\"\"  @schema  class SessionQC(dj.Computed):     definition = \"\"\"     -&gt; Session                # Inherits both, adds nothing     ---     passed : bool     \"\"\"     def make(self, key):         self.insert1({**key, 'passed': True})  dj.Diagram(schema) Out[5]: <p>In this diagram:</p> <ul> <li><code>Subject</code> is underlined \u2014 introduces the Subject dimension</li> <li><code>Session</code> is underlined \u2014 introduces the Session dimension (within each Subject)</li> <li><code>SessionQC</code> is not underlined \u2014 exists in the Session dimension space, adds no new dimension</li> </ul> <p>Why this matters: Dimensions determine attribute lineage. Primary key attributes trace back to the dimension where they originated, enabling semantic matching for safe joins.</p> In\u00a0[6]: Copied! <pre>@schema\nclass Student(dj.Manual):\n    definition = \"\"\"\n    student_id : int64\n    ---\n    name : varchar(60)\n    \"\"\"\n\n@schema\nclass Course(dj.Manual):\n    definition = \"\"\"\n    course_code : char(8)\n    ---\n    title : varchar(100)\n    \"\"\"\n\n@schema\nclass Enrollment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Student\n    -&gt; Course\n    ---\n    grade : enum('A','B','C','D','F')\n    \"\"\"\n\ndj.Diagram(Student) + dj.Diagram(Course) + dj.Diagram(Enrollment)\n</pre> @schema class Student(dj.Manual):     definition = \"\"\"     student_id : int64     ---     name : varchar(60)     \"\"\"  @schema class Course(dj.Manual):     definition = \"\"\"     course_code : char(8)     ---     title : varchar(100)     \"\"\"  @schema class Enrollment(dj.Manual):     definition = \"\"\"     -&gt; Student     -&gt; Course     ---     grade : enum('A','B','C','D','F')     \"\"\"  dj.Diagram(Student) + dj.Diagram(Course) + dj.Diagram(Enrollment) Out[6]: <p>Equivalent ER Diagram:</p> <p>DataJoint vs ER: Both show the association table pattern. DataJoint's converging solid lines immediately indicate the composite primary key.</p> <p>Note: <code>Enrollment</code> is not underlined \u2014 it exists in the space defined by Student \u00d7 Course dimensions.</p> In\u00a0[7]: Copied! <pre>@schema\nclass Person(dj.Manual):\n    definition = \"\"\"\n    person_id : int64\n    ---\n    name : varchar(60)\n    \"\"\"\n\n@schema\nclass Marriage(dj.Manual):\n    definition = \"\"\"\n    marriage_id : int64\n    ---\n    -&gt; Person.proj(spouse1='person_id')\n    -&gt; Person.proj(spouse2='person_id')\n    marriage_date : date\n    \"\"\"\n\ndj.Diagram(Person) + dj.Diagram(Marriage)\n</pre> @schema class Person(dj.Manual):     definition = \"\"\"     person_id : int64     ---     name : varchar(60)     \"\"\"  @schema class Marriage(dj.Manual):     definition = \"\"\"     marriage_id : int64     ---     -&gt; Person.proj(spouse1='person_id')     -&gt; Person.proj(spouse2='person_id')     marriage_date : date     \"\"\"  dj.Diagram(Person) + dj.Diagram(Marriage) Out[7]: <p>The orange dots between <code>Person</code> and <code>Marriage</code> indicate that projections renamed the foreign key attributes (<code>spouse1</code> and <code>spouse2</code> both reference <code>person_id</code>).</p> <p>Tip: In Jupyter, hover over orange dots to see the projection expression.</p> In\u00a0[8]: Copied! <pre># Entire schema\ndj.Diagram(schema)\n</pre> # Entire schema dj.Diagram(schema) Out[8]: In\u00a0[9]: Copied! <pre># Session and 1 level upstream (dependencies)\ndj.Diagram(Session) - 1\n</pre> # Session and 1 level upstream (dependencies) dj.Diagram(Session) - 1 Out[9]: In\u00a0[10]: Copied! <pre># Subject and 2 levels downstream (dependents)\ndj.Diagram(Subject) + 2\n</pre> # Subject and 2 levels downstream (dependents) dj.Diagram(Subject) + 2 Out[10]: <p>Operation Reference:</p> Operation Meaning <code>dj.Diagram(schema)</code> Entire schema <code>dj.Diagram(Table) - N</code> Table + N levels upstream <code>dj.Diagram(Table) + N</code> Table + N levels downstream <code>D1 + D2</code> Union of two diagrams <code>D1 * D2</code> Intersection (common nodes) <p>Finding paths: Use intersection to find connection paths:</p> <pre>(dj.Diagram(upstream) + 100) * (dj.Diagram(downstream) - 100)\n</pre> In\u00a0[11]: Copied! <pre># Horizontal layout using config override\nwith dj.config.override(display__diagram_direction=\"LR\"):\n    display(dj.Diagram(schema))\n</pre> # Horizontal layout using config override with dj.config.override(display__diagram_direction=\"LR\"):     display(dj.Diagram(schema)) In\u00a0[12]: Copied! <pre># Generate Mermaid syntax\nprint((dj.Diagram(Subject) + 2).make_mermaid())\n</pre> # Generate Mermaid syntax print((dj.Diagram(Subject) + 2).make_mermaid()) <pre>flowchart LR\n    classDef manual fill:#90EE90,stroke:#006400\n    classDef lookup fill:#D3D3D3,stroke:#696969\n    classDef computed fill:#FFB6C1,stroke:#8B0000\n    classDef imported fill:#ADD8E6,stroke:#00008B\n    classDef part fill:#FFFFFF,stroke:#000000\n    classDef collapsed fill:#808080,stroke:#404040\n\n    subgraph __main__\n        SessionQC([SessionQC]):::computed\n        Session[Session]:::manual\n        Subject[Subject]:::manual\n    end\n\n    Session --&gt; SessionQC\n    Subject --&gt; Session\n</pre> <p>Copy this output into any Mermaid-compatible viewer (GitHub Markdown, MkDocs with mermaid plugin, https://mermaid.live) to render the diagram.</p> <p>Saving to file:</p> <pre>dj.Diagram(schema).save(\"pipeline.mmd\")  # .mmd or .mermaid extension\n</pre> In\u00a0[13]: Copied! <pre># Create a second schema for analysis\nhowto_analysis = dj.Schema('howto_analysis')\nhowto_analysis.drop(prompt=False)\nhowto_analysis = dj.Schema('howto_analysis')\n\n# Reference tables from the first schema\n@howto_analysis\nclass Experimenter(dj.Manual):\n    definition = \"\"\"\n    experimenter : varchar(32)\n    ---\n    email : varchar(100)\n    \"\"\"\n\n@howto_analysis\nclass Experiment(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject                    # Cross-schema reference\n    -&gt; Experimenter\n    experiment_date : date\n    ---\n    notes : varchar(1000)\n    \"\"\"\n\n@howto_analysis\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Experiment\n    ---\n    result : float64\n    \"\"\"\n    def make(self, key):\n        self.insert1({**key, 'result': 0.0})\n</pre> # Create a second schema for analysis howto_analysis = dj.Schema('howto_analysis') howto_analysis.drop(prompt=False) howto_analysis = dj.Schema('howto_analysis')  # Reference tables from the first schema @howto_analysis class Experimenter(dj.Manual):     definition = \"\"\"     experimenter : varchar(32)     ---     email : varchar(100)     \"\"\"  @howto_analysis class Experiment(dj.Manual):     definition = \"\"\"     -&gt; Subject                    # Cross-schema reference     -&gt; Experimenter     experiment_date : date     ---     notes : varchar(1000)     \"\"\"  @howto_analysis class Analysis(dj.Computed):     definition = \"\"\"     -&gt; Experiment     ---     result : float64     \"\"\"     def make(self, key):         self.insert1({**key, 'result': 0.0}) In\u00a0[14]: Copied! <pre># Combine both schemas - tables are automatically grouped\nmulti_schema_diagram = dj.Diagram(schema) + dj.Diagram(howto_analysis)\nmulti_schema_diagram\n</pre> # Combine both schemas - tables are automatically grouped multi_schema_diagram = dj.Diagram(schema) + dj.Diagram(howto_analysis) multi_schema_diagram Out[14]: <p>Tables are grouped by their database schema automatically. The group label shows the Python module name when available (following the DataJoint convention of one module per schema).</p> <p>Multi-schema diagrams are useful for:</p> <ul> <li>Visualizing pipelines spanning multiple schemas</li> <li>Understanding which tables belong to which module</li> <li>Documentation for multi-module architectures</li> </ul> In\u00a0[15]: Copied! <pre># Show schema1 expanded, schema2 collapsed into a single node\ndj.Diagram(schema) + dj.Diagram(howto_analysis).collapse()\n</pre> # Show schema1 expanded, schema2 collapsed into a single node dj.Diagram(schema) + dj.Diagram(howto_analysis).collapse() Out[15]: <p>The collapsed node shows the module name and table count. Edges from the expanded schema connect to the collapsed node.</p> <p>\"Expanded wins\" rule: If a table appears in both a collapsed and non-collapsed diagram, it stays expanded. This applies even when expanding a single table from a collapsed schema:</p> In\u00a0[16]: Copied! <pre># \"Expanded wins\": Experimenter is expanded even though howto_analysis is collapsed\ndj.Diagram(Subject) + dj.Diagram(Experimenter) + dj.Diagram(howto_analysis).collapse()\n</pre> # \"Expanded wins\": Experimenter is expanded even though howto_analysis is collapsed dj.Diagram(Subject) + dj.Diagram(Experimenter) + dj.Diagram(howto_analysis).collapse() Out[16]: <p>Preserving directionality: Collapsing middle layers of a pipeline preserves the DAG structure. The collapsed node sits between expanded tables, maintaining edge directions:</p> In\u00a0[17]: Copied! <pre># Create a separate schema for this example\nsandwich = dj.Schema('howto_sandwich')\nsandwich.drop(prompt=False)\nsandwich = dj.Schema('howto_sandwich')\n\n# Linear pipeline: RawData -&gt; Filtered -&gt; Normalized -&gt; FinalResult\n@sandwich\nclass RawData(dj.Manual):\n    definition = \"\"\"\n    data_id : int32\n    \"\"\"\n\n@sandwich\nclass Filtered(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    filtered_value : float32\n    \"\"\"\n    def make(self, key): pass\n\n@sandwich\nclass Normalized(dj.Computed):\n    definition = \"\"\"\n    -&gt; Filtered\n    ---\n    normalized_value : float32\n    \"\"\"\n    def make(self, key): pass\n\n@sandwich\nclass FinalResult(dj.Computed):\n    definition = \"\"\"\n    -&gt; Normalized\n    ---\n    result : float32\n    \"\"\"\n    def make(self, key): pass\n\n# Sandwich collapse: expand top and bottom, collapse middle processing steps\ndj.Diagram(RawData) + dj.Diagram(FinalResult) + dj.Diagram(sandwich).collapse()\n</pre> # Create a separate schema for this example sandwich = dj.Schema('howto_sandwich') sandwich.drop(prompt=False) sandwich = dj.Schema('howto_sandwich')  # Linear pipeline: RawData -&gt; Filtered -&gt; Normalized -&gt; FinalResult @sandwich class RawData(dj.Manual):     definition = \"\"\"     data_id : int32     \"\"\"  @sandwich class Filtered(dj.Computed):     definition = \"\"\"     -&gt; RawData     ---     filtered_value : float32     \"\"\"     def make(self, key): pass  @sandwich class Normalized(dj.Computed):     definition = \"\"\"     -&gt; Filtered     ---     normalized_value : float32     \"\"\"     def make(self, key): pass  @sandwich class FinalResult(dj.Computed):     definition = \"\"\"     -&gt; Normalized     ---     result : float32     \"\"\"     def make(self, key): pass  # Sandwich collapse: expand top and bottom, collapse middle processing steps dj.Diagram(RawData) + dj.Diagram(FinalResult) + dj.Diagram(sandwich).collapse() Out[17]: In\u00a0[18]: Copied! <pre># Import the demo modules\nfrom demo_modules import acquisition, processing, analysis\n\n# Activate schemas (creates tables on first run)\nacquisition.schema.activate('demo_acquisition')\nprocessing.schema.activate('demo_processing')\nanalysis.schema.activate('demo_analysis')\n\n# Drop and recreate for clean state\nfor s in [analysis.schema, processing.schema, acquisition.schema]:\n    s.drop(prompt=False)\nacquisition.schema.activate('demo_acquisition')\nprocessing.schema.activate('demo_processing')\nanalysis.schema.activate('demo_analysis')\n\n# Full pipeline diagram - all modules expanded\n# Note: dj.Diagram(module) works when the module has a `schema` attribute\nfull_pipeline = dj.Diagram(acquisition) + dj.Diagram(processing) + dj.Diagram(analysis)\nfull_pipeline\n</pre> # Import the demo modules from demo_modules import acquisition, processing, analysis  # Activate schemas (creates tables on first run) acquisition.schema.activate('demo_acquisition') processing.schema.activate('demo_processing') analysis.schema.activate('demo_analysis')  # Drop and recreate for clean state for s in [analysis.schema, processing.schema, acquisition.schema]:     s.drop(prompt=False) acquisition.schema.activate('demo_acquisition') processing.schema.activate('demo_processing') analysis.schema.activate('demo_analysis')  # Full pipeline diagram - all modules expanded # Note: dj.Diagram(module) works when the module has a `schema` attribute full_pipeline = dj.Diagram(acquisition) + dj.Diagram(processing) + dj.Diagram(analysis) full_pipeline Out[18]: <p>The full diagram shows all three modules with cross-schema references:</p> <ul> <li><code>acquisition</code> provides core tables (<code>Lab</code>, <code>Subject</code>, <code>Session</code>)</li> <li><code>processing</code> references <code>Session</code> from acquisition</li> <li><code>analysis</code> references <code>Subject</code> from acquisition AND <code>ProcessingParams</code> from processing</li> </ul> <p>Now let's see collapse in action:</p> In\u00a0[19]: Copied! <pre># Two schemas collapsed: acquisition expanded, downstream modules collapsed\n# This shows acquisition's internal structure while abstracting processing &amp; analysis\ndj.Diagram(acquisition) + dj.Diagram(processing).collapse() + dj.Diagram(analysis).collapse()\n</pre> # Two schemas collapsed: acquisition expanded, downstream modules collapsed # This shows acquisition's internal structure while abstracting processing &amp; analysis dj.Diagram(acquisition) + dj.Diagram(processing).collapse() + dj.Diagram(analysis).collapse() Out[19]: In\u00a0[20]: Copied! <pre># \"Expanded wins\": Subject stays expanded even though analysis references it\ndj.Diagram(acquisition.Subject) + dj.Diagram(analysis).collapse()\n</pre> # \"Expanded wins\": Subject stays expanded even though analysis references it dj.Diagram(acquisition.Subject) + dj.Diagram(analysis).collapse() Out[20]: In\u00a0[21]: Copied! <pre># Schema-level DAG: all modules collapsed\n# Shows the dependency structure between modules at a glance\ndj.Diagram(acquisition).collapse() + dj.Diagram(processing).collapse() + dj.Diagram(analysis).collapse()\n</pre> # Schema-level DAG: all modules collapsed # Shows the dependency structure between modules at a glance dj.Diagram(acquisition).collapse() + dj.Diagram(processing).collapse() + dj.Diagram(analysis).collapse() Out[21]: <p>Key observations:</p> <ol> <li>Collapsed nodes show table count \u2014 e.g., \"processing (3 tables)\"</li> <li>Cross-schema edges preserved \u2014 Dependencies between modules are shown as edges between collapsed nodes</li> <li>\"Expanded wins\" \u2014 If you explicitly include a table (like <code>Subject</code>), it stays expanded even if a collapsed schema references it</li> <li>Schema-level DAG \u2014 Collapsing all schemas reveals the high-level module dependency graph, useful for understanding pipeline architecture</li> </ol> In\u00a0[22]: Copied! <pre># Cleanup all schemas\n# Demo modules\nfor s in [analysis.schema, processing.schema, acquisition.schema]:\n    if s.is_activated():\n        s.drop(prompt=False)\n# Earlier examples\nsandwich.drop(prompt=False)\nhowto_analysis.drop(prompt=False)\nschema.drop(prompt=False)\n</pre> # Cleanup all schemas # Demo modules for s in [analysis.schema, processing.schema, acquisition.schema]:     if s.is_activated():         s.drop(prompt=False) # Earlier examples sandwich.drop(prompt=False) howto_analysis.drop(prompt=False) schema.drop(prompt=False)"},{"location":"how-to/read-diagrams/#read-schema-diagrams","title":"Read Schema Diagrams\u00b6","text":"<p>DataJoint diagrams visualize schema structure as directed acyclic graphs (DAGs). This guide teaches you to:</p> <ul> <li>Interpret line styles and their semantic meaning</li> <li>Recognize dimensions (underlined vs non-underlined tables)</li> <li>Use diagram operations to explore large schemas</li> <li>Compare DataJoint notation to traditional ER diagrams</li> </ul>"},{"location":"how-to/read-diagrams/#quick-reference","title":"Quick Reference\u00b6","text":"Line Style Relationship Child's Primary Key Thick Solid \u2501\u2501\u2501 Extension Parent PK only (one-to-one) Thin Solid \u2500\u2500\u2500 Containment Parent PK + own fields (one-to-many) Dashed \u2504\u2504\u2504 Reference Own independent PK (one-to-many) <p>Key principle: Solid lines mean the parent's identity becomes part of the child's identity. Dashed lines mean the child maintains independent identity.</p>"},{"location":"how-to/read-diagrams/#thick-solid-line-extension-one-to-one","title":"Thick Solid Line: Extension (One-to-One)\u00b6","text":"<p>The foreign key is the entire primary key. The child extends the parent.</p>"},{"location":"how-to/read-diagrams/#thin-solid-line-containment-one-to-many","title":"Thin Solid Line: Containment (One-to-Many)\u00b6","text":"<p>The foreign key is part of the primary key, with additional fields.</p>"},{"location":"how-to/read-diagrams/#dashed-line-reference-one-to-many","title":"Dashed Line: Reference (One-to-Many)\u00b6","text":"<p>The foreign key is a secondary attribute (below the <code>---</code> line).</p>"},{"location":"how-to/read-diagrams/#dimensions-and-underlined-names","title":"Dimensions and Underlined Names\u00b6","text":"<p>A dimension is a new entity type introduced by a table that defines new primary key attributes. Each underlined table introduces exactly one dimension\u2014even if it has multiple new PK attributes, together they identify one new entity type.</p> Visual Meaning Underlined Introduces a new dimension (new entity type) Not underlined Exists in the space defined by dimensions from referenced tables <p>Key rules:</p> <ul> <li>Computed tables never introduce dimensions (always non-underlined)</li> <li>Part tables can introduce dimensions (may be underlined)</li> </ul>"},{"location":"how-to/read-diagrams/#many-to-many-converging-lines","title":"Many-to-Many: Converging Lines\u00b6","text":"<p>Many-to-many relationships appear as tables with multiple solid lines converging.</p>"},{"location":"how-to/read-diagrams/#orange-dots-renamed-foreign-keys","title":"Orange Dots: Renamed Foreign Keys\u00b6","text":"<p>When referencing the same table multiple times, use <code>.proj()</code> to rename. Orange dots indicate renamed FKs.</p>"},{"location":"how-to/read-diagrams/#diagram-operations","title":"Diagram Operations\u00b6","text":"<p>Filter and combine diagrams to explore large schemas:</p>"},{"location":"how-to/read-diagrams/#layout-direction","title":"Layout Direction\u00b6","text":"<p>New in DataJoint 2.1</p> <p>Control the flow direction of diagrams via configuration:</p> Direction Description <code>\"TB\"</code> Top to bottom (default) <code>\"LR\"</code> Left to right"},{"location":"how-to/read-diagrams/#mermaid-output","title":"Mermaid Output\u00b6","text":"<p>New in DataJoint 2.1</p> <p>Generate Mermaid syntax for embedding diagrams in Markdown documentation, GitHub, or web pages:</p>"},{"location":"how-to/read-diagrams/#multi-schema-pipelines","title":"Multi-Schema Pipelines\u00b6","text":"<p>Real-world pipelines often span multiple schemas (modules).</p> <p>New in DataJoint 2.1: Tables are automatically grouped into visual clusters by schema, with the Python module name shown as the group label.</p>"},{"location":"how-to/read-diagrams/#collapsing-schemas","title":"Collapsing Schemas\u00b6","text":"<p>New in DataJoint 2.1</p> <p>For high-level pipeline views, collapse entire schemas into single nodes using <code>.collapse()</code>. This is useful for showing relationships between modules without the detail of individual tables.</p>"},{"location":"how-to/read-diagrams/#extended-example-multi-module-pipeline","title":"Extended Example: Multi-Module Pipeline\u00b6","text":"<p>Here's a realistic example with three modules that have cross-schema dependencies:</p> <p><code>demo_modules/acquisition.py</code> - Core data acquisition:</p> <pre>@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"lab : varchar(32) ...\"\"\"\n\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"subject_id : varchar(16) --- -&gt; Lab ...\"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"-&gt; Subject  session_date : date ...\"\"\"\n</pre> <p><code>demo_modules/processing.py</code> - Data processing (references acquisition):</p> <pre>@schema\nclass ProcessingParams(dj.Lookup):\n    definition = \"\"\"params_id : int16 ...\"\"\"\n\n@schema\nclass ProcessedSession(dj.Computed):\n    definition = \"\"\"-&gt; acquisition.Session  -&gt; ProcessingParams ...\"\"\"\n\n@schema\nclass EventDetection(dj.Computed):\n    definition = \"\"\"-&gt; ProcessedSession  event_id : int32 ...\"\"\"\n</pre> <p><code>demo_modules/analysis.py</code> - Analysis (references both modules):</p> <pre>@schema\nclass AnalysisParams(dj.Lookup):\n    definition = \"\"\"analysis_id : int16 ...\"\"\"\n\n@schema\nclass SubjectAnalysis(dj.Computed):\n    definition = \"\"\"-&gt; acquisition.Subject  -&gt; AnalysisParams ...\"\"\"\n\n@schema\nclass CrossSessionAnalysis(dj.Computed):\n    definition = \"\"\"-&gt; acquisition.Subject  -&gt; processing.ProcessingParams ...\"\"\"\n</pre>"},{"location":"how-to/read-diagrams/#what-diagrams-dont-show","title":"What Diagrams Don't Show\u00b6","text":"<p>Diagrams do NOT show these FK modifiers:</p> Modifier Effect Must Check Definition <code>[nullable]</code> Optional reference <code>-&gt; [nullable] Parent</code> <code>[unique]</code> One-to-one on secondary FK <code>-&gt; [unique] Parent</code> <p>A dashed line could be any of:</p> <ul> <li>Required one-to-many (default)</li> <li>Optional one-to-many (<code>[nullable]</code>)</li> <li>Required one-to-one (<code>[unique]</code>)</li> <li>Optional one-to-one (<code>[nullable, unique]</code>)</li> </ul> <p>Always check the table definition to see modifiers.</p>"},{"location":"how-to/read-diagrams/#datajoint-vs-traditional-er-notation","title":"DataJoint vs Traditional ER Notation\u00b6","text":"Feature Chen's ER Crow's Foot DataJoint Cardinality Numbers Line symbols Line style Direction None None Top-to-bottom Cycles Allowed Allowed Not allowed PK cascade Not shown Not shown Solid lines Identity sharing Not indicated Not indicated Thick solid New dimensions Not indicated Not indicated Underlined <p>Why DataJoint differs:</p> <ol> <li>DAG structure \u2014 No cycles means schemas read as workflows (top-to-bottom)</li> <li>Line semantics \u2014 Immediately reveals relationship type</li> <li>Executable \u2014 Diagram is generated from schema, cannot drift out of sync</li> </ol>"},{"location":"how-to/read-diagrams/#summary","title":"Summary\u00b6","text":"Visual Meaning Thick solid One-to-one extension Thin solid One-to-many containment Dashed Reference (independent identity) Underlined Introduces new dimension Orange dots Renamed FK via <code>.proj()</code> Colors Green=Manual, Gray=Lookup, Red=Computed, Blue=Imported Grouped boxes Tables grouped by schema/module 3D box (gray) Collapsed schema (2.1+) Feature Method Layout direction <code>dj.config.display.diagram_direction</code> Mermaid output <code>.make_mermaid()</code> Collapse schema <code>.collapse()</code> (2.1+)"},{"location":"how-to/read-diagrams/#related","title":"Related\u00b6","text":"<ul> <li>Diagram Specification</li> <li>Entity Integrity: Dimensions</li> <li>Semantic Matching</li> <li>Schema Design Tutorial</li> </ul>"},{"location":"how-to/run-computations/","title":"Run Computations","text":"<p>Execute automated computations with <code>populate()</code>.</p>"},{"location":"how-to/run-computations/#basic-usage","title":"Basic Usage","text":"<pre><code># Populate all missing entries\nProcessedData.populate()\n\n# With progress display\nProcessedData.populate(display_progress=True)\n</code></pre>"},{"location":"how-to/run-computations/#restrict-what-to-compute","title":"Restrict What to Compute","text":"<pre><code># Only specific subjects\nProcessedData.populate(Subject &amp; \"sex = 'M'\")\n\n# Only recent sessions\nProcessedData.populate(Session &amp; \"session_date &gt; '2026-01-01'\")\n\n# Specific key\nProcessedData.populate({'subject_id': 'M001', 'session_idx': 1})\n</code></pre>"},{"location":"how-to/run-computations/#limit-number-of-jobs","title":"Limit Number of Jobs","text":"<pre><code># Process at most 100 entries\nProcessedData.populate(limit=100)\n</code></pre>"},{"location":"how-to/run-computations/#error-handling","title":"Error Handling","text":"<pre><code># Continue on errors (log but don't stop)\nProcessedData.populate(suppress_errors=True)\n\n# Check what failed\nfailed = ProcessedData.jobs &amp; \"status = 'error'\"\nprint(failed)\n\n# Clear errors to retry\nfailed.delete()\nProcessedData.populate()\n</code></pre>"},{"location":"how-to/run-computations/#when-to-use-distributed-mode","title":"When to Use Distributed Mode","text":"<p>Choose your populate strategy based on your workload and infrastructure:</p>"},{"location":"how-to/run-computations/#use-populate-default-when","title":"Use <code>populate()</code> (Default) When:","text":"<ul> <li>\u2705 Single worker - Only one process computing at a time</li> <li>\u2705 Very fast computations - Each make() completes in &lt; 1 second</li> <li>\u2705 Small job count - Processing &lt; 100 entries</li> <li>\u2705 Development/testing - Iterating on make() logic</li> </ul> <p>Advantages:</p> <ul> <li>Simplest approach (no job management overhead)</li> <li>Immediate execution (no reservation delay)</li> <li>Easy debugging (errors stop execution)</li> </ul> <p>Example: <pre><code># Simple, direct execution\nProcessedData.populate()\n</code></pre></p>"},{"location":"how-to/run-computations/#use-populatereserve_jobstrue-when","title":"Use <code>populate(reserve_jobs=True)</code> When:","text":"<ul> <li>\u2705 Multiple workers - Running on multiple machines or processes</li> <li>\u2705 Computations &gt; 1 second - Job reservation overhead (~100ms) becomes negligible</li> <li>\u2705 Production pipelines - Need fault tolerance and monitoring</li> <li>\u2705 Worker crashes expected - Jobs can be resumed</li> </ul> <p>Advantages:</p> <ul> <li>Prevents duplicate work between workers</li> <li>Fault tolerance (crashed jobs can be retried)</li> <li>Job status tracking (<code>ProcessedData.jobs</code>)</li> <li>Error isolation (one failure doesn't stop others)</li> </ul> <p>Example: <pre><code># Distributed mode with job coordination\nProcessedData.populate(reserve_jobs=True)\n</code></pre></p> <p>Job reservation overhead: ~100ms per job Worth it when: Computations take &gt; 1 second (overhead becomes &lt; 10%)</p>"},{"location":"how-to/run-computations/#use-populatereserve_jobstrue-processesn-when","title":"Use <code>populate(reserve_jobs=True, processes=N)</code> When:","text":"<ul> <li>\u2705 Multi-core machine - Want to use all CPU cores</li> <li>\u2705 CPU-bound tasks - Computations are CPU-intensive, not I/O</li> <li>\u2705 Independent computations - No shared state between jobs</li> </ul> <p>Advantages:</p> <ul> <li>Parallel execution on single machine</li> <li>No network coordination needed</li> <li>Combines job safety with parallelism</li> </ul> <p>Example: <pre><code># Use 4 CPU cores\nProcessedData.populate(reserve_jobs=True, processes=4)\n</code></pre></p> <p>Caution: Don't use more processes than CPU cores (causes context switching overhead)</p>"},{"location":"how-to/run-computations/#decision-tree","title":"Decision Tree","text":"<pre><code>How many workers?\n\u251c\u2500 One \u2192 populate()\n\u2514\u2500 Multiple \u2192 Continue...\n\nHow long per computation?\n\u251c\u2500 &lt; 1 second \u2192 populate() (overhead not worth it)\n\u2514\u2500 &gt; 1 second \u2192 Continue...\n\nNeed fault tolerance?\n\u251c\u2500 Yes \u2192 populate(reserve_jobs=True)\n\u2514\u2500 No \u2192 populate() (simpler)\n\nMultiple cores on one machine?\n\u2514\u2500 Yes \u2192 populate(reserve_jobs=True, processes=N)\n</code></pre>"},{"location":"how-to/run-computations/#distributed-computing","title":"Distributed Computing","text":"<p>For multi-worker coordination:</p> <pre><code># Worker 1 (on machine A)\nProcessedData.populate(reserve_jobs=True)\n\n# Worker 2 (on machine B)\nProcessedData.populate(reserve_jobs=True)\n\n# Workers coordinate automatically via database\n# Each reserves different keys, no duplicates\n</code></pre>"},{"location":"how-to/run-computations/#check-progress","title":"Check Progress","text":"<pre><code># What's left to compute\nremaining = ProcessedData.key_source - ProcessedData\nprint(f\"{len(remaining)} entries remaining\")\n\n# View job status\nProcessedData.jobs\n</code></pre>"},{"location":"how-to/run-computations/#the-make-method","title":"The <code>make()</code> Method","text":"<pre><code>@schema\nclass ProcessedData(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    result : float64\n    \"\"\"\n\n    def make(self, key):\n        # 1. Fetch input data\n        raw = (RawData &amp; key).fetch1('data')\n\n        # 2. Compute\n        result = process(raw)\n\n        # 3. Insert\n        self.insert1({**key, 'result': result})\n</code></pre>"},{"location":"how-to/run-computations/#three-part-make-for-long-computations","title":"Three-Part Make for Long Computations","text":"<p>For computations taking hours or days:</p> <pre><code>@schema\nclass LongComputation(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    result : float64\n    \"\"\"\n\n    def make_fetch(self, key, **kwargs):\n        \"\"\"Fetch input data (outside transaction).\n\n        kwargs are passed from populate(make_kwargs={...}).\n        \"\"\"\n        data = (RawData &amp; key).fetch1('data')\n        return (data,)\n\n    def make_compute(self, key, fetched):\n        \"\"\"Perform computation (outside transaction)\"\"\"\n        (data,) = fetched\n        result = expensive_computation(data)\n        return (result,)\n\n    def make_insert(self, key, fetched, computed):\n        \"\"\"Insert results (inside brief transaction)\"\"\"\n        (result,) = computed\n        self.insert1({**key, 'result': result})\n</code></pre>"},{"location":"how-to/run-computations/#custom-key-source","title":"Custom Key Source","text":"<pre><code>@schema\nclass FilteredComputation(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    result : float64\n    \"\"\"\n\n    @property\n    def key_source(self):\n        # Only compute for high-quality data\n        return (RawData &amp; 'quality &gt; 0.8') - self\n</code></pre>"},{"location":"how-to/run-computations/#populate-options","title":"Populate Options","text":"Option Default Description <code>restriction</code> <code>None</code> Filter what to compute <code>limit</code> <code>None</code> Max entries to process <code>display_progress</code> <code>False</code> Show progress bar <code>reserve_jobs</code> <code>False</code> Reserve jobs for distributed computing <code>suppress_errors</code> <code>False</code> Continue on errors"},{"location":"how-to/run-computations/#see-also","title":"See Also","text":"<ul> <li>Computation Model \u2014 How computation works</li> <li>Distributed Computing \u2014 Multi-worker setup</li> <li>Handle Errors \u2014 Error recovery</li> </ul>"},{"location":"how-to/testing/","title":"Testing DataJoint Packages","text":"<p>Best practices for integration and unit testing in packages that depend on DataJoint.</p>"},{"location":"how-to/testing/#principles","title":"Principles","text":"<ol> <li> <p>Use real databases \u2014 Test against MySQL or PostgreSQL, not mocks. DataJoint's behavior depends on database semantics (transactions, foreign keys, type coercion).</p> </li> <li> <p>Isolate tests \u2014 Each test should use its own schema to avoid interference. Clean up on both success and failure.</p> </li> <li> <p>Reuse connections \u2014 Create connections once per session, not per test. Database connections are expensive.</p> </li> <li> <p>Test logic, not volume \u2014 Use minimal data that exercises your code paths. Integration tests should be fast.</p> </li> </ol>"},{"location":"how-to/testing/#database-setup","title":"Database Setup","text":""},{"location":"how-to/testing/#local-development","title":"Local Development","text":"<p>Use Docker Compose to run test databases:</p> <pre><code># docker-compose.yaml\nservices:\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_USER: datajoint\n      POSTGRES_PASSWORD: datajoint\n    ports:\n      - \"5432:5432\"\n</code></pre> <pre><code>docker compose up -d\n</code></pre>"},{"location":"how-to/testing/#environment-variables","title":"Environment Variables","text":"<p>Configure DataJoint via environment variables for flexibility across environments:</p> <pre><code>export DJ_HOST=localhost\nexport DJ_PORT=5432\nexport DJ_USER=datajoint\nexport DJ_PASS=datajoint\nexport DJ_BACKEND=postgresql\n</code></pre>"},{"location":"how-to/testing/#pytest-fixtures","title":"Pytest Fixtures","text":""},{"location":"how-to/testing/#connection-fixture-session-scoped","title":"Connection Fixture (Session-Scoped)","text":"<p>Create the connection once per test session:</p> <pre><code># conftest.py\nimport os\nimport pytest\nimport datajoint as dj\n\n\n@pytest.fixture(scope=\"session\")\ndef dj_connection():\n    \"\"\"Configure and return DataJoint connection for the test session.\"\"\"\n    dj.config[\"database.host\"] = os.getenv(\"DJ_HOST\", \"localhost\")\n    dj.config[\"database.port\"] = int(os.getenv(\"DJ_PORT\", 5432))\n    dj.config[\"database.user\"] = os.getenv(\"DJ_USER\", \"datajoint\")\n    dj.config[\"database.password\"] = os.getenv(\"DJ_PASS\", \"datajoint\")\n    dj.config[\"database.use_tls\"] = os.getenv(\"DJ_USE_TLS\", \"false\").lower() == \"true\"\n\n    backend = os.getenv(\"DJ_BACKEND\", \"postgresql\")\n    if backend == \"postgresql\":\n        dj.config[\"database.backend\"] = \"postgresql\"\n\n    # Verify connection works\n    dj.conn()\n\n    yield dj.conn()\n</code></pre>"},{"location":"how-to/testing/#schema-fixture-function-scoped","title":"Schema Fixture (Function-Scoped)","text":"<p>Create a fresh schema for each test, with automatic cleanup:</p> <pre><code>@pytest.fixture\ndef schema(dj_connection):\n    \"\"\"Create a temporary schema for a single test.\"\"\"\n    import uuid\n\n    # Unique schema name to avoid collisions in parallel runs\n    schema_name = f\"test_{uuid.uuid4().hex[:8]}\"\n    schema = dj.Schema(schema_name)\n\n    yield schema\n\n    # Cleanup: drop schema even if test fails\n    schema.drop(prompt=False)\n</code></pre>"},{"location":"how-to/testing/#schema-fixture-module-scoped","title":"Schema Fixture (Module-Scoped)","text":"<p>For tests that share table definitions, use module scope:</p> <pre><code>@pytest.fixture(scope=\"module\")\ndef shared_schema(dj_connection):\n    \"\"\"Schema shared across tests in a module.\"\"\"\n    schema_name = f\"test_{uuid.uuid4().hex[:8]}\"\n    schema = dj.Schema(schema_name)\n\n    yield schema\n\n    schema.drop(prompt=False)\n</code></pre>"},{"location":"how-to/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"how-to/testing/#basic-table-test","title":"Basic Table Test","text":"<pre><code>def test_insert_and_fetch(schema):\n    @schema\n    class Subject(dj.Manual):\n        definition = \"\"\"\n        subject_id : int32\n        ---\n        name : varchar(100)\n        \"\"\"\n\n    # Insert\n    Subject.insert1({\"subject_id\": 1, \"name\": \"Alice\"})\n\n    # Verify\n    result = Subject.fetch1()\n    assert result[\"name\"] == \"Alice\"\n</code></pre>"},{"location":"how-to/testing/#testing-computed-tables","title":"Testing Computed Tables","text":"<pre><code>def test_computed_table(schema):\n    @schema\n    class Source(dj.Manual):\n        definition = \"\"\"\n        id : int32\n        ---\n        value : float64\n        \"\"\"\n\n    @schema\n    class Computed(dj.Computed):\n        definition = \"\"\"\n        -&gt; Source\n        ---\n        doubled : float64\n        \"\"\"\n\n        def make(self, key):\n            value = (Source &amp; key).fetch1(\"value\")\n            self.insert1({**key, \"doubled\": value * 2})\n\n    # Setup\n    Source.insert1({\"id\": 1, \"value\": 3.5})\n\n    # Run computation\n    Computed.populate()\n\n    # Verify\n    assert Computed.fetch1(\"doubled\") == 7.0\n</code></pre>"},{"location":"how-to/testing/#testing-foreign-key-constraints","title":"Testing Foreign Key Constraints","text":"<pre><code>def test_referential_integrity(schema):\n    @schema\n    class Parent(dj.Manual):\n        definition = \"\"\"\n        parent_id : int32\n        \"\"\"\n\n    @schema\n    class Child(dj.Manual):\n        definition = \"\"\"\n        -&gt; Parent\n        child_id : int32\n        \"\"\"\n\n    # Insert parent first\n    Parent.insert1({\"parent_id\": 1})\n    Child.insert1({\"parent_id\": 1, \"child_id\": 1})\n\n    # Verify FK violation raises error\n    with pytest.raises(dj.errors.IntegrityError):\n        Child.insert1({\"parent_id\": 999, \"child_id\": 2})\n</code></pre>"},{"location":"how-to/testing/#testing-cascading-deletes","title":"Testing Cascading Deletes","text":"<pre><code>def test_cascade_delete(schema):\n    @schema\n    class Parent(dj.Manual):\n        definition = \"\"\"\n        parent_id : int32\n        \"\"\"\n\n    @schema\n    class Child(dj.Manual):\n        definition = \"\"\"\n        -&gt; Parent\n        child_id : int32\n        \"\"\"\n\n    Parent.insert1({\"parent_id\": 1})\n    Child.insert([\n        {\"parent_id\": 1, \"child_id\": 1},\n        {\"parent_id\": 1, \"child_id\": 2},\n    ])\n\n    assert len(Child()) == 2\n\n    # Delete parent cascades to children\n    (Parent &amp; {\"parent_id\": 1}).delete(prompt=False)\n\n    assert len(Child()) == 0\n</code></pre>"},{"location":"how-to/testing/#testing-with-object-storage","title":"Testing with Object Storage","text":"<p>For tables using blob storage, configure a test store:</p> <pre><code>import tempfile\n\n@pytest.fixture(scope=\"session\")\ndef blob_store(dj_connection):\n    \"\"\"Configure a temporary blob store for tests.\"\"\"\n    store_path = tempfile.mkdtemp(prefix=\"dj_test_store_\")\n\n    dj.config[\"stores\"] = {\n        \"test\": {\n            \"protocol\": \"file\",\n            \"location\": store_path,\n        }\n    }\n\n    yield store_path\n\n    # Cleanup\n    import shutil\n    shutil.rmtree(store_path, ignore_errors=True)\n\n\ndef test_blob_storage(schema, blob_store):\n    import numpy as np\n\n    @schema\n    class ArrayData(dj.Manual):\n        definition = \"\"\"\n        id : int32\n        ---\n        data : &lt;blob@test&gt;\n        \"\"\"\n\n    arr = np.array([1, 2, 3])\n    ArrayData.insert1({\"id\": 1, \"data\": arr})\n\n    fetched = ArrayData.fetch1(\"data\")\n    assert np.array_equal(fetched, arr)\n</code></pre>"},{"location":"how-to/testing/#cicd-configuration","title":"CI/CD Configuration","text":""},{"location":"how-to/testing/#github-actions","title":"GitHub Actions","text":"<pre><code># .github/workflows/test.yaml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:15\n        env:\n          POSTGRES_USER: datajoint\n          POSTGRES_PASSWORD: datajoint\n        ports:\n          - 5432:5432\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Install dependencies\n        run: pip install -e \".[test]\"\n\n      - name: Run tests\n        env:\n          DJ_HOST: localhost\n          DJ_PORT: 5432\n          DJ_USER: datajoint\n          DJ_PASS: datajoint\n          DJ_BACKEND: postgresql\n        run: pytest tests/ -v\n</code></pre>"},{"location":"how-to/testing/#testing-against-multiple-backends","title":"Testing Against Multiple Backends","text":"<pre><code>jobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        include:\n          - backend: postgresql\n            image: postgres:15\n            port: 5432\n          - backend: mysql\n            image: mysql:8.0\n            port: 3306\n\n    services:\n      db:\n        image: ${{ matrix.image }}\n        # ... service config\n\n    steps:\n      - name: Run tests\n        env:\n          DJ_BACKEND: ${{ matrix.backend }}\n          DJ_PORT: ${{ matrix.port }}\n        run: pytest tests/ -v\n</code></pre>"},{"location":"how-to/testing/#common-patterns","title":"Common Patterns","text":""},{"location":"how-to/testing/#parameterized-tests","title":"Parameterized Tests","text":"<p>Test the same logic with different inputs:</p> <pre><code>@pytest.mark.parametrize(\"value,expected\", [\n    (1, 2),\n    (0, 0),\n    (-5, -10),\n])\ndef test_doubling(schema, value, expected):\n    @schema\n    class Numbers(dj.Manual):\n        definition = \"\"\"\n        id : int32\n        ---\n        value : int32\n        \"\"\"\n\n    @schema\n    class Doubled(dj.Computed):\n        definition = \"\"\"\n        -&gt; Numbers\n        ---\n        result : int32\n        \"\"\"\n        def make(self, key):\n            v = (Numbers &amp; key).fetch1(\"value\")\n            self.insert1({**key, \"result\": v * 2})\n\n    Numbers.insert1({\"id\": 1, \"value\": value})\n    Doubled.populate()\n\n    assert Doubled.fetch1(\"result\") == expected\n</code></pre>"},{"location":"how-to/testing/#testing-error-conditions","title":"Testing Error Conditions","text":"<pre><code>def test_duplicate_insert_raises(schema):\n    @schema\n    class Data(dj.Manual):\n        definition = \"\"\"\n        id : int32\n        \"\"\"\n\n    Data.insert1({\"id\": 1})\n\n    with pytest.raises(dj.errors.DuplicateError):\n        Data.insert1({\"id\": 1})\n</code></pre>"},{"location":"how-to/testing/#fixtures-for-sample-data","title":"Fixtures for Sample Data","text":"<pre><code>@pytest.fixture\ndef sample_subjects(schema):\n    \"\"\"Create Subject table with sample data.\"\"\"\n    @schema\n    class Subject(dj.Manual):\n        definition = \"\"\"\n        subject_id : varchar(16)\n        ---\n        species : varchar(50)\n        \"\"\"\n\n    Subject.insert([\n        {\"subject_id\": \"M001\", \"species\": \"mouse\"},\n        {\"subject_id\": \"M002\", \"species\": \"mouse\"},\n        {\"subject_id\": \"R001\", \"species\": \"rat\"},\n    ])\n\n    return Subject\n</code></pre>"},{"location":"how-to/testing/#tips","title":"Tips","text":"<ol> <li> <p>Use <code>prompt=False</code> \u2014 Always pass <code>prompt=False</code> to <code>delete()</code> and <code>drop()</code> in tests to avoid interactive prompts.</p> </li> <li> <p>Unique schema names \u2014 Use UUIDs or timestamps in schema names to allow parallel test execution.</p> </li> <li> <p>Don't test DataJoint itself \u2014 Trust that DataJoint works. Test your application logic, not basic insert/fetch operations.</p> </li> <li> <p>Seed random data \u2014 If using random test data, set a fixed seed for reproducibility:    <pre><code>import random\nrandom.seed(42)\n</code></pre></p> </li> <li> <p>Skip slow tests \u2014 Mark slow integration tests for optional execution:    <pre><code>@pytest.mark.slow\ndef test_large_computation(schema):\n    ...\n</code></pre> <pre><code>pytest -m \"not slow\"  # Skip slow tests\npytest -m slow        # Run only slow tests\n</code></pre></p> </li> </ol>"},{"location":"how-to/use-cli/","title":"Use the Command-Line Interface","text":"<p>Start an interactive Python REPL with DataJoint pre-loaded.</p> <p>The <code>dj</code> command provides quick access to DataJoint for exploring schemas, running queries, and testing connections without writing scripts.</p>"},{"location":"how-to/use-cli/#start-the-repl","title":"Start the REPL","text":"<pre><code>dj\n</code></pre> <p>This opens a Python REPL with <code>dj</code> (DataJoint) already imported:</p> <pre><code>DataJoint 2.0.0 REPL\nType 'dj.' and press Tab for available functions.\n\n&gt;&gt;&gt; dj.conn()  # Connect to database\n&gt;&gt;&gt; dj.list_schemas()  # List available schemas\n</code></pre>"},{"location":"how-to/use-cli/#specify-database-credentials","title":"Specify Database Credentials","text":"<p>Override config file settings from the command line:</p> <pre><code>dj --host localhost:3306 --user root --password secret\n</code></pre> Option Description <code>--host HOST</code> Database host as <code>host:port</code> <code>-u</code>, <code>--user USER</code> Database username <code>-p</code>, <code>--password PASS</code> Database password <p>Credentials from command-line arguments override values in config files.</p>"},{"location":"how-to/use-cli/#load-schemas-as-virtual-modules","title":"Load Schemas as Virtual Modules","text":"<p>Load database schemas directly into the REPL namespace:</p> <pre><code>dj -s my_lab:lab -s my_analysis:analysis\n</code></pre> <p>The format is <code>schema_name:alias</code> where:</p> <ul> <li><code>schema_name</code> is the database schema name</li> <li><code>alias</code> is the variable name in the REPL</li> </ul> <p>This outputs:</p> <pre><code>DataJoint 2.0.0 REPL\nType 'dj.' and press Tab for available functions.\n\nLoaded schemas:\n  lab -&gt; my_lab\n  analysis -&gt; my_analysis\n\n&gt;&gt;&gt; lab.Subject.to_dicts()  # Query Subject table\n&gt;&gt;&gt; dj.Diagram(lab.schema)  # View schema diagram\n</code></pre>"},{"location":"how-to/use-cli/#common-workflows","title":"Common Workflows","text":""},{"location":"how-to/use-cli/#explore-an-existing-schema","title":"Explore an Existing Schema","text":"<pre><code>dj -s production_db:db\n</code></pre> <pre><code>&gt;&gt;&gt; list(db.schema)  # List all tables\n&gt;&gt;&gt; db.Experiment().to_dicts()[:5]  # Preview data\n&gt;&gt;&gt; dj.Diagram(db.schema)  # Visualize structure\n</code></pre>"},{"location":"how-to/use-cli/#quick-data-check","title":"Quick Data Check","text":"<pre><code>dj --host db.example.com -s my_lab:lab\n</code></pre> <pre><code>&gt;&gt;&gt; len(lab.Session())  # Count sessions\n&gt;&gt;&gt; lab.Session.describe()  # Show table definition\n</code></pre>"},{"location":"how-to/use-cli/#test-connection","title":"Test Connection","text":"<pre><code>dj --host localhost:3306 --user testuser --password testpass\n</code></pre> <pre><code>&gt;&gt;&gt; dj.conn()  # Verify connection works\n&gt;&gt;&gt; dj.list_schemas()  # Check accessible schemas\n</code></pre>"},{"location":"how-to/use-cli/#version-information","title":"Version Information","text":"<p>Display DataJoint version:</p> <pre><code>dj --version\n</code></pre>"},{"location":"how-to/use-cli/#help","title":"Help","text":"<p>Display all options:</p> <pre><code>dj --help\n</code></pre>"},{"location":"how-to/use-cli/#entry-points","title":"Entry Points","text":"<p>The CLI is available as both <code>dj</code> and <code>datajoint</code>:</p> <pre><code>dj --version\ndatajoint --version  # Same command\n</code></pre>"},{"location":"how-to/use-cli/#programmatic-usage","title":"Programmatic Usage","text":"<p>The CLI function can also be called from Python:</p> <pre><code>from datajoint.cli import cli\n\n# Show version and exit\ncli([\"--version\"])\n\n# Start REPL with schemas\ncli([\"-s\", \"my_lab:lab\"])\n</code></pre>"},{"location":"how-to/use-npy-codec/","title":"Use the <code>&lt;npy&gt;</code> Codec","text":"<p>Store NumPy arrays with lazy loading and metadata access.</p>"},{"location":"how-to/use-npy-codec/#overview","title":"Overview","text":"<p>The <code>&lt;npy@&gt;</code> codec stores NumPy arrays as portable <code>.npy</code> files in object storage. On fetch, you get an <code>NpyRef</code> that provides metadata without downloading.</p> <p>Key benefits: - Access shape, dtype, size without I/O - Lazy loading - download only when needed - Memory mapping - random access to large arrays - Safe bulk fetch - inspect before downloading - Portable <code>.npy</code> format</p>"},{"location":"how-to/use-npy-codec/#quick-start","title":"Quick Start","text":""},{"location":"how-to/use-npy-codec/#1-configure-a-store","title":"1. Configure a Store","text":"<pre><code>import datajoint as dj\n\n# Add store configuration\ndj.config.object_storage.stores['mystore'] = {\n    'protocol': 's3',\n    'endpoint': 'localhost:9000',\n    'bucket': 'my-bucket',\n    'access_key': 'access_key',\n    'secret_key': 'secret_key',\n    'location': 'data',\n}\n</code></pre> <p>Or in <code>datajoint.json</code>: <pre><code>{\n  \"object_storage\": {\n    \"stores\": {\n      \"mystore\": {\n        \"protocol\": \"s3\",\n        \"endpoint\": \"s3.amazonaws.com\",\n        \"bucket\": \"my-bucket\",\n        \"location\": \"data\"\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"how-to/use-npy-codec/#2-define-table-with-npy","title":"2. Define Table with <code>&lt;npy@&gt;</code>","text":"<pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int32\n    ---\n    waveform : &lt;npy@mystore&gt;\n    \"\"\"\n</code></pre>"},{"location":"how-to/use-npy-codec/#3-insert-arrays","title":"3. Insert Arrays","text":"<pre><code>import numpy as np\n\nRecording.insert1({\n    'recording_id': 1,\n    'waveform': np.random.randn(1000, 32),\n})\n</code></pre>"},{"location":"how-to/use-npy-codec/#4-fetch-with-lazy-loading","title":"4. Fetch with Lazy Loading","text":"<pre><code># Returns NpyRef, not array\nref = (Recording &amp; 'recording_id=1').fetch1('waveform')\n\n# Metadata without download\nprint(ref.shape)   # (1000, 32)\nprint(ref.dtype)   # float64\n\n# Load when ready\narr = ref.load()\n</code></pre>"},{"location":"how-to/use-npy-codec/#npyref-reference","title":"NpyRef Reference","text":""},{"location":"how-to/use-npy-codec/#metadata-properties-no-io","title":"Metadata Properties (No I/O)","text":"<pre><code>ref.shape      # Tuple of dimensions\nref.dtype      # NumPy dtype\nref.ndim       # Number of dimensions\nref.size       # Total elements\nref.nbytes     # Total bytes\nref.path       # Storage path\nref.store      # Store name\nref.is_loaded  # Whether data is cached\n</code></pre>"},{"location":"how-to/use-npy-codec/#loading-methods","title":"Loading Methods","text":"<pre><code># Explicit load (recommended)\narr = ref.load()\n\n# Via NumPy functions (auto-loads)\nmean = np.mean(ref)\nstd = np.std(ref, axis=0)\n\n# Via conversion (auto-loads)\narr = np.asarray(ref)\n\n# Indexing (loads then indexes)\nfirst_row = ref[0]\nsnippet = ref[100:200, :]\n</code></pre>"},{"location":"how-to/use-npy-codec/#memory-mapping","title":"Memory Mapping","text":"<p>For large arrays, use <code>mmap_mode</code> to access data without loading it all into memory:</p> <pre><code># Memory-mapped loading (random access)\narr = ref.load(mmap_mode='r')\n\n# Only reads the portion you access\nslice = arr[1000:2000, :]  # Efficient for large arrays\n</code></pre> <p>Modes: - <code>'r'</code> - Read-only (recommended) - <code>'r+'</code> - Read-write - <code>'c'</code> - Copy-on-write (changes not saved)</p> <p>Performance: - Local filesystem stores: mmaps directly (no copy) - Remote stores (S3): downloads to cache first, then mmaps</p>"},{"location":"how-to/use-npy-codec/#common-patterns","title":"Common Patterns","text":""},{"location":"how-to/use-npy-codec/#bulk-fetch-with-filtering","title":"Bulk Fetch with Filtering","text":"<pre><code># Fetch all - returns NpyRefs, not arrays\nresults = MyTable.to_dicts()\n\n# Filter by metadata (no downloads)\nlarge = [r for r in results if r['data'].shape[0] &gt; 1000]\n\n# Load only what you need\nfor rec in large:\n    arr = rec['data'].load()\n    process(arr)\n</code></pre>"},{"location":"how-to/use-npy-codec/#computed-tables","title":"Computed Tables","text":"<pre><code>@schema\nclass ProcessedData(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    result : &lt;npy@mystore&gt;\n    \"\"\"\n\n    def make(self, key):\n        # Fetch lazy reference\n        ref = (RawData &amp; key).fetch1('raw')\n\n        # NumPy functions auto-load\n        result = np.fft.fft(ref, axis=1)\n\n        self.insert1({**key, 'result': result})\n</code></pre>"},{"location":"how-to/use-npy-codec/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code># Process recordings one at a time\nfor key in Recording.keys():\n    ref = (Recording &amp; key).fetch1('data')\n\n    # Check size before loading\n    if ref.nbytes &gt; 1e9:  # &gt; 1 GB\n        print(f\"Skipping large recording: {ref.nbytes/1e9:.1f} GB\")\n        continue\n\n    process(ref.load())\n</code></pre>"},{"location":"how-to/use-npy-codec/#comparison-with-blob","title":"Comparison with <code>&lt;blob@&gt;</code>","text":"Aspect <code>&lt;npy@&gt;</code> <code>&lt;blob@&gt;</code> On fetch NpyRef (lazy) Array (eager) Metadata access Without download Must download Memory mapping Yes, via <code>mmap_mode</code> No Addressing Schema-addressed Hash-addressed Deduplication No Yes Format <code>.npy</code> (portable) DJ blob (Python) Best for Large arrays, lazy loading Small arrays, dedup"},{"location":"how-to/use-npy-codec/#when-to-use-each","title":"When to Use Each","text":"<p>Use <code>&lt;npy@&gt;</code> when: - Arrays are large (&gt; 10 MB) - You need to inspect shape/dtype before loading - Fetching many rows but processing few - Random access to slices of very large arrays (memory mapping) - Interoperability matters (non-Python tools)</p> <p>Use <code>&lt;blob@&gt;</code> when: - Arrays are small (&lt; 10 MB) - Same arrays appear in multiple rows (deduplication) - Storing non-array Python objects (dicts, lists)</p>"},{"location":"how-to/use-npy-codec/#supported-array-types","title":"Supported Array Types","text":"<p>The <code>&lt;npy&gt;</code> codec supports any NumPy array except object dtype:</p> <pre><code># Supported\nnp.array([1, 2, 3], dtype=np.int32)          # Integer\nnp.array([1.0, 2.0], dtype=np.float64)       # Float\nnp.array([True, False], dtype=np.bool_)      # Boolean\nnp.array([1+2j, 3+4j], dtype=np.complex128)  # Complex\nnp.zeros((10, 10, 10))                       # N-dimensional\nnp.array(42)                                 # 0-dimensional scalar\n\n# Structured arrays\ndt = np.dtype([('x', np.float64), ('y', np.float64)])\nnp.array([(1.0, 2.0), (3.0, 4.0)], dtype=dt)\n\n# NOT supported\nnp.array([{}, []], dtype=object)  # Object dtype\n</code></pre>"},{"location":"how-to/use-npy-codec/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/use-npy-codec/#store-not-configured","title":"\"Store not configured\"","text":"<p>Ensure your store is configured before using <code>&lt;npy@store&gt;</code>:</p> <pre><code>dj.config.object_storage.stores['store'] = {...}\n</code></pre>"},{"location":"how-to/use-npy-codec/#requires-store-only","title":"\"requires @ (store only)\"","text":"<p>The <code>&lt;npy&gt;</code> codec requires the <code>@</code> modifier:</p> <pre><code># Wrong\ndata : &lt;npy&gt;\n\n# Correct\ndata : &lt;npy@&gt;\ndata : &lt;npy@mystore&gt;\n</code></pre>"},{"location":"how-to/use-npy-codec/#memory-issues-with-large-arrays","title":"Memory issues with large arrays","text":"<p>Use lazy loading or memory mapping to control memory:</p> <pre><code># Check size before loading\nif ref.nbytes &gt; available_memory:\n    # Use memory mapping for random access\n    arr = ref.load(mmap_mode='r')\n    # Process in chunks\n    for i in range(0, len(arr), chunk_size):\n        process(arr[i:i+chunk_size])\nelse:\n    arr = ref.load()\n</code></pre>"},{"location":"how-to/use-npy-codec/#see-also","title":"See Also","text":"<ul> <li>Use Object Storage - Complete storage guide</li> <li>Configure Object Storage - Store setup</li> <li><code>&lt;npy&gt;</code> Codec Specification - Full spec</li> </ul>"},{"location":"how-to/use-object-storage/","title":"Use Object Storage","text":"<p>Store large data objects as part of your Object-Augmented Schema.</p>"},{"location":"how-to/use-object-storage/#object-augmented-schema-oas","title":"Object-Augmented Schema (OAS)","text":"<p>An Object-Augmented Schema extends relational tables with object storage as a unified system. The relational database stores metadata, references, and small values while large objects (arrays, files, datasets) are stored in object storage. DataJoint maintains referential integrity across both storage layers\u2014when you delete a row, its associated objects are cleaned up automatically.</p> <p>OAS supports two addressing schemes:</p> Addressing Location Path Derived From Object Type Use Case Hash-addressed Object store Content hash (MD5) Individual/atomic Single blobs, single files, attachments (with deduplication) Schema-addressed Object store Schema structure Complex/multi-part Zarr arrays, HDF5 datasets, multi-file objects (browsable paths) <p>Key distinction:</p> <ul> <li>Hash-addressed (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>) stores individual, atomic objects - one object per field</li> <li>Schema-addressed (<code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>) can store complex, multi-part objects like Zarr (directory structures with multiple files)</li> </ul> <p>Data can also be stored in-table directly in the database column (no <code>@</code> modifier).</p> <p>For complete details, see the Type System specification.</p>"},{"location":"how-to/use-object-storage/#when-to-use-object-storage","title":"When to Use Object Storage","text":"<p>Use the <code>@</code> modifier for:</p> <ul> <li>Large arrays (images, videos, neural recordings)</li> <li>File attachments</li> <li>Zarr arrays and HDF5 files</li> <li>Any data too large for efficient database storage</li> </ul>"},{"location":"how-to/use-object-storage/#in-table-vs-object-store","title":"In-Table vs Object Store","text":"<pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : uuid\n    ---\n    metadata : &lt;blob&gt;           # In-table: stored in database column\n    raw_data : &lt;blob@&gt;          # Object store: hash-addressed\n    waveforms : &lt;npy@&gt;          # Object store: schema-addressed (lazy)\n    \"\"\"\n</code></pre> Syntax Storage Best For <code>&lt;blob&gt;</code> Database Small Python objects (typically &lt; 1-10 MB) <code>&lt;attach&gt;</code> Database Small files with filename (typically &lt; 1-10 MB) <code>&lt;blob@&gt;</code> Default store Large Python objects (hash-addressed, with dedup) <code>&lt;attach@&gt;</code> Default store Large files with filename (hash-addressed, with dedup) <code>&lt;npy@&gt;</code> Default store NumPy arrays (schema-addressed, lazy, navigable) <code>&lt;blob@store&gt;</code> Named store Specific storage tier"},{"location":"how-to/use-object-storage/#store-data","title":"Store Data","text":"<p>Insert works the same regardless of storage location:</p> <pre><code>import numpy as np\n\nRecording.insert1({\n    'recording_id': uuid.uuid4(),\n    'metadata': {'channels': 32, 'rate': 30000},\n    'raw_data': np.random.randn(32, 30000)  # ~7.7 MB array\n})\n</code></pre> <p>DataJoint automatically routes to the configured store.</p>"},{"location":"how-to/use-object-storage/#retrieve-data","title":"Retrieve Data","text":"<p>Fetch works transparently:</p> <pre><code>data = (Recording &amp; key).fetch1('raw_data')\n# Returns the numpy array, regardless of where it was stored\n</code></pre>"},{"location":"how-to/use-object-storage/#named-stores","title":"Named Stores","text":"<p>Use different stores for different data types:</p> <pre><code>@schema\nclass Experiment(dj.Manual):\n    definition = \"\"\"\n    experiment_id : uuid\n    ---\n    raw_video : &lt;blob@raw&gt;        # Fast local storage\n    processed : &lt;blob@archive&gt;    # S3 for long-term\n    \"\"\"\n</code></pre> <p>Configure stores in <code>datajoint.json</code>:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"raw\",\n    \"raw\": {\n      \"protocol\": \"file\",\n      \"location\": \"/fast/storage\"\n    },\n    \"archive\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"archive\",\n      \"location\": \"project-data\"\n    }\n  }\n}\n</code></pre>"},{"location":"how-to/use-object-storage/#hash-addressed-storage","title":"Hash-Addressed Storage","text":"<p><code>&lt;blob@&gt;</code> and <code>&lt;attach@&gt;</code> use hash-addressed storage:</p> <ul> <li>Objects are stored by their content hash (MD5)</li> <li>Identical data is stored once (automatic deduplication)</li> <li>Multiple rows can reference the same object</li> <li>Immutable\u2014changing data creates a new object</li> </ul> <pre><code># These two inserts store the same array only once\ndata = np.zeros((1000, 1000))\nTable.insert1({'id': 1, 'array': data})\nTable.insert1({'id': 2, 'array': data})  # References same object\n</code></pre>"},{"location":"how-to/use-object-storage/#schema-addressed-storage","title":"Schema-Addressed Storage","text":"<p><code>&lt;npy@&gt;</code> and <code>&lt;object@&gt;</code> use schema-addressed storage:</p> <ul> <li>Objects stored at paths that mirror database schema: <code>{schema}/{table}/{pk}/{attribute}.npy</code></li> <li>Browsable organization in object storage</li> <li>One object per entity (no deduplication)</li> <li>Supports lazy loading with metadata access</li> </ul> <pre><code>@schema\nclass Dataset(dj.Manual):\n    definition = \"\"\"\n    dataset_id : uuid\n    ---\n    zarr_array : &lt;object@&gt;      # Zarr array stored by path\n    \"\"\"\n</code></pre> <p>Use schema-addressed storage for:</p> <ul> <li>Zarr arrays (chunked, appendable)</li> <li>HDF5 files</li> <li>Large datasets requiring streaming access</li> </ul>"},{"location":"how-to/use-object-storage/#write-directly-to-object-storage","title":"Write Directly to Object Storage","text":"<p>For large datasets like multi-GB imaging recordings, avoid intermediate copies by writing directly to object storage with <code>staged_insert1</code>:</p> <pre><code>import zarr\n\n@schema\nclass ImagingSession(dj.Manual):\n    definition = \"\"\"\n    subject_id : int32\n    session_id : int32\n    ---\n    n_frames : int32\n    frame_rate : float32\n    frames : &lt;object@&gt;\n    \"\"\"\n\n# Write Zarr directly to object storage\nwith ImagingSession.staged_insert1 as staged:\n    # 1. Set primary key values first\n    staged.rec['subject_id'] = 1\n    staged.rec['session_id'] = 1\n\n    # 2. Get storage handle\n    store = staged.store('frames', '.zarr')\n\n    # 3. Write directly (no local copy)\n    z = zarr.open(store, mode='w', shape=(1000, 512, 512),\n                  chunks=(10, 512, 512), dtype='int32')\n    for i in range(1000):\n        z[i] = acquire_frame()  # Write frame-by-frame\n\n    # 4. Set remaining attributes\n    staged.rec['n_frames'] = 1000\n    staged.rec['frame_rate'] = 30.0\n\n# Record inserted with computed metadata on successful exit\n</code></pre> <p>The <code>staged_insert1</code> context manager:</p> <ul> <li>Writes directly to the object store (no intermediate files)</li> <li>Computes metadata (size, manifest) automatically on exit</li> <li>Cleans up storage if an error occurs (atomic)</li> <li>Requires primary key values before calling <code>store()</code> or <code>open()</code></li> </ul> <p>Use <code>staged.store(field, ext)</code> for FSMap access (Zarr), or <code>staged.open(field, ext)</code> for file-like access.</p>"},{"location":"how-to/use-object-storage/#attachments","title":"Attachments","text":"<p>Preserve original filenames with <code>&lt;attach@&gt;</code>:</p> <pre><code>@schema\nclass Document(dj.Manual):\n    definition = \"\"\"\n    doc_id : uuid\n    ---\n    report : &lt;attach@&gt;          # Preserves filename\n    \"\"\"\n\n# Insert with AttachFileType\nfrom datajoint import AttachFileType\nDocument.insert1({\n    'doc_id': uuid.uuid4(),\n    'report': AttachFileType('/path/to/report.pdf')\n})\n</code></pre>"},{"location":"how-to/use-object-storage/#numpy-arrays-with-npy","title":"NumPy Arrays with <code>&lt;npy@&gt;</code>","text":"<p>The <code>&lt;npy@&gt;</code> codec stores NumPy arrays as portable <code>.npy</code> files with lazy loading:</p> <pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int32\n    ---\n    waveform : &lt;npy@mystore&gt;    # NumPy array, schema-addressed\n    \"\"\"\n\n# Insert - just pass the array\nRecording.insert1({\n    'recording_id': 1,\n    'waveform': np.random.randn(1000, 32),\n})\n\n# Fetch returns NpyRef (lazy)\nref = (Recording &amp; 'recording_id=1').fetch1('waveform')\n</code></pre>"},{"location":"how-to/use-object-storage/#npyref-lazy-array-reference","title":"NpyRef: Lazy Array Reference","text":"<p><code>NpyRef</code> provides metadata without downloading:</p> <pre><code>ref = (Recording &amp; key).fetch1('waveform')\n\n# Metadata access - NO download\nref.shape    # (1000, 32)\nref.dtype    # float64\nref.nbytes   # 256000\nref.is_loaded  # False\n\n# Explicit loading\narr = ref.load()    # Downloads and caches\nref.is_loaded       # True\n\n# Numpy integration (triggers download)\nresult = np.mean(ref)           # Uses __array__ protocol\nresult = np.asarray(ref) + 1    # Convert then operate\n</code></pre>"},{"location":"how-to/use-object-storage/#bulk-fetch-safety","title":"Bulk Fetch Safety","text":"<p>Fetching many rows doesn't download until you access each array:</p> <pre><code># Fetch 1000 recordings - NO downloads yet\nresults = Recording.to_dicts()\n\n# Inspect metadata without downloading\nfor rec in results:\n    ref = rec['waveform']\n    if ref.shape[0] &gt; 500:     # Check without download\n        process(ref.load())     # Download only what you need\n</code></pre>"},{"location":"how-to/use-object-storage/#lazy-loading-with-objectref","title":"Lazy Loading with ObjectRef","text":"<p><code>&lt;object@&gt;</code> and <code>&lt;filepath@&gt;</code> return lazy references:</p> <pre><code>ref = (Dataset &amp; key).fetch1('zarr_array')\n\n# Open for streaming access\nwith ref.open() as f:\n    data = zarr.open(f)\n\n# Or download to local path\nlocal_path = ref.download('/tmp/data')\n</code></pre>"},{"location":"how-to/use-object-storage/#storage-best-practices","title":"Storage Best Practices","text":""},{"location":"how-to/use-object-storage/#choose-the-right-codec","title":"Choose the Right Codec","text":"Data Type Codec Addressing Lazy Best For NumPy arrays <code>&lt;npy@&gt;</code> Schema Yes Arrays needing lazy load, metadata inspection Python objects <code>&lt;blob&gt;</code> or <code>&lt;blob@&gt;</code> In-table or Hash No Dicts, lists, arrays (use <code>@</code> for large/dedup) File attachments <code>&lt;attach&gt;</code> or <code>&lt;attach@&gt;</code> In-table or Hash No Files with filename preserved (use <code>@</code> for large/dedup) Zarr/HDF5 <code>&lt;object@&gt;</code> Schema Yes Chunked arrays, streaming access File references <code>&lt;filepath@&gt;</code> External Yes References to external files"},{"location":"how-to/use-object-storage/#size-guidelines","title":"Size Guidelines","text":"<p>Technical limits: - MySQL: In-table blobs up to 4 GiB (<code>LONGBLOB</code>) - PostgreSQL: In-table blobs unlimited (<code>BYTEA</code>)</p> <p>Practical recommendations (consider accessibility, cost, performance): - &lt; 1-10 MB: In-table storage (<code>&lt;blob&gt;</code>) often sufficient - 10-100 MB: Object store (<code>&lt;blob@&gt;</code> with dedup, or <code>&lt;npy@&gt;</code> for arrays) - &gt; 100 MB: Schema-addressed (<code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>) for streaming and lazy loading</p>"},{"location":"how-to/use-object-storage/#store-tiers","title":"Store Tiers","text":"<p>Configure stores for different access patterns:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"hot\",\n    \"hot\": {\n      \"protocol\": \"file\",\n      \"location\": \"/ssd/data\"\n    },\n    \"warm\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"project-data\",\n      \"location\": \"active\"\n    },\n    \"cold\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"archive\",\n      \"location\": \"long-term\"\n    }\n  }\n}\n</code></pre>"},{"location":"how-to/use-object-storage/#see-also","title":"See Also","text":"<ul> <li>Configure Object Storage \u2014 Storage setup</li> <li>Create Custom Codecs \u2014 Domain-specific types</li> <li>Manage Large Data \u2014 Working with blobs</li> </ul>"},{"location":"how-to/use-plugin-codecs/","title":"Use Plugin Codecs","text":"<p>Install and use plugin codec packages to extend DataJoint's type system.</p>"},{"location":"how-to/use-plugin-codecs/#overview","title":"Overview","text":"<p>Plugin codecs are distributed as separate Python packages that extend DataJoint's type system. They add support for domain-specific data types without modifying DataJoint itself. Once installed, they register automatically via Python's entry point system and work seamlessly with DataJoint.</p> <p>Benefits:</p> <ul> <li>Automatic registration via entry points - no code changes needed</li> <li>Domain-specific types maintained independently</li> <li>Clean separation of core framework from specialized formats</li> <li>Easy to share across projects and teams</li> </ul>"},{"location":"how-to/use-plugin-codecs/#quick-start","title":"Quick Start","text":""},{"location":"how-to/use-plugin-codecs/#1-install-the-codec-package","title":"1. Install the Codec Package","text":"<pre><code>pip install dj-zarr-codecs\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#2-use-in-table-definitions","title":"2. Use in Table Definitions","text":"<pre><code>import datajoint as dj\n\nschema = dj.Schema('my_schema')\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int\n    ---\n    waveform : &lt;zarr@&gt;  # Automatically available after install\n    \"\"\"\n</code></pre> <p>That's it! No imports or registration needed. The codec is automatically discovered via Python's entry point system.</p>"},{"location":"how-to/use-plugin-codecs/#example-zarr-array-storage","title":"Example: Zarr Array Storage","text":"<p>The <code>dj-zarr-codecs</code> package adds support for storing NumPy arrays in Zarr format with schema-addressed paths.</p>"},{"location":"how-to/use-plugin-codecs/#installation","title":"Installation","text":"<pre><code>pip install dj-zarr-codecs\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#configuration","title":"Configuration","text":"<p>Configure object storage for external data:</p> <pre><code>import datajoint as dj\n\ndj.config['stores'] = {\n    'mystore': {\n        'protocol': 's3',\n        'endpoint': 's3.amazonaws.com',\n        'bucket': 'my-bucket',\n        'location': 'data',\n    }\n}\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#basic-usage","title":"Basic Usage","text":"<pre><code>import numpy as np\n\nschema = dj.Schema('neuroscience')\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int\n    ---\n    waveform : &lt;zarr@mystore&gt;  # Store as Zarr array\n    \"\"\"\n\n# Insert NumPy array\nRecording.insert1({\n    'recording_id': 1,\n    'waveform': np.random.randn(1000, 32),\n})\n\n# Fetch returns zarr.Array (read-only)\nzarr_array = (Recording &amp; {'recording_id': 1}).fetch1('waveform')\n\n# Use with NumPy\nmean_waveform = np.mean(zarr_array, axis=0)\n\n# Access Zarr features\nprint(zarr_array.shape)   # (1000, 32)\nprint(zarr_array.chunks)  # Zarr chunking info\nprint(zarr_array.dtype)   # float64\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#storage-structure","title":"Storage Structure","text":"<p>Zarr arrays are stored with schema-addressed paths that mirror your database structure:</p> <pre><code>s3://my-bucket/data/\n\u2514\u2500\u2500 neuroscience/           # Schema name\n    \u2514\u2500\u2500 recording/          # Table name\n        \u2514\u2500\u2500 recording_id=1/ # Primary key\n            \u2514\u2500\u2500 waveform.zarr/  # Field name + .zarr extension\n                \u251c\u2500\u2500 .zarray\n                \u2514\u2500\u2500 0.0\n</code></pre> <p>This organization makes object storage browsable and self-documenting.</p>"},{"location":"how-to/use-plugin-codecs/#when-to-use-zarr","title":"When to Use <code>&lt;zarr@&gt;</code>","text":"<p>Use <code>&lt;zarr@&gt;</code> when:</p> <ul> <li>Arrays are large (&gt; 10 MB)</li> <li>You need chunked access patterns</li> <li>Compression is beneficial</li> <li>Cross-language compatibility matters (any Zarr library can read)</li> <li>You want browsable, organized storage paths</li> </ul> <p>Use <code>&lt;npy@&gt;</code> instead when:</p> <ul> <li>You need lazy loading with metadata inspection before download</li> <li>Memory mapping is important</li> <li>Storage format simplicity is preferred</li> </ul> <p>Use <code>&lt;blob@&gt;</code> instead when:</p> <ul> <li>Arrays are small (&lt; 10 MB)</li> <li>Deduplication of repeated values is important</li> <li>Storing mixed Python objects (not just arrays)</li> </ul>"},{"location":"how-to/use-plugin-codecs/#finding-plugin-codecs","title":"Finding Plugin Codecs","text":""},{"location":"how-to/use-plugin-codecs/#datajoint-maintained-codecs","title":"DataJoint-Maintained Codecs","text":"<ul> <li>dj-zarr-codecs \u2014 Zarr array storage for general numpy arrays</li> <li>dj-photon-codecs \u2014 Photon-limited movies with Anscombe transformation and compression</li> </ul>"},{"location":"how-to/use-plugin-codecs/#community-codecs","title":"Community Codecs","text":"<p>Check PyPI for packages with the <code>datajoint</code> keyword:</p> <pre><code>pip search datajoint codec\n</code></pre> <p>Or browse GitHub: https://github.com/topics/datajoint</p>"},{"location":"how-to/use-plugin-codecs/#domain-specific-examples","title":"Domain-Specific Examples","text":"<p>Neuroscience:</p> <ul> <li>Spike train formats (NEO, NWB)</li> <li>Neural network models</li> <li>Connectivity matrices</li> </ul> <p>Imaging:</p> <ul> <li>Photon-limited movies (calcium imaging, low-light microscopy)</li> <li>OME-TIFF, OME-ZARR</li> <li>DICOM medical images</li> <li>Point cloud data</li> </ul> <p>Genomics:</p> <ul> <li>BAM/SAM alignments</li> <li>VCF variant calls</li> <li>Phylogenetic trees</li> </ul>"},{"location":"how-to/use-plugin-codecs/#verifying-installation","title":"Verifying Installation","text":"<p>Check that a codec is registered:</p> <pre><code>import datajoint as dj\n\n# List all available codecs\nprint(dj.list_codecs())\n# ['blob', 'attach', 'hash', 'object', 'npy', 'filepath', 'zarr', ...]\n\n# Check specific codec\nassert 'zarr' in dj.list_codecs()\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#how-auto-registration-works","title":"How Auto-Registration Works","text":"<p>Plugin codecs use Python's entry point system for automatic discovery. When you install a codec package, it registers itself via <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"datajoint.codecs\"]\nzarr = \"dj_zarr_codecs:ZarrCodec\"\n</code></pre> <p>DataJoint discovers these entry points at import time, so the codec is immediately available after <code>pip install</code>.</p> <p>No manual registration needed \u2014 unlike DataJoint 0.x which required <code>dj.register_codec()</code>.</p>"},{"location":"how-to/use-plugin-codecs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/use-plugin-codecs/#unknown-codec-zarr","title":"\"Unknown codec: \\&lt;zarr&gt;\"","text":"<p>The codec package is not installed or not found. Verify installation:</p> <pre><code>pip list | grep dj-zarr-codecs\n</code></pre> <p>If installed but not working:</p> <pre><code># Force entry point reload\nimport importlib.metadata\nimportlib.metadata.entry_points().select(group='datajoint.codecs')\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#codec-not-found-after-installation","title":"Codec Not Found After Installation","text":"<p>Restart your Python session or kernel. Entry points are discovered at import time:</p> <pre><code># Restart kernel, then:\nimport datajoint as dj\nprint('zarr' in dj.list_codecs())  # Should be True\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#version-conflicts","title":"Version Conflicts","text":"<p>Check compatibility with your DataJoint version:</p> <pre><code>pip show dj-zarr-codecs\n# Requires: datajoint&gt;=2.0.0\n</code></pre> <p>Upgrade DataJoint if needed:</p> <pre><code>pip install --upgrade datajoint\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#versioning-and-backward-compatibility","title":"Versioning and Backward Compatibility","text":"<p>Plugin codecs evolve over time. Following versioning best practices ensures your data remains accessible across codec updates.</p>"},{"location":"how-to/use-plugin-codecs/#built-in-vs-plugin-codec-versioning","title":"Built-in vs Plugin Codec Versioning","text":"<p>Built-in codecs (<code>&lt;blob&gt;</code>, <code>&lt;npy@&gt;</code>, <code>&lt;object@&gt;</code>, etc.) are versioned with DataJoint: - \u2705 Shipped with datajoint-python - \u2705 Versioned by DataJoint release (2.0.0, 2.1.0, 3.0.0) - \u2705 Upgraded when you upgrade DataJoint - \u2705 Stability guaranteed by DataJoint's semantic versioning - \u274c No explicit codec_version field needed - DataJoint version is the codec version</p> <p>Plugin codecs (dj-zarr-codecs, dj-photon-codecs, etc.) have independent lifecycles: - \u2705 Installed separately from DataJoint - \u2705 Independent version numbers (0.1.0 \u2192 1.0.0 \u2192 2.0.0) - \u2705 Users choose when to upgrade - \u2705 Must include explicit codec_version field for backward compatibility</p> <p>Why the difference?</p> <p>Plugin codecs evolve independently and need to handle data encoded by different plugin versions. Built-in codecs are part of DataJoint's API surface and evolve with the framework itself. When you upgrade DataJoint 2.0 \u2192 3.0, you expect potential breaking changes. When you upgrade a plugin 1.0 \u2192 2.0 while keeping DataJoint 2.0, backward compatibility is critical.</p> <p>How built-in codecs handle versioning:</p> <p>Built-in formats have intrinsic versioning - the format version is embedded in the data itself:</p> <ul> <li><code>&lt;blob&gt;</code> \u2014 Protocol header (<code>mYm\\0</code> or <code>dj0\\0</code>) at start of blob</li> <li><code>&lt;npy@&gt;</code> \u2014 NumPy format version in <code>.npy</code> file header</li> <li><code>&lt;object@&gt;</code> \u2014 Self-describing directory structure</li> <li><code>&lt;attach&gt;</code> \u2014 Filename + content (format-agnostic)</li> </ul> <p>When DataJoint needs to change a built-in codec's format, it can detect the old format from the embedded version information and handle migration transparently. This is why built-in codecs don't need an explicit <code>codec_version</code> field in database metadata.</p>"},{"location":"how-to/use-plugin-codecs/#version-strategy","title":"Version Strategy","text":"<p>Two version numbers matter for plugin codecs:</p> <ol> <li> <p>Package version (semantic versioning: <code>0.1.0</code>, <code>1.0.0</code>, <code>2.0.0</code>)</p> <ul> <li>For codec package releases</li> <li>Follows standard semantic versioning</li> </ul> </li> <li> <p>Data format version (stored with each encoded value)</p> <ul> <li>Tracks storage format changes</li> <li>Enables decode() to handle multiple formats</li> </ul> </li> </ol>"},{"location":"how-to/use-plugin-codecs/#implementing-versioning","title":"Implementing Versioning","text":"<p>Include version in encoded metadata:</p> <pre><code>def encode(self, value, *, key=None, store_name=None):\n    # ... encoding logic ...\n\n    return {\n        \"path\": path,\n        \"store\": store_name,\n        \"codec_version\": \"1.0\",  # Data format version\n        \"shape\": list(value.shape),\n        \"dtype\": str(value.dtype),\n    }\n</code></pre> <p>Handle multiple versions in decode:</p> <pre><code>def decode(self, stored, *, key=None):\n    version = stored.get(\"codec_version\", \"1.0\")  # Default for old data\n\n    if version == \"2.0\":\n        return self._decode_v2(stored)\n    elif version == \"1.0\":\n        return self._decode_v1(stored)\n    else:\n        raise DataJointError(f\"Unsupported codec version: {version}\")\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#when-to-bump-versions","title":"When to Bump Versions","text":"<p>Bump data format version when: - \u2705 Changing storage structure or encoding algorithm - \u2705 Modifying metadata schema - \u2705 Changing compression parameters that affect decode</p> <p>Don't bump for: - \u274c Bug fixes that don't affect stored data format - \u274c Performance improvements to encode/decode logic - \u274c Adding new optional features (store version in attributes instead)</p>"},{"location":"how-to/use-plugin-codecs/#backward-compatibility-patterns","title":"Backward Compatibility Patterns","text":"<p>Pattern 1: Version dispatch in decode()</p> <pre><code>class MyCodec(SchemaCodec):\n    name = \"mycodec\"\n    CURRENT_VERSION = \"2.0\"\n\n    def encode(self, value, *, key=None, store_name=None):\n        # Always encode with current version\n        metadata = {\n            \"codec_version\": self.CURRENT_VERSION,\n            # ... other metadata ...\n        }\n        return metadata\n\n    def decode(self, stored, *, key=None):\n        version = stored.get(\"codec_version\", \"1.0\")\n\n        if version == \"2.0\":\n            # Current version - optimized path\n            return self._decode_current(stored)\n        elif version == \"1.0\":\n            # Legacy version - compatibility path\n            return self._decode_legacy_v1(stored)\n        else:\n            raise DataJointError(\n                f\"Cannot decode {self.name} version {version}. \"\n                f\"Upgrade codec package or migrate data.\"\n            )\n</code></pre> <p>Pattern 2: Zarr attributes for feature versions</p> <p>For codecs using Zarr (like dj-zarr-codecs, dj-photon-codecs):</p> <pre><code>def encode(self, value, *, key=None, store_name=None):\n    # ... write to Zarr ...\n\n    z = zarr.open(store_map, mode=\"r+\")\n    z.attrs[\"codec_version\"] = \"2.0\"\n    z.attrs[\"codec_name\"] = self.name\n    z.attrs[\"feature_flags\"] = [\"compression\", \"chunking\"]\n\n    return {\n        \"path\": path,\n        \"store\": store_name,\n        \"codec_version\": \"2.0\",  # Also in DB for quick access\n    }\n\ndef decode(self, stored, *, key=None):\n    z = zarr.open(store_map, mode=\"r\")\n    version = z.attrs.get(\"codec_version\", \"1.0\")\n\n    # Handle version-specific decoding\n    if version == \"2.0\":\n        return z  # Return Zarr array directly\n    else:\n        return self._migrate_v1_to_v2(z)\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#migration-strategies","title":"Migration Strategies","text":"<p>Strategy 1: Lazy migration (recommended)</p> <p>Old data is migrated when accessed:</p> <pre><code>def decode(self, stored, *, key=None):\n    version = stored.get(\"codec_version\", \"1.0\")\n\n    if version == \"1.0\":\n        # Decode old format\n        data = self._decode_v1(stored)\n\n        # Optionally: re-encode to new format in background\n        # (requires database write access)\n        return data\n\n    return self._decode_current(stored)\n</code></pre> <p>Strategy 2: Explicit migration script</p> <p>For breaking changes, provide migration tools:</p> <pre><code># migration_tool.py\ndef migrate_table_to_v2(table, field_name):\n    \"\"\"Migrate all rows to codec version 2.0.\"\"\"\n    for key in table.fetch(\"KEY\"):\n        # Fetch with old codec\n        data = (table &amp; key).fetch1(field_name)\n\n        # Re-insert with new codec (triggers encode)\n        table.update1({**key, field_name: data})\n</code></pre> <p>Strategy 3: Deprecation warnings</p> <pre><code>def decode(self, stored, *, key=None):\n    version = stored.get(\"codec_version\", \"1.0\")\n\n    if version == \"1.0\":\n        import warnings\n        warnings.warn(\n            f\"Reading {self.name} v1.0 data. Support will be removed in v3.0. \"\n            f\"Please migrate: pip install {self.name}-migrate &amp;&amp; migrate-data\",\n            DeprecationWarning\n        )\n        return self._decode_v1(stored)\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#real-world-example-dj-photon-codecs-evolution","title":"Real-World Example: dj-photon-codecs Evolution","text":"<p>Version 1.0 (current): - Stores Anscombe-transformed data - Fixed compression (Blosc zstd level 5) - Fixed chunking (100 frames)</p> <p>Hypothetical Version 2.0 (backward compatible): <pre><code>def encode(self, value, *, key=None, store_name=None):\n    # New: configurable compression\n    compression_level = getattr(self, 'compression_level', 5)\n\n    zarr.save_array(\n        store_map,\n        transformed,\n        compressor=zarr.Blosc(cname=\"zstd\", clevel=compression_level),\n    )\n\n    z = zarr.open(store_map, mode=\"r+\")\n    z.attrs[\"codec_version\"] = \"2.0\"\n    z.attrs[\"compression_level\"] = compression_level\n\n    return {\n        \"path\": path,\n        \"codec_version\": \"2.0\",  # &lt;-- NEW\n        # ... rest same ...\n    }\n\ndef decode(self, stored, *, key=None):\n    z = zarr.open(store_map, mode=\"r\")\n    version = z.attrs.get(\"codec_version\", \"1.0\")\n\n    # Both versions return zarr.Array - fully compatible!\n    if version in (\"1.0\", \"2.0\"):\n        return z\n    else:\n        raise DataJointError(f\"Unsupported version: {version}\")\n</code></pre></p>"},{"location":"how-to/use-plugin-codecs/#testing-version-compatibility","title":"Testing Version Compatibility","text":"<p>Include tests for version compatibility:</p> <pre><code>def test_decode_v1_data():\n    \"\"\"Ensure new codec can read old data.\"\"\"\n    # Load fixture with v1.0 data\n    old_data = load_v1_fixture()\n\n    # Decode with current codec\n    codec = PhotonCodec()\n    result = codec.decode(old_data)\n\n    assert result.shape == (1000, 512, 512)\n    assert result.dtype == np.float64\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#package-version-guidelines","title":"Package Version Guidelines","text":"<p>Follow semantic versioning for codec packages:</p> <ul> <li>Patch (0.1.0 \u2192 0.1.1): Bug fixes, no data format changes</li> <li>Minor (0.1.0 \u2192 0.2.0): New features, backward compatible</li> <li>Major (0.1.0 \u2192 1.0.0): Breaking changes (may require migration)</li> </ul> <p>Example changelog:</p> <pre><code>v2.0.0 (2026-02-01) - BREAKING\n  - Changed default compression from zstd-5 to zstd-3\n  - Data format v2.0 (can still read v1.0)\n  - Migration guide: docs/migration-v2.md\n\nv1.1.0 (2026-01-15)\n  - Added configurable chunk sizes (backward compatible)\n  - Data format still v1.0\n\nv1.0.1 (2026-01-10)\n  - Fixed edge case in Anscombe inverse transform\n  - Data format unchanged (v1.0)\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#creating-your-own-codecs","title":"Creating Your Own Codecs","text":"<p>If you need a codec that doesn't exist yet, see:</p> <ul> <li>Create Custom Codecs \u2014 Step-by-step guide</li> <li>Codec API Specification \u2014 Technical reference</li> <li>Custom Codecs Explanation \u2014 Design concepts</li> </ul> <p>Consider publishing your codec as a package so others can benefit!</p>"},{"location":"how-to/use-plugin-codecs/#best-practices","title":"Best Practices","text":""},{"location":"how-to/use-plugin-codecs/#1-install-codecs-with-your-project","title":"1. Install Codecs with Your Project","text":"<p>Add plugin codecs to your project dependencies:</p> <p>requirements.txt: <pre><code>datajoint&gt;=2.0.0\ndj-zarr-codecs&gt;=0.1.0\n</code></pre></p> <p>pyproject.toml: <pre><code>dependencies = [\n    \"datajoint&gt;=2.0.0\",\n    \"dj-zarr-codecs&gt;=0.1.0\",\n]\n</code></pre></p>"},{"location":"how-to/use-plugin-codecs/#2-document-codec-requirements","title":"2. Document Codec Requirements","text":"<p>In your pipeline documentation, specify required codecs:</p> <pre><code>\"\"\"\nMy Pipeline\n===========\n\nRequirements:\n- datajoint&gt;=2.0.0\n- dj-zarr-codecs&gt;=0.1.0  # For waveform storage\n\nInstall:\n    pip install datajoint dj-zarr-codecs\n\"\"\"\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#3-pin-versions-for-reproducibility","title":"3. Pin Versions for Reproducibility","text":"<p>Use exact versions in production:</p> <pre><code>dj-zarr-codecs==0.1.0  # Exact version\n</code></pre> <p>Use minimum versions in libraries:</p> <pre><code>dj-zarr-codecs&gt;=0.1.0  # Minimum version\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#4-test-codec-availability","title":"4. Test Codec Availability","text":"<p>Add checks in your pipeline setup:</p> <pre><code>import datajoint as dj\n\nREQUIRED_CODECS = ['zarr']\n\ndef check_requirements():\n    available = dj.list_codecs()\n    missing = [c for c in REQUIRED_CODECS if c not in available]\n\n    if missing:\n        raise ImportError(\n            f\"Missing required codecs: {missing}\\n\"\n            f\"Install with: pip install dj-zarr-codecs\"\n        )\n\ncheck_requirements()\n</code></pre>"},{"location":"how-to/use-plugin-codecs/#see-also","title":"See Also","text":"<ul> <li>Use Object Storage \u2014 Object storage configuration</li> <li>Create Custom Codecs \u2014 Build your own codecs</li> <li>Type System \u2014 Complete type reference</li> <li>dj-zarr-codecs Repository \u2014 General Zarr array storage</li> <li>dj-photon-codecs Repository \u2014 Photon-limited movies with compression</li> </ul>"},{"location":"how-to/demo_modules/__init__/","title":"init","text":"<p>Demo modules for diagram collapse example</p>"},{"location":"how-to/demo_modules/acquisition/","title":"Acquisition","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nacquisition.py - Data acquisition tables\n\nThis module defines the core acquisition schema with Subject and Session tables.\nTables are declared but schema must be activated before use.\n\"\"\"\nimport datajoint as dj\n</pre> \"\"\" acquisition.py - Data acquisition tables  This module defines the core acquisition schema with Subject and Session tables. Tables are declared but schema must be activated before use. \"\"\" import datajoint as dj In\u00a0[\u00a0]: Copied! <pre># Create schema without activating - caller must activate before use\nschema = dj.Schema()\n</pre> # Create schema without activating - caller must activate before use schema = dj.Schema() In\u00a0[\u00a0]: Copied! <pre>@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"\n    lab : varchar(32)\n    ---\n    institution : varchar(100)\n    \"\"\"\n</pre> @schema class Lab(dj.Manual):     definition = \"\"\"     lab : varchar(32)     ---     institution : varchar(100)     \"\"\" In\u00a0[\u00a0]: Copied! <pre>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    -&gt; Lab\n    species : varchar(50)\n    sex : enum('M', 'F', 'U')\n    \"\"\"\n</pre> @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : varchar(16)     ---     -&gt; Lab     species : varchar(50)     sex : enum('M', 'F', 'U')     \"\"\" In\u00a0[\u00a0]: Copied! <pre>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_date : date\n    ---\n    session_notes : varchar(1000)\n    \"\"\"\n</pre> @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_date : date     ---     session_notes : varchar(1000)     \"\"\""},{"location":"how-to/demo_modules/analysis/","title":"Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nanalysis.py - Analysis tables\n\nThis module defines analysis tables that aggregate results across sessions.\nTables are declared but schema must be activated before use.\n\"\"\"\nimport datajoint as dj\nfrom . import acquisition, processing\n</pre> \"\"\" analysis.py - Analysis tables  This module defines analysis tables that aggregate results across sessions. Tables are declared but schema must be activated before use. \"\"\" import datajoint as dj from . import acquisition, processing In\u00a0[\u00a0]: Copied! <pre># Create schema without activating - caller must activate before use\nschema = dj.Schema()\n</pre> # Create schema without activating - caller must activate before use schema = dj.Schema() In\u00a0[\u00a0]: Copied! <pre>@schema\nclass AnalysisParams(dj.Lookup):\n    definition = \"\"\"\n    analysis_id : int16\n    ---\n    method : varchar(50)\n    \"\"\"\n    contents = [(1, 'correlation'), (2, 'regression')]\n</pre> @schema class AnalysisParams(dj.Lookup):     definition = \"\"\"     analysis_id : int16     ---     method : varchar(50)     \"\"\"     contents = [(1, 'correlation'), (2, 'regression')] In\u00a0[\u00a0]: Copied! <pre>@schema\nclass SubjectAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; acquisition.Subject\n    -&gt; AnalysisParams\n    ---\n    result : float32\n    confidence : float32\n    \"\"\"\n\n    def make(self, key):\n        self.insert1({**key, 'result': 0.8, 'confidence': 0.95})\n</pre> @schema class SubjectAnalysis(dj.Computed):     definition = \"\"\"     -&gt; acquisition.Subject     -&gt; AnalysisParams     ---     result : float32     confidence : float32     \"\"\"      def make(self, key):         self.insert1({**key, 'result': 0.8, 'confidence': 0.95}) In\u00a0[\u00a0]: Copied! <pre>@schema\nclass CrossSessionAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; acquisition.Subject\n    -&gt; processing.ProcessingParams\n    ---\n    aggregate_score : float32\n    \"\"\"\n\n    def make(self, key):\n        self.insert1({**key, 'aggregate_score': 0.9})\n</pre> @schema class CrossSessionAnalysis(dj.Computed):     definition = \"\"\"     -&gt; acquisition.Subject     -&gt; processing.ProcessingParams     ---     aggregate_score : float32     \"\"\"      def make(self, key):         self.insert1({**key, 'aggregate_score': 0.9})"},{"location":"how-to/demo_modules/processing/","title":"Processing","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nprocessing.py - Data processing tables\n\nThis module defines computed tables that process raw acquisition data.\nTables are declared but schema must be activated before use.\n\"\"\"\nimport datajoint as dj\nfrom . import acquisition\n</pre> \"\"\" processing.py - Data processing tables  This module defines computed tables that process raw acquisition data. Tables are declared but schema must be activated before use. \"\"\" import datajoint as dj from . import acquisition In\u00a0[\u00a0]: Copied! <pre># Create schema without activating - caller must activate before use\nschema = dj.Schema()\n</pre> # Create schema without activating - caller must activate before use schema = dj.Schema() In\u00a0[\u00a0]: Copied! <pre>@schema\nclass ProcessingParams(dj.Lookup):\n    definition = \"\"\"\n    params_id : int16\n    ---\n    filter_cutoff : float32\n    threshold : float32\n    \"\"\"\n    contents = [(1, 30.0, 2.5), (2, 50.0, 3.0)]\n</pre> @schema class ProcessingParams(dj.Lookup):     definition = \"\"\"     params_id : int16     ---     filter_cutoff : float32     threshold : float32     \"\"\"     contents = [(1, 30.0, 2.5), (2, 50.0, 3.0)] In\u00a0[\u00a0]: Copied! <pre>@schema\nclass ProcessedSession(dj.Computed):\n    definition = \"\"\"\n    -&gt; acquisition.Session\n    -&gt; ProcessingParams\n    ---\n    n_events : int32\n    quality_score : float32\n    \"\"\"\n\n    def make(self, key):\n        self.insert1({**key, 'n_events': 100, 'quality_score': 0.95})\n</pre> @schema class ProcessedSession(dj.Computed):     definition = \"\"\"     -&gt; acquisition.Session     -&gt; ProcessingParams     ---     n_events : int32     quality_score : float32     \"\"\"      def make(self, key):         self.insert1({**key, 'n_events': 100, 'quality_score': 0.95}) In\u00a0[\u00a0]: Copied! <pre>@schema\nclass EventDetection(dj.Computed):\n    definition = \"\"\"\n    -&gt; ProcessedSession\n    event_id : int32\n    ---\n    event_time : float32\n    amplitude : float32\n    \"\"\"\n\n    def make(self, key):\n        pass\n</pre> @schema class EventDetection(dj.Computed):     definition = \"\"\"     -&gt; ProcessedSession     event_id : int32     ---     event_time : float32     amplitude : float32     \"\"\"      def make(self, key):         pass"},{"location":"reference/","title":"Reference","text":"<p>Specifications, API documentation, and technical details.</p>"},{"location":"reference/#specifications","title":"Specifications","text":"<p>Detailed specifications of DataJoint's behavior and semantics.</p> <ul> <li>Primary Key Rules \u2014 How primary keys are determined in query results</li> <li>Semantic Matching \u2014 Attribute lineage and homologous matching</li> <li>Type System \u2014 Core types, codecs, and storage modes</li> <li>Codec API \u2014 Creating custom attribute types</li> <li>Object Store Configuration \u2014 Store configuration, path generation, and integrated storage models</li> <li>AutoPopulate \u2014 Jobs 2.0 specification</li> <li>Fetch API \u2014 Data retrieval methods</li> <li>Job Metadata \u2014 Hidden job tracking columns</li> </ul>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":"<ul> <li>Configuration \u2014 All <code>dj.config</code> options</li> <li>Definition Syntax \u2014 Table definition grammar</li> <li>Operators \u2014 Query operator summary</li> <li>Errors \u2014 Exception types and meanings</li> </ul>"},{"location":"reference/#elements","title":"Elements","text":"<p>Curated pipeline modules for neurophysiology experiments.</p> <ul> <li>DataJoint Elements \u2014 Pre-built pipelines for calcium imaging, electrophysiology, behavior tracking, and more</li> </ul>"},{"location":"reference/#api-documentation","title":"API Documentation","text":"<p>Auto-generated from source code docstrings.</p> <ul> <li>API Index</li> </ul>"},{"location":"reference/configuration/","title":"Configuration Reference","text":"<p>DataJoint configuration options and settings.</p>"},{"location":"reference/configuration/#configuration-sources","title":"Configuration Sources","text":"<p>Configuration is loaded in priority order:</p> <ol> <li>Environment variables (highest priority)</li> <li>Secrets directory (<code>.secrets/</code>)</li> <li>Config file (<code>datajoint.json</code>)</li> <li>Defaults (lowest priority)</li> </ol>"},{"location":"reference/configuration/#database-settings","title":"Database Settings","text":"Setting Environment Default Description <code>database.backend</code> <code>DJ_BACKEND</code> <code>mysql</code> Database backend: <code>mysql</code> or <code>postgresql</code> (new in 2.1) <code>database.host</code> <code>DJ_HOST</code> <code>localhost</code> Database server hostname <code>database.port</code> <code>DJ_PORT</code> <code>3306</code>/<code>5432</code> Database server port (auto-detects from backend) <code>database.user</code> <code>DJ_USER</code> \u2014 Database username (required) <code>database.password</code> <code>DJ_PASS</code> \u2014 Database password (required) <code>database.reconnect</code> \u2014 <code>True</code> Auto-reconnect on connection loss <code>database.use_tls</code> <code>DJ_USE_TLS</code> <code>None</code> Enable TLS encryption (env var new in 2.1) <code>database.database_prefix</code> <code>DJ_DATABASE_PREFIX</code> <code>\"\"</code> Prefix for database/schema names <code>database.create_tables</code> <code>DJ_CREATE_TABLES</code> <code>True</code> Default for <code>Schema(create_tables=)</code>. Set <code>False</code> for production mode"},{"location":"reference/configuration/#connection-settings","title":"Connection Settings","text":"Setting Default Description <code>connection.init_function</code> <code>None</code> SQL function to run on connect <code>connection.charset</code> <code>\"\"</code> Character set (pymysql default)"},{"location":"reference/configuration/#stores-configuration","title":"Stores Configuration","text":"<p>Unified storage configuration for all in-store types (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;filepath@&gt;</code>, <code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>).</p> <p>Default stores:</p> <p>DataJoint uses two default settings to reflect the architectural distinction between integrated and reference storage:</p> Setting Default Description <code>stores.default</code> \u2014 Default store for integrated storage (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>) <code>stores.filepath_default</code> \u2014 Default store for filepath references (<code>&lt;filepath@&gt;</code>) \u2014 often different from <code>stores.default</code> <p>Why separate defaults? Hash and schema-addressed storage are integrated into the Object-Augmented Schema (OAS)\u2014DataJoint manages paths, lifecycle, and integrity. Filepath storage is user-managed references to existing files\u2014DataJoint only stores the path. These are architecturally distinct and often use different storage locations.</p> <p>Common settings (all protocols):</p> Setting Required Description <code>stores.&lt;name&gt;.protocol</code> Yes Storage protocol: <code>file</code>, <code>s3</code>, <code>gcs</code>, <code>azure</code> <code>stores.&lt;name&gt;.location</code> Yes Base path or prefix (includes project context) <code>stores.&lt;name&gt;.hash_prefix</code> No Path prefix for hash-addressed section (default: <code>\"_hash\"</code>) <code>stores.&lt;name&gt;.schema_prefix</code> No Path prefix for schema-addressed section (default: <code>\"_schema\"</code>) <code>stores.&lt;name&gt;.filepath_prefix</code> No Required path prefix for filepath section, or <code>null</code> for unrestricted (default: <code>null</code>) <code>stores.&lt;name&gt;.subfolding</code> No Directory nesting for hash-addressed storage, e.g., <code>[2, 2]</code> (default: no subfolding) <code>stores.&lt;name&gt;.partition_pattern</code> No Path partitioning for schema-addressed storage, e.g., <code>\"subject_id/session_date\"</code> (default: no partitioning) <code>stores.&lt;name&gt;.token_length</code> No Random token length for schema-addressed filenames (default: <code>8</code>) <p>Storage sections:</p> <p>Each store is divided into sections defined by prefix configuration. The <code>*_prefix</code> parameters set the path prefix for each storage section:</p> <ul> <li><code>hash_prefix</code>: Defines the hash-addressed section for <code>&lt;blob@&gt;</code> and <code>&lt;attach@&gt;</code> (default: <code>\"_hash\"</code>)</li> <li><code>schema_prefix</code>: Defines the schema-addressed section for <code>&lt;object@&gt;</code> and <code>&lt;npy@&gt;</code> (default: <code>\"_schema\"</code>)</li> <li><code>filepath_prefix</code>: Optionally restricts the filepath section for <code>&lt;filepath@&gt;</code> (default: <code>null</code> = unrestricted)</li> </ul> <p>Prefixes must be mutually exclusive (no prefix can be a parent/child of another). This allows mapping DataJoint to existing storage layouts:</p> <pre><code>{\n  \"stores\": {\n    \"legacy\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/existing_storage\",\n      \"hash_prefix\": \"content_addressed\",      // Path prefix for hash section\n      \"schema_prefix\": \"structured_data\",       // Path prefix for schema section\n      \"filepath_prefix\": \"raw_files\"            // Path prefix for filepath section\n    }\n  }\n}\n</code></pre> <p>S3-specific settings:</p> Setting Required Description <code>stores.&lt;name&gt;.endpoint</code> Yes S3 endpoint URL (e.g., <code>s3.amazonaws.com</code>) <code>stores.&lt;name&gt;.bucket</code> Yes Bucket name <code>stores.&lt;name&gt;.access_key</code> Yes S3 access key ID <code>stores.&lt;name&gt;.secret_key</code> Yes S3 secret access key <code>stores.&lt;name&gt;.secure</code> No Use HTTPS (default: <code>True</code>) <p>GCS-specific settings:</p> Setting Required Description <code>stores.&lt;name&gt;.bucket</code> Yes GCS bucket name <code>stores.&lt;name&gt;.token</code> Yes Authentication token path <code>stores.&lt;name&gt;.project</code> No GCS project ID <p>Azure-specific settings:</p> Setting Required Description <code>stores.&lt;name&gt;.container</code> Yes Azure container name <code>stores.&lt;name&gt;.account_name</code> Yes Storage account name <code>stores.&lt;name&gt;.account_key</code> Yes Storage account key <code>stores.&lt;name&gt;.connection_string</code> No Alternative to account_name + account_key <p>How storage methods use stores:</p> <ul> <li>Hash-addressed (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>): <code>{location}/{hash_prefix}/{schema}/{hash}</code> with optional subfolding</li> <li>Schema-addressed (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>): <code>{location}/{schema_prefix}/{partition}/{schema}/{table}/{key}/{field}.{token}.{ext}</code> with optional partitioning</li> <li>Filepath (<code>&lt;filepath@&gt;</code>): <code>{location}/{filepath_prefix}/{user_path}</code> (user-managed, cannot use hash or schema prefixes)</li> </ul> <p>All storage methods share the same stores and default store. DataJoint reserves the configured <code>hash_prefix</code> and <code>schema_prefix</code> sections for managed storage; <code>&lt;filepath@&gt;</code> references can use any other paths (unless <code>filepath_prefix</code> is configured to restrict them).</p> <p>Path structure examples:</p> <p>Without partitioning: <pre><code>{location}/_hash/{schema}/ab/cd/abcd1234...                    # hash-addressed with subfolding\n{location}/_schema/{schema}/{table}/{key}/data.x8f2a9b1.zarr   # schema-addressed, no partitioning\n</code></pre></p> <p>With <code>partition_pattern: \"subject_id/session_date\"</code>: <pre><code>{location}/_schema/subject_id=042/session_date=2024-01-15/{schema}/{table}/{remaining_key}/data.x8f2a9b1.zarr\n</code></pre></p> <p>If table lacks partition attributes, it follows normal path structure.</p> <p>Credentials should be stored in secrets:</p> <pre><code>.secrets/\n\u251c\u2500\u2500 stores.main.access_key\n\u251c\u2500\u2500 stores.main.secret_key\n\u251c\u2500\u2500 stores.archive.access_key\n\u2514\u2500\u2500 stores.archive.secret_key\n</code></pre>"},{"location":"reference/configuration/#jobs-settings","title":"Jobs Settings","text":"Setting Default Description <code>jobs.auto_refresh</code> <code>True</code> Auto-refresh job queue on populate <code>jobs.keep_completed</code> <code>False</code> Retain success records in jobs table <code>jobs.stale_timeout</code> <code>3600</code> Seconds before stale job cleanup <code>jobs.default_priority</code> <code>5</code> Default priority (0-255, lower = more urgent) <code>jobs.version_method</code> <code>None</code> Version tracking: <code>git</code>, <code>none</code>, or <code>None</code> (disabled) <code>jobs.add_job_metadata</code> <code>False</code> Add hidden metadata to computed tables <code>jobs.allow_new_pk_fields_in_computed_tables</code> <code>False</code> Allow non-FK primary key fields"},{"location":"reference/configuration/#display-settings","title":"Display Settings","text":"Setting Environment Default Description <code>display.limit</code> \u2014 <code>12</code> Max rows to display <code>display.width</code> \u2014 <code>14</code> Column width <code>display.show_tuple_count</code> \u2014 <code>True</code> Show row count in output <code>display.diagram_direction</code> <code>DJ_DIAGRAM_DIRECTION</code> <code>LR</code> Diagram layout: <code>LR</code> (left-right) or <code>TB</code> (top-bottom) (new in 2.1)"},{"location":"reference/configuration/#top-level-settings","title":"Top-Level Settings","text":"Setting Environment Default Description <code>loglevel</code> <code>DJ_LOG_LEVEL</code> <code>INFO</code> Log level: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code> <code>safemode</code> \u2014 <code>True</code> Require confirmation for destructive operations <code>enable_python_native_blobs</code> \u2014 <code>True</code> Allow Python-native blob serialization <code>cache</code> \u2014 <code>None</code> Path for query result cache <code>query_cache</code> \u2014 <code>None</code> Path for compiled query cache <code>download_path</code> \u2014 <code>.</code> Download location for attachments/filepaths"},{"location":"reference/configuration/#example-configuration","title":"Example Configuration","text":""},{"location":"reference/configuration/#datajointjson-non-sensitive-settings","title":"datajoint.json (Non-sensitive settings)","text":"<pre><code>{\n    \"database.host\": \"mysql.example.com\",\n    \"database.port\": 3306,\n    \"stores\": {\n        \"default\": \"main\",\n        \"filepath_default\": \"raw_data\",\n        \"main\": {\n            \"protocol\": \"s3\",\n            \"endpoint\": \"s3.amazonaws.com\",\n            \"bucket\": \"datajoint-bucket\",\n            \"location\": \"neuroscience-lab/production\",\n            \"partition_pattern\": \"subject_id/session_date\",\n            \"token_length\": 8\n        },\n        \"archive\": {\n            \"protocol\": \"s3\",\n            \"endpoint\": \"s3.amazonaws.com\",\n            \"bucket\": \"archive-bucket\",\n            \"location\": \"neuroscience-lab/long-term\",\n            \"subfolding\": [2, 2]\n        },\n        \"raw_data\": {\n            \"protocol\": \"file\",\n            \"location\": \"/mnt/acquisition\",\n            \"filepath_prefix\": \"recordings\"\n        }\n    },\n    \"jobs\": {\n        \"add_job_metadata\": true\n    }\n}\n</code></pre>"},{"location":"reference/configuration/#secrets-credentials-never-commit","title":".secrets/ (Credentials - never commit!)","text":"<pre><code>.secrets/\n\u251c\u2500\u2500 database.user                      # analyst\n\u251c\u2500\u2500 database.password                  # dbpass123\n\u251c\u2500\u2500 stores.main.access_key             # AKIAIOSFODNN7EXAMPLE\n\u251c\u2500\u2500 stores.main.secret_key             # wJalrXUtnFEMI/K7MDENG...\n\u251c\u2500\u2500 stores.archive.access_key          # AKIAIOSFODNN8EXAMPLE\n\u2514\u2500\u2500 stores.archive.secret_key          # xKbmsYVuoGFNJ/L8NEOH...\n</code></pre> <p>Add <code>.secrets/</code> to <code>.gitignore</code>:</p> <pre><code>echo \".secrets/\" &gt;&gt; .gitignore\n</code></pre>"},{"location":"reference/configuration/#environment-variables-alternative-to-secrets","title":"Environment Variables (Alternative to .secrets/)","text":"<pre><code># Database\nexport DJ_HOST=mysql.example.com\nexport DJ_USER=analyst\nexport DJ_PASS=secret\n</code></pre> <p>Note: Per-store credentials must be configured in <code>datajoint.json</code> or <code>.secrets/</code> \u2014 environment variable overrides are not supported for nested store configurations.</p>"},{"location":"reference/configuration/#programmatic-access","title":"Programmatic Access","text":""},{"location":"reference/configuration/#reading-and-writing-settings","title":"Reading and Writing Settings","text":"<p>Access settings using dot notation on <code>dj.config</code>:</p> <pre><code>import datajoint as dj\n\n# Read settings\nprint(dj.config.database.host)\nprint(dj.config.display.diagram_direction)\n\n# Write settings\ndj.config.database.host = \"mysql.example.com\"\ndj.config.display.diagram_direction = \"TB\"\n</code></pre>"},{"location":"reference/configuration/#temporary-overrides","title":"Temporary Overrides","text":"<p>Use <code>dj.config.override()</code> to temporarily change settings within a context:</p> <pre><code>with dj.config.override(safemode=False):\n    # safemode is False here\n    (Table &amp; key).delete()\n# safemode is restored to original value\n</code></pre> <p>Nested settings syntax: Since Python doesn't allow dots in keyword argument names, use double underscores (<code>__</code>) to access nested settings in <code>override()</code>:</p> <pre><code># These are equivalent:\ndj.config.display.diagram_direction = \"TB\"  # direct assignment\n\nwith dj.config.override(display__diagram_direction=\"TB\"):  # in override()\n    ...\n</code></pre> <p>The double underscore maps to the dot-notation path: <code>display__diagram_direction</code> \u2192 <code>display.diagram_direction</code>.</p>"},{"location":"reference/configuration/#api-reference","title":"API Reference","text":"<p>See Settings API for programmatic access.</p>"},{"location":"reference/definition-syntax/","title":"Table Definition Syntax","text":"<p>DataJoint's declarative table definition language.</p>"},{"location":"reference/definition-syntax/#basic-structure","title":"Basic Structure","text":"<pre><code>@schema\nclass TableName(dj.Manual):\n    definition = \"\"\"\n    # Table comment\n    primary_attr1 : type    # comment\n    primary_attr2 : type    # comment\n    ---\n    secondary_attr1 : type  # comment\n    secondary_attr2 = default : type  # comment with default\n    \"\"\"\n</code></pre>"},{"location":"reference/definition-syntax/#grammar","title":"Grammar","text":"<pre><code>definition     = [comment] pk_section \"---\" secondary_section\npk_section     = attribute_line+\nsecondary_section = attribute_line*\n\nattribute_line = [foreign_key | attribute]\nforeign_key    = \"-&gt;\" [modifiers] table_reference [alias]\nmodifiers      = \"[\" modifier (\",\" modifier)* \"]\"\nmodifier       = \"nullable\" | \"unique\"\nattribute      = [default \"=\"] name \":\" type [# comment]\n\ndefault        = NULL | literal | CURRENT_TIMESTAMP\ntype           = core_type | codec_type | native_type\ncore_type      = int32 | float64 | varchar(n) | ...\ncodec_type     = \"&lt;\" name [\"@\" [store]] \"&gt;\"\n</code></pre>"},{"location":"reference/definition-syntax/#foreign-keys","title":"Foreign Keys","text":"<pre><code>-&gt; ParentTable                    # Inherit all PK attributes\n-&gt; ParentTable.proj(new='old')    # Rename attributes\n-&gt; [nullable] ParentTable         # Optional reference (secondary only)\n-&gt; [unique] ParentTable           # One-to-one constraint\n-&gt; [nullable, unique] ParentTable # Optional one-to-one\n</code></pre>"},{"location":"reference/definition-syntax/#modifiers","title":"Modifiers","text":"Modifier Effect Position <code>[nullable]</code> FK attributes can be NULL Secondary only <code>[unique]</code> Creates UNIQUE INDEX on FK Primary or secondary <code>[nullable, unique]</code> Optional one-to-one Secondary only <p>Note: Multiple rows can have NULL in a <code>[nullable, unique]</code> FK because SQL's UNIQUE constraint does not consider NULLs equal.</p>"},{"location":"reference/definition-syntax/#attribute-types","title":"Attribute Types","text":""},{"location":"reference/definition-syntax/#core-types","title":"Core Types","text":"<pre><code>mouse_id : int32                  # 32-bit integer\nweight : float64                  # 64-bit float\nname : varchar(100)               # Variable string up to 100 chars\nis_active : bool                  # Boolean\ncreated : datetime                # Date and time\ndata : json                       # JSON document\n</code></pre>"},{"location":"reference/definition-syntax/#codec-types","title":"Codec Types","text":"<pre><code>image : &lt;blob&gt;                    # Serialized Python object (in DB)\nlarge_array : &lt;blob@&gt;             # Serialized Python object (external)\nconfig_file : &lt;attach&gt;            # File attachment (in DB)\ndata_file : &lt;attach@archive&gt;      # File attachment (named store)\nzarr_data : &lt;object@&gt;             # Path-addressed folder\nraw_path : &lt;filepath@raw&gt;         # Portable file reference\n</code></pre>"},{"location":"reference/definition-syntax/#defaults","title":"Defaults","text":"<pre><code>status = \"pending\" : varchar(20)  # String default\ncount = 0 : int32                 # Numeric default\nnotes = '' : varchar(1000)        # Empty string default (preferred for strings)\ncreated = CURRENT_TIMESTAMP : datetime  # Auto-timestamp\nratio = NULL : float64            # Nullable (only NULL can be default)\n</code></pre> <p>Nullable attributes: An attribute is nullable if and only if its default is <code>NULL</code>. DataJoint does not allow other defaults for nullable attributes\u2014this prevents ambiguity about whether an attribute is optional. For strings, prefer empty string <code>''</code> as the default rather than <code>NULL</code>.</p>"},{"location":"reference/definition-syntax/#comments","title":"Comments","text":"<pre><code># Table-level comment (first line)\nmouse_id : int32    # Inline attribute comment\n</code></pre>"},{"location":"reference/definition-syntax/#indexes","title":"Indexes","text":"<pre><code>definition = \"\"\"\n    ...\n    ---\n    ...\n    INDEX (attr1)                 # Single-column index\n    INDEX (attr1, attr2)          # Composite index\n    UNIQUE INDEX (email)          # Unique constraint\n    \"\"\"\n</code></pre>"},{"location":"reference/definition-syntax/#complete-example","title":"Complete Example","text":"<pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    # Experimental session\n    -&gt; Subject\n    session_idx : int32           # Session number for this subject\n    ---\n    session_date : date           # Date of session\n    -&gt; [nullable] Experimenter    # Optional experimenter\n    -&gt; [unique] Protocol          # Each protocol used at most once per session\n    notes = '' : varchar(1000)    # Session notes\n    start_time : datetime         # Session start\n    duration : float64            # Duration in minutes\n    INDEX (session_date)\n    \"\"\"\n</code></pre>"},{"location":"reference/definition-syntax/#validation","title":"Validation","text":"<p>DataJoint validates definitions at declaration time:</p> <ul> <li>Primary key must have at least one attribute</li> <li>Attribute names must be valid identifiers</li> <li>Types must be recognized</li> <li>Foreign key references must exist</li> <li>No circular dependencies allowed</li> </ul>"},{"location":"reference/definition-syntax/#see-also","title":"See Also","text":"<ul> <li>Primary Keys \u2014 Key determination rules</li> <li>Type System \u2014 Type architecture</li> <li>Codec API \u2014 Custom types</li> </ul>"},{"location":"reference/errors/","title":"Error Reference","text":"<p>DataJoint exception classes and their meanings.</p>"},{"location":"reference/errors/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>Exception\n\u2514\u2500\u2500 DataJointError\n    \u251c\u2500\u2500 LostConnectionError\n    \u251c\u2500\u2500 QueryError\n    \u2502   \u251c\u2500\u2500 QuerySyntaxError\n    \u2502   \u251c\u2500\u2500 AccessError\n    \u2502   \u251c\u2500\u2500 DuplicateError\n    \u2502   \u251c\u2500\u2500 IntegrityError\n    \u2502   \u251c\u2500\u2500 UnknownAttributeError\n    \u2502   \u2514\u2500\u2500 MissingAttributeError\n    \u251c\u2500\u2500 MissingTableError\n    \u251c\u2500\u2500 MissingExternalFile\n    \u2514\u2500\u2500 BucketInaccessible\n</code></pre>"},{"location":"reference/errors/#base-exception","title":"Base Exception","text":""},{"location":"reference/errors/#datajointerror","title":"DataJointError","text":"<p>Base class for all DataJoint-specific errors.</p> <pre><code>try:\n    # DataJoint operation\nexcept dj.DataJointError as e:\n    print(f\"DataJoint error: {e}\")\n</code></pre>"},{"location":"reference/errors/#connection-errors","title":"Connection Errors","text":""},{"location":"reference/errors/#lostconnectionerror","title":"LostConnectionError","text":"<p>Database connection was lost during operation.</p> <p>Common causes: - Network interruption - Server timeout - Server restart</p> <p>Resolution: - Check network connectivity - Reconnect with <code>dj.conn().connect()</code></p>"},{"location":"reference/errors/#query-errors","title":"Query Errors","text":""},{"location":"reference/errors/#querysyntaxerror","title":"QuerySyntaxError","text":"<p>Invalid query syntax.</p> <p>Common causes: - Malformed restriction string - Invalid attribute reference - SQL syntax error in projection</p>"},{"location":"reference/errors/#accesserror","title":"AccessError","text":"<p>Insufficient database privileges.</p> <p>Common causes: - User lacks SELECT/INSERT/DELETE privileges - Schema access not granted</p> <p>Resolution: - Contact database administrator - Check user grants</p>"},{"location":"reference/errors/#duplicateerror","title":"DuplicateError","text":"<p>Attempt to insert duplicate primary key.</p> <pre><code>try:\n    table.insert1({'id': 1, 'name': 'Alice'})\n    table.insert1({'id': 1, 'name': 'Bob'})  # Raises DuplicateError\nexcept dj.errors.DuplicateError:\n    print(\"Entry already exists\")\n</code></pre> <p>Resolution: - Use <code>insert(..., skip_duplicates=True)</code> - Use <code>insert(..., replace=True)</code> to update - Check if entry exists before inserting</p>"},{"location":"reference/errors/#integrityerror","title":"IntegrityError","text":"<p>Foreign key constraint violation.</p> <p>Common causes: - Inserting row with non-existent parent - Parent row deletion blocked by children</p> <p>Resolution: - Insert parent rows first - Use cascade delete for parent</p>"},{"location":"reference/errors/#unknownattributeerror","title":"UnknownAttributeError","text":"<p>Referenced attribute doesn't exist.</p> <pre><code># Raises UnknownAttributeError\ntable.to_arrays('nonexistent_column')\n</code></pre> <p>Resolution: - Check <code>table.heading</code> for available attributes - Verify spelling</p>"},{"location":"reference/errors/#missingattributeerror","title":"MissingAttributeError","text":"<p>Required attribute not provided in insert.</p> <pre><code># Raises MissingAttributeError if 'name' is required\ntable.insert1({'id': 1})  # Missing 'name'\n</code></pre> <p>Resolution: - Provide all required attributes - Set default values in definition</p>"},{"location":"reference/errors/#table-errors","title":"Table Errors","text":""},{"location":"reference/errors/#missingtableerror","title":"MissingTableError","text":"<p>Table not declared in database.</p> <p>Common causes: - Schema not created - Table class not instantiated - Database dropped</p> <p>Resolution: - Check schema exists: <code>schema.is_activated()</code> - Verify table declaration</p>"},{"location":"reference/errors/#storage-errors","title":"Storage Errors","text":""},{"location":"reference/errors/#missingexternalfile","title":"MissingExternalFile","text":"<p>External file managed by DataJoint is missing.</p> <p>Common causes: - File manually deleted from store - Store misconfigured - Network/permission issues</p> <p>Resolution: - Check store configuration - Verify file exists at expected path - Run garbage collection audit</p>"},{"location":"reference/errors/#bucketinaccessible","title":"BucketInaccessible","text":"<p>S3 bucket cannot be accessed.</p> <p>Common causes: - Invalid credentials - Bucket doesn't exist - Network/firewall issues</p> <p>Resolution: - Verify AWS credentials - Check bucket name and region - Test with AWS CLI</p>"},{"location":"reference/errors/#handling-errors","title":"Handling Errors","text":""},{"location":"reference/errors/#catching-specific-errors","title":"Catching Specific Errors","text":"<pre><code>import datajoint as dj\n\ntry:\n    table.insert1(data)\nexcept dj.errors.DuplicateError:\n    print(\"Entry exists, skipping\")\nexcept dj.errors.IntegrityError:\n    print(\"Parent entry missing\")\nexcept dj.DataJointError as e:\n    print(f\"Other DataJoint error: {e}\")\n</code></pre>"},{"location":"reference/errors/#error-information","title":"Error Information","text":"<pre><code>try:\n    table.insert1(data)\nexcept dj.DataJointError as e:\n    print(f\"Error type: {type(e).__name__}\")\n    print(f\"Message: {e}\")\n    print(f\"Args: {e.args}\")\n</code></pre>"},{"location":"reference/errors/#see-also","title":"See Also","text":"<ul> <li>API: errors module</li> </ul>"},{"location":"reference/operators/","title":"Query Operators Reference","text":"<p>DataJoint provides a small set of operators for querying data. All operators return new query expressions without modifying the original\u2014queries are immutable and composable.</p>"},{"location":"reference/operators/#operator-summary","title":"Operator Summary","text":"Operator Syntax Description Restriction <code>A &amp; condition</code> Select rows matching condition Anti-restriction <code>A - condition</code> Select rows NOT matching condition Projection <code>A.proj(...)</code> Select, rename, or compute attributes Join <code>A * B</code> Combine tables on matching attributes Extension <code>A.extend(B)</code> Add attributes from B, keeping all rows of A Aggregation <code>A.aggr(B, ...)</code> Group B by A's primary key and compute summaries Union <code>A + B</code> Combine entity sets"},{"location":"reference/operators/#restriction","title":"Restriction (<code>&amp;</code>)","text":"<p>Select rows that match a condition.</p> <pre><code># String condition (SQL expression)\nSession &amp; \"session_date &gt; '2024-01-01'\"\nSession &amp; \"duration BETWEEN 30 AND 60\"\n\n# Dictionary (exact match)\nSession &amp; {'subject_id': 'M001'}\nSession &amp; {'subject_id': 'M001', 'session_idx': 1}\n\n# Query expression (matching keys)\nSession &amp; Subject                    # Sessions for subjects in Subject table\nSession &amp; (Subject &amp; \"sex = 'M'\")    # Sessions for male subjects\n\n# List (OR of conditions)\nSession &amp; [{'subject_id': 'M001'}, {'subject_id': 'M002'}]\n</code></pre> <p>Chaining: Multiple restrictions combine with AND: <pre><code>Session &amp; \"duration &gt; 30\" &amp; {\"experimenter\": \"alice\"}\n</code></pre></p>"},{"location":"reference/operators/#top-n-rows-djtop","title":"Top N Rows (<code>dj.Top</code>)","text":"<p>Restrict to the top N rows with optional ordering:</p> <pre><code># First row by primary key\nSession &amp; dj.Top()\n\n# First 10 rows by primary key (ascending)\nSession &amp; dj.Top(10)\n\n# First 10 rows by primary key (descending)\nSession &amp; dj.Top(10, 'KEY DESC')\n\n# Top 5 by score descending\nResult &amp; dj.Top(5, 'score DESC')\n\n# Top 10 most recent sessions\nSession &amp; dj.Top(10, 'session_date DESC')\n\n# Pagination: skip 20, take 10\nSession &amp; dj.Top(10, 'session_date DESC', offset=20)\n\n# All rows ordered (no limit)\nSession &amp; dj.Top(None, 'session_date DESC')\n</code></pre> <p>Parameters: - <code>limit</code> (default=1): Maximum rows. Use <code>None</code> for no limit. - <code>order_by</code> (default=\"KEY\"): Attribute(s) to sort by. <code>\"KEY\"</code> expands to all primary key attributes. Add <code>DESC</code> for descending order (e.g., <code>\"KEY DESC\"</code>, <code>\"score DESC\"</code>). Use <code>None</code> to inherit existing order. - <code>offset</code> (default=0): Rows to skip.</p> <p>Chaining Tops: When chaining multiple Top restrictions, the second Top can inherit the first's ordering by using <code>order_by=None</code>:</p> <pre><code># First Top sets the order, second inherits it\n(Session &amp; dj.Top(100, 'date DESC')) &amp; dj.Top(10, order_by=None)\n# Result: top 10 of top 100 by date descending\n</code></pre> <p>Note: <code>dj.Top</code> can only be used with restriction (<code>&amp;</code>), not with anti-restriction (<code>-</code>).</p>"},{"location":"reference/operators/#anti-restriction-","title":"Anti-Restriction (<code>-</code>)","text":"<p>Select rows that do NOT match a condition.</p> <pre><code># Subjects without any sessions\nSubject - Session\n\n# Sessions not from subject M001\nSession - {'subject_id': 'M001'}\n\n# Sessions without trials\nSession - Trial\n</code></pre>"},{"location":"reference/operators/#projection-proj","title":"Projection (<code>.proj()</code>)","text":"<p>Select, rename, or compute attributes. Primary key is always included.</p> <pre><code># Primary key only\nSubject.proj()\n\n# Specific attributes\nSubject.proj('species', 'sex')\n\n# All attributes\nSubject.proj(...)\n\n# All except some\nSubject.proj(..., '-notes', '-internal_id')\n\n# Rename attribute\nSubject.proj(animal_species='species')\n\n# Computed attribute (SQL expression)\nSubject.proj(weight_kg='weight / 1000')\nSession.proj(year='YEAR(session_date)')\nTrial.proj(is_correct='response = stimulus')\n</code></pre>"},{"location":"reference/operators/#join","title":"Join (<code>*</code>)","text":"<p>Combine tables on shared attributes. DataJoint matches attributes by semantic matching\u2014only attributes with the same name AND same origin (through foreign keys) are matched.</p> <pre><code># Join Subject and Session on subject_id\nSubject * Session\n\n# Three-way join\nSubject * Session * Experimenter\n\n# Join then restrict\n(Subject * Session) &amp; \"sex = 'M'\"\n\n# Restrict then join (equivalent)\n(Subject &amp; \"sex = 'M'\") * Session\n</code></pre> <p>Primary key of result: Determined by functional dependencies between operands. See Query Algebra Specification for details.</p>"},{"location":"reference/operators/#extension-extend","title":"Extension (<code>.extend()</code>)","text":"<p>Add attributes from another table while preserving all rows. This is useful for adding optional attributes.</p> <pre><code># Add experimenter info to sessions\n# Sessions without an experimenter get NULL values\nSession.extend(Experimenter)\n</code></pre> <p>Requirement: The left operand must \"determine\" the right operand\u2014all of B's primary key attributes must exist in A.</p>"},{"location":"reference/operators/#aggregation-aggr","title":"Aggregation (<code>.aggr()</code>)","text":"<p>Group one entity type by another and compute summary statistics.</p> <pre><code># Count trials per session\nSession.aggr(Session.Trial, n_trials='count(trial_idx)')\n\n# Multiple aggregates\nSession.aggr(\n    Session.Trial,\n    n_trials='count(trial_idx)',\n    n_correct='sum(correct)',\n    avg_rt='avg(reaction_time)',\n    min_rt='min(reaction_time)',\n    max_rt='max(reaction_time)'\n)\n\n# Count sessions per subject\nSubject.aggr(Session, n_sessions='count(session_idx)')\n</code></pre> <p>Default behavior: Keeps all rows from the grouping table (left operand), even those without matches. Use <code>count(pk_attribute)</code> to get 0 for entities without matches.</p> <pre><code># All subjects, including those with 0 sessions\nSubject.aggr(Session, n_sessions='count(session_idx)')\n\n# Only subjects with at least one session\nSubject.aggr(Session, n_sessions='count(session_idx)', exclude_nonmatching=True)\n</code></pre>"},{"location":"reference/operators/#common-aggregate-functions","title":"Common Aggregate Functions","text":"Function Description <code>count(attr)</code> Count non-NULL values <code>count(*)</code> Count all rows (including NULL) <code>sum(attr)</code> Sum of values <code>avg(attr)</code> Average <code>min(attr)</code> Minimum <code>max(attr)</code> Maximum <code>std(attr)</code> Standard deviation <code>group_concat(attr)</code> Concatenate values"},{"location":"reference/operators/#union","title":"Union (<code>+</code>)","text":"<p>Combine entity sets from two tables with the same primary key.</p> <pre><code># All subjects that are either mice or rats\nMouse + Rat\n</code></pre> <p>Requirements: - Same primary key attributes - No overlapping secondary attributes</p>"},{"location":"reference/operators/#universal-set-dju","title":"Universal Set (<code>dj.U()</code>)","text":"<p>Create ad-hoc groupings or extract unique values.</p>"},{"location":"reference/operators/#unique-values","title":"Unique Values","text":"<pre><code># Unique species\ndj.U('species') &amp; Subject\n\n# Unique (year, month) combinations\ndj.U('year', 'month') &amp; Session.proj(year='YEAR(session_date)', month='MONTH(session_date)')\n</code></pre>"},{"location":"reference/operators/#aggregation-by-non-primary-key-attributes","title":"Aggregation by Non-Primary-Key Attributes","text":"<pre><code># Count sessions by date (session_date is not a primary key)\ndj.U('session_date').aggr(Session, n='count(session_idx)')\n\n# Count by experimenter\ndj.U('experimenter_id').aggr(Session, n='count(session_idx)')\n</code></pre>"},{"location":"reference/operators/#universal-aggregation-single-row-result","title":"Universal Aggregation (Single Row Result)","text":"<pre><code># Total count across all sessions\ndj.U().aggr(Session, total='count(*)')\n\n# Global statistics\ndj.U().aggr(Trial,\n    total='count(*)',\n    avg_rt='avg(reaction_time)',\n    std_rt='std(reaction_time)'\n)\n</code></pre>"},{"location":"reference/operators/#operator-precedence","title":"Operator Precedence","text":"<p>Python operator precedence applies:</p> Precedence Operator Operation Highest <code>*</code> Join <code>+</code>, <code>-</code> Union, Anti-restriction Lowest <code>&amp;</code> Restriction <p>Use parentheses to make intent clear:</p> <pre><code># Join happens before restriction\nSubject * Session &amp; condition    # Same as: (Subject * Session) &amp; condition\n\n# Use parentheses to restrict first\n(Subject &amp; condition) * Session\n</code></pre>"},{"location":"reference/operators/#semantic-matching","title":"Semantic Matching","text":"<p>DataJoint uses semantic matching for joins and restrictions by query expression. Attributes match only if they have:</p> <ol> <li>The same name</li> <li>The same origin (traced through foreign key lineage)</li> </ol> <p>This prevents accidental matches on attributes that happen to share names but represent different things (like generic <code>id</code> columns in unrelated tables).</p> <pre><code># These match on subject_id because Session references Subject\nSubject * Session  # Correct: subject_id has same lineage\n\n# These would error if both have 'name' from different origins\nStudent * Course   # Error if both define their own 'name' attribute\n</code></pre> <p>Resolution: Rename attributes to avoid conflicts: <pre><code>Student * Course.proj(..., course_name='name')\n</code></pre></p>"},{"location":"reference/operators/#see-also","title":"See Also","text":"<ul> <li>Query Algebra Specification \u2014 Complete formal specification</li> <li>Fetch API \u2014 Retrieving query results</li> <li>Queries Tutorial \u2014 Hands-on examples</li> </ul>"},{"location":"reference/specs/","title":"Specifications","text":"<p>Formal specifications of DataJoint's data model and behavior.</p> <p>These documents define how DataJoint works at a detailed level. They serve as authoritative references for:</p> <ul> <li>Understanding exact behavior of operations</li> <li>Implementing compatible tools and extensions</li> <li>Debugging complex scenarios</li> </ul>"},{"location":"reference/specs/#how-to-use-these-specifications","title":"How to Use These Specifications","text":"<p>If you're new to DataJoint: Start with the tutorials and how-to guides before diving into specifications. Specs are technical references, not learning materials.</p> <p>If you're implementing features: Use specs as authoritative sources for behavior. Start with dependencies (see below) and work up to your target specification.</p> <p>If you're debugging: Specs clarify exact behavior when documentation or examples are ambiguous.</p>"},{"location":"reference/specs/#reading-order","title":"Reading Order","text":""},{"location":"reference/specs/#start-here","title":"Start Here","text":"<ol> <li>Database Backends \u2014 Supported databases (MySQL, PostgreSQL)</li> <li>Table Declaration \u2014 How to define tables</li> <li>Primary Keys \u2014 Key propagation rules</li> <li>Type System \u2014 Three-layer type architecture</li> </ol> <p>Next: Choose based on your needs: - Working with data? \u2192 Data Operations - Building queries? \u2192 Query Algebra - Using large data? \u2192 Object Storage</p>"},{"location":"reference/specs/#query-algebra","title":"Query Algebra","text":"<p>Prerequisites: Table Declaration, Primary Keys</p> <ol> <li>Query Operators \u2014 Restrict, proj, join, aggr, union</li> <li>Semantic Matching \u2014 Attribute lineage</li> <li>Fetch API \u2014 Data retrieval</li> </ol>"},{"location":"reference/specs/#data-operations","title":"Data Operations","text":"<p>Prerequisites: Table Declaration</p> <ol> <li>Data Manipulation \u2014 Insert, update, delete</li> <li>AutoPopulate \u2014 Jobs 2.0 system</li> <li>Job Metadata \u2014 Hidden job tracking columns</li> </ol>"},{"location":"reference/specs/#object-storage","title":"Object Storage","text":"<p>Prerequisites: Type System</p> <ol> <li>Object Store Configuration \u2014 Store setup</li> <li>Codec API \u2014 Custom type implementation</li> <li><code>&lt;npy&gt;</code> Codec \u2014 NumPy array storage</li> </ol>"},{"location":"reference/specs/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>Master-Part Relationships \u2014 Compositional modeling</li> <li>Virtual Schemas \u2014 Schema introspection without source</li> </ol>"},{"location":"reference/specs/#document-structure","title":"Document Structure","text":"<p>Each specification follows a consistent structure:</p> <ol> <li>Overview \u2014 What this specifies</li> <li>User Guide \u2014 Practical usage</li> <li>API Reference \u2014 Methods and signatures</li> <li>Concepts \u2014 Definitions and rules</li> <li>Implementation Details \u2014 Internal behavior</li> <li>Examples \u2014 Concrete code samples</li> <li>Best Practices \u2014 Recommendations</li> </ol>"},{"location":"reference/specs/#specifications-by-topic","title":"Specifications by Topic","text":""},{"location":"reference/specs/#schema-definition","title":"Schema Definition","text":"Specification Prerequisites Related How-To Related Explanation Table Declaration None Define Tables Relational Workflow Model Master-Part Relationships Table Declaration Model Relationships Data Pipelines Virtual Schemas Table Declaration \u2014 \u2014 <p>Key concepts: Table tiers (Manual, Lookup, Imported, Computed, Part), foreign keys, dependency graphs, compositional modeling</p>"},{"location":"reference/specs/#query-algebra_1","title":"Query Algebra","text":"Specification Prerequisites Related How-To Related Explanation Query Operators Table Declaration, Primary Keys Query Data Query Algebra Semantic Matching Query Operators Model Relationships Query Algebra Primary Keys Table Declaration Design Primary Keys Entity Integrity Fetch API Query Operators Fetch Results \u2014 Diagram Table Declaration Read Diagrams \u2014 <p>Key concepts: Restriction (<code>&amp;</code>, <code>-</code>), projection (<code>.proj()</code>), join (<code>*</code>), aggregation (<code>.aggr()</code>), union, universal set (<code>U()</code>), attribute lineage, schema visualization</p>"},{"location":"reference/specs/#type-system","title":"Type System","text":"Specification Prerequisites Related How-To Related Explanation Type System None Choose a Storage Type Type System Codec API Type System Create Custom Codec Custom Codecs <code>&lt;npy&gt;</code> Codec Type System Use Object Storage \u2014 <p>Key concepts: Native types (MySQL), core types (portable), codec types (Python objects), in-table vs object storage, addressing schemes</p>"},{"location":"reference/specs/#object-storage_1","title":"Object Storage","text":"Specification Prerequisites Related How-To Related Explanation Object Store Configuration Type System Configure Object Storage Data Pipelines (OAS) <p>Key concepts: Hash-addressed storage (deduplication), schema-addressed storage (browsable paths), filepath storage (user-managed), store configuration, path generation</p>"},{"location":"reference/specs/#data-operations_1","title":"Data Operations","text":"Specification Prerequisites Related How-To Related Explanation Data Manipulation Table Declaration Insert Data Normalization AutoPopulate Table Declaration, Data Manipulation Run Computations, Distributed Computing Computation Model Job Metadata AutoPopulate Handle Errors Computation Model <p>Key concepts: Insert patterns, transactional integrity, workflow normalization, Jobs 2.0, job coordination, populate(), make() method, job states</p>"},{"location":"reference/specs/autopopulate/","title":"AutoPopulate Specification","text":""},{"location":"reference/specs/autopopulate/#overview","title":"Overview","text":"<p>AutoPopulate is DataJoint's mechanism for automated computation. Tables that inherit from <code>dj.Computed</code> or <code>dj.Imported</code> automatically populate themselves by executing a <code>make()</code> method for each entry defined by their dependencies.</p> <p>This specification covers: - The populate process and key source calculation - Transaction management and atomicity - The <code>make()</code> method and tripartite pattern - Part tables in computed results - Distributed computing with job reservation</p>"},{"location":"reference/specs/autopopulate/#1-auto-populated-tables","title":"1. Auto-Populated Tables","text":""},{"location":"reference/specs/autopopulate/#11-table-types","title":"1.1 Table Types","text":"Type Base Class Purpose Computed <code>dj.Computed</code> Results derived from other DataJoint tables Imported <code>dj.Imported</code> Data ingested from external sources (files, instruments) <p>Both types share the same AutoPopulate mechanism. The distinction is semantic\u2014<code>Imported</code> indicates external data sources while <code>Computed</code> indicates derivation from existing tables.</p>"},{"location":"reference/specs/autopopulate/#12-basic-structure","title":"1.2 Basic Structure","text":"<pre><code>@schema\nclass FilteredImage(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawImage\n    ---\n    filtered : &lt;blob&gt;\n    \"\"\"\n\n    def make(self, key):\n        # Fetch source data\n        raw = (RawImage &amp; key).fetch1('image')\n\n        # Compute result\n        filtered = apply_filter(raw)\n\n        # Insert result\n        self.insert1({**key, 'filtered': filtered})\n</code></pre>"},{"location":"reference/specs/autopopulate/#13-primary-key-constraint","title":"1.3 Primary Key Constraint","text":"<p>Auto-populated tables must have primary keys composed entirely of foreign key references:</p> <pre><code># Correct: all PK attributes from foreign keys\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    -&gt; AnalysisMethod\n    ---\n    result : float64\n    \"\"\"\n\n# Error: non-FK primary key attribute\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    method : varchar(32)   # Not allowed - use FK to lookup table\n    ---\n    result : float64\n    \"\"\"\n</code></pre> <p>Rationale: This ensures each computed entry is uniquely determined by its upstream dependencies, enabling automatic key source calculation and precise job tracking.</p>"},{"location":"reference/specs/autopopulate/#2-key-source-calculation","title":"2. Key Source Calculation","text":""},{"location":"reference/specs/autopopulate/#21-definition","title":"2.1 Definition","text":"<p>The <code>key_source</code> property defines which entries should exist in the table\u2014the complete set of primary keys that <code>make()</code> should be called with.</p>"},{"location":"reference/specs/autopopulate/#22-automatic-key-source","title":"2.2 Automatic Key Source","text":"<p>By default, DataJoint automatically calculates <code>key_source</code> as the join of all tables referenced by foreign keys in the primary key:</p> <pre><code>@schema\nclass SpikeDetection(dj.Computed):\n    definition = \"\"\"\n    -&gt; Recording\n    -&gt; DetectionMethod\n    ---\n    spike_times : &lt;blob&gt;\n    \"\"\"\n    # Automatic key_source = Recording * DetectionMethod\n</code></pre> <p>Calculation rules: 1. Identify all foreign keys in the primary key section 2. Join the referenced tables: <code>Parent1 * Parent2 * ...</code> 3. Project to primary key attributes only</p> <p>For a table with definition: <pre><code>-&gt; Session\n-&gt; Probe\n-&gt; SortingMethod\n---\nunits : &lt;blob&gt;\n</code></pre></p> <p>The automatic <code>key_source</code> is: <pre><code>Session * Probe * SortingMethod\n</code></pre></p> <p>This produces all valid combinations of (session, probe, method) that could be computed.</p>"},{"location":"reference/specs/autopopulate/#23-custom-key-source","title":"2.3 Custom Key Source","text":"<p>Override <code>key_source</code> to customize which entries to compute:</p> <pre><code>@schema\nclass QualityAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    score : float64\n    \"\"\"\n\n    @property\n    def key_source(self):\n        # Only process sessions marked as 'good'\n        return Session &amp; \"quality = 'good'\"\n</code></pre> <p>Common customizations:</p> <pre><code># Filter by condition\n@property\ndef key_source(self):\n    return Session &amp; \"status = 'complete'\"\n\n# Restrict to specific combinations\n@property\ndef key_source(self):\n    return Recording * Method &amp; \"method_name != 'deprecated'\"\n\n# Add complex logic\n@property\ndef key_source(self):\n    # Only sessions with enough trials\n    good_sessions = dj.U('session_id').aggr(\n        Trial, n='count(*)') &amp; 'n &gt;= 100'\n    return Session &amp; good_sessions\n</code></pre>"},{"location":"reference/specs/autopopulate/#24-pending-entries","title":"2.4 Pending Entries","text":"<p>Entries to be computed = <code>key_source - self</code>:</p> <pre><code># Entries that should exist but don't yet\npending = table.key_source - table\n\n# Check how many entries need computing\nn_pending = len(table.key_source - table)\n</code></pre>"},{"location":"reference/specs/autopopulate/#3-the-populate-process","title":"3. The Populate Process","text":""},{"location":"reference/specs/autopopulate/#31-basic-populate","title":"3.1 Basic Populate","text":"<p>The <code>populate()</code> method iterates through pending entries and calls <code>make()</code> for each:</p> <pre><code># Populate all pending entries\nFilteredImage.populate()\n</code></pre> <p>Execution flow (direct mode):</p> <pre><code>1. Calculate pending keys: key_source - self\n2. Apply restrictions: pending &amp; restrictions\n3. For each key in pending:\n   a. Start transaction\n   b. Call make(key)\n   c. Commit transaction (or rollback on error)\n4. Return summary\n</code></pre>"},{"location":"reference/specs/autopopulate/#32-method-signature","title":"3.2 Method Signature","text":"<pre><code>def populate(\n    self,\n    *restrictions,\n    suppress_errors: bool = False,\n    return_exception_objects: bool = False,\n    reserve_jobs: bool = False,\n    max_calls: int = None,\n    display_progress: bool = False,\n    processes: int = 1,\n    make_kwargs: dict = None,\n    priority: int = None,\n    refresh: bool = None,\n) -&gt; dict\n</code></pre>"},{"location":"reference/specs/autopopulate/#33-parameters","title":"3.3 Parameters","text":"Parameter Default Description <code>*restrictions</code> \u2014 Filter <code>key_source</code> to subset of entries <code>suppress_errors</code> <code>False</code> Continue on errors instead of raising <code>return_exception_objects</code> <code>False</code> Return exception objects vs strings <code>reserve_jobs</code> <code>False</code> Enable job reservation for distributed computing <code>max_calls</code> <code>None</code> Maximum number of <code>make()</code> calls <code>display_progress</code> <code>False</code> Show progress bar <code>processes</code> <code>1</code> Number of parallel worker processes <code>make_kwargs</code> <code>None</code> Additional kwargs passed to <code>make()</code> <code>priority</code> <code>None</code> Process only jobs at this priority or more urgent <code>refresh</code> <code>None</code> Refresh jobs queue before processing"},{"location":"reference/specs/autopopulate/#34-common-usage-patterns","title":"3.4 Common Usage Patterns","text":"<pre><code># Populate everything\nAnalysis.populate()\n\n# Populate specific subjects\nAnalysis.populate(Subject &amp; \"subject_id &lt; 10\")\n\n# Populate with progress bar\nAnalysis.populate(display_progress=True)\n\n# Populate limited batch\nAnalysis.populate(max_calls=100)\n\n# Populate with error collection\nerrors = Analysis.populate(suppress_errors=True)\n\n# Parallel populate (single machine)\nAnalysis.populate(processes=4)\n</code></pre>"},{"location":"reference/specs/autopopulate/#35-return-value","title":"3.5 Return Value","text":"<pre><code>result = Analysis.populate()\n# {\n#     'success': 150,    # Entries successfully computed\n#     'error': 3,        # Entries that failed\n#     'skip': 0,         # Entries skipped (already exist)\n# }\n</code></pre>"},{"location":"reference/specs/autopopulate/#4-the-make-method","title":"4. The make() Method","text":""},{"location":"reference/specs/autopopulate/#41-basic-pattern","title":"4.1 Basic Pattern","text":"<p>The <code>make()</code> method computes and inserts one entry:</p> <pre><code>def make(self, key):\n    \"\"\"\n    Compute and insert one entry.\n\n    Parameters\n    ----------\n    key : dict\n        Primary key values identifying which entry to compute.\n    \"\"\"\n    # 1. Fetch source data\n    source_data = (SourceTable &amp; key).fetch1()\n\n    # 2. Compute result\n    result = compute(source_data)\n\n    # 3. Insert result\n    self.insert1({**key, **result})\n</code></pre>"},{"location":"reference/specs/autopopulate/#42-requirements","title":"4.2 Requirements","text":"<ul> <li>Must insert: <code>make()</code> must insert exactly one row matching the key</li> <li>Idempotent: Same input should produce same output</li> <li>Atomic: Runs within a transaction\u2014all or nothing</li> <li>Self-contained: Should not depend on external state that changes</li> </ul>"},{"location":"reference/specs/autopopulate/#43-accessing-source-data","title":"4.3 Accessing Source Data","text":"<pre><code>def make(self, key):\n    # Fetch single row\n    data = (SourceTable &amp; key).fetch1()\n\n    # Fetch specific attributes\n    image, timestamp = (Recording &amp; key).fetch1('image', 'timestamp')\n\n    # Fetch multiple rows (e.g., trials for a session)\n    trials = (Trial &amp; key).to_dicts()\n\n    # Join multiple sources\n    combined = (TableA * TableB &amp; key).to_dicts()\n</code></pre> <p>Upstream-only convention: Inside <code>make()</code>, fetch only from tables that are strictly upstream in the pipeline\u2014tables referenced by foreign keys in the definition, their ancestors, and their part tables. This ensures reproducibility: computed results depend only on their declared dependencies.</p> <p>This convention is not currently enforced programmatically but is critical for pipeline integrity. Some pipelines violate this rule for operational reasons, which makes them non-reproducible. A future release may programmatically enforce upstream-only fetches inside <code>make()</code>.</p>"},{"location":"reference/specs/autopopulate/#44-tripartite-make-pattern","title":"4.4 Tripartite Make Pattern","text":"<p>For long-running computations, use the tripartite pattern to separate fetch, compute, and insert phases. This enables better transaction management for jobs that take minutes or hours.</p> <p>Method-based tripartite:</p> <pre><code>@schema\nclass HeavyComputation(dj.Computed):\n    definition = \"\"\"\n    -&gt; Recording\n    ---\n    result : &lt;blob&gt;\n    \"\"\"\n\n    def make_fetch(self, key, **kwargs):\n        \"\"\"Fetch all required data (runs in transaction).\n\n        kwargs are passed from populate(make_kwargs={...}).\n        \"\"\"\n        return (Recording &amp; key).fetch1('raw_data')\n\n    def make_compute(self, key, data):\n        \"\"\"Perform computation (runs outside transaction).\"\"\"\n        # Long-running computation - no database locks held\n        return heavy_algorithm(data)\n\n    def make_insert(self, key, result):\n        \"\"\"Insert results (runs in transaction).\"\"\"\n        self.insert1({**key, 'result': result})\n</code></pre> <p>Generator-based tripartite:</p> <pre><code>def make(self, key):\n    # Phase 1: Fetch (in transaction)\n    data = (Recording &amp; key).fetch1('raw_data')\n\n    yield  # Exit transaction, release locks\n\n    # Phase 2: Compute (outside transaction)\n    result = heavy_algorithm(data)  # May take hours\n\n    yield  # Re-enter transaction\n\n    # Phase 3: Insert (in transaction)\n    self.insert1({**key, 'result': result})\n</code></pre> <p>When to use tripartite: - Computation takes more than a few seconds - You want to avoid holding database locks during computation - Working with external resources (files, APIs) that may be slow</p>"},{"location":"reference/specs/autopopulate/#45-additional-make-arguments","title":"4.5 Additional make() Arguments","text":"<p>Pass extra arguments via <code>make_kwargs</code>:</p> <pre><code>@schema\nclass ConfigurableAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    result : float64\n    \"\"\"\n\n    def make(self, key, threshold=0.5, method='default'):\n        data = (Session &amp; key).fetch1('data')\n        result = analyze(data, threshold=threshold, method=method)\n        self.insert1({**key, 'result': result})\n\n# Call with custom parameters\nConfigurableAnalysis.populate(make_kwargs={'threshold': 0.8})\n</code></pre> <p>Tripartite pattern: When using the method-based tripartite pattern, <code>make_kwargs</code> are passed to <code>make_fetch()</code>:</p> <pre><code>def make_fetch(self, key, verbose=False, **kwargs):\n    if verbose:\n        print(f\"Fetching {key}\")\n    return (Source &amp; key).fetch1('data')\n</code></pre> <p>Anti-pattern warning: Passing arguments that affect the computed result breaks reproducibility\u2014all inputs should come from <code>fetch</code> calls inside <code>make()</code>. If a parameter affects results, it should be stored in a lookup table and referenced via foreign key.</p> <p>Acceptable use: Directives that don't affect results, such as: - <code>verbose=True</code> for logging - <code>gpu_id=0</code> for device selection - <code>n_workers=4</code> for parallelization</p>"},{"location":"reference/specs/autopopulate/#5-transaction-management","title":"5. Transaction Management","text":""},{"location":"reference/specs/autopopulate/#51-automatic-transactions","title":"5.1 Automatic Transactions","text":"<p>Each <code>make()</code> call runs within an automatic transaction:</p> <pre><code># Pseudocode for populate loop\nfor key in pending_keys:\n    connection.start_transaction()\n    try:\n        self.make(key)\n        connection.commit()\n    except Exception:\n        connection.rollback()\n        raise  # or log if suppress_errors=True\n</code></pre>"},{"location":"reference/specs/autopopulate/#52-atomicity-guarantees","title":"5.2 Atomicity Guarantees","text":"<ul> <li>All or nothing: If <code>make()</code> fails, no partial data is inserted</li> <li>Isolation: Concurrent workers see consistent state</li> <li>Rollback on error: Any exception rolls back the transaction</li> </ul> <pre><code>def make(self, key):\n    # If this succeeds...\n    self.insert1({**key, 'step1': result1})\n\n    # But this fails...\n    self.Part.insert(part_data)  # Raises exception\n\n    # Both inserts are rolled back - table unchanged\n</code></pre>"},{"location":"reference/specs/autopopulate/#53-transaction-scope","title":"5.3 Transaction Scope","text":"<p>Simple make (single transaction): <pre><code>BEGIN TRANSACTION\n  \u2514\u2500\u2500 make(key)\n       \u251c\u2500\u2500 fetch source data\n       \u251c\u2500\u2500 compute\n       \u2514\u2500\u2500 insert result\nCOMMIT\n</code></pre></p> <p>Tripartite make (single transaction): <pre><code>[No transaction]\n  \u251c\u2500\u2500 make_fetch(key)           # Fetch source data\n  \u2514\u2500\u2500 make_compute(key, data)   # Long-running computation\n\nBEGIN TRANSACTION\n  \u251c\u2500\u2500 make_fetch(key)           # Repeat fetch, verify unchanged\n  \u2514\u2500\u2500 make_insert(key, result)  # Insert computed result\nCOMMIT\n</code></pre></p> <p>This pattern allows long computations without holding database locks, while ensuring data consistency by verifying the source data hasn't changed before inserting.</p>"},{"location":"reference/specs/autopopulate/#54-nested-operations","title":"5.4 Nested Operations","text":"<p>Inserts within <code>make()</code> share the same transaction:</p> <pre><code>def make(self, key):\n    # Main table insert\n    self.insert1({**key, 'summary': summary})\n\n    # Part table inserts - same transaction\n    self.Part1.insert(part1_data)\n    self.Part2.insert(part2_data)\n\n    # All three inserts commit together or roll back together\n</code></pre>"},{"location":"reference/specs/autopopulate/#55-manual-transaction-control","title":"5.5 Manual Transaction Control","text":"<p>For complex scenarios, use explicit transactions:</p> <pre><code>def make(self, key):\n    # Fetch outside transaction\n    data = (Source &amp; key).to_dicts()\n\n    # Explicit transaction for insert\n    with dj.conn().transaction:\n        self.insert1({**key, 'result': compute(data)})\n        self.Part.insert(parts)\n</code></pre>"},{"location":"reference/specs/autopopulate/#6-part-tables","title":"6. Part Tables","text":""},{"location":"reference/specs/autopopulate/#61-part-tables-in-computed-tables","title":"6.1 Part Tables in Computed Tables","text":"<p>Computed tables can have Part tables for detailed results:</p> <pre><code>@schema\nclass SpikeSorting(dj.Computed):\n    definition = \"\"\"\n    -&gt; Recording\n    ---\n    n_units : int\n    \"\"\"\n\n    class Unit(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        unit_id : int\n        ---\n        waveform : &lt;blob&gt;\n        spike_times : &lt;blob&gt;\n        \"\"\"\n\n    def make(self, key):\n        # Compute spike sorting\n        units = sort_spikes((Recording &amp; key).fetch1('data'))\n\n        # Insert master entry\n        self.insert1({**key, 'n_units': len(units)})\n\n        # Insert part entries\n        self.Unit.insert([\n            {**key, 'unit_id': i, **unit}\n            for i, unit in enumerate(units)\n        ])\n</code></pre>"},{"location":"reference/specs/autopopulate/#62-transaction-behavior","title":"6.2 Transaction Behavior","text":"<p>Master and part inserts share the same transaction:</p> <pre><code>def make(self, key):\n    self.insert1({**key, 'summary': s})  # Master\n    self.Part.insert(parts)               # Parts\n\n    # If Part.insert fails, master insert is also rolled back\n</code></pre>"},{"location":"reference/specs/autopopulate/#63-fetching-part-data","title":"6.3 Fetching Part Data","text":"<pre><code># Fetch master with parts\nmaster = (SpikeSorting &amp; key).fetch1()\nparts = (SpikeSorting.Unit &amp; key).to_dicts()\n\n# Join master and parts\ncombined = (SpikeSorting * SpikeSorting.Unit &amp; key).to_dicts()\n</code></pre>"},{"location":"reference/specs/autopopulate/#64-key-source-with-parts","title":"6.4 Key Source with Parts","text":"<p>The key source is based on the master table's primary key only:</p> <pre><code># key_source returns master keys, not part keys\nSpikeSorting.key_source  # Recording keys\n</code></pre>"},{"location":"reference/specs/autopopulate/#65-deleting-computed-parts","title":"6.5 Deleting Computed Parts","text":"<p>Deleting master entries cascades to parts:</p> <pre><code># Deletes SpikeSorting entry AND all SpikeSorting.Unit entries\n(SpikeSorting &amp; key).delete()\n</code></pre>"},{"location":"reference/specs/autopopulate/#7-progress-and-monitoring","title":"7. Progress and Monitoring","text":""},{"location":"reference/specs/autopopulate/#71-progress-method","title":"7.1 Progress Method","text":"<p>Check computation progress:</p> <pre><code># Simple progress\nremaining, total = Analysis.progress()\nprint(f\"{remaining}/{total} entries remaining\")\n\n# With display\nAnalysis.progress(display=True)\n# Analysis: 150/200 (75%) [===========&gt;    ]\n</code></pre>"},{"location":"reference/specs/autopopulate/#72-display-progress-during-populate","title":"7.2 Display Progress During Populate","text":"<pre><code>Analysis.populate(display_progress=True)\n# [################----] 80% 160/200 [00:15&lt;00:04]\n</code></pre>"},{"location":"reference/specs/autopopulate/#8-direct-mode-vs-distributed-mode","title":"8. Direct Mode vs Distributed Mode","text":""},{"location":"reference/specs/autopopulate/#81-direct-mode-default","title":"8.1 Direct Mode (Default)","text":"<p>When <code>reserve_jobs=False</code> (default):</p> <pre><code>Analysis.populate()  # Direct mode\n</code></pre> <p>Characteristics: - Calculates <code>key_source - self</code> on each call - No job tracking or status persistence - Simple and efficient for single-worker scenarios - No coordination overhead</p> <p>Best for: - Interactive development - Single-worker pipelines - Small to medium datasets</p>"},{"location":"reference/specs/autopopulate/#82-distributed-mode","title":"8.2 Distributed Mode","text":"<p>When <code>reserve_jobs=True</code>:</p> <pre><code>Analysis.populate(reserve_jobs=True)  # Distributed mode\n</code></pre> <p>Characteristics: - Uses per-table jobs queue for coordination - Workers reserve jobs before processing - Full status tracking (pending, reserved, error, success) - Enables monitoring and recovery</p> <p>Best for: - Multi-worker distributed computing - Long-running pipelines - Production environments with monitoring needs</p>"},{"location":"reference/specs/autopopulate/#9-per-table-jobs-system","title":"9. Per-Table Jobs System","text":""},{"location":"reference/specs/autopopulate/#91-jobs-table","title":"9.1 Jobs Table","text":"<p>Each auto-populated table has an associated jobs table:</p> <pre><code>Table: Analysis\nJobs:  ~~analysis\n</code></pre> <p>Access via the <code>.jobs</code> property:</p> <pre><code>Analysis.jobs              # Jobs table\nAnalysis.jobs.pending      # Pending jobs\nAnalysis.jobs.errors       # Failed jobs\nAnalysis.jobs.progress()   # Status summary\n</code></pre>"},{"location":"reference/specs/autopopulate/#92-jobs-table-structure","title":"9.2 Jobs Table Structure","text":"<pre><code># Job queue for Analysis\n&lt;primary key attributes from FK references&gt;\n---\nstatus : enum('pending', 'reserved', 'success', 'error', 'ignore')\npriority : int16                    # Lower = more urgent (0 = highest)\ncreated_time : timestamp\nscheduled_time : timestamp          # Process on or after this time\nreserved_time : timestamp           # When reserved\ncompleted_time : timestamp          # When completed\nduration : float64                  # Execution time in seconds\nerror_message : varchar(2047)       # Truncated error\nerror_stack : &lt;blob&gt;                # Full traceback\nuser : varchar(255)                 # Database user\nhost : varchar(255)                 # Worker hostname\npid : int64                        # Process ID\nconnection_id : int64              # MySQL connection ID\nversion : varchar(255)              # Code version\n</code></pre>"},{"location":"reference/specs/autopopulate/#93-job-statuses","title":"9.3 Job Statuses","text":"Status Description <code>pending</code> Queued and ready to process <code>reserved</code> Currently being processed by a worker <code>success</code> Completed successfully (when <code>jobs.keep_completed=True</code>) <code>error</code> Failed with error details <code>ignore</code> Manually marked to skip <pre><code>stateDiagram-v2\n    state \"(none)\" as none1\n    state \"(none)\" as none2\n    none1 --&gt; pending : refresh()\n    none1 --&gt; ignore : ignore()\n    pending --&gt; reserved : reserve()\n    reserved --&gt; none2 : complete()\n    reserved --&gt; success : complete()*\n    reserved --&gt; error : error()\n    success --&gt; pending : refresh()*\n    error --&gt; none2 : delete()\n    success --&gt; none2 : delete()\n    ignore --&gt; none2 : delete()\n</code></pre> <p>Transitions:</p> Method Description <code>refresh()</code> Adds new jobs as <code>pending</code>; re-pends <code>success</code> jobs if key is in <code>key_source</code> but not in target <code>ignore()</code> Marks a key as <code>ignore</code> (can be called on keys not yet in jobs table) <code>reserve()</code> Marks a <code>pending</code> job as <code>reserved</code> before calling <code>make()</code> <code>complete()</code> Deletes job (default) or marks as <code>success</code> (when <code>jobs.keep_completed=True</code>) <code>error()</code> Marks <code>reserved</code> job as <code>error</code> with message and stack trace <code>delete()</code> Removes job entry; use <code>(jobs &amp; condition).delete()</code> pattern <p>Notes:</p> <ul> <li><code>ignore</code> is set manually via <code>jobs.ignore(key)</code> and skipped by <code>populate()</code> and <code>refresh()</code></li> <li>To reset an ignored job: <code>jobs.ignored.delete(); jobs.refresh()</code></li> </ul>"},{"location":"reference/specs/autopopulate/#94-jobs-api","title":"9.4 Jobs API","text":"<pre><code># Refresh job queue (sync with key_source)\nAnalysis.jobs.refresh()\n\n# Status queries\nAnalysis.jobs.pending       # Pending jobs\nAnalysis.jobs.reserved      # Currently processing\nAnalysis.jobs.errors        # Failed jobs\nAnalysis.jobs.ignored       # Skipped jobs\nAnalysis.jobs.completed     # Success jobs (if kept)\n\n# Progress summary\nAnalysis.jobs.progress()\n# {'pending': 150, 'reserved': 3, 'success': 847, 'error': 12, 'total': 1012}\n\n# Manual control\nAnalysis.jobs.ignore(key)                    # Skip a job\n(Analysis.jobs &amp; condition).delete()         # Remove jobs\nAnalysis.jobs.errors.delete()                # Clear errors\n</code></pre>"},{"location":"reference/specs/autopopulate/#10-priority-and-scheduling","title":"10. Priority and Scheduling","text":""},{"location":"reference/specs/autopopulate/#101-priority","title":"10.1 Priority","text":"<p>Lower values = higher priority (0 is most urgent):</p> <pre><code># Urgent jobs (priority 0)\nAnalysis.jobs.refresh(priority=0)\n\n# Normal jobs (default priority 5)\nAnalysis.jobs.refresh()\n\n# Background jobs (priority 10)\nAnalysis.jobs.refresh(priority=10)\n\n# Urgent jobs for specific data\nAnalysis.jobs.refresh(Subject &amp; \"priority='urgent'\", priority=0)\n</code></pre>"},{"location":"reference/specs/autopopulate/#102-scheduling","title":"10.2 Scheduling","text":"<p>Delay job availability using server time:</p> <pre><code># Available in 2 hours\nAnalysis.jobs.refresh(delay=2*60*60)\n\n# Available tomorrow\nAnalysis.jobs.refresh(delay=24*60*60)\n</code></pre> <p>Jobs with <code>scheduled_time &gt; now</code> are not processed by <code>populate()</code>.</p>"},{"location":"reference/specs/autopopulate/#11-distributed-computing","title":"11. Distributed Computing","text":""},{"location":"reference/specs/autopopulate/#111-basic-pattern","title":"11.1 Basic Pattern","text":"<p>Multiple workers can run simultaneously:</p> <pre><code># Worker 1\nAnalysis.populate(reserve_jobs=True)\n\n# Worker 2 (different machine/process)\nAnalysis.populate(reserve_jobs=True)\n\n# Worker 3\nAnalysis.populate(reserve_jobs=True)\n</code></pre>"},{"location":"reference/specs/autopopulate/#112-execution-flow-distributed","title":"11.2 Execution Flow (Distributed)","text":"<pre><code>1. Refresh jobs queue (if auto_refresh=True)\n2. Fetch pending jobs ordered by (priority, scheduled_time)\n3. For each job:\n   a. Mark as 'reserved'\n   b. Start transaction\n   c. Call make(key)\n   d. Commit transaction\n   e. Mark as 'success' or delete job\n   f. On error: mark as 'error' with details\n</code></pre>"},{"location":"reference/specs/autopopulate/#113-conflict-resolution","title":"11.3 Conflict Resolution","text":"<p>When two workers reserve the same job simultaneously:</p> <ol> <li>Both reservations succeed (optimistic, no locking)</li> <li>Both call <code>make()</code> for the same key</li> <li>First worker's transaction commits</li> <li>Second worker gets duplicate key error (silently ignored)</li> <li>First worker marks job complete</li> </ol> <p>This is acceptable because: - The <code>make()</code> transaction guarantees data integrity - Conflicts are rare with job reservation - Wasted computation is minimal vs locking overhead</p>"},{"location":"reference/specs/autopopulate/#12-error-handling","title":"12. Error Handling","text":""},{"location":"reference/specs/autopopulate/#121-default-behavior","title":"12.1 Default Behavior","text":"<p>Errors stop populate and raise the exception:</p> <pre><code>Analysis.populate()  # Stops on first error\n</code></pre>"},{"location":"reference/specs/autopopulate/#122-suppressing-errors","title":"12.2 Suppressing Errors","text":"<p>Continue processing despite errors:</p> <pre><code>errors = Analysis.populate(\n    suppress_errors=True,\n    return_exception_objects=True\n)\n# errors contains list of (key, exception) tuples\n</code></pre>"},{"location":"reference/specs/autopopulate/#123-error-recovery-distributed-mode","title":"12.3 Error Recovery (Distributed Mode)","text":"<pre><code># View errors\nfor err in Analysis.jobs.errors.to_dicts():\n    print(f\"Key: {err}, Error: {err['error_message']}\")\n\n# Clear and retry\nAnalysis.jobs.errors.delete()\nAnalysis.jobs.refresh()\nAnalysis.populate(reserve_jobs=True)\n</code></pre>"},{"location":"reference/specs/autopopulate/#124-stale-and-orphaned-jobs","title":"12.4 Stale and Orphaned Jobs","text":"<p>Stale jobs: Keys no longer in <code>key_source</code> (upstream deleted) <pre><code>Analysis.jobs.refresh(stale_timeout=3600)  # Clean up after 1 hour\n</code></pre></p> <p>Orphaned jobs: Reserved jobs whose worker crashed <pre><code>Analysis.jobs.refresh(orphan_timeout=3600)  # Reset after 1 hour\n</code></pre></p>"},{"location":"reference/specs/autopopulate/#13-configuration","title":"13. Configuration","text":"<pre><code>dj.config['jobs.auto_refresh'] = True       # Auto-refresh on populate\ndj.config['jobs.keep_completed'] = False    # Retain success records\ndj.config['jobs.stale_timeout'] = 3600      # Seconds before stale cleanup\ndj.config['jobs.default_priority'] = 5      # Default priority (lower=urgent)\ndj.config['jobs.version'] = None            # Version string ('git' for auto)\ndj.config['jobs.add_job_metadata'] = False  # Add hidden metadata columns\n</code></pre>"},{"location":"reference/specs/autopopulate/#14-hidden-job-metadata","title":"14. Hidden Job Metadata","text":"<p>When <code>config['jobs.add_job_metadata'] = True</code>, auto-populated tables receive hidden columns:</p> Column Type Description <code>_job_start_time</code> <code>datetime(3)</code> When computation began <code>_job_duration</code> <code>float64</code> Duration in seconds <code>_job_version</code> <code>varchar(64)</code> Code version <pre><code># Fetch with job metadata\nAnalysis().to_arrays('result', '_job_duration')\n\n# Query slow computations\nslow = Analysis &amp; '_job_duration &gt; 3600'\n</code></pre>"},{"location":"reference/specs/autopopulate/#15-migration-from-legacy-datajoint","title":"15. Migration from Legacy DataJoint","text":"<p>DataJoint 2.0 replaces the schema-level <code>~jobs</code> table with per-table <code>~~table_name</code> jobs tables. See the Migration Guide for details.</p>"},{"location":"reference/specs/autopopulate/#16-quick-reference","title":"16. Quick Reference","text":""},{"location":"reference/specs/autopopulate/#161-common-operations","title":"16.1 Common Operations","text":"<pre><code># Basic populate (direct mode)\nTable.populate()\nTable.populate(restriction)\nTable.populate(max_calls=100, display_progress=True)\n\n# Distributed populate\nTable.populate(reserve_jobs=True)\n\n# Check progress\nremaining, total = Table.progress()\nTable.jobs.progress()  # Detailed status\n\n# Error handling\nTable.populate(suppress_errors=True)\nTable.jobs.errors.to_dicts()\nTable.jobs.errors.delete()\n\n# Priority control\nTable.jobs.refresh(priority=0)  # Urgent\nTable.jobs.refresh(delay=3600)  # Scheduled\n</code></pre>"},{"location":"reference/specs/autopopulate/#162-make-patterns","title":"16.2 make() Patterns","text":"<pre><code># Simple make\ndef make(self, key):\n    data = (Source &amp; key).fetch1()\n    self.insert1({**key, 'result': compute(data)})\n\n# With parts\ndef make(self, key):\n    self.insert1({**key, 'summary': s})\n    self.Part.insert(parts)\n\n# Tripartite (generator)\ndef make(self, key):\n    data = (Source &amp; key).fetch1()\n    yield  # Release transaction\n    result = heavy_compute(data)\n    yield  # Re-acquire transaction\n    self.insert1({**key, 'result': result})\n\n# Tripartite (methods) - kwargs passed to make_fetch\ndef make_fetch(self, key, **kwargs): return data\ndef make_compute(self, key, data): return result\ndef make_insert(self, key, result): self.insert1(...)\n</code></pre>"},{"location":"reference/specs/codec-api/","title":"Codec API Specification","text":"<p>This document specifies the DataJoint Codec API for creating custom attribute types. For the complete type system architecture (core types, built-in codecs, storage modes), see the Type System Specification.</p>"},{"location":"reference/specs/codec-api/#overview","title":"Overview","text":"<p>Codecs define bidirectional conversion between Python objects and database storage. They enable storing complex data types (graphs, models, custom formats) while maintaining DataJoint's query capabilities.</p> <pre><code>flowchart LR\n    A[\"Python Object(e.g. Graph)\"] -- encode --&gt; B[\"Storage Type(e.g. bytes)\"]\n    B -- decode --&gt; A\n</code></pre>"},{"location":"reference/specs/codec-api/#two-patterns-for-custom-codecs","title":"Two Patterns for Custom Codecs","text":"<p>There are two approaches for creating custom codecs:</p> Pattern When to Use Base Class Type Chaining Transform Python objects, use existing storage <code>dj.Codec</code> SchemaCodec Subclassing Custom file formats with schema-addressed paths <code>dj.SchemaCodec</code>"},{"location":"reference/specs/codec-api/#pattern-1-type-chaining-most-common","title":"Pattern 1: Type Chaining (Most Common)","text":"<p>Chain to an existing codec for storage. Your codec transforms objects; the chained codec handles storage.</p> <pre><code>import datajoint as dj\nimport networkx as nx\n\nclass GraphCodec(dj.Codec):\n    \"\"\"Store NetworkX graphs.\"\"\"\n\n    name = \"graph\"  # Use as &lt;graph&gt; in definitions\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"  # Delegate to blob for serialization\n\n    def encode(self, graph, *, key=None, store_name=None):\n        return {\n            'nodes': list(graph.nodes(data=True)),\n            'edges': list(graph.edges(data=True)),\n        }\n\n    def decode(self, stored, *, key=None):\n        G = nx.Graph()\n        G.add_nodes_from(stored['nodes'])\n        G.add_edges_from(stored['edges'])\n        return G\n\n# Use in table definition\n@schema\nclass Connectivity(dj.Manual):\n    definition = '''\n    conn_id : int\n    ---\n    network : &lt;graph&gt;      # in-table storage\n    network_ext : &lt;graph@&gt;  # object store\n    '''\n</code></pre>"},{"location":"reference/specs/codec-api/#pattern-2-schemacodec-subclassing-file-formats","title":"Pattern 2: SchemaCodec Subclassing (File Formats)","text":"<p>For custom file formats that need schema-addressed storage paths.</p> <pre><code>import datajoint as dj\n\nclass ParquetCodec(dj.SchemaCodec):\n    \"\"\"Store DataFrames as Parquet files.\"\"\"\n\n    name = \"parquet\"\n\n    # get_dtype inherited: returns \"json\", requires @\n\n    def encode(self, df, *, key=None, store_name=None):\n        import io\n        schema, table, field, pk = self._extract_context(key)\n        path, _ = self._build_path(schema, table, field, pk, ext=\".parquet\")\n        backend = self._get_backend(store_name)\n\n        buffer = io.BytesIO()\n        df.to_parquet(buffer)\n        backend.put_buffer(buffer.getvalue(), path)\n\n        return {\"path\": path, \"store\": store_name, \"shape\": list(df.shape)}\n\n    def decode(self, stored, *, key=None):\n        return ParquetRef(stored, self._get_backend(stored.get(\"store\")))\n\n# Use in table definition (store only)\n@schema\nclass Results(dj.Manual):\n    definition = '''\n    result_id : int\n    ---\n    data : &lt;parquet@&gt;\n    '''\n</code></pre>"},{"location":"reference/specs/codec-api/#the-codec-base-class","title":"The Codec Base Class","text":"<p>All custom codecs inherit from <code>dj.Codec</code>:</p> <pre><code>class Codec(ABC):\n    \"\"\"Base class for codec types.\"\"\"\n\n    name: str | None = None  # Required: unique identifier\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        \"\"\"Return the storage dtype.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def encode(self, value, *, key=None, store_name=None) -&gt; Any:\n        \"\"\"Encode Python value for storage.\"\"\"\n        ...\n\n    @abstractmethod\n    def decode(self, stored, *, key=None) -&gt; Any:\n        \"\"\"Decode stored value back to Python.\"\"\"\n        ...\n\n    def validate(self, value) -&gt; None:\n        \"\"\"Optional: validate value before encoding.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/specs/codec-api/#the-schemacodec-base-class","title":"The SchemaCodec Base Class","text":"<p>For schema-addressed storage (file formats), inherit from <code>dj.SchemaCodec</code>:</p> <pre><code>class SchemaCodec(Codec, register=False):\n    \"\"\"Base class for schema-addressed codecs.\"\"\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        \"\"\"Store only, returns 'json'.\"\"\"\n        if not is_store:\n            raise DataJointError(f\"&lt;{self.name}&gt; requires @ (store only)\")\n        return \"json\"\n\n    def _extract_context(self, key: dict) -&gt; tuple[str, str, str, dict]:\n        \"\"\"Parse key into (schema, table, field, primary_key).\"\"\"\n        ...\n\n    def _build_path(self, schema, table, field, pk, ext=None) -&gt; tuple[str, str]:\n        \"\"\"Build schema-addressed path: {schema}/{table}/{pk}/{field}{ext}\"\"\"\n        ...\n\n    def _get_backend(self, store_name: str = None):\n        \"\"\"Get storage backend by name.\"\"\"\n        ...\n</code></pre>"},{"location":"reference/specs/codec-api/#required-components","title":"Required Components","text":""},{"location":"reference/specs/codec-api/#1-the-name-attribute","title":"1. The <code>name</code> Attribute","text":"<p>The <code>name</code> class attribute is a unique identifier used in table definitions with <code>&lt;name&gt;</code> syntax:</p> <pre><code>class MyCodec(dj.Codec):\n    name = \"mycodec\"  # Use as &lt;mycodec&gt; in definitions\n</code></pre> <p>Naming conventions: - Use lowercase with underscores: <code>spike_train</code>, <code>graph_embedding</code> - Avoid generic names that might conflict: prefer <code>lab_model</code> over <code>model</code> - Names must be unique across all registered codecs</p>"},{"location":"reference/specs/codec-api/#2-the-get_dtype-method","title":"2. The <code>get_dtype()</code> Method","text":"<p>Returns the underlying storage type. The <code>is_store</code> parameter indicates whether the <code>@</code> modifier is present in the table definition:</p> <pre><code>def get_dtype(self, is_store: bool) -&gt; str:\n    \"\"\"\n    Args:\n        is_store: True if @ modifier present (e.g., &lt;mycodec@store&gt;)\n\n    Returns:\n        - A core type: \"bytes\", \"json\", \"varchar(N)\", \"int32\", etc.\n        - Another codec: \"&lt;blob&gt;\", \"&lt;hash&gt;\", etc.\n\n    Raises:\n        DataJointError: If store not supported but @ is present\n    \"\"\"\n</code></pre> <p>Examples:</p> <pre><code># Simple: always store as bytes\ndef get_dtype(self, is_store: bool) -&gt; str:\n    return \"bytes\"\n\n# Different behavior for in-table/store\ndef get_dtype(self, is_store: bool) -&gt; str:\n    return \"&lt;hash&gt;\" if is_store else \"bytes\"\n\n# Store-only codec\ndef get_dtype(self, is_store: bool) -&gt; str:\n    if not is_store:\n        raise DataJointError(\"&lt;object&gt; requires @ (store only)\")\n    return \"json\"\n</code></pre>"},{"location":"reference/specs/codec-api/#3-the-encode-method","title":"3. The <code>encode()</code> Method","text":"<p>Converts Python objects to the format expected by <code>get_dtype()</code>:</p> <pre><code>def encode(self, value: Any, *, key: dict | None = None, store_name: str | None = None) -&gt; Any:\n    \"\"\"\n    Args:\n        value: The Python object to store\n        key: Primary key values (for context-dependent encoding)\n        store_name: Target store name (for in-store storage)\n\n    Returns:\n        Value in the format expected by get_dtype()\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/codec-api/#4-the-decode-method","title":"4. The <code>decode()</code> Method","text":"<p>Converts stored values back to Python objects:</p> <pre><code>def decode(self, stored: Any, *, key: dict | None = None) -&gt; Any:\n    \"\"\"\n    Args:\n        stored: Data retrieved from storage\n        key: Primary key values (for context-dependent decoding)\n\n    Returns:\n        The reconstructed Python object\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/codec-api/#5-the-validate-method-optional","title":"5. The <code>validate()</code> Method (Optional)","text":"<p>Called automatically before <code>encode()</code> during INSERT operations:</p> <pre><code>def validate(self, value: Any) -&gt; None:\n    \"\"\"\n    Args:\n        value: The value to validate\n\n    Raises:\n        TypeError: If the value has an incompatible type\n        ValueError: If the value fails domain validation\n    \"\"\"\n    if not isinstance(value, ExpectedType):\n        raise TypeError(f\"Expected ExpectedType, got {type(value).__name__}\")\n</code></pre>"},{"location":"reference/specs/codec-api/#auto-registration","title":"Auto-Registration","text":"<p>Codecs automatically register when their class is defined. No decorator needed:</p> <pre><code># This codec is registered automatically when the class is defined\nclass MyCodec(dj.Codec):\n    name = \"mycodec\"\n    # ...\n</code></pre>"},{"location":"reference/specs/codec-api/#skipping-registration","title":"Skipping Registration","text":"<p>For abstract base classes that shouldn't be registered:</p> <pre><code>class BaseCodec(dj.Codec, register=False):\n    \"\"\"Abstract base - not registered.\"\"\"\n    name = None  # Or omit entirely\n\nclass ConcreteCodec(BaseCodec):\n    name = \"concrete\"  # This one IS registered\n    # ...\n</code></pre>"},{"location":"reference/specs/codec-api/#registration-timing","title":"Registration Timing","text":"<p>Codecs are registered at class definition time. Ensure your codec classes are imported before any table definitions that use them:</p> <pre><code># myproject/codecs.py\nclass GraphCodec(dj.Codec):\n    name = \"graph\"\n    ...\n\n# myproject/tables.py\nimport myproject.codecs  # Ensure codecs are registered\n\n@schema\nclass Networks(dj.Manual):\n    definition = '''\n    id : int\n    ---\n    network : &lt;graph&gt;\n    '''\n</code></pre>"},{"location":"reference/specs/codec-api/#codec-composition-chaining","title":"Codec Composition (Chaining)","text":"<p>Codecs can delegate to other codecs by returning <code>&lt;codec_name&gt;</code> from <code>get_dtype()</code>. This enables layered functionality:</p> <pre><code>class CompressedJsonCodec(dj.Codec):\n    \"\"\"Compress JSON data with zlib.\"\"\"\n\n    name = \"zjson\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"  # Delegate serialization to blob codec\n\n    def encode(self, value, *, key=None, store_name=None):\n        import json, zlib\n        json_bytes = json.dumps(value).encode('utf-8')\n        return zlib.compress(json_bytes)\n\n    def decode(self, stored, *, key=None):\n        import json, zlib\n        json_bytes = zlib.decompress(stored)\n        return json.loads(json_bytes.decode('utf-8'))\n</code></pre>"},{"location":"reference/specs/codec-api/#how-chaining-works","title":"How Chaining Works","text":"<p>When DataJoint encounters <code>&lt;zjson&gt;</code>:</p> <ol> <li>Calls <code>ZjsonCodec.get_dtype(is_store=False)</code> \u2192 returns <code>\"&lt;blob&gt;\"</code></li> <li>Calls <code>BlobCodec.get_dtype(is_store=False)</code> \u2192 returns <code>\"bytes\"</code></li> <li>Final storage type is <code>bytes</code> (LONGBLOB in MySQL)</li> </ol> <p>During INSERT: 1. <code>ZjsonCodec.encode()</code> converts Python dict \u2192 compressed bytes 2. <code>BlobCodec.encode()</code> packs bytes \u2192 DJ blob format 3. Stored in database</p> <p>During FETCH: 1. Read from database 2. <code>BlobCodec.decode()</code> unpacks DJ blob \u2192 compressed bytes 3. <code>ZjsonCodec.decode()</code> decompresses \u2192 Python dict</p>"},{"location":"reference/specs/codec-api/#built-in-codec-chains","title":"Built-in Codec Chains","text":"<p>DataJoint's built-in codecs form these chains:</p> Codec Chain Final Storage <code>&lt;blob&gt;</code> <code>&lt;blob&gt;</code> \u2192 <code>bytes</code> Inline <code>&lt;blob@&gt;</code> <code>&lt;blob&gt;</code> \u2192 <code>&lt;hash&gt;</code> \u2192 <code>json</code> Store (hash-addressed) <code>&lt;attach&gt;</code> <code>&lt;attach&gt;</code> \u2192 <code>bytes</code> Inline <code>&lt;attach@&gt;</code> <code>&lt;attach&gt;</code> \u2192 <code>&lt;hash&gt;</code> \u2192 <code>json</code> Store (hash-addressed) <code>&lt;hash@&gt;</code> <code>&lt;hash&gt;</code> \u2192 <code>json</code> Store only (hash-addressed) <code>&lt;object@&gt;</code> <code>&lt;object&gt;</code> \u2192 <code>json</code> Store only (schema-addressed) <code>&lt;npy@&gt;</code> <code>&lt;npy&gt;</code> \u2192 <code>json</code> Store only (schema-addressed) <code>&lt;filepath@&gt;</code> <code>&lt;filepath&gt;</code> \u2192 <code>json</code> Store only (external ref)"},{"location":"reference/specs/codec-api/#store-name-propagation","title":"Store Name Propagation","text":"<p>When using object storage (<code>@</code>), the store name propagates through the chain:</p> <pre><code># Table definition\ndata : &lt;mycodec@coldstore&gt;\n\n# Resolution:\n# 1. MyCodec.get_dtype(is_store=True) \u2192 \"&lt;blob&gt;\"\n# 2. BlobCodec.get_dtype(is_store=True) \u2192 \"&lt;hash&gt;\"\n# 3. HashCodec.get_dtype(is_store=True) \u2192 \"json\"\n# 4. store_name=\"coldstore\" passed to HashCodec.encode()\n</code></pre>"},{"location":"reference/specs/codec-api/#plugin-system-entry-points","title":"Plugin System (Entry Points)","text":"<p>Codecs can be distributed as installable packages using Python entry points.</p>"},{"location":"reference/specs/codec-api/#package-structure","title":"Package Structure","text":"<pre><code>dj-graph-codecs/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src/\n    \u2514\u2500\u2500 dj_graph_codecs/\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 codecs.py\n</code></pre>"},{"location":"reference/specs/codec-api/#pyprojecttoml","title":"pyproject.toml","text":"<pre><code>[project]\nname = \"dj-graph-codecs\"\nversion = \"1.0.0\"\ndependencies = [\"datajoint&gt;=2.0\", \"networkx\"]\n\n[project.entry-points.\"datajoint.codecs\"]\ngraph = \"dj_graph_codecs.codecs:GraphCodec\"\nweighted_graph = \"dj_graph_codecs.codecs:WeightedGraphCodec\"\n</code></pre>"},{"location":"reference/specs/codec-api/#codec-implementation","title":"Codec Implementation","text":"<pre><code># src/dj_graph_codecs/codecs.py\nimport datajoint as dj\nimport networkx as nx\n\nclass GraphCodec(dj.Codec):\n    name = \"graph\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n\n    def encode(self, graph, *, key=None, store_name=None):\n        return {\n            'nodes': list(graph.nodes(data=True)),\n            'edges': list(graph.edges(data=True)),\n        }\n\n    def decode(self, stored, *, key=None):\n        G = nx.Graph()\n        G.add_nodes_from(stored['nodes'])\n        G.add_edges_from(stored['edges'])\n        return G\n\nclass WeightedGraphCodec(dj.Codec):\n    name = \"weighted_graph\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n\n    def encode(self, graph, *, key=None, store_name=None):\n        return {\n            'nodes': list(graph.nodes(data=True)),\n            'edges': [(u, v, d) for u, v, d in graph.edges(data=True)],\n        }\n\n    def decode(self, stored, *, key=None):\n        G = nx.Graph()\n        G.add_nodes_from(stored['nodes'])\n        for u, v, d in stored['edges']:\n            G.add_edge(u, v, **d)\n        return G\n</code></pre>"},{"location":"reference/specs/codec-api/#usage-after-installation","title":"Usage After Installation","text":"<pre><code>pip install dj-graph-codecs\n</code></pre> <pre><code># Codecs are automatically discovered and available\n@schema\nclass Networks(dj.Manual):\n    definition = '''\n    network_id : int\n    ---\n    topology : &lt;graph&gt;\n    weights : &lt;weighted_graph&gt;\n    '''\n</code></pre>"},{"location":"reference/specs/codec-api/#entry-point-discovery","title":"Entry Point Discovery","text":"<p>DataJoint loads entry points lazily when a codec is first requested:</p> <ol> <li>Check explicit registry (codecs defined in current process)</li> <li>Load entry points from <code>datajoint.codecs</code> group</li> <li>Also checks legacy <code>datajoint.types</code> group for compatibility</li> </ol>"},{"location":"reference/specs/codec-api/#api-reference","title":"API Reference","text":""},{"location":"reference/specs/codec-api/#module-functions","title":"Module Functions","text":"<pre><code>import datajoint as dj\n\n# List all registered codec names\ndj.list_codecs()  # Returns: ['blob', 'hash', 'object', 'attach', 'filepath', ...]\n\n# Get a codec instance by name\ncodec = dj.get_codec(\"blob\")\ncodec = dj.get_codec(\"&lt;blob&gt;\")  # Angle brackets are optional\ncodec = dj.get_codec(\"&lt;blob@store&gt;\")  # Store parameter is stripped\n</code></pre>"},{"location":"reference/specs/codec-api/#internal-functions-for-advanced-use","title":"Internal Functions (for advanced use)","text":"<pre><code>from datajoint.codecs import (\n    is_codec_registered,  # Check if codec exists\n    unregister_codec,     # Remove codec (testing only)\n    resolve_dtype,        # Resolve codec chain\n    parse_type_spec,      # Parse \"&lt;name@store&gt;\" syntax\n)\n</code></pre>"},{"location":"reference/specs/codec-api/#built-in-codecs","title":"Built-in Codecs","text":"<p>DataJoint provides these built-in codecs. See the Type System Specification for detailed behavior and implementation.</p> Codec Inline Store Addressing Description <code>&lt;blob&gt;</code> <code>bytes</code> <code>&lt;hash@&gt;</code> Hash DataJoint serialization for Python objects <code>&lt;attach&gt;</code> <code>bytes</code> <code>&lt;hash@&gt;</code> Hash File attachments with filename preserved <code>&lt;hash@&gt;</code> N/A <code>json</code> Hash Hash-addressed storage with MD5 deduplication <code>&lt;object@&gt;</code> N/A <code>json</code> Schema Schema-addressed storage for files/folders <code>&lt;npy@&gt;</code> N/A <code>json</code> Schema Schema-addressed storage for numpy arrays <code>&lt;filepath@&gt;</code> N/A <code>json</code> Reference Reference to existing files in store <p>Addressing schemes: - Hash-addressed: Path from content hash. Automatic deduplication. - Schema-addressed: Path mirrors database structure. One location per entity.</p>"},{"location":"reference/specs/codec-api/#complete-examples","title":"Complete Examples","text":""},{"location":"reference/specs/codec-api/#example-1-simple-serialization","title":"Example 1: Simple Serialization","text":"<pre><code>import datajoint as dj\nimport numpy as np\n\nclass SpikeTrainCodec(dj.Codec):\n    \"\"\"Efficient storage for sparse spike timing data.\"\"\"\n\n    name = \"spike_train\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n\n    def validate(self, value):\n        if not isinstance(value, np.ndarray):\n            raise TypeError(\"Expected numpy array of spike times\")\n        if value.ndim != 1:\n            raise ValueError(\"Spike train must be 1-dimensional\")\n        if len(value) &gt; 1 and not np.all(np.diff(value) &gt;= 0):\n            raise ValueError(\"Spike times must be sorted\")\n\n    def encode(self, spike_times, *, key=None, store_name=None):\n        # Store as differences (smaller values, better compression)\n        return np.diff(spike_times, prepend=0).astype(np.float32)\n\n    def decode(self, stored, *, key=None):\n        # Reconstruct original spike times\n        return np.cumsum(stored).astype(np.float64)\n</code></pre>"},{"location":"reference/specs/codec-api/#example-2-in-store-storage","title":"Example 2: In-Store Storage","text":"<pre><code>import datajoint as dj\nimport pickle\n\nclass ModelCodec(dj.Codec):\n    \"\"\"Store ML models with optional in-store storage.\"\"\"\n\n    name = \"model\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        # Use hash-addressed storage for large models\n        return \"&lt;hash&gt;\" if is_store else \"&lt;blob&gt;\"\n\n    def encode(self, model, *, key=None, store_name=None):\n        return pickle.dumps(model, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def decode(self, stored, *, key=None):\n        return pickle.loads(stored)\n\n    def validate(self, value):\n        # Check that model has required interface\n        if not hasattr(value, 'predict'):\n            raise TypeError(\"Model must have a predict() method\")\n</code></pre> <p>Usage: <pre><code>@schema\nclass Models(dj.Manual):\n    definition = '''\n    model_id : int\n    ---\n    small_model : &lt;model&gt;         # In-table storage\n    large_model : &lt;model@&gt;        # In-store (default store)\n    archive_model : &lt;model@cold&gt;  # In-store (specific store)\n    '''\n</code></pre></p>"},{"location":"reference/specs/codec-api/#example-3-json-with-schema-validation","title":"Example 3: JSON with Schema Validation","text":"<pre><code>import datajoint as dj\nimport jsonschema\n\nclass ConfigCodec(dj.Codec):\n    \"\"\"Store validated JSON configuration.\"\"\"\n\n    name = \"config\"\n\n    SCHEMA = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"version\": {\"type\": \"integer\", \"minimum\": 1},\n            \"settings\": {\"type\": \"object\"},\n        },\n        \"required\": [\"version\", \"settings\"],\n    }\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"json\"\n\n    def validate(self, value):\n        jsonschema.validate(value, self.SCHEMA)\n\n    def encode(self, config, *, key=None, store_name=None):\n        return config  # JSON type handles serialization\n\n    def decode(self, stored, *, key=None):\n        return stored\n</code></pre>"},{"location":"reference/specs/codec-api/#example-4-context-dependent-encoding","title":"Example 4: Context-Dependent Encoding","text":"<pre><code>import datajoint as dj\n\nclass VersionedDataCodec(dj.Codec):\n    \"\"\"Handle different encoding versions based on primary key.\"\"\"\n\n    name = \"versioned\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n\n    def encode(self, value, *, key=None, store_name=None):\n        version = key.get(\"schema_version\", 1) if key else 1\n        if version &gt;= 2:\n            return {\"v\": 2, \"data\": self._encode_v2(value)}\n        return {\"v\": 1, \"data\": self._encode_v1(value)}\n\n    def decode(self, stored, *, key=None):\n        version = stored.get(\"v\", 1)\n        if version &gt;= 2:\n            return self._decode_v2(stored[\"data\"])\n        return self._decode_v1(stored[\"data\"])\n\n    def _encode_v1(self, value):\n        return value\n\n    def _decode_v1(self, data):\n        return data\n\n    def _encode_v2(self, value):\n        # New encoding format\n        return {\"optimized\": True, \"payload\": value}\n\n    def _decode_v2(self, data):\n        return data[\"payload\"]\n</code></pre>"},{"location":"reference/specs/codec-api/#example-5-external-only-codec","title":"Example 5: External-Only Codec","text":"<pre><code>import datajoint as dj\nfrom pathlib import Path\n\nclass ZarrCodec(dj.Codec):\n    \"\"\"Store Zarr arrays in object storage.\"\"\"\n\n    name = \"zarr\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        if not is_store:\n            raise dj.DataJointError(\"&lt;zarr&gt; requires @ (in-store only)\")\n        return \"&lt;object&gt;\"  # Delegate to object storage\n\n    def encode(self, value, *, key=None, store_name=None):\n        import zarr\n        import tempfile\n\n        # If already a path, pass through\n        if isinstance(value, (str, Path)):\n            return str(value)\n\n        # If zarr array, save to temp and return path\n        if isinstance(value, zarr.Array):\n            tmpdir = tempfile.mkdtemp()\n            path = Path(tmpdir) / \"data.zarr\"\n            zarr.save(path, value)\n            return str(path)\n\n        raise TypeError(f\"Expected zarr.Array or path, got {type(value)}\")\n\n    def decode(self, stored, *, key=None):\n        # ObjectCodec returns ObjectRef, use its fsmap for zarr\n        import zarr\n        return zarr.open(stored.fsmap, mode='r')\n</code></pre>"},{"location":"reference/specs/codec-api/#best-practices","title":"Best Practices","text":""},{"location":"reference/specs/codec-api/#1-choose-appropriate-storage-types","title":"1. Choose Appropriate Storage Types","text":"Data Type Recommended <code>get_dtype()</code> Python objects (dicts, arrays) <code>\"&lt;blob&gt;\"</code> Large binary data <code>\"&lt;hash&gt;\"</code> (external) Files/folders (Zarr, HDF5) <code>\"&lt;object&gt;\"</code> (external) Simple JSON-serializable <code>\"json\"</code> Short strings <code>\"varchar(N)\"</code> Numeric identifiers <code>\"int32\"</code>, <code>\"int64\"</code>"},{"location":"reference/specs/codec-api/#2-handle-none-values","title":"2. Handle None Values","text":"<p>Nullable columns may pass <code>None</code> to your codec:</p> <pre><code>def encode(self, value, *, key=None, store_name=None):\n    if value is None:\n        return None  # Pass through for nullable columns\n    return self._actual_encode(value)\n\ndef decode(self, stored, *, key=None):\n    if stored is None:\n        return None\n    return self._actual_decode(stored)\n</code></pre>"},{"location":"reference/specs/codec-api/#3-test-round-trips","title":"3. Test Round-Trips","text":"<p>Always verify that <code>decode(encode(x)) == x</code>:</p> <pre><code>def test_codec_roundtrip():\n    codec = MyCodec()\n\n    test_values = [\n        {\"key\": \"value\"},\n        [1, 2, 3],\n        np.array([1.0, 2.0]),\n    ]\n\n    for original in test_values:\n        encoded = codec.encode(original)\n        decoded = codec.decode(encoded)\n        assert decoded == original or np.array_equal(decoded, original)\n</code></pre>"},{"location":"reference/specs/codec-api/#4-include-validation","title":"4. Include Validation","text":"<p>Catch errors early with <code>validate()</code>:</p> <pre><code>def validate(self, value):\n    if not isinstance(value, ExpectedType):\n        raise TypeError(f\"Expected ExpectedType, got {type(value).__name__}\")\n\n    if not self._is_valid(value):\n        raise ValueError(\"Value fails validation constraints\")\n</code></pre>"},{"location":"reference/specs/codec-api/#5-document-expected-formats","title":"5. Document Expected Formats","text":"<p>Include docstrings explaining input/output formats:</p> <pre><code>class MyCodec(dj.Codec):\n    \"\"\"\n    Store MyType objects.\n\n    Input format (encode):\n        MyType instance with attributes: x, y, z\n\n    Storage format:\n        Dict with keys: 'x', 'y', 'z'\n\n    Output format (decode):\n        MyType instance reconstructed from storage\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/codec-api/#6-consider-versioning","title":"6. Consider Versioning","text":"<p>If your encoding format might change:</p> <pre><code>def encode(self, value, *, key=None, store_name=None):\n    return {\n        \"_version\": 2,\n        \"_data\": self._encode_v2(value),\n    }\n\ndef decode(self, stored, *, key=None):\n    version = stored.get(\"_version\", 1)\n    data = stored.get(\"_data\", stored)\n\n    if version == 1:\n        return self._decode_v1(data)\n    return self._decode_v2(data)\n</code></pre>"},{"location":"reference/specs/codec-api/#error-handling","title":"Error Handling","text":""},{"location":"reference/specs/codec-api/#common-errors","title":"Common Errors","text":"Error Cause Solution <code>Unknown codec: &lt;name&gt;</code> Codec not registered Import module defining codec before table definition <code>Codec &lt;name&gt; already registered</code> Duplicate name Use unique names; check for conflicts <code>&lt;codec&gt; requires @</code> In-store-only codec used without @ Add <code>@</code> or <code>@store</code> to attribute type <code>Circular codec reference</code> Codec chain forms a loop Check <code>get_dtype()</code> return values"},{"location":"reference/specs/codec-api/#debugging","title":"Debugging","text":"<pre><code># Check what codecs are registered\nprint(dj.list_codecs())\n\n# Inspect a codec\ncodec = dj.get_codec(\"mycodec\")\nprint(f\"Name: {codec.name}\")\nprint(f\"In-table dtype: {codec.get_dtype(is_store=False)}\")\nprint(f\"In-store dtype: {codec.get_dtype(is_store=True)}\")\n\n# Resolve full chain\nfrom datajoint.codecs import resolve_dtype\nfinal_type, chain, store = resolve_dtype(\"&lt;mycodec@store&gt;\")\nprint(f\"Final storage type: {final_type}\")\nprint(f\"Codec chain: {[c.name for c in chain]}\")\nprint(f\"Store: {store}\")\n</code></pre>"},{"location":"reference/specs/data-manipulation/","title":"DataJoint Data Manipulation Specification","text":""},{"location":"reference/specs/data-manipulation/#overview","title":"Overview","text":"<p>This document specifies data manipulation operations in DataJoint Python: insert, update, and delete. These operations maintain referential integrity across the pipeline while supporting the workflow normalization paradigm.</p>"},{"location":"reference/specs/data-manipulation/#1-workflow-normalization-philosophy","title":"1. Workflow Normalization Philosophy","text":""},{"location":"reference/specs/data-manipulation/#11-insert-and-delete-as-primary-operations","title":"1.1 Insert and Delete as Primary Operations","text":"<p>DataJoint pipelines are designed around insert and delete as the primary data manipulation operations:</p> <pre><code>Insert: Add complete entities (rows) to tables\nDelete: Remove entities and all dependent data (cascading)\n</code></pre> <p>This design maintains referential integrity at the entity level\u2014each row represents a complete, self-consistent unit of data.</p>"},{"location":"reference/specs/data-manipulation/#12-updates-as-surgical-corrections","title":"1.2 Updates as Surgical Corrections","text":"<p>Updates are intentionally limited to the <code>update1()</code> method, which modifies a single row at a time. This is by design:</p> <ul> <li>Updates bypass the normal workflow</li> <li>They can create inconsistencies with derived data</li> <li>They should be used sparingly for corrective operations</li> </ul> <p>Appropriate uses of update1(): - Fixing data entry errors - Correcting metadata after the fact - Administrative annotations</p> <p>Inappropriate uses: - Regular workflow operations - Batch modifications - Anything that should trigger recomputation</p>"},{"location":"reference/specs/data-manipulation/#13-the-recomputation-pattern","title":"1.3 The Recomputation Pattern","text":"<p>When source data changes, the correct pattern is:</p> <pre><code># 1. Delete the incorrect data (cascades to all derived tables)\n(SourceTable &amp; {\"key\": value}).delete()\n\n# 2. Insert the corrected data\nSourceTable.insert1(corrected_row)\n\n# 3. Recompute derived tables\nDerivedTable.populate()\n</code></pre> <p>This ensures all derived data remains consistent with its sources.</p>"},{"location":"reference/specs/data-manipulation/#2-insert-operations","title":"2. Insert Operations","text":""},{"location":"reference/specs/data-manipulation/#21-insert-method","title":"2.1 <code>insert()</code> Method","text":"<p>Signature: <pre><code>def insert(\n    self,\n    rows,\n    replace=False,\n    skip_duplicates=False,\n    ignore_extra_fields=False,\n    allow_direct_insert=None,\n    chunk_size=None,\n)\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>rows</code> iterable \u2014 Data to insert <code>replace</code> bool <code>False</code> Replace existing rows with matching PK <code>skip_duplicates</code> bool <code>False</code> Silently skip duplicate keys <code>ignore_extra_fields</code> bool <code>False</code> Ignore fields not in table <code>allow_direct_insert</code> bool <code>None</code> Allow insert into auto-populated tables <code>chunk_size</code> int <code>None</code> Insert in batches of this size"},{"location":"reference/specs/data-manipulation/#22-accepted-input-formats","title":"2.2 Accepted Input Formats","text":"Format Example List of dicts <code>[{\"id\": 1, \"name\": \"Alice\"}, ...]</code> pandas DataFrame <code>pd.DataFrame({\"id\": [1, 2], \"name\": [\"A\", \"B\"]})</code> polars DataFrame <code>pl.DataFrame({\"id\": [1, 2], \"name\": [\"A\", \"B\"]})</code> numpy structured array <code>np.array([(1, \"A\")], dtype=[(\"id\", int), (\"name\", \"U10\")])</code> QueryExpression <code>OtherTable.proj(...)</code> (INSERT...SELECT) Path to CSV <code>Path(\"data.csv\")</code>"},{"location":"reference/specs/data-manipulation/#23-basic-usage","title":"2.3 Basic Usage","text":"<pre><code># Single row\nSubject.insert1({\"subject_id\": 1, \"name\": \"Mouse001\", \"dob\": \"2024-01-15\"})\n\n# Multiple rows\nSubject.insert([\n    {\"subject_id\": 1, \"name\": \"Mouse001\", \"dob\": \"2024-01-15\"},\n    {\"subject_id\": 2, \"name\": \"Mouse002\", \"dob\": \"2024-01-16\"},\n])\n\n# From DataFrame\ndf = pd.DataFrame({\"subject_id\": [1, 2], \"name\": [\"M1\", \"M2\"], \"dob\": [\"2024-01-15\", \"2024-01-16\"]})\nSubject.insert(df)\n\n# From query (INSERT...SELECT)\nActiveSubjects.insert(Subject &amp; \"status = 'active'\")\n</code></pre>"},{"location":"reference/specs/data-manipulation/#24-handling-duplicates","title":"2.4 Handling Duplicates","text":"<pre><code># Error on duplicate (default)\nSubject.insert1({\"subject_id\": 1, ...})  # Raises DuplicateError if exists\n\n# Skip duplicates silently\nSubject.insert(rows, skip_duplicates=True)\n\n# Replace existing rows\nSubject.insert(rows, replace=True)\n</code></pre> <p>Difference between skip and replace: - <code>skip_duplicates</code>: Keeps existing row unchanged - <code>replace</code>: Overwrites existing row with new values</p>"},{"location":"reference/specs/data-manipulation/#25-extra-fields","title":"2.5 Extra Fields","text":"<pre><code># Error on extra fields (default)\nSubject.insert1({\"subject_id\": 1, \"unknown_field\": \"x\"})  # Raises error\n\n# Ignore extra fields\nSubject.insert1({\"subject_id\": 1, \"unknown_field\": \"x\"}, ignore_extra_fields=True)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#26-auto-populated-tables","title":"2.6 Auto-Populated Tables","text":"<p>Computed and Imported tables normally only accept inserts from their <code>make()</code> method:</p> <pre><code># Raises DataJointError by default\nComputedTable.insert1({\"key\": 1, \"result\": 42})\n\n# Explicit override\nComputedTable.insert1({\"key\": 1, \"result\": 42}, allow_direct_insert=True)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#27-chunked-insertion","title":"2.7 Chunked Insertion","text":"<p>For large datasets, insert in batches:</p> <pre><code># Insert 10,000 rows at a time\nSubject.insert(large_dataset, chunk_size=10000)\n</code></pre> <p>Each chunk is a separate transaction. If interrupted, completed chunks persist.</p>"},{"location":"reference/specs/data-manipulation/#28-insert1-method","title":"2.8 <code>insert1()</code> Method","text":"<p>Convenience wrapper for single-row inserts:</p> <pre><code>def insert1(self, row, **kwargs)\n</code></pre> <p>Equivalent to <code>insert((row,), **kwargs)</code>.</p>"},{"location":"reference/specs/data-manipulation/#29-staged-insert-for-large-objects","title":"2.9 Staged Insert for Large Objects","text":"<p>For large objects (Zarr arrays, HDF5 files), use staged insert to write directly to object storage:</p> <pre><code>with table.staged_insert1 as staged:\n    # Set primary key and metadata\n    staged.rec[\"session_id\"] = 123\n    staged.rec[\"timestamp\"] = datetime.now()\n\n    # Write large data directly to storage\n    zarr_path = staged.store(\"raw_data\", \".zarr\")\n    z = zarr.open(zarr_path, mode=\"w\")\n    z[:] = large_array\n    staged.rec[\"raw_data\"] = z\n\n# Row automatically inserted on successful exit\n# Storage cleaned up if exception occurs\n</code></pre>"},{"location":"reference/specs/data-manipulation/#3-update-operations","title":"3. Update Operations","text":""},{"location":"reference/specs/data-manipulation/#31-update1-method","title":"3.1 <code>update1()</code> Method","text":"<p>Signature: <pre><code>def update1(self, row: dict) -&gt; None\n</code></pre></p> <p>Parameters: - <code>row</code>: Dictionary containing all primary key values plus attributes to update</p>"},{"location":"reference/specs/data-manipulation/#32-basic-usage","title":"3.2 Basic Usage","text":"<pre><code># Update a single attribute\nSubject.update1({\"subject_id\": 1, \"name\": \"NewName\"})\n\n# Update multiple attributes\nSubject.update1({\n    \"subject_id\": 1,\n    \"name\": \"NewName\",\n    \"notes\": \"Updated on 2024-01-15\"\n})\n</code></pre>"},{"location":"reference/specs/data-manipulation/#33-requirements","title":"3.3 Requirements","text":"<ol> <li>Complete primary key: All PK attributes must be provided</li> <li>Exactly one match: Must match exactly one existing row</li> <li>No restrictions: Cannot call on restricted table</li> </ol> <pre><code># Error: incomplete primary key\nSubject.update1({\"name\": \"NewName\"})\n\n# Error: row doesn't exist\nSubject.update1({\"subject_id\": 999, \"name\": \"Ghost\"})\n\n# Error: cannot update restricted table\n(Subject &amp; \"subject_id &gt; 10\").update1({...})\n</code></pre>"},{"location":"reference/specs/data-manipulation/#34-resetting-to-default","title":"3.4 Resetting to Default","text":"<p>Setting an attribute to <code>None</code> resets it to its default value:</p> <pre><code># Reset 'notes' to its default (NULL if nullable)\nSubject.update1({\"subject_id\": 1, \"notes\": None})\n</code></pre>"},{"location":"reference/specs/data-manipulation/#35-when-to-use-updates","title":"3.5 When to Use Updates","text":"<p>Appropriate: <pre><code># Fix a typo in metadata\nSubject.update1({\"subject_id\": 1, \"name\": \"Mouse001\"})  # Was \"Mous001\"\n\n# Add a note to an existing record\nSession.update1({\"session_id\": 5, \"notes\": \"Excluded from analysis\"})\n</code></pre></p> <p>Inappropriate (use delete + insert + populate instead): <pre><code># DON'T: Update source data that affects computed results\nTrial.update1({\"trial_id\": 1, \"stimulus\": \"new_stim\"})  # Computed tables now stale!\n\n# DO: Delete and recompute\n(Trial &amp; {\"trial_id\": 1}).delete()  # Cascades to computed tables\nTrial.insert1({\"trial_id\": 1, \"stimulus\": \"new_stim\"})\nComputedResults.populate()\n</code></pre></p>"},{"location":"reference/specs/data-manipulation/#36-why-no-bulk-update","title":"3.6 Why No Bulk Update?","text":"<p>DataJoint intentionally does not provide <code>update()</code> for multiple rows:</p> <ol> <li>Consistency: Bulk updates easily create inconsistencies with derived data</li> <li>Auditability: Single-row updates are explicit and traceable</li> <li>Workflow: The insert/delete pattern maintains referential integrity</li> </ol> <p>If you need to update many rows, iterate explicitly:</p> <pre><code>for key in (Subject &amp; condition).keys():\n    Subject.update1({**key, \"status\": \"archived\"})\n</code></pre>"},{"location":"reference/specs/data-manipulation/#4-delete-operations","title":"4. Delete Operations","text":""},{"location":"reference/specs/data-manipulation/#41-delete-method","title":"4.1 <code>delete()</code> Method","text":"<p>Signature: <pre><code>def delete(\n    self,\n    transaction: bool = True,\n    prompt: bool | None = None,\n    part_integrity: str = \"enforce\",\n) -&gt; int\n</code></pre></p> <p>Parameters:</p> Parameter Type Default Description <code>transaction</code> bool <code>True</code> Wrap in atomic transaction <code>prompt</code> bool <code>None</code> Prompt for confirmation (default: config setting) <code>part_integrity</code> str <code>\"enforce\"</code> Master-part integrity policy (see below) <p><code>part_integrity</code> values:</p> Value Behavior <code>\"enforce\"</code> Error if parts would be deleted without masters <code>\"ignore\"</code> Allow deleting parts without masters (breaks integrity) <code>\"cascade\"</code> Also delete masters when parts are deleted <p>Returns: Number of deleted rows from the primary table.</p>"},{"location":"reference/specs/data-manipulation/#42-cascade-behavior","title":"4.2 Cascade Behavior","text":"<p>Delete automatically cascades to all dependent tables:</p> <pre><code># Deleting a subject deletes all their sessions, trials, and computed results\n(Subject &amp; {\"subject_id\": 1}).delete()\n</code></pre> <p>Cascade order: 1. Identify all tables with foreign keys referencing target 2. Recursively delete matching rows in child tables 3. Delete rows in target table</p>"},{"location":"reference/specs/data-manipulation/#43-basic-usage","title":"4.3 Basic Usage","text":"<pre><code># Delete specific rows\n(Subject &amp; {\"subject_id\": 1}).delete()\n\n# Delete matching a condition\n(Session &amp; \"session_date &lt; '2024-01-01'\").delete()\n\n# Delete all rows (use with caution!)\nSubject.delete()\n</code></pre>"},{"location":"reference/specs/data-manipulation/#44-safe-mode","title":"4.4 Safe Mode","text":"<p>When <code>prompt=True</code> (default from config):</p> <pre><code>About to delete:\n  Subject: 1 rows\n  Session: 5 rows\n  Trial: 150 rows\n  ProcessedData: 150 rows\n\nCommit deletes? [yes, No]:\n</code></pre> <p>Disable for automated scripts:</p> <pre><code>Subject.delete(prompt=False)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#45-transaction-control","title":"4.5 Transaction Control","text":"<pre><code># Atomic delete (default) - all or nothing\n(Subject &amp; condition).delete(transaction=True)\n\n# Non-transactional (for nested transactions)\n(Subject &amp; condition).delete(transaction=False)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#46-part-table-constraints","title":"4.6 Part Table Constraints","text":"<p>Cannot delete from part tables without deleting from master (by default):</p> <pre><code># Error: cannot delete part without master\nSession.Recording.delete()\n\n# Allow breaking master-part integrity\nSession.Recording.delete(part_integrity=\"ignore\")\n\n# Delete parts AND cascade up to delete master\nSession.Recording.delete(part_integrity=\"cascade\")\n</code></pre> <p><code>part_integrity</code> parameter:</p> Value Behavior <code>\"enforce\"</code> (default) Error if parts would be deleted without masters <code>\"ignore\"</code> Allow deleting parts without masters (breaks integrity) <code>\"cascade\"</code> Also delete masters when parts are deleted (maintains integrity)"},{"location":"reference/specs/data-manipulation/#47-delete_quick-method","title":"4.7 <code>delete_quick()</code> Method","text":"<p>Fast delete without cascade or confirmation:</p> <pre><code>def delete_quick(self, get_count: bool = False) -&gt; int | None\n</code></pre> <p>Use cases: - Internal cleanup - Tables with no dependents - When you've already handled dependencies</p> <p>Behavior: - No cascade to child tables - No user confirmation - Fails on FK constraint violation</p> <pre><code># Quick delete (fails if has dependents)\n(TempTable &amp; condition).delete_quick()\n\n# Get count of deleted rows\nn = (TempTable &amp; condition).delete_quick(get_count=True)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#5-validation","title":"5. Validation","text":""},{"location":"reference/specs/data-manipulation/#51-validate-method","title":"5.1 <code>validate()</code> Method","text":"<p>Pre-validate rows before insertion:</p> <pre><code>def validate(self, rows, *, ignore_extra_fields=False) -&gt; ValidationResult\n</code></pre> <p>Returns: <code>ValidationResult</code> with: - <code>is_valid</code>: Boolean indicating all rows passed - <code>errors</code>: List of (row_idx, field_name, error_message) - <code>rows_checked</code>: Number of rows validated</p>"},{"location":"reference/specs/data-manipulation/#52-usage","title":"5.2 Usage","text":"<pre><code>result = Subject.validate(rows)\n\nif result:\n    Subject.insert(rows)\nelse:\n    print(result.summary())\n    # Row 3, field 'dob': Invalid date format\n    # Row 7, field 'subject_id': Missing required field\n</code></pre>"},{"location":"reference/specs/data-manipulation/#53-validations-performed","title":"5.3 Validations Performed","text":"Check Description Field existence All fields must exist in table NULL constraints Required fields must have values Primary key completeness All PK fields must be present UUID format Valid UUID string or object JSON serializability JSON fields must be serializable Codec validation Custom type validation via codecs"},{"location":"reference/specs/data-manipulation/#54-limitations","title":"5.4 Limitations","text":"<p>These constraints are only checked at database level: - Foreign key references - Unique constraints (beyond PK) - Custom CHECK constraints</p>"},{"location":"reference/specs/data-manipulation/#6-part-tables","title":"6. Part Tables","text":""},{"location":"reference/specs/data-manipulation/#61-inserting-into-part-tables","title":"6.1 Inserting into Part Tables","text":"<p>Part tables are inserted via their master:</p> <pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    session_id : int\n    ---\n    date : date\n    \"\"\"\n\n    class Recording(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        recording_id : int\n        ---\n        duration : float\n        \"\"\"\n\n# Insert master with parts\nSession.insert1({\"session_id\": 1, \"date\": \"2024-01-15\"})\nSession.Recording.insert([\n    {\"session_id\": 1, \"recording_id\": 1, \"duration\": 60.0},\n    {\"session_id\": 1, \"recording_id\": 2, \"duration\": 45.5},\n])\n</code></pre>"},{"location":"reference/specs/data-manipulation/#62-deleting-with-part-tables","title":"6.2 Deleting with Part Tables","text":"<p>Deleting master cascades to parts:</p> <pre><code># Deletes session AND all its recordings\n(Session &amp; {\"session_id\": 1}).delete()\n</code></pre> <p>Cannot delete parts independently (by default):</p> <pre><code># Error\nSession.Recording.delete()\n\n# Allow breaking master-part integrity\nSession.Recording.delete(part_integrity=\"ignore\")\n\n# Or cascade up to also delete master\nSession.Recording.delete(part_integrity=\"cascade\")\n</code></pre>"},{"location":"reference/specs/data-manipulation/#7-transaction-handling","title":"7. Transaction Handling","text":""},{"location":"reference/specs/data-manipulation/#71-implicit-transactions","title":"7.1 Implicit Transactions","text":"<p>Single operations are atomic:</p> <pre><code>Subject.insert1(row)  # Atomic\nSubject.update1(row)  # Atomic\nSubject.delete()      # Atomic (by default)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#72-explicit-transactions","title":"7.2 Explicit Transactions","text":"<p>For multi-table operations:</p> <pre><code>with dj.conn().transaction:\n    Parent.insert1(parent_row)\n    Child.insert(child_rows)\n    # Commits on successful exit\n    # Rolls back on exception\n</code></pre>"},{"location":"reference/specs/data-manipulation/#73-chunked-inserts-and-transactions","title":"7.3 Chunked Inserts and Transactions","text":"<p>With <code>chunk_size</code>, each chunk is a separate transaction:</p> <pre><code># Each chunk of 1000 rows commits independently\nSubject.insert(large_dataset, chunk_size=1000)\n</code></pre> <p>If interrupted, completed chunks persist.</p>"},{"location":"reference/specs/data-manipulation/#8-error-handling","title":"8. Error Handling","text":""},{"location":"reference/specs/data-manipulation/#81-common-errors","title":"8.1 Common Errors","text":"Error Cause Resolution <code>DuplicateError</code> Primary key already exists Use <code>skip_duplicates=True</code> or <code>replace=True</code> <code>IntegrityError</code> Foreign key constraint violated Insert parent rows first <code>MissingAttributeError</code> Required field not provided Include all required fields <code>UnknownAttributeError</code> Field not in table Use <code>ignore_extra_fields=True</code> or fix field name <code>DataJointError</code> Various validation failures Check error message for details"},{"location":"reference/specs/data-manipulation/#82-error-recovery-pattern","title":"8.2 Error Recovery Pattern","text":"<pre><code>try:\n    Subject.insert(rows)\nexcept dj.errors.DuplicateError as e:\n    # Handle specific duplicate\n    print(f\"Duplicate: {e}\")\nexcept dj.errors.IntegrityError as e:\n    # Missing parent reference\n    print(f\"Missing parent: {e}\")\nexcept dj.DataJointError as e:\n    # Other DataJoint errors\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"reference/specs/data-manipulation/#9-best-practices","title":"9. Best Practices","text":""},{"location":"reference/specs/data-manipulation/#91-prefer-insertdelete-over-update","title":"9.1 Prefer Insert/Delete Over Update","text":"<pre><code># Good: Delete and reinsert\n(Trial &amp; key).delete()\nTrial.insert1(corrected_trial)\nDerivedTable.populate()\n\n# Avoid: Update that creates stale derived data\nTrial.update1({**key, \"value\": new_value})  # Derived tables now inconsistent!\n</code></pre>"},{"location":"reference/specs/data-manipulation/#92-validate-before-insert","title":"9.2 Validate Before Insert","text":"<pre><code>result = Subject.validate(rows)\nif not result:\n    raise ValueError(result.summary())\nSubject.insert(rows)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#93-use-transactions-for-related-inserts","title":"9.3 Use Transactions for Related Inserts","text":"<pre><code>with dj.conn().transaction:\n    session_key = Session.insert1(session_data, skip_duplicates=True)\n    Session.Recording.insert(recordings)\n    Session.Stimulus.insert(stimuli)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#94-batch-inserts-for-performance","title":"9.4 Batch Inserts for Performance","text":"<pre><code># Good: Single insert call\nSubject.insert(all_rows)\n\n# Avoid: Loop of insert1 calls\nfor row in all_rows:\n    Subject.insert1(row)  # Slow!\n</code></pre>"},{"location":"reference/specs/data-manipulation/#95-safe-deletion-in-production","title":"9.5 Safe Deletion in Production","text":"<pre><code># Always use prompt in interactive sessions\n(Subject &amp; condition).delete(prompt=True)\n\n# Disable only in tested automated scripts\n(Subject &amp; condition).delete(prompt=False)\n</code></pre>"},{"location":"reference/specs/data-manipulation/#10-quick-reference","title":"10. Quick Reference","text":"Operation Method Cascades Transaction Typical Use Insert one <code>insert1()</code> \u2014 Implicit Adding single entity Insert many <code>insert()</code> \u2014 Per-chunk Bulk data loading Insert large object <code>staged_insert1</code> \u2014 On exit Zarr, HDF5 files Update one <code>update1()</code> \u2014 Implicit Surgical corrections Delete <code>delete()</code> Yes Optional Removing entities Delete quick <code>delete_quick()</code> No No Internal cleanup Validate <code>validate()</code> \u2014 \u2014 Pre-insert check"},{"location":"reference/specs/database-backends/","title":"Database Backends Specification","text":"<p>New in 2.1</p> <p>PostgreSQL support was introduced in DataJoint 2.1.</p> <p>DataJoint supports multiple database backends through a unified adapter architecture.</p>"},{"location":"reference/specs/database-backends/#supported-backends","title":"Supported Backends","text":"Backend Minimum Version Default Port Status MySQL 8.0 3306 Production PostgreSQL 15 5432 Production"},{"location":"reference/specs/database-backends/#configuration","title":"Configuration","text":"<p>Select the backend via configuration:</p> <pre><code>import datajoint as dj\n\ndj.config['database.backend'] = 'mysql'      # Default\ndj.config['database.backend'] = 'postgresql' # PostgreSQL\n</code></pre> <p>Or via environment variable:</p> <pre><code>export DJ_BACKEND=postgresql\n</code></pre>"},{"location":"reference/specs/database-backends/#adapter-architecture","title":"Adapter Architecture","text":"<p>DataJoint uses database adapters to generate backend-specific SQL while maintaining a consistent API.</p> <pre><code>flowchart TB\n    subgraph api[DataJoint API]\n        tables[Tables, Queries, Schemas]\n    end\n\n    subgraph adapter[Database Adapter]\n        gen[SQL Generation, Type Mapping]\n    end\n\n    subgraph backends[Backend Adapters]\n        mysql[MySQL Adapter]\n        postgres[PostgreSQL Adapter]\n    end\n\n    api --&gt; adapter\n    adapter --&gt; mysql\n    adapter --&gt; postgres\n</code></pre>"},{"location":"reference/specs/database-backends/#backend-compatibility","title":"Backend Compatibility","text":""},{"location":"reference/specs/database-backends/#fully-compatible","title":"Fully Compatible","text":"<p>The following features work identically across all backends:</p> <ul> <li>Table definitions: Same definition syntax for all backends</li> <li>Core types: <code>int32</code>, <code>float64</code>, <code>varchar</code>, <code>datetime</code>, etc.</li> <li>Codec types: <code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>, <code>&lt;object@&gt;</code>, etc.</li> <li>Query operations: Restriction, projection, join, aggregation</li> <li>Foreign keys: Inheritance, nullable, unique modifiers</li> <li>Indexes: Single-column, composite, unique</li> <li>Auto-populate: Jobs queue, distributed computation</li> </ul>"},{"location":"reference/specs/database-backends/#backend-specific-behavior","title":"Backend-Specific Behavior","text":"Feature MySQL PostgreSQL Default port 3306 5432 Schema = Database Yes Yes (uses schemas) JSON operators <code>-&gt;</code>, <code>-&gt;&gt;</code> <code>-&gt;</code>, <code>-&gt;&gt;</code> BLOB storage <code>LONGBLOB</code> <code>BYTEA</code> Boolean type <code>TINYINT(1)</code> <code>BOOLEAN</code>"},{"location":"reference/specs/database-backends/#string-quoting","title":"String Quoting","text":"<p>MySQL and PostgreSQL handle quotes differently in SQL:</p> Quote Type MySQL PostgreSQL Single quotes <code>'...'</code> String literals String literals Double quotes <code>\"...\"</code> String literals Identifiers (column names) Backticks <code>`...`</code> Identifiers Not supported <p>This affects restriction strings. MySQL accepts both quote styles for string values, but PostgreSQL interprets double quotes as identifier (column) references.</p> <pre><code># MySQL only - double quotes work as string literals\nTable &amp; 'name = \"Alice\"'\n\n# Both backends - single quotes for string literals\nTable &amp; \"name = 'Alice'\"\n</code></pre> <p>PostgreSQL migration: Replace double quotes with single quotes inside SQL restriction strings:</p> <pre><code># Before (MySQL)\nTable &amp; 'strain = \"C57BL/6\"'\nTable &amp; 'date &gt; \"2024-01-01\"'\n\n# After (PostgreSQL compatible)\nTable &amp; \"strain = 'C57BL/6'\"\nTable &amp; \"date &gt; '2024-01-01'\"\n</code></pre> <p>Dictionary restrictions handle quoting automatically but only support equality:</p> <pre><code>Table &amp; {'name': 'Alice'}  # Equality only - backend-agnostic\n</code></pre> <p>For range comparisons (<code>&gt;</code>, <code>&lt;</code>, <code>LIKE</code>, etc.), use string restrictions with single-quoted values.</p>"},{"location":"reference/specs/database-backends/#sql-function-translation","title":"SQL Function Translation","text":"<p>DataJoint automatically translates certain SQL functions between backends, allowing portable code:</p> Function MySQL PostgreSQL String aggregation <code>GROUP_CONCAT(col)</code> <code>STRING_AGG(col, ',')</code> String aggregation with separator <code>GROUP_CONCAT(col SEPARATOR ';')</code> <code>STRING_AGG(col, ';')</code> <p>You can use either syntax in your code\u2014DataJoint translates to the appropriate form:</p> <pre><code># Both work on both backends\nPerson.aggr(Proficiency, languages='GROUP_CONCAT(lang_code)')\nPerson.aggr(Proficiency, languages=\"STRING_AGG(lang_code, ',')\")\n</code></pre> <p>The translation is bidirectional:</p> <ul> <li>On PostgreSQL: <code>GROUP_CONCAT(col)</code> \u2192 <code>STRING_AGG(col::text, ',')</code></li> <li>On MySQL: <code>STRING_AGG(col, ',')</code> \u2192 <code>GROUP_CONCAT(col)</code></li> </ul>"},{"location":"reference/specs/database-backends/#type-mapping","title":"Type Mapping","text":"<p>DataJoint core types map to native database types:</p> Core Type MySQL PostgreSQL <code>int8</code> <code>TINYINT</code> <code>SMALLINT</code> <code>int16</code> <code>SMALLINT</code> <code>SMALLINT</code> <code>int32</code> <code>INT</code> <code>INTEGER</code> <code>int64</code> <code>BIGINT</code> <code>BIGINT</code> <code>float32</code> <code>FLOAT</code> <code>REAL</code> <code>float64</code> <code>DOUBLE</code> <code>DOUBLE PRECISION</code> <code>bool</code> <code>TINYINT(1)</code> <code>BOOLEAN</code> <code>varchar(n)</code> <code>VARCHAR(n)</code> <code>VARCHAR(n)</code> <code>char(n)</code> <code>CHAR(n)</code> <code>CHAR(n)</code> <code>date</code> <code>DATE</code> <code>DATE</code> <code>datetime</code> <code>DATETIME</code> <code>TIMESTAMP</code> <code>datetime(n)</code> <code>DATETIME(n)</code> <code>TIMESTAMP(n)</code> <code>json</code> <code>JSON</code> <code>JSONB</code> <code>uuid</code> <code>BINARY(16)</code> <code>UUID</code> <code>bytes</code> <code>LONGBLOB</code> <code>BYTEA</code>"},{"location":"reference/specs/database-backends/#connection-management","title":"Connection Management","text":"<p>Both backends support the same connection patterns:</p> <pre><code># Singleton connection (default)\nconn = dj.conn()\n\n# Context manager (explicit lifecycle)\nwith dj.Connection(**creds) as conn:\n    schema = dj.Schema('my_schema', connection=conn)\n\n# Reset connection\nconn = dj.conn(reset=True)\n</code></pre>"},{"location":"reference/specs/database-backends/#testing-against-multiple-backends","title":"Testing Against Multiple Backends","text":"<p>For development and CI/CD, you can test against both backends:</p> <pre><code># Test MySQL only\nDJ_BACKEND=mysql pytest tests/\n\n# Test PostgreSQL only\nDJ_BACKEND=postgresql pytest tests/\n</code></pre>"},{"location":"reference/specs/database-backends/#migration-between-backends","title":"Migration Between Backends","text":"<p>DataJoint does not provide automatic migration between backends. To migrate data:</p> <ol> <li>Export data using <code>fetch()</code> and pandas DataFrames</li> <li>Create new schemas on the target backend</li> <li>Import data using <code>insert()</code></li> </ol> <p>Schema definitions are portable\u2014the same Python class definitions work on both backends.</p>"},{"location":"reference/specs/database-backends/#see-also","title":"See Also","text":"<ul> <li>Configure Database \u2014 Connection setup</li> <li>Type System \u2014 Core type definitions</li> <li>Table Declaration \u2014 Definition syntax</li> </ul>"},{"location":"reference/specs/diagram/","title":"Diagram Specification","text":"<p>Schema visualization as directed acyclic graphs.</p>"},{"location":"reference/specs/diagram/#overview","title":"Overview","text":"<p><code>dj.Diagram</code> visualizes DataJoint schemas as directed graphs showing tables and their foreign key relationships. It provides multiple output formats including SVG, PNG, and Mermaid syntax.</p>"},{"location":"reference/specs/diagram/#design-principles","title":"Design Principles","text":"<ol> <li>Multiple output formats: Graphviz (SVG/PNG) and Mermaid for different use cases</li> <li>Graph algebra: Combine and filter diagrams with set operators</li> <li>Visual encoding: Table tiers distinguished by shape and color</li> <li>Flexible layout: Configurable direction and schema grouping</li> </ol>"},{"location":"reference/specs/diagram/#api-reference","title":"API Reference","text":""},{"location":"reference/specs/diagram/#constructor","title":"Constructor","text":"<pre><code>dj.Diagram(source, context=None)\n</code></pre> Parameter Type Default Description <code>source</code> Table, Schema, module \u2014 Source to visualize <code>context</code> dict None Namespace for class name resolution"},{"location":"reference/specs/diagram/#layout-direction","title":"Layout Direction","text":"<p>New in 2.1</p> <p>Configurable layout direction was added in DataJoint 2.1.</p> <p>Layout direction is controlled via configuration:</p> <pre><code># Check current direction\ndj.config.display.diagram_direction  # \"TB\" or \"LR\"\n\n# Set globally\ndj.config.display.diagram_direction = \"LR\"\n\n# Override temporarily\nwith dj.config.override(display__diagram_direction=\"LR\"):\n    dj.Diagram(schema).draw()\n</code></pre> Value Description <code>\"TB\"</code> Top to bottom (default) <code>\"LR\"</code> Left to right"},{"location":"reference/specs/diagram/#class-method","title":"Class Method","text":"<pre><code>dj.Diagram.from_sequence(sequence)\n</code></pre> <p>Create a combined diagram from multiple sources. Equivalent to <code>Diagram(a) + Diagram(b) + ...</code>.</p>"},{"location":"reference/specs/diagram/#operators","title":"Operators","text":"<p>Diagrams support set algebra for combining and filtering:</p> Operator Description Example <code>diag + n</code> Expand n levels downstream (children) <code>dj.Diagram(Mouse) + 2</code> <code>diag - n</code> Expand n levels upstream (parents) <code>dj.Diagram(Neuron) - 2</code> <code>diag1 + diag2</code> Union of two diagrams <code>dj.Diagram(Mouse) + dj.Diagram(Session)</code> <code>diag1 - diag2</code> Difference (remove nodes) <code>dj.Diagram(schema) - dj.Diagram(Lookup)</code> <code>diag1 * diag2</code> Intersection <code>dj.Diagram(schema1) * dj.Diagram(schema2)</code>"},{"location":"reference/specs/diagram/#common-patterns","title":"Common Patterns","text":"<pre><code># Show table with immediate parents and children\ndj.Diagram(MyTable) + 1 - 1\n\n# Show entire schema\ndj.Diagram(schema)\n\n# Show all tables downstream of a source\ndj.Diagram(SourceTable) + 10\n\n# Show ancestry of a computed table\ndj.Diagram(ComputedTable) - 10\n</code></pre> <p>Note: Order matters. <code>diagram + 1 - 1</code> may differ from <code>diagram - 1 + 1</code>.</p>"},{"location":"reference/specs/diagram/#collapsing-schemas","title":"Collapsing Schemas","text":"<p>New in 2.1</p> <p>The <code>collapse()</code> method was added in DataJoint 2.1.</p> <pre><code>diag.collapse()\n</code></pre> <p>Mark a diagram for collapsing when combined with other diagrams. Collapsed schemas appear as single nodes showing the table count.</p> <pre><code># Show schema1 expanded, schema2 as a single collapsed node\ndj.Diagram(schema1) + dj.Diagram(schema2).collapse()\n</code></pre> <p>\"Expanded wins\" rule: If a node appears in both a collapsed and non-collapsed diagram, it stays expanded. This allows you to show specific tables from a schema while collapsing the rest.</p> <pre><code># Subject is expanded, rest of analysis schema is collapsed\ndj.Diagram(Subject) + dj.Diagram(analysis).collapse()\n</code></pre>"},{"location":"reference/specs/diagram/#output-methods","title":"Output Methods","text":""},{"location":"reference/specs/diagram/#graphviz-output","title":"Graphviz Output","text":"Method Returns Description <code>make_svg()</code> <code>IPython.SVG</code> SVG for Jupyter display <code>make_png()</code> <code>BytesIO</code> PNG image bytes <code>make_image()</code> <code>ndarray</code> NumPy array (matplotlib) <code>make_dot()</code> <code>pydot.Dot</code> Graphviz DOT object"},{"location":"reference/specs/diagram/#mermaid-output","title":"Mermaid Output","text":"<p>New in 2.1</p> <p>Mermaid output was added in DataJoint 2.1.</p> <pre><code>make_mermaid() -&gt; str\n</code></pre> <p>Generates Mermaid flowchart syntax for embedding in Markdown, GitHub, or web documentation. Tables are grouped into subgraphs by schema.</p>"},{"location":"reference/specs/diagram/#display-methods","title":"Display Methods","text":"Method Description <code>draw()</code> Display with matplotlib <code>_repr_svg_()</code> Jupyter notebook auto-display"},{"location":"reference/specs/diagram/#file-output","title":"File Output","text":"<pre><code>save(filename, format=None)\n</code></pre> Parameter Type Description <code>filename</code> str Output file path <code>format</code> str <code>\"png\"</code>, <code>\"svg\"</code>, or <code>\"mermaid\"</code>. Inferred from extension if None. <p>Supported extensions: <code>.png</code>, <code>.svg</code>, <code>.mmd</code>, <code>.mermaid</code></p>"},{"location":"reference/specs/diagram/#visual-encoding","title":"Visual Encoding","text":""},{"location":"reference/specs/diagram/#table-tiers","title":"Table Tiers","text":"<p>Each table tier has a distinct visual style:</p> Tier Shape Fill Color Font Color Manual rectangle green dark green Lookup plain text gray black Computed ellipse red dark red Imported ellipse blue dark blue Part plain text transparent black"},{"location":"reference/specs/diagram/#edge-styles","title":"Edge Styles","text":"Style Meaning Solid line Primary foreign key Dashed line Non-primary foreign key Thick line Master-Part relationship Thin line Multi-valued foreign key"},{"location":"reference/specs/diagram/#node-labels","title":"Node Labels","text":"<ul> <li>Underlined: Table introduces new primary key attributes</li> <li>Plain: Table inherits all primary key attributes from parents</li> </ul>"},{"location":"reference/specs/diagram/#schema-grouping","title":"Schema Grouping","text":"<p>New in 2.1</p> <p>Automatic schema grouping was added in DataJoint 2.1.</p> <p>Tables are automatically grouped into visual clusters by their database schema. The cluster label shows the Python module name when available (following the DataJoint convention of one module per schema), otherwise the database schema name.</p> <pre><code># Multi-schema diagram - tables automatically grouped\ncombined = dj.Diagram(schema1) + dj.Diagram(schema2)\ncombined.draw()\n\n# Save with grouping\ncombined.save(\"pipeline.svg\")\n</code></pre> <p>This is useful when visualizing multi-schema pipelines to see which tables belong to which module.</p>"},{"location":"reference/specs/diagram/#examples","title":"Examples","text":""},{"location":"reference/specs/diagram/#basic-usage","title":"Basic Usage","text":"<pre><code>import datajoint as dj\n\n# Diagram from a single table\ndj.Diagram(Mouse)\n\n# Diagram from entire schema\ndj.Diagram(schema)\n\n# Diagram from module\ndj.Diagram(my_pipeline_module)\n</code></pre>"},{"location":"reference/specs/diagram/#layout-direction_1","title":"Layout Direction","text":"<pre><code># Horizontal layout using config override\nwith dj.config.override(display__diagram_direction=\"LR\"):\n    dj.Diagram(schema).draw()\n\n# Or set globally\ndj.config.display.diagram_direction = \"LR\"\ndj.Diagram(schema).save(\"pipeline.svg\")\n</code></pre>"},{"location":"reference/specs/diagram/#saving-diagrams","title":"Saving Diagrams","text":"<pre><code>diag = dj.Diagram(schema)\n\n# Save as SVG\ndiag.save(\"pipeline.svg\")\n\n# Save as PNG\ndiag.save(\"pipeline.png\")\n\n# Save as Mermaid\ndiag.save(\"pipeline.mmd\")\n\n# Explicit format\ndiag.save(\"output.txt\", format=\"mermaid\")\n</code></pre>"},{"location":"reference/specs/diagram/#mermaid-output_1","title":"Mermaid Output","text":"<pre><code>print(dj.Diagram(schema).make_mermaid())\n</code></pre> <p>Output: <pre><code>flowchart TB\n    classDef manual fill:#90EE90,stroke:#006400\n    classDef lookup fill:#D3D3D3,stroke:#696969\n    classDef computed fill:#FFB6C1,stroke:#8B0000\n    classDef imported fill:#ADD8E6,stroke:#00008B\n    classDef part fill:#FFFFFF,stroke:#000000\n\n    subgraph my_pipeline\n        Mouse[Mouse]:::manual\n        Session[Session]:::manual\n        Neuron([Neuron]):::computed\n    end\n    Mouse --&gt; Session\n    Session --&gt; Neuron\n</code></pre></p>"},{"location":"reference/specs/diagram/#combining-diagrams","title":"Combining Diagrams","text":"<pre><code># Union of schemas\ncombined = dj.Diagram(schema1) + dj.Diagram(schema2)\n\n# Intersection\ncommon = dj.Diagram(schema1) * dj.Diagram(schema2)\n\n# From sequence\ncombined = dj.Diagram.from_sequence([schema1, schema2, schema3])\n</code></pre>"},{"location":"reference/specs/diagram/#dependencies","title":"Dependencies","text":"<p>Diagram visualization requires optional dependencies:</p> <pre><code>pip install matplotlib pygraphviz\n</code></pre> <p>If dependencies are missing, <code>dj.Diagram</code> displays a warning and provides a stub class.</p>"},{"location":"reference/specs/diagram/#see-also","title":"See Also","text":"<ul> <li>How to Read Diagrams</li> <li>Query Algebra</li> <li>Table Declaration</li> </ul>"},{"location":"reference/specs/fetch-api/","title":"DataJoint 2.0 Fetch API Specification","text":""},{"location":"reference/specs/fetch-api/#overview","title":"Overview","text":"<p>DataJoint 2.0 replaces the complex <code>fetch()</code> method with a set of explicit, composable output methods. This provides better discoverability, clearer intent, and more efficient iteration.</p>"},{"location":"reference/specs/fetch-api/#design-principles","title":"Design Principles","text":"<ol> <li>Explicit over implicit: Each output format has its own method</li> <li>Composable: Use existing <code>.proj()</code> for column selection</li> <li>Lazy iteration: Single cursor streaming instead of fetch-all-keys</li> <li>Modern formats: First-class support for polars and Arrow</li> </ol>"},{"location":"reference/specs/fetch-api/#new-api-reference","title":"New API Reference","text":""},{"location":"reference/specs/fetch-api/#output-methods","title":"Output Methods","text":"Method Returns Description <code>to_dicts()</code> <code>list[dict]</code> All rows as list of dictionaries <code>to_pandas()</code> <code>DataFrame</code> pandas DataFrame with primary key as index <code>to_polars()</code> <code>polars.DataFrame</code> polars DataFrame (requires <code>datajoint[polars]</code>) <code>to_arrow()</code> <code>pyarrow.Table</code> PyArrow Table (requires <code>datajoint[arrow]</code>) <code>to_arrays()</code> <code>np.ndarray</code> numpy structured array (recarray) <code>to_arrays('a', 'b')</code> <code>tuple[array, array]</code> Tuple of arrays for specific columns <code>keys()</code> <code>list[dict]</code> Primary key values only <code>fetch1()</code> <code>dict</code> Single row as dict (raises if not exactly 1) <code>fetch1('a', 'b')</code> <code>tuple</code> Single row attribute values <code>head(limit=25)</code> <code>list[dict]</code> Preview first N entries <code>tail(limit=25)</code> <code>list[dict]</code> Preview last N entries <code>cursor(as_dict=False)</code> <code>cursor</code> Raw database cursor for manual iteration"},{"location":"reference/specs/fetch-api/#common-parameters","title":"Common Parameters","text":"<p>All output methods accept these optional parameters:</p> <pre><code>table.to_dicts(\n    order_by=None,      # str or list: column(s) to sort by, e.g. \"KEY\", \"name DESC\"\n    limit=None,         # int: maximum rows to return\n    offset=None,        # int: rows to skip\n    squeeze=False,      # bool: remove singleton dimensions from arrays\n)\n</code></pre> <p>For in-store types (attachments, filepaths), files are downloaded to <code>config[\"download_path\"]</code>. Use <code>config.override()</code> to change:</p> <pre><code>with dj.config.override(download_path=\"/data\"):\n    data = table.to_dicts()\n</code></pre>"},{"location":"reference/specs/fetch-api/#iteration","title":"Iteration","text":"<pre><code># Lazy streaming - yields one dict per row from database cursor\nfor row in table:\n    process(row)  # row is a dict\n</code></pre>"},{"location":"reference/specs/fetch-api/#migration-guide","title":"Migration Guide","text":""},{"location":"reference/specs/fetch-api/#basic-fetch-operations","title":"Basic Fetch Operations","text":"Old Pattern (1.x) New Pattern (2.0) <code>table.fetch()</code> <code>table.to_arrays()</code> or <code>table.to_dicts()</code> <code>table.fetch(format=\"array\")</code> <code>table.to_arrays()</code> <code>table.fetch(format=\"frame\")</code> <code>table.to_pandas()</code> <code>table.fetch(as_dict=True)</code> <code>table.to_dicts()</code>"},{"location":"reference/specs/fetch-api/#attribute-fetching","title":"Attribute Fetching","text":"Old Pattern (1.x) New Pattern (2.0) <code>table.fetch('a')</code> <code>table.to_arrays('a')</code> <code>a, b = table.fetch('a', 'b')</code> <code>a, b = table.to_arrays('a', 'b')</code> <code>table.fetch('a', 'b', as_dict=True)</code> <code>table.proj('a', 'b').to_dicts()</code>"},{"location":"reference/specs/fetch-api/#primary-key-fetching","title":"Primary Key Fetching","text":"Old Pattern (1.x) New Pattern (2.0) <code>table.fetch('KEY')</code> <code>table.keys()</code> <code>table.fetch(dj.key)</code> <code>table.keys()</code> <code>keys, a = table.fetch('KEY', 'a')</code> See note below <p>For mixed KEY + attribute fetch: <pre><code># Old: keys, a = table.fetch('KEY', 'a')\n# New: Combine keys() with to_arrays()\nkeys = table.keys()\na = table.to_arrays('a')\n# Or use to_dicts() which includes all columns\n</code></pre></p>"},{"location":"reference/specs/fetch-api/#ordering-limiting-offset","title":"Ordering, Limiting, Offset","text":"Old Pattern (1.x) New Pattern (2.0) <code>table.fetch(order_by='name')</code> <code>table.to_arrays(order_by='name')</code> <code>table.fetch(limit=10)</code> <code>table.to_arrays(limit=10)</code> <code>table.fetch(order_by='KEY', limit=10, offset=5)</code> <code>table.to_arrays(order_by='KEY', limit=10, offset=5)</code>"},{"location":"reference/specs/fetch-api/#single-row-fetch-fetch1","title":"Single Row Fetch (fetch1)","text":"Old Pattern (1.x) New Pattern (2.0) <code>table.fetch1()</code> <code>table.fetch1()</code> (unchanged) <code>a, b = table.fetch1('a', 'b')</code> <code>a, b = table.fetch1('a', 'b')</code> (unchanged) <code>table.fetch1('KEY')</code> <code>table.fetch1()</code> then extract pk columns"},{"location":"reference/specs/fetch-api/#configuration","title":"Configuration","text":"Old Pattern (1.x) New Pattern (2.0) <code>dj.config['fetch_format'] = 'frame'</code> Use <code>.to_pandas()</code> explicitly <code>with dj.config.override(fetch_format='frame'):</code> Use <code>.to_pandas()</code> in the block"},{"location":"reference/specs/fetch-api/#iteration_1","title":"Iteration","text":"Old Pattern (1.x) New Pattern (2.0) <code>for row in table:</code> <code>for row in table:</code> (same syntax, now lazy!) <code>list(table)</code> <code>table.to_dicts()</code>"},{"location":"reference/specs/fetch-api/#column-selection-with-proj","title":"Column Selection with proj()","text":"<p>Use <code>.proj()</code> for column selection, then apply output method:</p> <pre><code># Select specific columns\ntable.proj('col1', 'col2').to_pandas()\ntable.proj('col1', 'col2').to_dicts()\n\n# Computed columns\ntable.proj(total='price * quantity').to_pandas()\n</code></pre>"},{"location":"reference/specs/fetch-api/#removed-features","title":"Removed Features","text":""},{"location":"reference/specs/fetch-api/#removed-methods-and-parameters","title":"Removed Methods and Parameters","text":"<ul> <li><code>fetch()</code> method - use explicit output methods</li> <li><code>fetch('KEY')</code> - use <code>keys()</code></li> <li><code>dj.key</code> class - use <code>keys()</code> method</li> <li><code>format=</code> parameter - use explicit methods</li> <li><code>as_dict=</code> parameter - use <code>to_dicts()</code></li> <li><code>config['fetch_format']</code> setting - use explicit methods</li> </ul>"},{"location":"reference/specs/fetch-api/#removed-imports","title":"Removed Imports","text":"<pre><code># Old (removed)\nfrom datajoint import key\nresult = table.fetch(dj.key)\n\n# New\nresult = table.keys()\n</code></pre>"},{"location":"reference/specs/fetch-api/#examples","title":"Examples","text":""},{"location":"reference/specs/fetch-api/#example-1-basic-data-retrieval","title":"Example 1: Basic Data Retrieval","text":"<pre><code># Get all data as DataFrame\ndf = Experiment().to_pandas()\n\n# Get all data as list of dicts\nrows = Experiment().to_dicts()\n\n# Get all data as numpy array\narr = Experiment().to_arrays()\n</code></pre>"},{"location":"reference/specs/fetch-api/#example-2-filtered-and-sorted-query","title":"Example 2: Filtered and Sorted Query","text":"<pre><code># Get recent experiments, sorted by date\nrecent = (Experiment() &amp; \"date &gt; '2024-01-01'\").to_pandas(\n    order_by='date DESC',\n    limit=100\n)\n</code></pre>"},{"location":"reference/specs/fetch-api/#example-3-specific-columns","title":"Example 3: Specific Columns","text":"<pre><code># Fetch specific columns as arrays\nnames, dates = Experiment().to_arrays('name', 'date')\n\n# Or with primary key included\nnames, dates = Experiment().to_arrays('name', 'date', include_key=True)\n</code></pre>"},{"location":"reference/specs/fetch-api/#example-4-primary-keys-for-iteration","title":"Example 4: Primary Keys for Iteration","text":"<pre><code># Get keys for restriction\nkeys = Experiment().keys()\nfor key in keys:\n    process(Session() &amp; key)\n</code></pre>"},{"location":"reference/specs/fetch-api/#example-5-single-row","title":"Example 5: Single Row","text":"<pre><code># Get one row as dict\nrow = (Experiment() &amp; key).fetch1()\n\n# Get specific attributes\nname, date = (Experiment() &amp; key).fetch1('name', 'date')\n</code></pre>"},{"location":"reference/specs/fetch-api/#example-6-lazy-iteration","title":"Example 6: Lazy Iteration","text":"<pre><code># Stream rows efficiently (single database cursor)\nfor row in Experiment():\n    if should_process(row):\n        process(row)\n    if done:\n        break  # Early termination - no wasted fetches\n</code></pre>"},{"location":"reference/specs/fetch-api/#example-7-modern-dataframe-libraries","title":"Example 7: Modern DataFrame Libraries","text":"<pre><code># Polars (fast, modern)\nimport polars as pl\ndf = Experiment().to_polars()\nresult = df.filter(pl.col('value') &gt; 100).group_by('category').agg(pl.mean('value'))\n\n# PyArrow (zero-copy interop)\ntable = Experiment().to_arrow()\n# Can convert to pandas or polars with zero copy\n</code></pre>"},{"location":"reference/specs/fetch-api/#performance-considerations","title":"Performance Considerations","text":""},{"location":"reference/specs/fetch-api/#lazy-iteration","title":"Lazy Iteration","text":"<p>The new iteration is significantly more efficient:</p> <pre><code># Old (1.x): N+1 queries\n# 1. fetch(\"KEY\") gets ALL keys\n# 2. fetch1() for EACH key\n\n# New (2.0): Single query\n# Streams rows from one cursor\nfor row in table:\n    ...\n</code></pre>"},{"location":"reference/specs/fetch-api/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li><code>to_dicts()</code>: Returns full list in memory</li> <li><code>for row in table:</code>: Streams one row at a time</li> <li><code>to_arrays(limit=N)</code>: Fetches only N rows</li> </ul>"},{"location":"reference/specs/fetch-api/#format-selection","title":"Format Selection","text":"Use Case Recommended Method Data analysis <code>to_pandas()</code> or <code>to_polars()</code> JSON API responses <code>to_dicts()</code> Numeric computation <code>to_arrays()</code> Large datasets <code>for row in table:</code> (streaming) Interop with other tools <code>to_arrow()</code>"},{"location":"reference/specs/fetch-api/#error-messages","title":"Error Messages","text":"<p>When attempting to use removed methods, users see helpful error messages:</p> <pre><code>&gt;&gt;&gt; table.fetch()\nAttributeError: fetch() has been removed in DataJoint 2.0.\nUse to_dicts(), to_pandas(), to_arrays(), or keys() instead.\nSee table.fetch.__doc__ for details.\n</code></pre>"},{"location":"reference/specs/fetch-api/#optional-dependencies","title":"Optional Dependencies","text":"<p>Install optional dependencies for additional output formats:</p> <pre><code># For polars support\npip install datajoint[polars]\n\n# For PyArrow support\npip install datajoint[arrow]\n\n# For both\npip install datajoint[polars,arrow]\n</code></pre>"},{"location":"reference/specs/job-metadata/","title":"Hidden Job Metadata in Computed Tables","text":""},{"location":"reference/specs/job-metadata/#overview","title":"Overview","text":"<p>Job execution metadata (start time, duration, code version) should be persisted in computed tables themselves, not just in ephemeral job entries. This is accomplished using hidden attributes.</p>"},{"location":"reference/specs/job-metadata/#motivation","title":"Motivation","text":"<p>The current job table (<code>~~table_name</code>) tracks execution metadata, but: 1. Job entries are deleted after completion (unless <code>keep_completed=True</code>) 2. Users often need to know when and with what code version each row was computed 3. This metadata should be transparent - not cluttering the user-facing schema</p> <p>Hidden attributes (prefixed with <code>_</code>) provide the solution: stored in the database but filtered from user-facing APIs.</p>"},{"location":"reference/specs/job-metadata/#hidden-job-metadata-attributes","title":"Hidden Job Metadata Attributes","text":"Attribute Type Description <code>_job_start_time</code> datetime(3) When computation began <code>_job_duration</code> float32 Computation duration in seconds <code>_job_version</code> varchar(64) Code version (e.g., git commit hash) <p>Design notes: - <code>_job_duration</code> (elapsed time) rather than <code>_job_completed_time</code> because duration is more informative for performance analysis - <code>varchar(64)</code> for version is sufficient for git hashes (40 chars for SHA-1, 7-8 for short hash) - <code>datetime(3)</code> provides millisecond precision</p>"},{"location":"reference/specs/job-metadata/#configuration","title":"Configuration","text":""},{"location":"reference/specs/job-metadata/#settings-structure","title":"Settings Structure","text":"<p>Job metadata is controlled via <code>config.jobs</code> settings:</p> <pre><code>class JobsSettings(BaseSettings):\n    \"\"\"Job queue configuration for AutoPopulate 2.0.\"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"DJ_JOBS_\",\n        case_sensitive=False,\n        extra=\"forbid\",\n        validate_assignment=True,\n    )\n\n    # Existing settings\n    auto_refresh: bool = Field(default=True, ...)\n    keep_completed: bool = Field(default=False, ...)\n    stale_timeout: int = Field(default=3600, ...)\n    default_priority: int = Field(default=5, ...)\n    version_method: Literal[\"git\", \"none\"] | None = Field(default=None, ...)\n    allow_new_pk_fields_in_computed_tables: bool = Field(default=False, ...)\n\n    # New setting for hidden job metadata\n    add_job_metadata: bool = Field(\n        default=False,\n        description=\"Add hidden job metadata attributes (_job_start_time, _job_duration, _job_version) \"\n        \"to Computed and Imported tables during declaration. Tables created without this setting \"\n        \"will not receive metadata updates during populate.\"\n    )\n</code></pre>"},{"location":"reference/specs/job-metadata/#access-patterns","title":"Access Patterns","text":"<pre><code>import datajoint as dj\n\n# Read setting\ndj.config.jobs.add_job_metadata  # False (default)\n\n# Enable programmatically\ndj.config.jobs.add_job_metadata = True\n\n# Enable via environment variable\n# DJ_JOBS_ADD_JOB_METADATA=true\n\n# Enable in config file (dj_config.yaml)\n# jobs:\n#   add_job_metadata: true\n\n# Temporary override\nwith dj.config.override(jobs={\"add_job_metadata\": True}):\n    schema(MyComputedTable)  # Declared with metadata columns\n</code></pre>"},{"location":"reference/specs/job-metadata/#setting-interactions","title":"Setting Interactions","text":"Setting Effect on Job Metadata <code>add_job_metadata=True</code> New Computed/Imported tables get hidden metadata columns <code>add_job_metadata=False</code> Tables declared without metadata columns (default) <code>version_method=\"git\"</code> <code>_job_version</code> populated with git short hash <code>version_method=\"none\"</code> <code>_job_version</code> left empty <code>version_method=None</code> <code>_job_version</code> left empty (same as \"none\")"},{"location":"reference/specs/job-metadata/#behavior-at-declaration-vs-populate","title":"Behavior at Declaration vs Populate","text":"<code>add_job_metadata</code> at declare <code>add_job_metadata</code> at populate Result True True Metadata columns created and populated True False Metadata columns exist but not populated False True No metadata columns, populate skips silently False False No metadata columns, normal behavior"},{"location":"reference/specs/job-metadata/#retrofitting-existing-tables","title":"Retrofitting Existing Tables","text":"<p>Tables created before enabling <code>add_job_metadata</code> do not have the hidden metadata columns. To add metadata columns to existing tables, use the migration utility (not automatic):</p> <pre><code>from datajoint.migrate import add_job_metadata_columns\n\n# Add hidden metadata columns to specific table\nadd_job_metadata_columns(MyComputedTable)\n\n# Add to all Computed/Imported tables in a schema\nadd_job_metadata_columns(schema)\n</code></pre> <p>This utility: - ALTERs the table to add the three hidden columns - Does NOT populate existing rows (metadata remains NULL) - Future <code>populate()</code> calls will populate metadata for new rows</p>"},{"location":"reference/specs/job-metadata/#behavior","title":"Behavior","text":""},{"location":"reference/specs/job-metadata/#declaration-time","title":"Declaration-time","text":"<p>When <code>config.jobs.add_job_metadata=True</code> and a Computed/Imported table is declared: - Hidden metadata columns are added to the table definition - Only master tables receive metadata columns; Part tables never get them</p>"},{"location":"reference/specs/job-metadata/#population-time","title":"Population-time","text":"<p>After <code>make()</code> completes successfully: 1. Check if the table has hidden metadata columns 2. If yes: UPDATE the just-inserted rows with start_time, duration, version 3. If no: Silently skip (no error, no ALTER)</p> <p>This applies to both: - Direct mode (<code>reserve_jobs=False</code>): Single-process populate - Distributed mode (<code>reserve_jobs=True</code>): Multi-worker with job table coordination</p>"},{"location":"reference/specs/job-metadata/#excluding-hidden-attributes-from-binary-operators","title":"Excluding Hidden Attributes from Binary Operators","text":""},{"location":"reference/specs/job-metadata/#problem-statement","title":"Problem Statement","text":"<p>If two tables have hidden attributes with the same name (e.g., both have <code>_job_start_time</code>), SQL's NATURAL JOIN would incorrectly match on them:</p> <pre><code>-- NATURAL JOIN matches ALL common attributes including hidden\nSELECT * FROM table_a NATURAL JOIN table_b\n-- Would incorrectly match on _job_start_time!\n</code></pre>"},{"location":"reference/specs/job-metadata/#solution-replace-natural-join-with-using-clause","title":"Solution: Replace NATURAL JOIN with USING Clause","text":"<p>Hidden attributes must be excluded from all binary operator considerations. The result of a join does not preserve hidden attributes from its operands.</p> <p>Pre-2.0 implementation: <pre><code>def from_clause(self):\n    clause = next(support)\n    for s, left in zip(support, self._left):\n        clause += \" NATURAL{left} JOIN {clause}\".format(...)\n</code></pre></p> <p>DataJoint 2.0 implementation: <pre><code>def from_clause(self):\n    clause = next(support)\n    for s, (left, using_attrs) in zip(support, self._joins):\n        if using_attrs:\n            using = \"USING ({})\".format(\", \".join(f\"`{a}`\" for a in using_attrs))\n            clause += \" {left}JOIN {s} {using}\".format(\n                left=\"LEFT \" if left else \"\",\n                s=s,\n                using=using\n            )\n        else:\n            # Cross join (no common non-hidden attributes)\n            clause += \" CROSS JOIN \" + s if not left else \" LEFT JOIN \" + s + \" ON TRUE\"\n    return clause\n</code></pre></p>"},{"location":"reference/specs/job-metadata/#changes-required","title":"Changes Required","text":""},{"location":"reference/specs/job-metadata/#1-queryexpression_left-queryexpression_joins","title":"1. <code>QueryExpression._left</code> \u2192 <code>QueryExpression._joins</code>","text":"<p>Replace <code>_left: List[bool]</code> with <code>_joins: List[Tuple[bool, List[str]]]</code></p> <p>Each join stores: - <code>left</code>: Whether it's a left join - <code>using_attrs</code>: Non-hidden common attributes to join on</p> <pre><code># Before\nresult._left = self._left + [left] + other._left\n\n# After\njoin_attributes = [n for n in self.heading.names if n in other.heading.names]\nresult._joins = self._joins + [(left, join_attributes)] + other._joins\n</code></pre>"},{"location":"reference/specs/job-metadata/#2-headingnames-existing-behavior","title":"2. <code>heading.names</code> (existing behavior)","text":"<p>Already filters out hidden attributes: <pre><code>@property\ndef names(self):\n    return [k for k in self.attributes]  # attributes excludes is_hidden=True\n</code></pre></p> <p>This ensures join attribute computation automatically excludes hidden attributes.</p>"},{"location":"reference/specs/job-metadata/#behavior-summary","title":"Behavior Summary","text":"Scenario Hidden Attributes Result <code>A * B</code> (join) Same hidden attr in both NOT matched - excluded from USING <code>A &amp; B</code> (restriction) Same hidden attr in both NOT matched <code>A - B</code> (anti-restriction) Same hidden attr in both NOT matched <code>A.proj()</code> Hidden attrs in A NOT projected (unless explicitly named) <code>A.to_dicts()</code> Hidden attrs in A NOT returned by default"},{"location":"reference/specs/job-metadata/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/specs/job-metadata/#1-declaration-declarepy","title":"1. Declaration (declare.py)","text":"<pre><code>def declare(full_table_name, definition, context):\n    # ... existing code ...\n\n    # Add hidden job metadata for auto-populated tables\n    if config.jobs.add_job_metadata and table_tier in (TableTier.COMPUTED, TableTier.IMPORTED):\n        # Only for master tables, not parts\n        if not is_part_table:\n            job_metadata_sql = [\n                \"`_job_start_time` datetime(3) DEFAULT NULL\",\n                \"`_job_duration` float DEFAULT NULL\",\n                \"`_job_version` varchar(64) DEFAULT ''\",\n            ]\n            attribute_sql.extend(job_metadata_sql)\n</code></pre>"},{"location":"reference/specs/job-metadata/#2-population-autopopulatepy","title":"2. Population (autopopulate.py)","text":"<pre><code>def _populate1(self, key, callback, use_jobs, jobs):\n    start_time = datetime.now()\n    version = _get_job_version()\n\n    # ... call make() ...\n\n    duration = time.time() - start_time.timestamp()\n\n    # Update job metadata if table has the hidden attributes\n    if self._has_job_metadata_attrs():\n        self._update_job_metadata(\n            key,\n            start_time=start_time,\n            duration=duration,\n            version=version\n        )\n\ndef _has_job_metadata_attrs(self):\n    \"\"\"Check if table has hidden job metadata columns.\"\"\"\n    hidden_attrs = self.heading._attributes  # includes hidden\n    return '_job_start_time' in hidden_attrs\n\ndef _update_job_metadata(self, key, start_time, duration, version):\n    \"\"\"Update hidden job metadata for the given key.\"\"\"\n    # UPDATE using primary key\n    pk_condition = make_condition(self, key, set())\n    self.connection.query(\n        f\"UPDATE {self.full_table_name} SET \"\n        f\"`_job_start_time`=%s, `_job_duration`=%s, `_job_version`=%s \"\n        f\"WHERE {pk_condition}\",\n        args=(start_time, duration, version[:64])\n    )\n</code></pre>"},{"location":"reference/specs/job-metadata/#3-job-table-jobspy","title":"3. Job table (jobs.py)","text":"<p>Update version field length: <pre><code>version=\"\"      : varchar(64)\n</code></pre></p>"},{"location":"reference/specs/job-metadata/#4-version-helper","title":"4. Version helper","text":"<pre><code>def _get_job_version() -&gt; str:\n    \"\"\"Get version string, truncated to 64 chars.\"\"\"\n    from .settings import config\n\n    method = config.jobs.version_method\n    if method is None or method == \"none\":\n        return \"\"\n    elif method == \"git\":\n        try:\n            result = subprocess.run(\n                [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],\n                capture_output=True,\n                text=True,\n                timeout=5,\n            )\n            return result.stdout.strip()[:64] if result.returncode == 0 else \"\"\n        except Exception:\n            return \"\"\n    return \"\"\n</code></pre>"},{"location":"reference/specs/job-metadata/#example-usage","title":"Example Usage","text":"<pre><code># Enable job metadata for new tables\ndj.config.jobs.add_job_metadata = True\n\n@schema\nclass ProcessedData(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    result : float\n    \"\"\"\n\n    def make(self, key):\n        # User code - unaware of hidden attributes\n        self.insert1({**key, 'result': compute(key)})\n\n# Job metadata automatically added and populated:\n# _job_start_time, _job_duration, _job_version\n\n# User-facing API unaffected:\nProcessedData().heading.names  # ['raw_data_id', 'result']\nProcessedData().to_dicts()  # Returns only visible attributes\n\n# Access hidden attributes explicitly if needed:\nProcessedData().to_arrays('_job_start_time', '_job_duration', '_job_version')\n</code></pre>"},{"location":"reference/specs/job-metadata/#summary-of-design-decisions","title":"Summary of Design Decisions","text":"Decision Resolution Configuration <code>config.jobs.add_job_metadata</code> (default False) Environment variable <code>DJ_JOBS_ADD_JOB_METADATA</code> Existing tables No automatic ALTER - silently skip metadata if columns absent Retrofitting Manual via <code>datajoint.migrate.add_job_metadata_columns()</code> utility Populate modes Record metadata in both direct and distributed modes Part tables No metadata columns - only master tables Version length varchar(64) in both jobs table and computed tables Binary operators Hidden attributes excluded via USING clause instead of NATURAL JOIN Failed makes N/A - transaction rolls back, no rows to update"},{"location":"reference/specs/master-part/","title":"Master-Part Relationships Specification","text":"<p>Looking for a task-oriented guide?</p> <p>See Master-Part Tables for step-by-step examples.</p>"},{"location":"reference/specs/master-part/#overview","title":"Overview","text":"<p>Master-Part relationships model compositional data where a master entity contains multiple detail records. Part tables provide a way to store variable-length, structured data associated with each master entity while maintaining strict referential integrity.</p>"},{"location":"reference/specs/master-part/#1-definition","title":"1. Definition","text":""},{"location":"reference/specs/master-part/#11-master-table","title":"1.1 Master Table","text":"<p>Any table class (<code>Manual</code>, <code>Lookup</code>, <code>Imported</code>, <code>Computed</code>) can serve as a master:</p> <pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    session_idx : int16\n    ---\n    session_date : date\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/master-part/#12-part-table","title":"1.2 Part Table","text":"<p>Part tables are nested classes inheriting from <code>dj.Part</code>:</p> <pre><code>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    session_idx : int16\n    ---\n    session_date : date\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32\n        ---\n        stimulus : varchar(32)\n        response : varchar(32)\n        \"\"\"\n</code></pre>"},{"location":"reference/specs/master-part/#13-sql-naming","title":"1.3 SQL Naming","text":"Python SQL Table Name <code>Session</code> <code>schema</code>.<code>session</code> <code>Session.Trial</code> <code>schema</code>.<code>session__trial</code> <p>Part tables use double underscore (<code>__</code>) separator in SQL.</p>"},{"location":"reference/specs/master-part/#14-master-reference","title":"1.4 Master Reference","text":"<p>Within a Part definition, reference the master using:</p> <pre><code>-&gt; master        # lowercase keyword (preferred)\n-&gt; Session       # explicit class name\n</code></pre> <p>The <code>-&gt; master</code> reference: - Automatically inherits master's primary key - Creates foreign key constraint to master - Enforces ON DELETE RESTRICT (by default)</p>"},{"location":"reference/specs/master-part/#2-integrity-constraints","title":"2. Integrity Constraints","text":""},{"location":"reference/specs/master-part/#21-compositional-integrity","title":"2.1 Compositional Integrity","text":"<p>Master-Part relationships enforce compositional integrity:</p> <ol> <li>Existence: Parts cannot exist without their master</li> <li>Cohesion: Parts should be deleted/dropped with their master</li> <li>Atomicity: Master and parts form a logical unit</li> </ol>"},{"location":"reference/specs/master-part/#22-foreign-key-behavior","title":"2.2 Foreign Key Behavior","text":"<p>Part tables have implicit foreign key to master:</p> <pre><code>FOREIGN KEY (master_pk) REFERENCES master_table (master_pk)\nON UPDATE CASCADE\nON DELETE RESTRICT\n</code></pre> <p>The <code>ON DELETE RESTRICT</code> prevents orphaned parts at the database level.</p>"},{"location":"reference/specs/master-part/#3-insert-operations","title":"3. Insert Operations","text":""},{"location":"reference/specs/master-part/#31-master-first-insertion","title":"3.1 Master-First Insertion","text":"<p>Master must exist before inserting parts:</p> <pre><code># Insert master\nSession.insert1({\n    'subject_id': 'M001',\n    'session_idx': 1,\n    'session_date': '2026-01-08'\n})\n\n# Insert parts\nSession.Trial.insert([\n    {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 1, 'stimulus': 'A', 'response': 'left'},\n    {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 2, 'stimulus': 'B', 'response': 'right'},\n])\n</code></pre>"},{"location":"reference/specs/master-part/#32-atomic-insertion","title":"3.2 Atomic Insertion","text":"<p>For atomic master+parts insertion, use transactions:</p> <pre><code>with dj.conn().transaction:\n    Session.insert1(master_data)\n    Session.Trial.insert(trials_data)\n</code></pre>"},{"location":"reference/specs/master-part/#33-computed-tables-with-parts","title":"3.3 Computed Tables with Parts","text":"<p>In <code>make()</code> methods, use <code>self.insert1()</code> for master and <code>self.PartName.insert()</code> for parts:</p> <pre><code>class ProcessedSession(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    n_trials : int32\n    \"\"\"\n\n    class TrialResult(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Session.Trial\n        ---\n        score : float32\n        \"\"\"\n\n    def make(self, key):\n        trials = (Session.Trial &amp; key).fetch()\n        results = process(trials)\n\n        self.insert1({**key, 'n_trials': len(trials)})\n        self.TrialResult.insert(results)\n</code></pre>"},{"location":"reference/specs/master-part/#4-delete-operations","title":"4. Delete Operations","text":""},{"location":"reference/specs/master-part/#41-cascade-from-master","title":"4.1 Cascade from Master","text":"<p>Deleting from master cascades to parts:</p> <pre><code># Deletes session AND all its trials\n(Session &amp; {'subject_id': 'M001', 'session_idx': 1}).delete()\n</code></pre>"},{"location":"reference/specs/master-part/#42-part-integrity-parameter","title":"4.2 Part Integrity Parameter","text":"<p>Direct deletion from Part tables is controlled by <code>part_integrity</code>:</p> <pre><code>def delete(self, part_integrity: str = \"enforce\", ...) -&gt; int\n</code></pre> Value Behavior <code>\"enforce\"</code> (default) Error if parts deleted without masters <code>\"ignore\"</code> Allow deleting parts without masters (breaks integrity) <code>\"cascade\"</code> Also delete masters when parts are deleted"},{"location":"reference/specs/master-part/#43-default-behavior-enforce","title":"4.3 Default Behavior (enforce)","text":"<pre><code># Error: Cannot delete from Part directly\nSession.Trial.delete()\n# DataJointError: Cannot delete from a Part directly.\n# Delete from master instead, or use part_integrity='ignore'\n# to break integrity, or part_integrity='cascade' to also delete master.\n</code></pre>"},{"location":"reference/specs/master-part/#44-breaking-integrity-ignore","title":"4.4 Breaking Integrity (ignore)","text":"<pre><code># Allow direct part deletion (master retains incomplete parts)\n(Session.Trial &amp; {'trial_idx': 1}).delete(part_integrity=\"ignore\")\n</code></pre> <p>Use cases: - Removing specific invalid trials - Partial data cleanup - Testing/debugging</p> <p>Warning: This leaves masters with incomplete part data.</p>"},{"location":"reference/specs/master-part/#45-cascade-to-master-cascade","title":"4.5 Cascade to Master (cascade)","text":"<pre><code># Delete parts AND their masters\n(Session.Trial &amp; condition).delete(part_integrity=\"cascade\")\n</code></pre> <p>Behavior: - Identifies affected masters - Deletes masters (which cascades to ALL their parts) - Maintains compositional integrity</p>"},{"location":"reference/specs/master-part/#46-behavior-matrix","title":"4.6 Behavior Matrix","text":"Operation Result <code>Master.delete()</code> Deletes master + all parts <code>Part.delete()</code> Error (default) <code>Part.delete(part_integrity=\"ignore\")</code> Deletes parts only <code>Part.delete(part_integrity=\"cascade\")</code> Deletes parts + masters"},{"location":"reference/specs/master-part/#5-drop-operations","title":"5. Drop Operations","text":""},{"location":"reference/specs/master-part/#51-drop-master","title":"5.1 Drop Master","text":"<p>Dropping a master table also drops all its part tables:</p> <pre><code>Session.drop()  # Drops Session AND Session.Trial\n</code></pre>"},{"location":"reference/specs/master-part/#52-drop-part-directly","title":"5.2 Drop Part Directly","text":"<p>Part tables cannot be dropped directly by default:</p> <pre><code>Session.Trial.drop()\n# DataJointError: Cannot drop a Part directly. Drop master instead,\n# or use part_integrity='ignore' to force.\n\n# Override with part_integrity=\"ignore\"\nSession.Trial.drop(part_integrity=\"ignore\")\n</code></pre> <p>Note: <code>part_integrity=\"cascade\"</code> is not supported for drop (too destructive).</p>"},{"location":"reference/specs/master-part/#53-schema-drop","title":"5.3 Schema Drop","text":"<p>Dropping schema drops all tables including masters and parts:</p> <pre><code>schema.drop(prompt=False)\n</code></pre>"},{"location":"reference/specs/master-part/#6-query-operations","title":"6. Query Operations","text":""},{"location":"reference/specs/master-part/#61-accessing-parts","title":"6.1 Accessing Parts","text":"<pre><code># From master class\nSession.Trial\n\n# From master instance\nsession = Session()\nsession.Trial\n</code></pre>"},{"location":"reference/specs/master-part/#62-joining-master-and-parts","title":"6.2 Joining Master and Parts","text":"<pre><code># All trials with session info\nSession * Session.Trial\n\n# Filtered\n(Session &amp; {'subject_id': 'M001'}) * Session.Trial\n</code></pre>"},{"location":"reference/specs/master-part/#63-aggregating-parts","title":"6.3 Aggregating Parts","text":"<pre><code># Count trials per session\nSession.aggr(Session.Trial, n_trials='count(trial_idx)')\n\n# Statistics\nSession.aggr(\n    Session.Trial,\n    n_trials='count(trial_idx)',\n    n_correct='sum(response = stimulus)'\n)\n</code></pre>"},{"location":"reference/specs/master-part/#7-best-practices","title":"7. Best Practices","text":""},{"location":"reference/specs/master-part/#71-when-to-use-part-tables","title":"7.1 When to Use Part Tables","text":"<p>Good use cases: - Trials within sessions - Electrodes within probes - Cells within imaging fields - Frames within videos - Rows within files</p> <p>Avoid when: - Parts have independent meaning (use regular FK instead) - Need to query parts without master context - Parts reference multiple masters</p>"},{"location":"reference/specs/master-part/#72-naming-conventions","title":"7.2 Naming Conventions","text":"<pre><code>class Master(dj.Manual):\n    class Detail(dj.Part):      # Singular, descriptive\n        ...\n    class Items(dj.Part):       # Or plural for collections\n        ...\n</code></pre>"},{"location":"reference/specs/master-part/#73-part-primary-keys","title":"7.3 Part Primary Keys","text":"<p>Include minimal additional keys beyond master reference:</p> <pre><code>class Session(dj.Manual):\n    definition = \"\"\"\n    session_id : int64\n    ---\n    ...\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32      # Only trial-specific key\n        ---\n        ...\n        \"\"\"\n</code></pre>"},{"location":"reference/specs/master-part/#74-avoiding-deep-nesting","title":"7.4 Avoiding Deep Nesting","text":"<p>Part tables cannot have their own parts. For hierarchical data:</p> <pre><code># Instead of nested parts, use separate tables with FKs\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"...\"\"\"\n    class Trial(dj.Part):\n        definition = \"\"\"...\"\"\"\n\n@schema\nclass TrialEvent(dj.Manual):  # Not a Part, but references Trial\n    definition = \"\"\"\n    -&gt; Session.Trial\n    event_idx : int16\n    ---\n    event_time : float32\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/master-part/#8-implementation-reference","title":"8. Implementation Reference","text":"File Purpose <code>user_tables.py</code> Part class definition <code>table.py</code> delete() with part_integrity <code>schemas.py</code> Part table decoration <code>declare.py</code> Part table SQL generation"},{"location":"reference/specs/master-part/#9-error-messages","title":"9. Error Messages","text":"Error Cause Solution \"Cannot delete from Part directly\" Called Part.delete() with part_integrity=\"enforce\" Delete from master, or use part_integrity=\"ignore\" or \"cascade\" \"Cannot drop Part directly\" Called Part.drop() with part_integrity=\"enforce\" Drop master table, or use part_integrity=\"ignore\" \"Attempt to delete part before master\" Cascade would delete part without master Use part_integrity=\"ignore\" or \"cascade\""},{"location":"reference/specs/npy-codec/","title":"<code>&lt;npy&gt;</code> Codec Specification","text":"<p>Schema-addressed storage for numpy arrays as portable <code>.npy</code> files.</p>"},{"location":"reference/specs/npy-codec/#overview","title":"Overview","text":"<p>The <code>&lt;npy@&gt;</code> codec stores numpy arrays as standard <code>.npy</code> files using schema-addressed paths that mirror the database structure. On fetch, it returns <code>NpyRef</code>\u2014a lazy reference that provides metadata access without downloading, and transparent numpy integration via the <code>__array__</code> protocol.</p> <p>Key characteristics:</p> <ul> <li>Store only: Requires <code>@</code> modifier (<code>&lt;npy@&gt;</code> or <code>&lt;npy@store&gt;</code>)</li> <li>Schema-addressed: Paths mirror database structure (<code>{schema}/{table}/{pk}/{attr}.npy</code>)</li> <li>Lazy loading: Shape/dtype available without download</li> <li>Transparent: Use directly in numpy operations</li> <li>Portable: Standard <code>.npy</code> format readable by numpy, MATLAB, etc.</li> </ul>"},{"location":"reference/specs/npy-codec/#quick-start","title":"Quick Start","text":"<pre><code>import datajoint as dj\nimport numpy as np\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int\n    ---\n    waveform : &lt;npy@&gt;\n    \"\"\"\n\n# Insert - just pass the array\nRecording.insert1({\n    'recording_id': 1,\n    'waveform': np.random.randn(1000, 32),\n})\n\n# Fetch - returns NpyRef (lazy)\nref = (Recording &amp; 'recording_id=1').fetch1('waveform')\n\n# Metadata without download\nref.shape   # (1000, 32)\nref.dtype   # float64\n\n# Use in numpy ops - downloads automatically\nmean = np.mean(ref, axis=0)\n\n# Or load explicitly\narr = ref.load()\n</code></pre>"},{"location":"reference/specs/npy-codec/#npyref-lazy-array-reference","title":"NpyRef: Lazy Array Reference","text":"<p>When you fetch an <code>&lt;npy@&gt;</code> attribute, you get an <code>NpyRef</code> object:</p> <pre><code>ref = (Recording &amp; key).fetch1('waveform')\ntype(ref)  # &lt;class 'datajoint.builtin_codecs.NpyRef'&gt;\n</code></pre>"},{"location":"reference/specs/npy-codec/#metadata-access-no-io","title":"Metadata Access (No I/O)","text":"<pre><code>ref.shape    # tuple: (1000, 32)\nref.dtype    # numpy.dtype: float64\nref.ndim     # int: 2\nref.size     # int: 32000\nref.nbytes   # int: 256000 (estimated)\nref.path     # str: \"my_schema/recording/recording_id=1/waveform.npy\"\nref.store    # str or None: store name\nref.is_loaded  # bool: False (until loaded)\n</code></pre>"},{"location":"reference/specs/npy-codec/#loading-data","title":"Loading Data","text":"<p>Explicit loading: <pre><code>arr = ref.load()  # Downloads, caches, returns np.ndarray\narr = ref.load()  # Returns cached copy (no re-download)\n</code></pre></p> <p>Implicit loading via <code>__array__</code>: <pre><code># These all trigger automatic download\nresult = ref + 1\nresult = np.mean(ref)\nresult = np.dot(ref, weights)\narr = np.asarray(ref)\n</code></pre></p> <p>Indexing/slicing: <pre><code>first_row = ref[0]      # Loads then indexes\nsubset = ref[100:200]   # Loads then slices\n</code></pre></p>"},{"location":"reference/specs/npy-codec/#memory-mapping","title":"Memory Mapping","text":"<p>For very large arrays, use <code>mmap_mode</code> to access data without loading it all:</p> <pre><code># Memory-mapped loading (random access)\narr = ref.load(mmap_mode='r')\n\n# Efficient random access - only reads needed portions\nslice = arr[1000:2000, :]\nchunk = arr[::100]\n</code></pre> <p>Modes: - <code>'r'</code> - Read-only (recommended) - <code>'r+'</code> - Read-write (modifications persist) - <code>'c'</code> - Copy-on-write (changes not saved)</p> <p>Performance characteristics: - Local filesystem stores: memory-maps the file directly (zero-copy) - Remote stores (S3, GCS): downloads to local cache first, then memory-maps</p> <p>When to use: - Arrays too large to fit in memory - Only need random access to portions of the array - Processing data in chunks</p>"},{"location":"reference/specs/npy-codec/#safe-bulk-fetch","title":"Safe Bulk Fetch","text":"<p>The lazy design protects against accidental mass downloads:</p> <pre><code># Fetch 10,000 recordings - NO downloads happen yet\nrecs = Recording.fetch()\n\n# Inspect without downloading\nfor rec in recs:\n    ref = rec['waveform']\n    print(f\"Shape: {ref.shape}, dtype: {ref.dtype}\")  # No I/O\n\n# Download only what you need\nlarge_arrays = [rec['waveform'] for rec in recs if rec['waveform'].shape[0] &gt; 1000]\nfor ref in large_arrays:\n    process(ref.load())  # Downloads here\n</code></pre>"},{"location":"reference/specs/npy-codec/#repr-for-debugging","title":"Repr for Debugging","text":"<pre><code>&gt;&gt;&gt; ref\nNpyRef(shape=(1000, 32), dtype=float64, not loaded)\n\n&gt;&gt;&gt; ref.load()\n&gt;&gt;&gt; ref\nNpyRef(shape=(1000, 32), dtype=float64, loaded)\n</code></pre>"},{"location":"reference/specs/npy-codec/#table-definition","title":"Table Definition","text":"<pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int\n    ---\n    waveform : &lt;npy@&gt;           # default store\n    spectrogram : &lt;npy@archive&gt;  # specific store\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/npy-codec/#storage-details","title":"Storage Details","text":""},{"location":"reference/specs/npy-codec/#addressing-scheme","title":"Addressing Scheme","text":"<p>The <code>&lt;npy@&gt;</code> codec uses schema-addressed storage, where paths mirror the database schema structure. This creates a browsable organization in object storage that reflects your data model.</p>"},{"location":"reference/specs/npy-codec/#type-chain","title":"Type Chain","text":"<pre><code>&lt;npy@&gt; \u2192 \"json\" (metadata stored in JSON column)\n</code></pre>"},{"location":"reference/specs/npy-codec/#file-format","title":"File Format","text":"<ul> <li>Format: NumPy <code>.npy</code> (version 1.0 or 2.0 depending on array size)</li> <li>Encoding: <code>numpy.save()</code> with <code>allow_pickle=False</code></li> <li>Extension: <code>.npy</code></li> </ul>"},{"location":"reference/specs/npy-codec/#schema-addressed-path-construction","title":"Schema-Addressed Path Construction","text":"<pre><code>{schema}/{table}/{primary_key_values}/{attribute}.npy\n</code></pre> <p>Example: <code>lab_ephys/recording/recording_id=1/waveform.npy</code></p> <p>This schema-addressed layout means you can browse the object store and understand the organization because it mirrors your database schema.</p>"},{"location":"reference/specs/npy-codec/#json-metadata","title":"JSON Metadata","text":"<p>The database column stores:</p> <pre><code>{\n  \"path\": \"lab_ephys/recording/recording_id=1/waveform.npy\",\n  \"store\": \"main\",\n  \"dtype\": \"float64\",\n  \"shape\": [1000, 32]\n}\n</code></pre>"},{"location":"reference/specs/npy-codec/#validation","title":"Validation","text":"<p>The codec validates on insert:</p> <ul> <li>Value must be <code>numpy.ndarray</code></li> <li>Array must not have <code>object</code> dtype</li> </ul> <pre><code># Valid\nRecording.insert1({'recording_id': 1, 'waveform': np.array([1, 2, 3])})\n\n# Invalid - not an array\nRecording.insert1({'recording_id': 1, 'waveform': [1, 2, 3]})\n# DataJointError: &lt;npy&gt; requires numpy.ndarray, got list\n\n# Invalid - object dtype\nRecording.insert1({'recording_id': 1, 'waveform': np.array([{}, []])})\n# DataJointError: &lt;npy&gt; does not support object dtype arrays\n</code></pre>"},{"location":"reference/specs/npy-codec/#direct-file-access","title":"Direct File Access","text":"<p>Files are stored at predictable paths and can be accessed directly:</p> <pre><code># Get the storage path\nref = (Recording &amp; 'recording_id=1').fetch1('waveform')\nprint(ref.path)  # \"my_schema/recording/recording_id=1/waveform.npy\"\n\n# Load directly with numpy (if you have store access)\narr = np.load('/path/to/store/my_schema/recording/recording_id=1/waveform.npy')\n</code></pre>"},{"location":"reference/specs/npy-codec/#comparison-with-other-codecs","title":"Comparison with Other Codecs","text":"Codec Format Addressing Lazy Memory Map Portability <code>&lt;npy@&gt;</code> <code>.npy</code> Schema Yes (NpyRef) Yes High (numpy, MATLAB) <code>&lt;object@&gt;</code> varies Schema Yes (ObjectRef) No Depends on content <code>&lt;blob@&gt;</code> pickle Hash No No Python only <code>&lt;hash@&gt;</code> raw bytes Hash No No N/A <p>Addressing schemes: - Schema-addressed: Path mirrors database structure. Browsable, one location per entity. - Hash-addressed: Path from content hash. Automatic deduplication.</p>"},{"location":"reference/specs/npy-codec/#when-to-use-npy","title":"When to Use <code>&lt;npy@&gt;</code>","text":"<p>Use <code>&lt;npy@&gt;</code> when: - Storing single numpy arrays - Interoperability matters (non-Python tools) - You want lazy loading with metadata inspection - Fetching many rows where not all arrays are needed - Random access to large arrays via memory mapping - Browsable object store organization is valuable</p> <p>Use <code>&lt;blob@&gt;</code> when: - Storing arbitrary Python objects (dicts, lists, mixed types) - Arrays are small and eager loading is fine - MATLAB compatibility with DataJoint's mYm format is needed - Deduplication is beneficial (hash-addressed)</p> <p>Use <code>&lt;object@&gt;</code> when: - Storing files/folders (Zarr, HDF5, multi-file outputs) - Content is not a single numpy array</p>"},{"location":"reference/specs/npy-codec/#limitations","title":"Limitations","text":"<ol> <li>Single array only: For multiple arrays, use separate attributes or <code>&lt;object@&gt;</code> with <code>.npz</code></li> <li>No compression: For compressed storage, use a custom codec with <code>numpy.savez_compressed</code></li> <li>No object dtype: Arrays containing arbitrary Python objects are not supported</li> <li>Store only: Cannot store in-table (database column)</li> </ol>"},{"location":"reference/specs/npy-codec/#see-also","title":"See Also","text":"<ul> <li>Type System Specification - Complete type system overview</li> <li>Codec API - Creating custom codecs</li> <li>Object Storage - Schema-addressed storage details</li> </ul>"},{"location":"reference/specs/object-store-configuration/","title":"Object Store Configuration Specification","text":"<p>This specification defines DataJoint's unified object store system, including store configuration, path generation algorithms, and storage models.</p>"},{"location":"reference/specs/object-store-configuration/#overview","title":"Overview","text":"<p>DataJoint's Object-Augmented Schema (OAS) integrates relational tables with object storage as a single coherent system. Large data objects are stored in file systems or cloud storage while maintaining full referential integrity with the relational database.</p>"},{"location":"reference/specs/object-store-configuration/#storage-models","title":"Storage Models","text":"<p>DataJoint 2.0 supports three storage models, all sharing the same store configuration:</p> Model Data Types Path Structure Integration Use Case Hash-addressed <code>&lt;blob@store&gt;</code>, <code>&lt;attach@store&gt;</code> Content-addressed by hash Integrated (OAS) Immutable data, automatic deduplication Schema-addressed <code>&lt;object@store&gt;</code>, <code>&lt;npy@store&gt;</code> Key-based hierarchical paths Integrated (OAS) Mutable data, streaming access, arrays Filepath <code>&lt;filepath@store&gt;</code> User-managed paths Reference User-managed files (no lifecycle management) <p>Key distinction:</p> <ul> <li>Hash-addressed and schema-addressed storage are integrated into the Object-Augmented Schema. DataJoint manages their lifecycle, paths, integrity, garbage collection, transaction safety, and deduplication.</li> <li>Filepath storage stores only the path string. DataJoint provides no lifecycle management, garbage collection, transaction safety, or deduplication. Users control file creation, organization, and lifecycle.</li> </ul> <p>Legacy note: DataJoint 0.14.x only supported hash-addressed (called \"external\") and filepath storage. Schema-addressed storage is new in 2.0.</p>"},{"location":"reference/specs/object-store-configuration/#store-configuration","title":"Store Configuration","text":""},{"location":"reference/specs/object-store-configuration/#minimal-configuration","title":"Minimal Configuration","text":"<p>Every store requires two fields:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/my-project\"\n    }\n  }\n}\n</code></pre> <p>This creates a store named <code>main</code> and designates it as the default.</p>"},{"location":"reference/specs/object-store-configuration/#default-store","title":"Default Store","text":"<p>DataJoint uses two default settings to reflect the architectural distinction between integrated and reference storage:</p>"},{"location":"reference/specs/object-store-configuration/#storesdefault-integrated-storage-oas","title":"stores.default \u2014 Integrated Storage (OAS)","text":"<p>The <code>stores.default</code> setting determines which store is used for integrated storage (hash-addressed and schema-addressed) when no store is specified:</p> <pre><code># These are equivalent when stores.default = \"main\"\nsignal : &lt;blob&gt;           # Uses stores.default\nsignal : &lt;blob@main&gt;      # Explicitly names store\n\narrays : &lt;object&gt;         # Uses stores.default\narrays : &lt;object@main&gt;    # Explicitly names store\n</code></pre> <p>Rules: - <code>stores.default</code> must be a string naming a configured store - Required for <code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>, <code>&lt;object&gt;</code>, <code>&lt;npy&gt;</code> without explicit <code>@store</code> - Each project typically uses one primary store for integrated data</p>"},{"location":"reference/specs/object-store-configuration/#storesfilepath_default-filepath-references","title":"stores.filepath_default \u2014 Filepath References","text":"<p>The <code>stores.filepath_default</code> setting determines which store is used for filepath references when no store is specified:</p> <pre><code># These are equivalent when stores.filepath_default = \"raw_data\"\nrecording : &lt;filepath@&gt;          # Uses stores.filepath_default\nrecording : &lt;filepath@raw_data&gt;  # Explicitly names store\n</code></pre> <p>Rules: - <code>stores.filepath_default</code> must be a string naming a configured store - Required for <code>&lt;filepath@&gt;</code> without explicit store name - Often configured differently from <code>stores.default</code> because filepath references are not part of OAS - Users manage file lifecycle and organization</p> <p>Why separate defaults?</p> <p>Integrated storage (hash, schema) is managed by DataJoint as part of the Object-Augmented Schema\u2014DataJoint controls paths, lifecycle, integrity, garbage collection, transaction safety, and deduplication. Filepath storage is user-managed\u2014DataJoint only stores the path string and provides no lifecycle management, garbage collection, transaction safety, or deduplication. These are architecturally distinct, so they often use different storage locations and require separate defaults.</p>"},{"location":"reference/specs/object-store-configuration/#complete-store-configuration","title":"Complete Store Configuration","text":"<p>A fully configured store specifying all sections:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"main\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"neuroscience-data\",\n      \"location\": \"lab-project-2024\",\n\n      \"hash_prefix\": \"blobs\",\n      \"schema_prefix\": \"arrays\",\n      \"filepath_prefix\": \"imported\",\n\n      \"subfolding\": [2, 2],\n      \"partition_pattern\": \"subject_id/session_date\",\n      \"token_length\": 8\n    }\n  }\n}\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#section-prefixes","title":"Section Prefixes","text":"<p>Each store is divided into sections controlled by prefix configuration. The <code>*_prefix</code> parameters define the path prefix for each storage section:</p> Configuration Parameter Default Storage Section Used By <code>hash_prefix</code> <code>\"_hash\"</code> Hash-addressed section <code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code> <code>schema_prefix</code> <code>\"_schema\"</code> Schema-addressed section <code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code> <code>filepath_prefix</code> <code>null</code> Filepath section (optional) <code>&lt;filepath@&gt;</code> <p>Validation rules: 1. All prefixes must be mutually exclusive (no nesting) 2. <code>hash_prefix</code> and <code>schema_prefix</code> are reserved for DataJoint 3. <code>filepath_prefix</code> is optional:    - <code>null</code> (default): filepaths can use any path except reserved sections    - <code>\"some/prefix\"</code>: all filepaths must start with this prefix</p> <p>Example with custom prefixes:</p> <pre><code>{\n  \"hash_prefix\": \"content_addressed\",\n  \"schema_prefix\": \"structured_data\",\n  \"filepath_prefix\": \"user_files\"\n}\n</code></pre> <p>Results in these sections: - <code>{location}/content_addressed/{schema}/{hash}</code> \u2014 hash-addressed - <code>{location}/structured_data/{schema}/{table}/{key}/</code> \u2014 schema-addressed - <code>{location}/user_files/{user_path}</code> \u2014 filepath (required prefix)</p>"},{"location":"reference/specs/object-store-configuration/#multiple-stores","title":"Multiple Stores","text":"<p>Configure multiple stores for different data types or storage tiers:</p> <pre><code>{\n  \"stores\": {\n    \"default\": \"main\",\n    \"filepath_default\": \"raw_data\",\n    \"main\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/fast-storage\",\n      \"hash_prefix\": \"blobs\",\n      \"schema_prefix\": \"arrays\"\n    },\n    \"archive\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"archive-bucket\",\n      \"location\": \"long-term-storage\",\n      \"hash_prefix\": \"archived_blobs\",\n      \"schema_prefix\": \"archived_arrays\",\n      \"subfolding\": [2, 2]\n    },\n    \"raw_data\": {\n      \"protocol\": \"file\",\n      \"location\": \"/data/acquisition\",\n      \"filepath_prefix\": \"recordings\"\n    }\n  }\n}\n</code></pre> <p>Use named stores in table definitions:</p> <pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : uuid\n    ---\n    metadata     : &lt;blob@main&gt;          # Fast storage, hash-addressed\n    raw_file     : &lt;filepath@raw_data&gt;  # Reference existing acquisition file\n    processed    : &lt;object@main&gt;        # Fast storage, schema-addressed\n    backup       : &lt;blob@archive&gt;       # Long-term storage\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#secret-management","title":"Secret Management","text":"<p>Store credentials separately from configuration files using the <code>.secrets/</code> directory.</p>"},{"location":"reference/specs/object-store-configuration/#secrets-directory-structure","title":"Secrets Directory Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 datajoint.json              # Non-sensitive configuration\n\u2514\u2500\u2500 .secrets/                   # Credentials (gitignored)\n    \u251c\u2500\u2500 .gitignore              # Ensures secrets aren't committed\n    \u251c\u2500\u2500 database.user\n    \u251c\u2500\u2500 database.password\n    \u251c\u2500\u2500 stores.main.access_key\n    \u251c\u2500\u2500 stores.main.secret_key\n    \u251c\u2500\u2500 stores.archive.access_key\n    \u2514\u2500\u2500 stores.archive.secret_key\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#configuration-priority","title":"Configuration Priority","text":"<p>DataJoint loads configuration in this order (highest priority first):</p> <ol> <li>Environment variables: <code>DJ_HOST</code>, <code>DJ_USER</code>, <code>DJ_PASS</code></li> <li>Secrets directory: <code>.secrets/database.user</code>, <code>.secrets/stores.main.access_key</code></li> <li>Config file: <code>datajoint.json</code></li> <li>Defaults: Built-in defaults</li> </ol>"},{"location":"reference/specs/object-store-configuration/#secrets-file-format","title":"Secrets File Format","text":"<p>Each secret file contains a single value (no quotes, no JSON):</p> <pre><code># .secrets/database.password\nmy_secure_password\n</code></pre> <pre><code># .secrets/stores.main.access_key\nAKIAIOSFODNN7EXAMPLE\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#per-store-credentials","title":"Per-Store Credentials","text":"<p>Store credentials use the naming pattern: <code>stores.&lt;name&gt;.&lt;attribute&gt;</code></p> <p>S3 stores: <pre><code>.secrets/stores.main.access_key\n.secrets/stores.main.secret_key\n</code></pre></p> <p>GCS stores: <pre><code>.secrets/stores.gcs_store.token\n</code></pre></p> <p>Azure stores: <pre><code>.secrets/stores.azure_store.account_key\n</code></pre></p>"},{"location":"reference/specs/object-store-configuration/#setting-up-secrets","title":"Setting Up Secrets","text":"<pre><code># Create secrets directory\nmkdir .secrets\necho \"*\" &gt; .secrets/.gitignore\n\n# Add credentials (no quotes)\necho \"analyst\" &gt; .secrets/database.user\necho \"dbpass123\" &gt; .secrets/database.password\necho \"AKIAIOSFODNN7EXAMPLE\" &gt; .secrets/stores.main.access_key\necho \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" &gt; .secrets/stores.main.secret_key\n\n# Verify .secrets/ is gitignored\ngit check-ignore .secrets/database.password  # Should output the path\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#template-generation","title":"Template Generation","text":"<p>Generate configuration templates:</p> <pre><code>import datajoint as dj\n\n# Create config file\ndj.config.save_template('datajoint.json')\n\n# Create config + secrets directory with placeholders\ndj.config.save_template('datajoint.json', create_secrets_dir=True)\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#path-generation","title":"Path Generation","text":""},{"location":"reference/specs/object-store-configuration/#hash-addressed-storage","title":"Hash-Addressed Storage","text":"<p>Data types: <code>&lt;blob@store&gt;</code>, <code>&lt;attach@store&gt;</code></p> <p>Path structure: <pre><code>{location}/{hash_prefix}/{schema_name}/{hash}[.ext]\n</code></pre></p> <p>With subfolding <code>[2, 2]</code>: <pre><code>{location}/{hash_prefix}/{schema_name}/{h1}{h2}/{h3}{h4}/{hash}[.ext]\n</code></pre></p> <p>Algorithm:</p> <ol> <li>Serialize value using codec-specific format</li> <li>Compute Blake2b hash of serialized data</li> <li>Encode hash as base32 (lowercase, no padding)</li> <li>Apply subfolding if configured</li> <li>Construct path: <code>{hash_prefix}/{schema}/{subfolded_hash}</code></li> <li>Store metadata in relational database as JSON</li> </ol> <p>Properties: - Immutable: Content defines path, cannot be changed - Deduplicated: Identical content stored once - Integrity: Hash validates content on retrieval</p> <p>Example:</p> <pre><code># Table definition\n@schema\nclass Experiment(dj.Manual):\n    definition = \"\"\"\n    experiment_id : int\n    ---\n    data : &lt;blob@main&gt;\n    \"\"\"\n\n# With config:\n# hash_prefix = \"blobs\"\n# location = \"/data/store\"\n# subfolding = [2, 2]\n\n# Insert\nExperiment.insert1({'experiment_id': 1, 'data': my_data})\n\n# Resulting path:\n# /data/store/blobs/my_schema/ab/cd/abcdef123456...\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#schema-addressed-storage","title":"Schema-Addressed Storage","text":"<p>Data types: <code>&lt;object@store&gt;</code>, <code>&lt;npy@store&gt;</code></p> <p>Path structure (no partitioning): <pre><code>{location}/{schema_prefix}/{schema_name}/{table_name}/{key_string}/{field_name}.{token}.{ext}\n</code></pre></p> <p>With partitioning: <pre><code>{location}/{schema_prefix}/{partition_path}/{schema_name}/{table_name}/{remaining_key}/{field_name}.{token}.{ext}\n</code></pre></p> <p>Algorithm:</p> <ol> <li>Extract primary key values from the row</li> <li>If partition pattern configured, extract partition attributes</li> <li>Build partition path from partition attributes (if any)</li> <li>Build remaining key string from non-partition primary key attributes</li> <li>Generate random token (default 8 characters)</li> <li>Construct full path</li> <li>Store path metadata in relational database as JSON</li> </ol> <p>Partition pattern format:</p> <pre><code>{\n  \"partition_pattern\": \"subject_id/session_date\"\n}\n</code></pre> <p>This creates paths like: <pre><code>{schema_prefix}/subject_id=042/session_date=2024-01-15/{schema}/{table}/{remaining_key}/\n</code></pre></p> <p>Key string encoding:</p> <p>Primary key values are encoded as: <code>{attr}={value}</code></p> <ul> <li>Multiple attributes joined with <code>/</code></li> <li>Values URL-encoded if necessary</li> <li>Order matches table definition</li> </ul> <p>Properties: - Mutable: Can overwrite by writing to same path - Streaming: fsspec integration for lazy loading - Organized: Hierarchical structure mirrors data relationships</p> <p>Example without partitioning:</p> <pre><code>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    subject_id : int\n    session_id : int\n    ---\n    neural_data : &lt;object@main&gt;\n    \"\"\"\n\n# With config:\n# schema_prefix = \"arrays\"\n# location = \"/data/store\"\n# token_length = 8\n\nRecording.insert1({\n    'subject_id': 42,\n    'session_id': 100,\n    'neural_data': zarr_array\n})\n\n# Resulting path:\n# /data/store/arrays/neuroscience/Recording/subject_id=42/session_id=100/neural_data.x8a7b2c4.zarr\n</code></pre> <p>Example with partitioning:</p> <pre><code># Same table, but with partition configuration:\n# partition_pattern = \"subject_id/session_date\"\n\n@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    subject_id : int\n    session_date : date\n    session_id : int\n    ---\n    neural_data : &lt;object@main&gt;\n    \"\"\"\n\nRecording.insert1({\n    'subject_id': 42,\n    'session_date': '2024-01-15',\n    'session_id': 100,\n    'neural_data': zarr_array\n})\n\n# Resulting path:\n# /data/store/arrays/subject_id=42/session_date=2024-01-15/neuroscience/Recording/session_id=100/neural_data.x8a7b2c4.zarr\n</code></pre> <p>Partition extraction:</p> <p>When a partition pattern is configured:</p> <ol> <li>Check if table has all partition attributes in primary key</li> <li>If yes: extract those attributes to partition path, remaining attributes to key path</li> <li>If no: use normal structure (no partitioning for this table)</li> </ol> <p>This allows a single <code>partition_pattern</code> to apply to multiple tables, with automatic fallback for tables lacking partition attributes.</p> <p>Path collision prevention:</p> <p>The random token ensures uniqueness: - 8 characters (default): 62^8 = ~218 trillion combinations - Collision probability negligible for typical table sizes - Token regenerated on each write</p>"},{"location":"reference/specs/object-store-configuration/#filepath-storage","title":"Filepath Storage","text":"<p>Data type: <code>&lt;filepath@store&gt;</code></p> <p>Path structure: <pre><code>{location}/{filepath_prefix}/{user_path}\n</code></pre></p> <p>Or if <code>filepath_prefix = null</code>: <pre><code>{location}/{user_path}\n</code></pre></p> <p>Algorithm:</p> <ol> <li>User provides relative path within store</li> <li>Validate path doesn't use reserved sections (<code>hash_prefix</code>, <code>schema_prefix</code>)</li> <li>If <code>filepath_prefix</code> configured, validate path starts with it</li> <li>Check file exists at <code>{location}/{user_path}</code></li> <li>Record path, size, and timestamp in JSON metadata</li> <li>No file copying occurs</li> </ol> <p>Properties: - Path-only storage: DataJoint stores path string, no file management - No lifecycle management: No garbage collection, transaction safety, or deduplication - User-managed: User controls file creation, organization, and lifecycle - Collision-prone: User responsible for avoiding name collisions - Flexible: Can reference existing files or create new ones</p> <p>Collision handling:</p> <p>DataJoint does not prevent filename collisions for filepath storage. Users must ensure:</p> <ol> <li>Unique paths for each referenced file</li> <li>No overwrites of files still referenced by database</li> <li>Coordination if multiple processes write to same store</li> </ol> <p>Strategies for avoiding collisions:</p> <pre><code># Strategy 1: Include primary key in path\nrecording_path = f\"subject_{subject_id}/session_{session_id}/data.bin\"\n\n# Strategy 2: Use UUIDs\nimport uuid\nrecording_path = f\"recordings/{uuid.uuid4()}.nwb\"\n\n# Strategy 3: Timestamps\nfrom datetime import datetime\nrecording_path = f\"data_{datetime.now().isoformat()}.dat\"\n\n# Strategy 4: Enforce via filepath_prefix\n# Config: \"filepath_prefix\": \"recordings\"\n# All paths must start with recordings/, organize within that namespace\n</code></pre> <p>Reserved sections:</p> <p>Filepath storage cannot use paths starting with configured <code>hash_prefix</code> or <code>schema_prefix</code>:</p> <pre><code># Invalid (default prefixes)\ntable.insert1({'id': 1, 'file': '_hash/data.bin'})       # ERROR\ntable.insert1({'id': 2, 'file': '_schema/data.zarr'})    # ERROR\n\n# Invalid (custom prefixes: hash_prefix=\"blobs\")\ntable.insert1({'id': 3, 'file': 'blobs/data.bin'})       # ERROR\n\n# Valid\ntable.insert1({'id': 4, 'file': 'raw/subject01/rec.bin'})  # OK\n</code></pre> <p>Example:</p> <pre><code>@schema\nclass RawRecording(dj.Manual):\n    definition = \"\"\"\n    recording_id : uuid\n    ---\n    acquisition_file : &lt;filepath@acquisition&gt;\n    \"\"\"\n\n# With config:\n# filepath_prefix = \"imported\"\n# location = \"/data/acquisition\"\n\n# File already exists at: /data/acquisition/imported/subject01/session001/data.nwb\n\nRawRecording.insert1({\n    'recording_id': my_uuid,\n    'acquisition_file': 'imported/subject01/session001/data.nwb'\n})\n\n# DataJoint validates file exists, stores reference\n# User responsible for ensuring path uniqueness across recordings\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#storage-type-comparison","title":"Storage Type Comparison","text":"Feature Hash-addressed Schema-addressed Filepath Mutability Immutable Mutable User-managed Deduplication Automatic None None Streaming No (load full) Yes (fsspec) Yes (fsspec) Organization Flat (by hash) Hierarchical (by key) User-defined Collision handling Automatic (by content) Automatic (token) User responsibility DataJoint manages lifecycle Yes Yes No Suitable for Immutable blobs Large mutable arrays Existing files"},{"location":"reference/specs/object-store-configuration/#protocol-specific-configuration","title":"Protocol-Specific Configuration","text":""},{"location":"reference/specs/object-store-configuration/#file-protocol","title":"File Protocol","text":"<pre><code>{\n  \"protocol\": \"file\",\n  \"location\": \"/data/my-project\",\n  \"hash_prefix\": \"blobs\",\n  \"schema_prefix\": \"arrays\",\n  \"filepath_prefix\": null\n}\n</code></pre> <p>Required: <code>protocol</code>, <code>location</code></p>"},{"location":"reference/specs/object-store-configuration/#s3-protocol","title":"S3 Protocol","text":"<pre><code>{\n  \"protocol\": \"s3\",\n  \"endpoint\": \"s3.amazonaws.com\",\n  \"bucket\": \"my-bucket\",\n  \"location\": \"my-project/production\",\n  \"secure\": true,\n  \"hash_prefix\": \"blobs\",\n  \"schema_prefix\": \"arrays\"\n}\n</code></pre> <p>Required: <code>protocol</code>, <code>endpoint</code>, <code>bucket</code>, <code>location</code>, <code>access_key</code>, <code>secret_key</code></p> <p>Credentials: Store in <code>.secrets/stores.&lt;name&gt;.access_key</code> and <code>.secrets/stores.&lt;name&gt;.secret_key</code></p>"},{"location":"reference/specs/object-store-configuration/#gcs-protocol","title":"GCS Protocol","text":"<pre><code>{\n  \"protocol\": \"gcs\",\n  \"bucket\": \"my-gcs-bucket\",\n  \"location\": \"my-project\",\n  \"project\": \"my-gcp-project\",\n  \"hash_prefix\": \"blobs\",\n  \"schema_prefix\": \"arrays\"\n}\n</code></pre> <p>Required: <code>protocol</code>, <code>bucket</code>, <code>location</code>, <code>token</code></p> <p>Credentials: Store in <code>.secrets/stores.&lt;name&gt;.token</code> (path to service account JSON)</p>"},{"location":"reference/specs/object-store-configuration/#azure-protocol","title":"Azure Protocol","text":"<pre><code>{\n  \"protocol\": \"azure\",\n  \"container\": \"my-container\",\n  \"location\": \"my-project\",\n  \"hash_prefix\": \"blobs\",\n  \"schema_prefix\": \"arrays\"\n}\n</code></pre> <p>Required: <code>protocol</code>, <code>container</code>, <code>location</code>, <code>account_name</code>, <code>account_key</code></p> <p>Credentials: Store in <code>.secrets/stores.&lt;name&gt;.account_key</code></p>"},{"location":"reference/specs/object-store-configuration/#migration-from-legacy-storage","title":"Migration from Legacy Storage","text":"<p>DataJoint 0.14.x used separate configuration systems:</p>"},{"location":"reference/specs/object-store-configuration/#legacy-external-storage-hash-addressed-integrated","title":"Legacy \"External\" Storage (Hash-addressed Integrated)","text":"<pre><code># 0.14.x config\ndj.config['stores'] = {\n    'my_store': {\n        'protocol': 's3',\n        'endpoint': 's3.amazonaws.com',\n        'bucket': 'my-bucket',\n        'location': 'my-project',\n        'access_key': 'XXX',\n        'secret_key': 'YYY'\n    }\n}\n\n# 0.14.x usage\ndata : external-my_store\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#20-equivalent","title":"2.0 Equivalent","text":"<pre><code>{\n  \"stores\": {\n    \"default\": \"my_store\",\n    \"my_store\": {\n      \"protocol\": \"s3\",\n      \"endpoint\": \"s3.amazonaws.com\",\n      \"bucket\": \"my-bucket\",\n      \"location\": \"my-project\",\n      \"hash_prefix\": \"_hash\"\n    }\n  }\n}\n</code></pre> <p>Credentials moved to <code>.secrets/</code>: <pre><code>.secrets/stores.my_store.access_key\n.secrets/stores.my_store.secret_key\n</code></pre></p> <pre><code># 2.0 usage (equivalent)\ndata : &lt;blob@my_store&gt;\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#new-in-20-schema-addressed-storage","title":"New in 2.0: Schema-addressed Storage","text":"<p>Schema-addressed storage (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>) is entirely new in DataJoint 2.0. No migration needed as this feature didn't exist in 0.14.x.</p>"},{"location":"reference/specs/object-store-configuration/#validation-and-testing","title":"Validation and Testing","text":""},{"location":"reference/specs/object-store-configuration/#verify-store-configuration","title":"Verify Store Configuration","text":"<pre><code>import datajoint as dj\n\n# Check default store\nspec = dj.config.get_store_spec()\nprint(f\"Default store: {dj.config['stores']['default']}\")\nprint(f\"Protocol: {spec['protocol']}\")\nprint(f\"Location: {spec['location']}\")\nprint(f\"Hash prefix: {spec['hash_prefix']}\")\nprint(f\"Schema prefix: {spec['schema_prefix']}\")\nprint(f\"Filepath prefix: {spec['filepath_prefix']}\")\n\n# Check named store\nspec = dj.config.get_store_spec('archive')\nprint(f\"Archive location: {spec['location']}\")\n\n# List all stores\nprint(f\"Configured stores: {list(dj.config['stores'].keys())}\")\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#test-storage-access","title":"Test Storage Access","text":"<pre><code>from datajoint.hash_registry import get_store_backend\n\n# Test backend connectivity\nbackend = get_store_backend('main')\nprint(f\"Backend type: {type(backend)}\")\n\n# For file protocol, check paths exist\nif spec['protocol'] == 'file':\n    import os\n    assert os.path.exists(spec['location']), f\"Location not found: {spec['location']}\"\n</code></pre>"},{"location":"reference/specs/object-store-configuration/#best-practices","title":"Best Practices","text":""},{"location":"reference/specs/object-store-configuration/#store-organization","title":"Store Organization","text":"<ol> <li>Use one default store for most data</li> <li>Add specialized stores for specific needs:<ul> <li><code>archive</code> \u2014 long-term cold storage</li> <li><code>fast</code> \u2014 high-performance tier</li> <li><code>shared</code> \u2014 cross-project data</li> <li><code>raw</code> \u2014 acquisition files (filepath only)</li> </ul> </li> </ol>"},{"location":"reference/specs/object-store-configuration/#prefix-configuration","title":"Prefix Configuration","text":"<ol> <li>Use defaults unless integrating with existing storage</li> <li>Choose meaningful names if customizing: <code>blobs</code>, <code>arrays</code>, <code>user_files</code></li> <li>Keep prefixes short to minimize path length</li> </ol>"},{"location":"reference/specs/object-store-configuration/#secret-management_1","title":"Secret Management","text":"<ol> <li>Never commit credentials to version control</li> <li>Use <code>.secrets/</code> directory for all credentials</li> <li>Set restrictive permissions: <code>chmod 700 .secrets</code></li> <li>Document required secrets in project README</li> </ol>"},{"location":"reference/specs/object-store-configuration/#partitioning-strategy","title":"Partitioning Strategy","text":"<ol> <li> <p>Choose partition attributes carefully:</p> <ul> <li>High cardinality (many unique values)</li> <li>Natural data organization (subject, date)</li> <li>Query patterns (often filtered by these attributes)</li> </ul> </li> <li> <p>Example patterns:</p> <ul> <li>Neuroscience: <code>subject_id/session_date</code></li> <li>Genomics: <code>sample_id/sequencing_run</code></li> <li>Microscopy: <code>experiment_id/imaging_session</code></li> </ul> </li> <li> <p>Avoid over-partitioning:</p> <ul> <li>Don't partition by high-cardinality unique IDs</li> <li>Limit to 2-3 partition levels</li> </ul> </li> </ol>"},{"location":"reference/specs/object-store-configuration/#filepath-usage","title":"Filepath Usage","text":"<ol> <li>Design naming conventions before inserting data</li> <li>Include unique identifiers in paths</li> <li>Document collision prevention strategy for the team</li> <li>Consider using <code>filepath_prefix</code> to enforce structure</li> </ol>"},{"location":"reference/specs/object-store-configuration/#see-also","title":"See Also","text":"<ul> <li>Configuration Reference \u2014 All configuration options</li> <li>Configure Object Stores \u2014 Setup guide</li> <li>Type System Specification \u2014 Data type definitions</li> <li>Codec API Specification \u2014 Codec implementation details</li> </ul>"},{"location":"reference/specs/primary-keys/","title":"Primary Key Rules in Relational Operators","text":"<p>In DataJoint, the result of each query operator produces a valid entity set with a well-defined entity type and primary key. This section specifies how the primary key is determined for each relational operator.</p>"},{"location":"reference/specs/primary-keys/#general-principle","title":"General Principle","text":"<p>The primary key of a query result identifies unique entities in that result. For most operators, the primary key is preserved from the left operand. For joins, the primary key depends on the functional dependencies between the operands.</p>"},{"location":"reference/specs/primary-keys/#integration-with-semantic-matching","title":"Integration with Semantic Matching","text":"<p>Primary key determination is applied after semantic compatibility is verified. The evaluation order is:</p> <ol> <li>Semantic Check: <code>assert_join_compatibility()</code> ensures all namesakes are homologous (same lineage)</li> <li>PK Determination: The functional dependency relationship is computed using attribute names</li> <li>Left Join Validation: If <code>left=True</code>, verify A \u2192 B</li> </ol> <p>This ordering is important because:</p> <ul> <li>After semantic matching passes, namesakes represent semantically equivalent attributes</li> <li>The name-based functional dependency check is therefore semantically valid</li> <li>Attribute names in the context of a semantically-valid join represent the same entity</li> </ul> <p>The functional dependency relationship is derived from the structure of primary keys and secondary attributes and their names in the two operands:</p> <ul> <li><code>A \u2192 B</code> holds when every attribute in PK(B) appears (by name) somewhere in A (either as a primary key or secondary attribute)</li> <li>After semantic matching passes, namesakes represent semantically equivalent attributes, so checking by name is valid</li> <li>Aliased attributes (same lineage, different names) don't participate in natural joins</li> </ul>"},{"location":"reference/specs/primary-keys/#notation","title":"Notation","text":"<p>In the examples below, <code>*</code> marks primary key attributes:</p> <ul> <li><code>A(x*, y*, z)</code> means A has primary key <code>{x, y}</code> and secondary attribute <code>z</code></li> <li><code>A \u2192 B</code> means \"A functionally determines B\" (defined below)</li> </ul>"},{"location":"reference/specs/primary-keys/#rules-by-operator","title":"Rules by Operator","text":"Operator Primary Key Rule <code>A &amp; B</code> (restriction) PK(A) \u2014 preserved from left operand <code>A - B</code> (anti-restriction) PK(A) \u2014 preserved from left operand <code>A.proj(...)</code> (projection) PK(A) \u2014 preserved from left operand <code>A.aggr(B, ...)</code> (aggregation) PK(A) \u2014 preserved from left operand <code>A.extend(B)</code> (extension) PK(A) \u2014 requires A \u2192 B <code>A * B</code> (join) Depends on functional dependencies (see below)"},{"location":"reference/specs/primary-keys/#join-primary-key-rule","title":"Join Primary Key Rule","text":"<p>The join operator requires special handling because it combines two entity sets. The primary key of <code>A * B</code> depends on the functional dependency relationship between the operands.</p>"},{"location":"reference/specs/primary-keys/#definitions","title":"Definitions","text":"<p>A \u2192 B (A functionally determines B): Every attribute in PK(B) is in A.</p> <pre><code>A \u2192 B  iff  \u2200b \u2208 PK(B): b \u2208 A\n</code></pre> <p>Since <code>PK(A) \u222a secondary(A) = all attributes in A</code>, this is equivalent to saying every attribute in B's primary key exists somewhere in A (as either a primary key or secondary attribute).</p> <p>Intuitively, <code>A \u2192 B</code> means that knowing A's primary key is sufficient to determine B's primary key through the functional dependencies implied by A's structure.</p> <p>B \u2192 A (B functionally determines A): Every attribute in PK(A) is in B.</p> <pre><code>B \u2192 A  iff  \u2200a \u2208 PK(A): a \u2208 B\n</code></pre>"},{"location":"reference/specs/primary-keys/#join-primary-key-algorithm","title":"Join Primary Key Algorithm","text":"<p>For <code>A * B</code>:</p> Condition PK(A * B) Attribute Order A \u2192 B PK(A) A's attributes first B \u2192 A (and not A \u2192 B) PK(B) B's attributes first Neither PK(A) \u222a PK(B) PK(A) first, then PK(B) \u2212 PK(A) <p>When both <code>A \u2192 B</code> and <code>B \u2192 A</code> hold, the left operand takes precedence (use PK(A)).</p>"},{"location":"reference/specs/primary-keys/#examples","title":"Examples","text":"<p>Example 1: B \u2192 A <pre><code>A: x*, y*\nB: x*, z*, y    (y is secondary in B, so z \u2192 y)\n</code></pre></p> <ul> <li>A \u2192 B? PK(B) = {x, z}. Is z in PK(A) or secondary in A? No (z not in A). No.</li> <li>B \u2192 A? PK(A) = {x, y}. Is y in PK(B) or secondary in B? Yes (secondary). Yes.</li> <li>Result: PK(A * B) = {x, z} with B's attributes first.</li> </ul> <p>Example 2: Both directions (bijection-like) <pre><code>A: x*, y*, z    (z is secondary in A)\nB: y*, z*, x    (x is secondary in B)\n</code></pre></p> <ul> <li>A \u2192 B? PK(B) = {y, z}. Is z in PK(A) or secondary in A? Yes (secondary). Yes.</li> <li>B \u2192 A? PK(A) = {x, y}. Is x in PK(B) or secondary in B? Yes (secondary). Yes.</li> <li>Both hold, prefer left operand: PK(A * B) = {x, y} with A's attributes first.</li> </ul> <p>Example 3: Neither direction <pre><code>A: x*, y*\nB: z*, x    (x is secondary in B)\n</code></pre></p> <ul> <li>A \u2192 B? PK(B) = {z}. Is z in PK(A) or secondary in A? No. No.</li> <li>B \u2192 A? PK(A) = {x, y}. Is y in PK(B) or secondary in B? No (y not in B). No.</li> <li>Result: PK(A * B) = {x, y, z} (union) with A's attributes first.</li> </ul> <p>Example 4: A \u2192 B (subordinate relationship) <pre><code>Session: session_id*\nTrial: session_id*, trial_num*    (references Session)\n</code></pre></p> <ul> <li>A \u2192 B? PK(Trial) = {session_id, trial_num}. Is trial_num in PK(Session) or secondary? No. No.</li> <li>B \u2192 A? PK(Session) = {session_id}. Is session_id in PK(Trial)? Yes. Yes.</li> <li>Result: PK(Session * Trial) = {session_id, trial_num} with Trial's attributes first.</li> </ul> <p>Join primary key determination:</p> <ul> <li><code>A * B</code> where <code>A \u2192 B</code>: result has PK(A)</li> <li><code>A * B</code> where <code>B \u2192 A</code> (not <code>A \u2192 B</code>): result has PK(B), B's attributes first</li> <li><code>A * B</code> where both <code>A \u2192 B</code> and <code>B \u2192 A</code>: result has PK(A) (left preference)</li> <li><code>A * B</code> where neither direction: result has PK(A) \u222a PK(B)</li> <li>Verify attribute ordering matches primary key source</li> <li>Verify non-commutativity: <code>A * B</code> vs <code>B * A</code> may differ in PK and order</li> </ul>"},{"location":"reference/specs/primary-keys/#design-tradeoff-predictability-vs-minimality","title":"Design Tradeoff: Predictability vs. Minimality","text":"<p>The join primary key rule prioritizes predictability over minimality. In some cases, the resulting primary key may not be minimal (i.e., it may contain functionally redundant attributes).</p> <p>Example of non-minimal result: <pre><code>A: x*, y*\nB: z*, x    (x is secondary in B, so z \u2192 x)\n</code></pre></p> <p>The mathematically minimal primary key for <code>A * B</code> would be <code>{y, z}</code> because:</p> <ul> <li><code>z \u2192 x</code> (from B's structure)</li> <li><code>{y, z} \u2192 {x, y, z}</code> (z gives us x, and we have y)</li> </ul> <p>However, <code>{y, z}</code> is problematic:</p> <ul> <li>It is not the primary key of either operand (A has <code>{x, y}</code>, B has <code>{z}</code>)</li> <li>It is not the union of the primary keys</li> <li>It represents a novel entity type that doesn't correspond to A, B, or their natural pairing</li> </ul> <p>This creates confusion: what kind of entity does <code>{y, z}</code> identify?</p> <p>The simplified rule produces <code>{x, y, z}</code> (the union), which:</p> <ul> <li>Is immediately recognizable as \"one A entity paired with one B entity\"</li> <li>Contains A's full primary key and B's full primary key</li> <li>May have redundancy (<code>x</code> is determined by <code>z</code>) but is semantically clear</li> </ul> <p>Rationale: Users can always project away redundant attributes if they need the minimal key. But starting with a predictable, interpretable primary key reduces confusion and errors.</p>"},{"location":"reference/specs/primary-keys/#attribute-ordering","title":"Attribute Ordering","text":"<p>The primary key attributes always appear first in the result's attribute list, followed by secondary attributes. When <code>B \u2192 A</code> (and not <code>A \u2192 B</code>), the join is conceptually reordered as <code>B * A</code> to maintain this invariant:</p> <ul> <li>If PK = PK(A): A's attributes appear first</li> <li>If PK = PK(B): B's attributes appear first</li> <li>If PK = PK(A) \u222a PK(B): PK(A) attributes first, then PK(B) \u2212 PK(A), then secondaries</li> </ul>"},{"location":"reference/specs/primary-keys/#non-commutativity","title":"Non-Commutativity","text":"<p>With these rules, join is not commutative in terms of:</p> <ol> <li>Primary key selection: <code>A * B</code> may have a different PK than <code>B * A</code> when functional dependency holds in one direction but not the other</li> <li>Attribute ordering: The left operand's attributes appear first (unless B \u2192 A)</li> </ol> <p>The result set (the actual rows returned) remains the same regardless of order, but the schema (primary key and attribute order) may differ.</p>"},{"location":"reference/specs/primary-keys/#left-join-constraint","title":"Left Join Constraint","text":"<p>For left joins (<code>A.join(B, left=True)</code>), the functional dependency A \u2192 B is required.</p> <p>Why this constraint exists:</p> <p>In a left join, all rows from A are retained even if there's no matching row in B. For unmatched rows, B's attributes are NULL. This creates a problem for primary key validity:</p> Scenario PK by inner join rule Left join problem A \u2192 B PK(A) \u2705 Safe \u2014 A's attrs always present B \u2192 A PK(B) \u274c B's PK attrs could be NULL Neither PK(A) \u222a PK(B) \u274c B's PK attrs could be NULL <p>Example of invalid left join: <pre><code>A: x*, y*           PK(A) = {x, y}\nB: x*, z*, y        PK(B) = {x, z}, y is secondary\n\nInner join: PK = {x, z} (B \u2192 A rule)\nLeft join attempt: FAILS because z could be NULL for unmatched A rows\n</code></pre></p> <p>Valid left join example: <pre><code>Session: session_id*, date\nTrial: session_id*, trial_num*, stimulus    (references Session)\n\nSession.join(Trial, left=True)  # OK: Session \u2192 Trial\n# PK = {session_id}, all sessions retained even without trials\n</code></pre></p> <p>Error message: <pre><code>DataJointError: Left join requires the left operand to determine the right operand (A \u2192 B).\nThe following attributes from the right operand's primary key are not determined by\nthe left operand: ['z']. Use an inner join or restructure the query.\n</code></pre></p>"},{"location":"reference/specs/primary-keys/#conceptual-note-left-join-as-extension","title":"Conceptual Note: Left Join as Extension","text":"<p>When <code>A \u2192 B</code>, the left join <code>A.join(B, left=True)</code> is conceptually distinct from the general join operator <code>A * B</code>. It is better understood as an extension operation rather than a join:</p> Aspect General Join (A * B) Left Join when A \u2192 B Conceptual model Cartesian product restricted to matching rows Extend A with attributes from B Row count May increase, decrease, or stay same Always equals len(A) Primary key Depends on functional dependencies Always PK(A) Relation to projection Different operation Variation of projection <p>The extension perspective:</p> <p>The operation <code>A.join(B, left=True)</code> when <code>A \u2192 B</code> is closer to projection than to join:</p> <ul> <li>It adds new attributes to A (like <code>A.proj(..., new_attr=...)</code>)</li> <li>It preserves all rows of A</li> <li>It preserves A's primary key</li> <li>It lacks the Cartesian product aspect that defines joins</li> </ul> <p>DataJoint provides an explicit <code>extend()</code> method for this pattern:</p> <pre><code># These are equivalent when A \u2192 B:\nA.join(B, left=True)\nA.extend(B)           # clearer intent: extend A with B's attributes\n</code></pre> <p>The <code>extend()</code> method:</p> <ul> <li>Requires <code>A \u2192 B</code> (raises <code>DataJointError</code> otherwise)</li> <li>Does not expose <code>allow_nullable_pk</code> (that's an internal mechanism)</li> <li>Expresses the semantic intent: \"add B's attributes to A's entities\"</li> </ul> <p>Relationship to aggregation:</p> <p>A similar argument applies to <code>A.aggr(B, ...)</code>:</p> <ul> <li>It preserves A's primary key</li> <li>It adds computed attributes derived from B</li> <li>It's conceptually a variation of projection with grouping</li> </ul> <p>Both <code>A.join(B, left=True)</code> (when A \u2192 B) and <code>A.aggr(B, ...)</code> can be viewed as projection-like operations that extend A's attributes while preserving its entity identity.</p>"},{"location":"reference/specs/primary-keys/#bypassing-the-left-join-constraint","title":"Bypassing the Left Join Constraint","text":""},{"location":"reference/specs/primary-keys/#the-allow_nullable_pk-parameter","title":"The <code>allow_nullable_pk</code> Parameter","text":"<p>The <code>join()</code> method accepts an <code>allow_nullable_pk</code> parameter that bypasses the <code>A \u2192 B</code> requirement for left joins:</p> <pre><code>def join(self, other, semantic_check=True, left=False, allow_nullable_pk=False):\n    ...\n</code></pre>"},{"location":"reference/specs/primary-keys/#why-the-constraint-exists","title":"Why the Constraint Exists","text":"<p>In a left join, unmatched rows from A have NULL values for all of B's attributes. If B's primary key attributes are part of the result's primary key, those NULLs violate entity integrity\u2014primary keys must not contain NULLs.</p> <p>The <code>A \u2192 B</code> requirement guarantees the result's primary key is <code>PK(A)</code>, which contains only A's attributes. Since A is the left operand in a left join, all its rows are retained with their original values, so the primary key cannot contain NULLs.</p>"},{"location":"reference/specs/primary-keys/#what-allow_nullable_pktrue-does","title":"What <code>allow_nullable_pk=True</code> Does","text":"<p>When the constraint is bypassed:</p> <ol> <li>The <code>A \u2192 B</code> check is skipped</li> <li>The result's primary key becomes <code>PK(A) \u222a PK(B)</code> (union of both)</li> <li>Primary key attributes from B may be NULL for unmatched rows</li> </ol> <pre><code># Normally blocked - A does not determine B\nA.join(B, left=True)  # Error: A \u2192 B not satisfied\n\n# Bypass the constraint\nA.join(B, left=True, allow_nullable_pk=True)  # PK = PK(A) \u222a PK(B)\n</code></pre>"},{"location":"reference/specs/primary-keys/#when-to-use-it","title":"When to Use It","text":"<p>This parameter is primarily internal. It exists because aggregation (<code>aggr</code>) needs a left join but has the opposite functional dependency requirement (<code>B \u2192 A</code> instead of <code>A \u2192 B</code>). The bypass is safe in aggregation because the <code>GROUP BY</code> clause resets the primary key to <code>PK(A)</code>.</p> <p>Users should generally avoid this parameter. If you need left join behavior without <code>A \u2192 B</code>:</p> <ul> <li>Use <code>extend()</code> if A should determine B (restructure your query)</li> <li>Use <code>aggr()</code> if you're computing aggregates</li> <li>Reconsider whether a left join is appropriate for your use case</li> </ul> <p>If you do use <code>allow_nullable_pk=True</code>, you must ensure subsequent operations (such as <code>GROUP BY</code> or projection) establish a valid, non-nullable primary key.</p>"},{"location":"reference/specs/primary-keys/#aggregation-exception","title":"Aggregation Exception","text":"<p><code>A.aggr(B)</code> (with default <code>exclude_nonmatching=False</code>) uses a left join internally but has the opposite requirement: B \u2192 A (the group expression B must have all of A's primary key attributes).</p> <p>This apparent contradiction is resolved by the <code>GROUP BY</code> clause:</p> <ol> <li>Aggregation requires B \u2192 A so that B can be grouped by A's primary key</li> <li>The intermediate left join <code>A LEFT JOIN B</code> would have an invalid PK under the normal left join rules</li> <li>Aggregation internally allows the invalid PK, producing PK(A) \u222a PK(B)</li> <li>The <code>GROUP BY PK(A)</code> clause then resets the primary key to PK(A)</li> <li>The final result has PK(A), which consists entirely of non-NULL values from A</li> </ol> <p>Note: The semantic check (homologous namesake validation) is still performed for aggregation's internal join. Only the primary key validity constraint is bypassed.</p> <p>Example: <pre><code>Session: session_id*, date\nTrial: session_id*, trial_num*, response_time    (references Session)\n\n# Aggregation (default keeps all rows)\nSession.aggr(Trial, avg_rt='avg(response_time)')\n\n# Internally: Session LEFT JOIN Trial (with invalid PK allowed)\n# Intermediate PK would be {session_id} \u222a {session_id, trial_num} = {session_id, trial_num}\n# But GROUP BY session_id resets PK to {session_id}\n# Result: All sessions, with avg_rt=NULL for sessions without trials\n</code></pre></p>"},{"location":"reference/specs/primary-keys/#universal-set-dju","title":"Universal Set <code>dj.U</code>","text":"<p><code>dj.U()</code> or <code>dj.U('attr1', 'attr2', ...)</code> represents the universal set of all possible values and lineages. </p>"},{"location":"reference/specs/primary-keys/#homology-with-dju","title":"Homology with <code>dj.U</code>","text":"<p>Since <code>dj.U</code> conceptually contains all possible lineages, its attributes are homologous to any namesake attribute in other expressions.</p>"},{"location":"reference/specs/primary-keys/#valid-operations","title":"Valid Operations","text":"<pre><code># Restriction: promotes a, b to PK; lineage transferred from A\ndj.U('a', 'b') &amp; A\n\n# Aggregation: groups by a, b\ndj.U('a', 'b').aggr(A, count='count(*)')\n</code></pre>"},{"location":"reference/specs/primary-keys/#invalid-operations","title":"Invalid Operations","text":"<pre><code># Anti-restriction: produces infinite set\ndj.U('a', 'b') - A  # DataJointError\n\n# Join: deprecated, use &amp; instead\ndj.U('a', 'b') * A  # DataJointError with migration guidance\n</code></pre>"},{"location":"reference/specs/query-algebra/","title":"DataJoint Query Algebra Specification","text":""},{"location":"reference/specs/query-algebra/#overview","title":"Overview","text":"<p>This document specifies the query algebra in DataJoint Python. Query expressions are composable objects that represent database queries. All operators return new QueryExpression objects without modifying the original\u2014expressions are immutable.</p>"},{"location":"reference/specs/query-algebra/#1-query-expression-fundamentals","title":"1. Query Expression Fundamentals","text":""},{"location":"reference/specs/query-algebra/#11-immutability","title":"1.1 Immutability","text":"<p>All query expressions are immutable. Every operator creates a new expression:</p> <pre><code>original = Session()\nrestricted = original &amp; \"session_date &gt; '2024-01-01'\"  # New object\n# original is unchanged\n</code></pre>"},{"location":"reference/specs/query-algebra/#12-primary-key-preservation","title":"1.2 Primary Key Preservation","text":"<p>Most operators preserve the primary key of their input. The exceptions are:</p> <ul> <li>Join: May expand or contract PK based on functional dependencies</li> <li>U &amp; table: Sets PK to U's attributes</li> </ul>"},{"location":"reference/specs/query-algebra/#13-lazy-evaluation","title":"1.3 Lazy Evaluation","text":"<p>Expressions are not executed until data is fetched:</p> <pre><code>expr = (Session * Trial) &amp; \"trial_type = 'test'\"  # No database query yet\ndata = expr.to_dicts()  # Query executed here\n</code></pre>"},{"location":"reference/specs/query-algebra/#2-restriction-and-","title":"2. Restriction (<code>&amp;</code> and <code>-</code>)","text":""},{"location":"reference/specs/query-algebra/#21-syntax","title":"2.1 Syntax","text":"<pre><code>result = expression &amp; condition    # Select matching rows\nresult = expression - condition    # Select non-matching rows (anti-restriction)\nresult = expression.restrict(condition, semantic_check=True)\n</code></pre>"},{"location":"reference/specs/query-algebra/#22-condition-types","title":"2.2 Condition Types","text":"Type Example Behavior String <code>\"x &gt; 5\"</code> SQL WHERE condition Dict <code>{\"status\": \"active\"}</code> Equality on attributes QueryExpression <code>OtherTable</code> Rows with matching keys in other table List/Tuple/Set <code>[cond1, cond2]</code> OR of conditions Boolean <code>True</code> / <code>False</code> No effect / empty result pandas.DataFrame <code>df</code> OR of row conditions numpy.void <code>record</code> Treated as dict"},{"location":"reference/specs/query-algebra/#23-string-conditions","title":"2.3 String Conditions","text":"<p>SQL expressions using attribute names:</p> <pre><code>Session &amp; \"session_date &gt; '2024-01-01'\"\nSession &amp; \"subject_id IN (1, 2, 3)\"\nSession &amp; \"notes LIKE '%test%'\"\nSession &amp; \"(x &gt; 0) AND (y &lt; 100)\"\n</code></pre>"},{"location":"reference/specs/query-algebra/#24-dictionary-conditions","title":"2.4 Dictionary Conditions","text":"<p>Attribute-value equality:</p> <pre><code>Session &amp; {\"subject_id\": 1}\nSession &amp; {\"subject_id\": 1, \"session_type\": \"training\"}\n</code></pre> <p>Multiple key-value pairs are combined with AND.</p> <p>Unmatched keys are silently ignored:</p> <pre><code># If 'nonexistent' is not an attribute of Session:\nSession &amp; {\"subject_id\": 1, \"nonexistent\": \"value\"}\n# Equivalent to:\nSession &amp; {\"subject_id\": 1}  # unmatched key skipped\n\n# If NO keys match, condition evaluates to True (all rows):\nSession &amp; {\"nonexistent\": \"value\"}  # returns all rows\n</code></pre> <p>This applies to: - Misspelled attribute names - Hidden attributes (prefixed with <code>_</code>) - Keys from a different table's schema</p> <p>Rationale: Enables restricting with dicts containing extra keys (e.g., a row fetched from another table) without explicitly filtering to matching attributes.</p> <p>Caution: Typos fail silently. Use string restrictions for explicit validation:</p> <pre><code># SQL error if attribute doesn't exist\nSession &amp; \"nonexistent = 'value'\"\n</code></pre>"},{"location":"reference/specs/query-algebra/#25-restriction-by-query-expression","title":"2.5 Restriction by Query Expression","text":"<p>Restrict to rows with matching primary keys in another expression:</p> <pre><code># Sessions that have at least one trial\nSession &amp; Trial\n\n# Sessions for active subjects only\nSession &amp; (Subject &amp; \"status = 'active'\")\n</code></pre>"},{"location":"reference/specs/query-algebra/#26-collection-conditions-or","title":"2.6 Collection Conditions (OR)","text":"<p>Lists, tuples, and sets create OR conditions:</p> <pre><code># Either condition matches\nSession &amp; [{\"subject_id\": 1}, {\"subject_id\": 2}]\n\n# Equivalent to\nSession &amp; \"subject_id IN (1, 2)\"\n</code></pre>"},{"location":"reference/specs/query-algebra/#27-anti-restriction","title":"2.7 Anti-Restriction","text":"<p>The <code>-</code> operator selects rows that do NOT match:</p> <pre><code># Sessions without any trials\nSession - Trial\n\n# Sessions not from subject 1\nSession - {\"subject_id\": 1}\n</code></pre>"},{"location":"reference/specs/query-algebra/#28-chaining-restrictions","title":"2.8 Chaining Restrictions","text":"<p>Sequential restrictions combine with AND:</p> <pre><code>(Session &amp; cond1) &amp; cond2\n# Equivalent to\nSession &amp; cond1 &amp; cond2\n</code></pre>"},{"location":"reference/specs/query-algebra/#29-semantic-matching","title":"2.9 Semantic Matching","text":"<p>With <code>semantic_check=True</code> (default), expression conditions match only on homologous namesakes\u2014attributes with the same name AND same lineage.</p> <pre><code># Default: semantic matching\nSession &amp; Trial\n\n# Disable semantic check (natural join on all namesakes)\nSession.restrict(Trial, semantic_check=False)\n</code></pre>"},{"location":"reference/specs/query-algebra/#210-algebraic-properties","title":"2.10 Algebraic Properties","text":"Property Value Primary Key Preserved: PK(result) = PK(input) Attributes Preserved: all attributes retained Entity Type Preserved"},{"location":"reference/specs/query-algebra/#211-error-conditions","title":"2.11 Error Conditions","text":"Condition Error Unknown attribute in string <code>UnknownAttributeError</code> Non-homologous namesakes <code>DataJointError</code> (semantic mismatch)"},{"location":"reference/specs/query-algebra/#3-projection-proj","title":"3. Projection (<code>.proj()</code>)","text":""},{"location":"reference/specs/query-algebra/#31-syntax","title":"3.1 Syntax","text":"<pre><code>result = expression.proj()                          # Primary key only\nresult = expression.proj(...)                       # All attributes\nresult = expression.proj('attr1', 'attr2')          # PK + specified\nresult = expression.proj(..., '-secret')            # All except secret\nresult = expression.proj(new_name='old_name')       # Rename\nresult = expression.proj(computed='x + y')          # Computed attribute\n</code></pre>"},{"location":"reference/specs/query-algebra/#32-attribute-selection","title":"3.2 Attribute Selection","text":"Syntax Meaning <code>'attr'</code> Include attribute <code>...</code> (Ellipsis) Include all secondary attributes <code>'-attr'</code> Exclude attribute (use with <code>...</code>) <p>Primary key attributes are always included, even if not specified.</p>"},{"location":"reference/specs/query-algebra/#33-renaming-attributes","title":"3.3 Renaming Attributes","text":"<pre><code># Rename 'name' to 'subject_name'\nSubject.proj(subject_name='name')\n\n# Duplicate attribute with new name (parentheses preserve original)\nSubject.proj('name', subject_name='(name)')\n</code></pre>"},{"location":"reference/specs/query-algebra/#34-computed-attributes","title":"3.4 Computed Attributes","text":"<p>Create new attributes from SQL expressions:</p> <pre><code># Arithmetic\nTrial.proj(speed='distance / duration')\n\n# Functions\nSession.proj(year='YEAR(session_date)')\n\n# Aggregation-like (per row)\nTrial.proj(centered='value - mean_value')\n</code></pre>"},{"location":"reference/specs/query-algebra/#35-primary-key-renaming","title":"3.5 Primary Key Renaming","text":"<p>Primary key attributes CAN be renamed:</p> <pre><code>Subject.proj(mouse_id='subject_id')\n# Result PK: (mouse_id,) instead of (subject_id,)\n</code></pre>"},{"location":"reference/specs/query-algebra/#36-excluding-attributes","title":"3.6 Excluding Attributes","text":"<p>Use <code>-</code> prefix with ellipsis to exclude:</p> <pre><code># All attributes except 'internal_notes'\nSession.proj(..., '-internal_notes')\n\n# Multiple exclusions\nSession.proj(..., '-notes', '-metadata')\n</code></pre> <p>Cannot exclude primary key attributes.</p>"},{"location":"reference/specs/query-algebra/#37-algebraic-properties","title":"3.7 Algebraic Properties","text":"Property Value Primary Key Preserved (may be renamed) Attributes Selected/computed subset Entity Type Preserved"},{"location":"reference/specs/query-algebra/#38-error-conditions","title":"3.8 Error Conditions","text":"Condition Error Attribute not found <code>UnknownAttributeError</code> Excluding PK attribute <code>DataJointError</code> Duplicate attribute name <code>DataJointError</code>"},{"location":"reference/specs/query-algebra/#4-join","title":"4. Join (<code>*</code>)","text":""},{"location":"reference/specs/query-algebra/#41-syntax","title":"4.1 Syntax","text":"<pre><code>result = A * B                                    # Inner join\nresult = A.join(B, semantic_check=True, left=False)\n</code></pre>"},{"location":"reference/specs/query-algebra/#42-parameters","title":"4.2 Parameters","text":"Parameter Default Description <code>semantic_check</code> <code>True</code> Match only homologous namesakes <code>left</code> <code>False</code> LEFT JOIN (preserve all rows from A)"},{"location":"reference/specs/query-algebra/#43-join-condition","title":"4.3 Join Condition","text":"<p>Joins match on all shared non-hidden attributes (namesakes):</p> <pre><code># If Session has (subject_id, session_id) and Trial has (subject_id, session_id, trial_id)\n# Join matches on (subject_id, session_id)\nSession * Trial\n</code></pre>"},{"location":"reference/specs/query-algebra/#44-primary-key-determination","title":"4.4 Primary Key Determination","text":"<p>The result's primary key depends on functional dependencies:</p> Condition Result PK Attribute Order A \u2192 B PK(A) A's attributes first B \u2192 A PK(B) B's attributes first Both PK(A) A's attributes first Neither PK(A) \u222a PK(B) A's PK, then B's additional PK <p>A \u2192 B means: All of B's primary key attributes exist in A (as PK or secondary).</p>"},{"location":"reference/specs/query-algebra/#45-examples","title":"4.5 Examples","text":"<pre><code># Session \u2192 Trial (Session's PK is subset of Trial's PK)\nSession * Trial\n# Result PK: (subject_id, session_id) \u2014 same as Session\n\n# Neither determines the other\nSubject * Experimenter\n# Result PK: (subject_id, experimenter_id) \u2014 union of PKs\n</code></pre>"},{"location":"reference/specs/query-algebra/#46-left-join","title":"4.6 Left Join","text":"<p>Preserve all rows from left operand:</p> <pre><code># All sessions, with trial data where available\nSession.join(Trial, left=True)\n</code></pre> <p>Constraint: Left join requires A \u2192 B to prevent NULL values in result's primary key.</p>"},{"location":"reference/specs/query-algebra/#47-semantic-matching","title":"4.7 Semantic Matching","text":"<p>With <code>semantic_check=True</code>, only homologous namesakes are matched:</p> <pre><code># Semantic join (default)\nTableA * TableB\n\n# Natural join (match all namesakes regardless of lineage)\nTableA.join(TableB, semantic_check=False)\n</code></pre>"},{"location":"reference/specs/query-algebra/#48-algebraic-properties","title":"4.8 Algebraic Properties","text":"Property Value Primary Key Depends on functional dependencies Attributes Union of both operands' attributes Commutativity Result rows same, but PK/order may differ"},{"location":"reference/specs/query-algebra/#49-error-conditions","title":"4.9 Error Conditions","text":"Condition Error Different database connections <code>DataJointError</code> Non-homologous namesakes (semantic mode) <code>DataJointError</code> Left join without A \u2192 B <code>DataJointError</code>"},{"location":"reference/specs/query-algebra/#5-aggregation-aggr","title":"5. Aggregation (<code>.aggr()</code>)","text":""},{"location":"reference/specs/query-algebra/#51-syntax","title":"5.1 Syntax","text":"<pre><code>result = A.aggr(B, ...)                           # All A attributes\nresult = A.aggr(B, 'attr1', 'attr2')              # PK + specified from A\nresult = A.aggr(B, ..., count='count(*)')         # With aggregate\nresult = A.aggr(B, ..., exclude_nonmatching=True) # Only rows with matches\n</code></pre>"},{"location":"reference/specs/query-algebra/#52-parameters","title":"5.2 Parameters","text":"Parameter Default Description <code>*attributes</code> \u2014 Attributes from A to include <code>exclude_nonmatching</code> <code>False</code> If True, exclude rows from A that have no matches in B (INNER JOIN). Default keeps all rows (LEFT JOIN). <code>**named_attributes</code> \u2014 Computed aggregates"},{"location":"reference/specs/query-algebra/#53-requirement","title":"5.3 Requirement","text":"<p>B must contain all primary key attributes of A. This enables grouping B's rows by A's primary key.</p>"},{"location":"reference/specs/query-algebra/#54-aggregate-functions","title":"5.4 Aggregate Functions","text":"<pre><code># Count\nSession.aggr(Trial, n_trials='count(*)')\n\n# Sum, average, min, max\nSession.aggr(Trial,\n    total='sum(score)',\n    avg_score='avg(score)',\n    best='max(score)',\n    worst='min(score)'\n)\n\n# Group concatenation\nSession.aggr(Trial, trial_list='group_concat(trial_id)')\n\n# Conditional count\nSession.aggr(Trial, n_correct='sum(correct = 1)')\n</code></pre>"},{"location":"reference/specs/query-algebra/#55-sql-equivalent","title":"5.5 SQL Equivalent","text":"<pre><code>SELECT A.pk1, A.pk2, A.secondary, agg_func(B.col) AS new_attr\nFROM A\n[LEFT] JOIN B USING (pk1, pk2)\nWHERE &lt;A restrictions&gt;\nGROUP BY A.pk1, A.pk2\nHAVING &lt;B restrictions&gt;\n</code></pre>"},{"location":"reference/specs/query-algebra/#56-restriction-behavior","title":"5.6 Restriction Behavior","text":"<p>Restrictions on A attributes \u2192 WHERE clause (before GROUP BY) Restrictions on B attributes \u2192 HAVING clause (after GROUP BY)</p> <pre><code># WHERE: only 2024 sessions, then count trials\n(Session &amp; \"YEAR(session_date) = 2024\").aggr(Trial, n='count(*)')\n\n# HAVING: sessions with more than 10 trials\nSession.aggr(Trial, n='count(*)') &amp; \"n &gt; 10\"\n</code></pre>"},{"location":"reference/specs/query-algebra/#57-default-behavior-keep-all-rows","title":"5.7 Default Behavior: Keep All Rows","text":"<p>By default (<code>exclude_nonmatching=False</code>), aggregation keeps all rows from A, even those without matches in B:</p> <pre><code># All sessions included; those without trials have n=0\nSession.aggr(Trial, n='count(trial_id)')\n\n# Only sessions that have at least one trial\nSession.aggr(Trial, n='count(trial_id)', exclude_nonmatching=True)\n</code></pre> <p>Note: Use <code>count(pk_attr)</code> rather than <code>count(*)</code> to correctly count 0 for sessions without trials. <code>count(*)</code> counts all rows including the NULL-filled left join row.</p>"},{"location":"reference/specs/query-algebra/#58-algebraic-properties","title":"5.8 Algebraic Properties","text":"Property Value Primary Key PK(A) \u2014 grouping expression's PK Entity Type Same as A"},{"location":"reference/specs/query-algebra/#59-error-conditions","title":"5.9 Error Conditions","text":"Condition Error B missing A's PK attributes <code>DataJointError</code> Semantic mismatch <code>DataJointError</code>"},{"location":"reference/specs/query-algebra/#6-extension-extend","title":"6. Extension (<code>.extend()</code>)","text":""},{"location":"reference/specs/query-algebra/#61-syntax","title":"6.1 Syntax","text":"<pre><code>result = A.extend(B)\nresult = A.extend(B, semantic_check=True)\n</code></pre>"},{"location":"reference/specs/query-algebra/#62-semantics","title":"6.2 Semantics","text":"<p>Extend is a left join that adds attributes from B while preserving A's entity identity:</p> <pre><code>A.extend(B)\n# Equivalent to:\nA.join(B, left=True)\n</code></pre>"},{"location":"reference/specs/query-algebra/#63-requirement","title":"6.3 Requirement","text":"<p>A must determine B (A \u2192 B). All of B's primary key attributes must exist in A.</p>"},{"location":"reference/specs/query-algebra/#64-use-case","title":"6.4 Use Case","text":"<p>Add optional attributes without losing rows:</p> <pre><code># Add experimenter info to sessions (some sessions may lack experimenter)\nSession.extend(Experimenter)\n</code></pre>"},{"location":"reference/specs/query-algebra/#65-algebraic-properties","title":"6.5 Algebraic Properties","text":"Property Value Primary Key PK(A) Attributes A's attributes + B's non-PK attributes Entity Type Same as A"},{"location":"reference/specs/query-algebra/#66-error-conditions","title":"6.6 Error Conditions","text":"Condition Error A does not determine B <code>DataJointError</code>"},{"location":"reference/specs/query-algebra/#7-union","title":"7. Union (<code>+</code>)","text":""},{"location":"reference/specs/query-algebra/#71-syntax","title":"7.1 Syntax","text":"<pre><code>result = A + B\n</code></pre>"},{"location":"reference/specs/query-algebra/#72-requirements","title":"7.2 Requirements","text":"<ol> <li>Same connection: Both from same database</li> <li>Same primary key: Identical PK attributes (names and types)</li> <li>No secondary attribute overlap: A and B cannot share secondary attributes</li> </ol>"},{"location":"reference/specs/query-algebra/#73-semantics","title":"7.3 Semantics","text":"<p>Combines entity sets from both operands:</p> <pre><code># All subjects that are either mice or rats\nMouse + Rat\n</code></pre>"},{"location":"reference/specs/query-algebra/#74-attribute-handling","title":"7.4 Attribute Handling","text":"Scenario Result PK only in both Union of PKs A has secondary attrs A's secondaries (NULL for B-only rows) B has secondary attrs B's secondaries (NULL for A-only rows) Overlapping PKs A's values take precedence"},{"location":"reference/specs/query-algebra/#75-sql-implementation","title":"7.5 SQL Implementation","text":"<pre><code>-- With secondary attributes\n(SELECT A.* FROM A LEFT JOIN B USING (pk))\nUNION\n(SELECT B.* FROM B WHERE (B.pk) NOT IN (SELECT A.pk FROM A))\n</code></pre>"},{"location":"reference/specs/query-algebra/#76-algebraic-properties","title":"7.6 Algebraic Properties","text":"Property Value Primary Key PK(A) = PK(B) Associative (A + B) + C = A + (B + C) Commutative A + B has same rows as B + A"},{"location":"reference/specs/query-algebra/#77-error-conditions","title":"7.7 Error Conditions","text":"Condition Error Different connections <code>DataJointError</code> Different primary keys <code>DataJointError</code> Overlapping secondary attributes <code>DataJointError</code>"},{"location":"reference/specs/query-algebra/#8-universal-sets-dju","title":"8. Universal Sets (<code>dj.U()</code>)","text":""},{"location":"reference/specs/query-algebra/#81-syntax","title":"8.1 Syntax","text":"<pre><code>dj.U()                    # Singular entity (one row, no attributes)\ndj.U('attr1', 'attr2')    # Set of all combinations\n</code></pre>"},{"location":"reference/specs/query-algebra/#82-unique-value-enumeration","title":"8.2 Unique Value Enumeration","text":"<p>Extract distinct values:</p> <pre><code># All unique last names\ndj.U('last_name') &amp; Student\n\n# All unique (year, month) combinations\ndj.U('year', 'month') &amp; Session.proj(year='YEAR(date)', month='MONTH(date)')\n</code></pre> <p>Result has specified attributes as primary key, with DISTINCT semantics.</p>"},{"location":"reference/specs/query-algebra/#83-universal-aggregation","title":"8.3 Universal Aggregation","text":"<p>Aggregate entire table (no grouping):</p> <pre><code># Count all students\ndj.U().aggr(Student, n='count(*)')\n# Result: single row with n = total count\n\n# Global statistics\ndj.U().aggr(Trial,\n    total='count(*)',\n    avg_score='avg(score)',\n    std_score='std(score)'\n)\n</code></pre>"},{"location":"reference/specs/query-algebra/#84-arbitrary-grouping","title":"8.4 Arbitrary Grouping","text":"<p>Group by attributes not in original PK:</p> <pre><code># Count students by graduation year\ndj.U('grad_year').aggr(Student, n='count(*)')\n\n# Monthly session counts\ndj.U('year', 'month').aggr(\n    Session.proj(year='YEAR(date)', month='MONTH(date)'),\n    n='count(*)'\n)\n</code></pre>"},{"location":"reference/specs/query-algebra/#85-primary-key-behavior","title":"8.5 Primary Key Behavior","text":"Usage Result PK <code>dj.U() &amp; table</code> Empty (single row) <code>dj.U('a', 'b') &amp; table</code> (a, b) <code>dj.U().aggr(table, ...)</code> Empty (single row) <code>dj.U('a').aggr(table, ...)</code> (a,)"},{"location":"reference/specs/query-algebra/#86-restrictions","title":"8.6 Restrictions","text":"<pre><code># U attributes must exist in the table\ndj.U('name') &amp; Student        # OK: 'name' in Student\ndj.U('invalid') &amp; Student     # Error: 'invalid' not found\n</code></pre>"},{"location":"reference/specs/query-algebra/#87-error-conditions","title":"8.7 Error Conditions","text":"Condition Error <code>table * dj.U()</code> <code>DataJointError</code> (use <code>&amp;</code> instead) <code>dj.U() - table</code> <code>DataJointError</code> (infinite set) U attributes not in table <code>DataJointError</code> <code>dj.U().aggr(..., exclude_nonmatching=False)</code> <code>DataJointError</code> (cannot keep all rows from infinite set)"},{"location":"reference/specs/query-algebra/#9-semantic-matching","title":"9. Semantic Matching","text":""},{"location":"reference/specs/query-algebra/#91-attribute-lineage","title":"9.1 Attribute Lineage","text":"<p>Every attribute has a lineage tracing to its original definition:</p> <pre><code>schema.table.attribute\n</code></pre> <p>Foreign key inheritance preserves lineage:</p> <pre><code>class Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject          # Inherits subject_id with Subject's lineage\n    session_id : int\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/query-algebra/#92-homologous-namesakes","title":"9.2 Homologous Namesakes","text":"<p>Two attributes are homologous namesakes if they have: 1. Same name 2. Same lineage (trace to same original definition)</p>"},{"location":"reference/specs/query-algebra/#93-non-homologous-namesakes","title":"9.3 Non-Homologous Namesakes","text":"<p>Attributes with same name but different lineage create semantic collisions:</p> <pre><code># Both have 'name' but from different origins\nStudent * Course  # Error if both have 'name' attribute\n</code></pre>"},{"location":"reference/specs/query-algebra/#94-resolution","title":"9.4 Resolution","text":"<p>Rename to avoid collisions:</p> <pre><code>Student * Course.proj(..., course_name='name')\n</code></pre>"},{"location":"reference/specs/query-algebra/#95-semantic-check-parameter","title":"9.5 Semantic Check Parameter","text":"Value Behavior <code>True</code> (default) Match only homologous namesakes; error on collisions <code>False</code> Natural join on all namesakes regardless of lineage"},{"location":"reference/specs/query-algebra/#10-operator-precedence","title":"10. Operator Precedence","text":"<p>Python operator precedence applies:</p> Precedence Operator Operation Highest <code>*</code> Join <code>+</code>, <code>-</code> Union, Anti-restriction Lowest <code>&amp;</code> Restriction <p>Use parentheses for clarity:</p> <pre><code>(Session &amp; condition) * Trial    # Restrict then join\nSession &amp; (Trial * Stimulus)     # Join then restrict\n</code></pre>"},{"location":"reference/specs/query-algebra/#11-subquery-generation","title":"11. Subquery Generation","text":"<p>Subqueries are generated automatically when needed:</p> Situation Subquery Created Restrict on computed attribute Yes Join on computed attribute Yes Aggregation operand Yes Union operand Yes Restriction after TOP Yes"},{"location":"reference/specs/query-algebra/#12-top-djtop","title":"12. Top (<code>dj.Top</code>)","text":""},{"location":"reference/specs/query-algebra/#121-syntax","title":"12.1 Syntax","text":"<pre><code>result = expression &amp; dj.Top()                          # First row by primary key\nresult = expression &amp; dj.Top(limit=5)                   # First 5 rows by primary key\nresult = expression &amp; dj.Top(5, 'score DESC')           # Top 5 by score descending\nresult = expression &amp; dj.Top(10, order_by='date DESC')  # Top 10 by date descending\nresult = expression &amp; dj.Top(5, offset=10)              # Skip 10, take 5\nresult = expression &amp; dj.Top(None, 'score DESC')        # All rows, ordered by score\n</code></pre>"},{"location":"reference/specs/query-algebra/#122-parameters","title":"12.2 Parameters","text":"Parameter Type Default Description <code>limit</code> <code>int</code> or <code>None</code> <code>1</code> Maximum rows to return. <code>None</code> = unlimited. <code>order_by</code> <code>str</code>, <code>list[str]</code>, or <code>None</code> <code>\"KEY\"</code> Ordering. <code>\"KEY\"</code> = primary key order. <code>None</code> = inherit existing order. <code>offset</code> <code>int</code> <code>0</code> Rows to skip before taking <code>limit</code>."},{"location":"reference/specs/query-algebra/#123-ordering-specification","title":"12.3 Ordering Specification","text":"Format Meaning <code>\"KEY\"</code> Order by primary key (ascending) <code>\"attr\"</code> Order by attribute (ascending) <code>\"attr DESC\"</code> Order by attribute (descending) <code>\"attr ASC\"</code> Order by attribute (ascending, explicit) <code>[\"attr1 DESC\", \"attr2\"]</code> Multiple columns <code>None</code> Inherit ordering from existing Top"},{"location":"reference/specs/query-algebra/#124-sql-equivalent","title":"12.4 SQL Equivalent","text":"<pre><code>SELECT * FROM table\nORDER BY order_by\nLIMIT limit OFFSET offset\n</code></pre>"},{"location":"reference/specs/query-algebra/#125-chaining-tops","title":"12.5 Chaining Tops","text":"<p>When multiple Tops are chained, behavior depends on the <code>order_by</code> parameter:</p> Scenario Behavior Second Top has <code>order_by=None</code> Merge: inherits ordering, limits combined Both Tops have identical <code>order_by</code> Merge: ordering preserved, limits combined Tops have different <code>order_by</code> Subquery: first Top executed, then second applied <p>Merge behavior: - <code>limit</code> = minimum of both limits - <code>offset</code> = sum of both offsets - <code>order_by</code> = preserved from first Top</p> <pre><code># Merge: same result, single query\n(Table &amp; dj.Top(10, \"score DESC\")) &amp; dj.Top(5, order_by=None)\n# Effective: Top(5, \"score DESC\", offset=0)\n\n# Merge with offsets\n(Table &amp; dj.Top(10, \"x\", offset=5)) &amp; dj.Top(3, order_by=None, offset=2)\n# Effective: Top(3, \"x\", offset=7)\n\n# Subquery: different orderings\n(Table &amp; dj.Top(10, \"score DESC\")) &amp; dj.Top(3, \"id ASC\")\n# First selects top 10 by score, then reorders those 10 by id and takes 3\n</code></pre>"},{"location":"reference/specs/query-algebra/#126-preview-and-limit","title":"12.6 Preview and Limit","text":"<p>When fetching with a <code>limit</code> parameter, the limit is applied as an additional Top that inherits existing ordering:</p> <pre><code># User applies custom ordering\nquery = Table &amp; dj.Top(order_by=\"score DESC\")\n\n# Preview respects the ordering\nquery.to_arrays(\"id\", \"score\", limit=5)  # Top 5 by score descending\n</code></pre> <p>Internally, <code>to_arrays(..., limit=N)</code> applies <code>dj.Top(N, order_by=None)</code>, which inherits the existing ordering.</p>"},{"location":"reference/specs/query-algebra/#127-use-cases","title":"12.7 Use Cases","text":"<p>Top N rows: <pre><code># Top 10 highest scores\nResult &amp; dj.Top(10, \"score DESC\")\n</code></pre></p> <p>Pagination: <pre><code># Page 3 (rows 20-29) sorted by date\nSession &amp; dj.Top(10, \"session_date DESC\", offset=20)\n</code></pre></p> <p>Sampling (deterministic): <pre><code># First 100 rows by primary key\nBigTable &amp; dj.Top(100)\n</code></pre></p> <p>Ordering without limit: <pre><code># All rows ordered by date\nSession &amp; dj.Top(None, \"session_date DESC\")\n</code></pre></p>"},{"location":"reference/specs/query-algebra/#128-algebraic-properties","title":"12.8 Algebraic Properties","text":"Property Value Primary Key Preserved: PK(result) = PK(input) Attributes Preserved: all attributes retained Entity Type Preserved Row Order Determined by <code>order_by</code>"},{"location":"reference/specs/query-algebra/#129-error-conditions","title":"12.9 Error Conditions","text":"Condition Error <code>limit</code> not int or None <code>TypeError</code> <code>order_by</code> not str, list[str], or None <code>TypeError</code> <code>offset</code> not int <code>TypeError</code> Top in OR list <code>DataJointError</code> Top in AndList <code>DataJointError</code>"},{"location":"reference/specs/query-algebra/#13-sql-transpilation","title":"13. SQL Transpilation","text":"<p>This section describes how DataJoint translates query expressions to SQL.</p>"},{"location":"reference/specs/query-algebra/#131-mysql-clause-evaluation-order","title":"13.1 MySQL Clause Evaluation Order","text":"<p>MySQL differs from standard SQL in clause evaluation:</p> <pre><code>Standard SQL: FROM \u2192 WHERE \u2192 GROUP BY \u2192 HAVING \u2192 SELECT\nMySQL:        FROM \u2192 WHERE \u2192 SELECT \u2192 GROUP BY \u2192 HAVING\n</code></pre> <p>This allows <code>GROUP BY</code> and <code>HAVING</code> clauses to use alias column names created by <code>SELECT</code>. DataJoint targets MySQL's behavior where column aliases can be used in <code>HAVING</code>.</p>"},{"location":"reference/specs/query-algebra/#132-queryexpression-properties","title":"13.2 QueryExpression Properties","text":"<p>Each <code>QueryExpression</code> represents a SQL <code>SELECT</code> statement with these properties:</p> Property SQL Clause Description <code>heading</code> <code>SELECT</code> Attributes and computed expressions <code>restriction</code> <code>WHERE</code> / <code>HAVING</code> Filter conditions <code>support</code> <code>FROM</code> Base tables and joined tables <code>_top</code> <code>ORDER BY</code> / <code>LIMIT</code> Ordering and pagination <code>_group_by</code> <code>GROUP BY</code> Grouping (for aggregations)"},{"location":"reference/specs/query-algebra/#133-modify-in-place-vs-subquery-wrapping","title":"13.3 Modify-in-Place vs. Subquery Wrapping","text":"<p>When an operator is applied to a QueryExpression, DataJoint either:</p> <ol> <li>Modifies in place: Adds/modifies clauses in the existing SELECT statement</li> <li>Wraps as subquery: Creates a new outer SELECT with the input as a FROM subquery</li> </ol> <p>The choice depends on SQL semantics\u2014whether the operation can be expressed by adding clauses, or whether the input must first be materialized as a derived table.</p>"},{"location":"reference/specs/query-algebra/#134-restriction-rules-and-","title":"13.4 Restriction Rules (<code>&amp;</code> and <code>-</code>)","text":"<p>Modify in place when restricting on: - Base table attributes (columns from FROM tables) - Primary key attributes (including those inherited via FK)</p> <p>Wrap as subquery when restricting on: - Computed/aliased attributes (created by <code>.proj()</code>) - Aggregated attributes (created by <code>.aggr()</code>) - Any attribute after <code>LIMIT</code>/<code>OFFSET</code> has been applied</p> <pre><code># Modify in place: condition added to WHERE\nSession &amp; \"session_date &gt; '2024-01-01'\"\n# SQL: SELECT ... FROM session WHERE session_date &gt; '2024-01-01'\n\n# Subquery required: restriction references computed alias\nSession.proj(year='YEAR(session_date)') &amp; \"year = 2024\"\n# SQL: SELECT * FROM (\n#        SELECT ..., YEAR(session_date) as year FROM session\n#      ) AS _s1 WHERE year = 2024\n</code></pre> <p>Aggregation special case: Restrictions after aggregation go to HAVING (MySQL) or require subquery (PostgreSQL):</p> <pre><code>Session.aggr(Trial, n='count(*)') &amp; \"n &gt; 10\"\n# MySQL:      ... GROUP BY pk HAVING n &gt; 10\n# PostgreSQL: SELECT * FROM (...GROUP BY pk) AS _s1 WHERE n &gt; 10\n</code></pre>"},{"location":"reference/specs/query-algebra/#135-projection-rules-proj","title":"13.5 Projection Rules (<code>.proj()</code>)","text":"<p>Modify in place when: - Selecting a subset of existing attributes - Renaming base attributes - Computing new attributes from base columns</p> <p>Wrap as subquery when: - Computing attribute from another computed attribute (alias of alias) - Projecting after <code>LIMIT</code>/<code>OFFSET</code> has been applied</p> <pre><code># Modify in place: adds computed column to SELECT\nSession.proj(year='YEAR(session_date)')\n# SQL: SELECT pk, YEAR(session_date) as year FROM session\n\n# Subquery required: decade depends on year alias\nSession.proj(year='YEAR(session_date)').proj(decade='year - year % 10')\n# SQL: SELECT pk, year - year % 10 as decade FROM (\n#        SELECT pk, YEAR(session_date) as year FROM session\n#      ) AS _s1\n</code></pre>"},{"location":"reference/specs/query-algebra/#1351-join-rules","title":"13.5.1 Join Rules (<code>*</code>)","text":"<p>Modify in place (merge both operands' clauses) when: - Both operands are base tables or simple projections - Join attributes are base columns (not computed) - Neither operand has <code>LIMIT</code>/<code>OFFSET</code> or <code>GROUP BY</code></p> <p>Wrap operand as subquery when operand has: - Computed attributes used in join condition - <code>LIMIT</code>/<code>OFFSET</code> applied - <code>GROUP BY</code> (is an aggregation result) - Is a union expression</p> <pre><code># Modify in place: merge FROM and WHERE clauses\nSession * Trial\n# SQL: SELECT ... FROM session JOIN trial USING (pk)\n\n# Subquery required: aggregation as join operand\nSession * Session.aggr(Trial, n='count(*)')\n# SQL: SELECT ... FROM session JOIN (\n#        SELECT pk, count(*) as n FROM session\n#        LEFT JOIN trial USING (pk) GROUP BY pk\n#      ) AS _s1 USING (pk)\n</code></pre>"},{"location":"reference/specs/query-algebra/#1352-aggregation-rules-aggr","title":"13.5.2 Aggregation Rules (<code>.aggr()</code>)","text":"<p>Aggregation produces a query with <code>GROUP BY</code>. The grouped expression is joined:</p> <pre><code>SELECT A.pk, A.secondary, agg_func(B.col) AS computed\nFROM A LEFT JOIN B USING (pk)\nWHERE &lt;restrictions on A attributes&gt;\nGROUP BY A.pk\nHAVING &lt;restrictions on B or computed attributes&gt;\n</code></pre> <p>Restrictions routing: - On grouping expression (A) attributes \u2192 <code>WHERE</code> (before GROUP BY) - On grouped expression (B) or computed attributes \u2192 <code>HAVING</code> (after GROUP BY)</p> <p>Aggregation result requires subquery when used as operand to: - Another join (<code>*</code>) - Further restriction on computed attributes - Another aggregation</p>"},{"location":"reference/specs/query-algebra/#1353-toplimit-rules-djtop","title":"13.5.3 Top/Limit Rules (<code>dj.Top</code>)","text":"<p>Any operation after Top requires subquery (the limited result must materialize):</p> <pre><code>(Session &amp; dj.Top(10, 'date DESC')) &amp; \"notes IS NOT NULL\"\n# SQL: SELECT * FROM (\n#        SELECT * FROM session ORDER BY date DESC LIMIT 10\n#      ) AS _s1 WHERE notes IS NOT NULL\n</code></pre> <p>Exception: Chaining Tops with compatible ordering can merge (see Section 12.5)</p>"},{"location":"reference/specs/query-algebra/#136-backend-specific-transpilation","title":"13.6 Backend-Specific Transpilation","text":"<p>DataJoint supports multiple database backends with differing SQL dialects. The adapter layer handles:</p>"},{"location":"reference/specs/query-algebra/#1361-function-translation","title":"13.6.1 Function Translation","text":"<p>Aggregate functions are translated bidirectionally:</p> DataJoint MySQL PostgreSQL <code>GROUP_CONCAT(col)</code> <code>GROUP_CONCAT(col)</code> <code>STRING_AGG(col::text, ',')</code> <code>STRING_AGG(col, sep)</code> <code>GROUP_CONCAT(col SEPARATOR sep)</code> <code>STRING_AGG(col, sep)</code> <p>Code using either syntax works on both backends.</p>"},{"location":"reference/specs/query-algebra/#1362-having-clause-alias-references","title":"13.6.2 HAVING Clause Alias References","text":"<p>MySQL allows column aliases in HAVING:</p> <pre><code>SELECT person_id, COUNT(*) as n_languages\nFROM proficiency\nGROUP BY person_id\nHAVING n_languages &gt;= 4   -- MySQL: OK, PostgreSQL: ERROR\n</code></pre> <p>PostgreSQL (following SQL standard) requires repeating the expression or using a subquery:</p> <pre><code>-- Option A: Repeat expression\nHAVING COUNT(*) &gt;= 4\n\n-- Option B: Subquery wrapper\nSELECT * FROM (\n  SELECT person_id, COUNT(*) as n_languages\n  FROM proficiency\n  GROUP BY person_id\n) AS subq\nWHERE n_languages &gt;= 4\n</code></pre> <p>DataJoint approach: When generating SQL for PostgreSQL, if the HAVING clause references computed attribute aliases, the aggregation is wrapped in a subquery and the HAVING condition becomes a WHERE clause on the outer query.</p> Backend HAVING with alias Generated SQL MySQL Direct <code>... GROUP BY pk HAVING alias &gt; 5</code> PostgreSQL Subquery wrapper <code>SELECT * FROM (...) AS subq WHERE alias &gt; 5</code> <p>This ensures consistent behavior across backends while maintaining query correctness.</p>"},{"location":"reference/specs/query-algebra/#1363-identifier-quoting","title":"13.6.3 Identifier Quoting","text":"Backend Identifier Quote Example MySQL Backtick <code>`table`.`column`</code> PostgreSQL Double quote <code>\"table\".\"column\"</code>"},{"location":"reference/specs/query-algebra/#1364-string-literal-quoting","title":"13.6.4 String Literal Quoting","text":"Backend String Literals Identifier Literals MySQL <code>'value'</code> or <code>\"value\"</code> <code>`name`</code> PostgreSQL <code>'value'</code> only <code>\"name\"</code> <p>In PostgreSQL, double quotes denote identifiers, not string literals. Dictionary restrictions handle this automatically; string restrictions must use single quotes for values.</p>"},{"location":"reference/specs/query-algebra/#137-union-sql","title":"13.7 Union SQL","text":"<p>Union performs an outer join:</p> <pre><code>(SELECT A.pk, A.secondary, NULL as B.secondary FROM A)\nUNION\n(SELECT B.pk, NULL as A.secondary, B.secondary FROM B\n WHERE B.pk NOT IN (SELECT pk FROM A))\n</code></pre> <p>All union inputs become subqueries except unrestricted unions.</p>"},{"location":"reference/specs/query-algebra/#138-query-backprojection","title":"13.8 Query Backprojection","text":"<p>Before execution, <code>finalize()</code> recursively projects out unnecessary attributes from all inputs. This optimization:</p> <ul> <li>Reduces data transfer (especially for blobs)</li> <li>Compensates for MySQL's query optimizer limitations</li> <li>Produces leaner queries for complex expressions</li> </ul>"},{"location":"reference/specs/query-algebra/#139-subquery-generation-summary","title":"13.9 Subquery Generation Summary","text":"Trigger Reason Strategy Restrict on computed attribute Alias must exist before WHERE references it Wrap input Restrict after LIMIT/OFFSET Limited result must materialize first Wrap input Project alias from alias First alias must materialize Wrap input Join on computed attribute Join condition can't reference SELECT alias Wrap operand Aggregation as join operand GROUP BY must complete first Wrap operand Restrict on aggregated attribute HAVING result must materialize (PostgreSQL) Wrap aggregation Union operands UNION requires complete subqueries Wrap both PostgreSQL HAVING with alias Standard SQL compliance Wrap + convert to WHERE"},{"location":"reference/specs/query-algebra/#14-implementation-reference","title":"14. Implementation Reference","text":"File Purpose <code>expression.py</code> QueryExpression base class, operators <code>condition.py</code> Restriction condition handling, Top class <code>heading.py</code> Attribute metadata and lineage <code>table.py</code> Table class, fetch interface <code>U.py</code> Universal set implementation"},{"location":"reference/specs/query-algebra/#15-quick-reference","title":"15. Quick Reference","text":"Operation Syntax Result PK Restrict <code>A &amp; cond</code> PK(A) Anti-restrict <code>A - cond</code> PK(A) Project <code>A.proj(...)</code> PK(A) Join <code>A * B</code> Depends on A\u2192B Aggregate <code>A.aggr(B, ...)</code> PK(A) Extend <code>A.extend(B)</code> PK(A) Union <code>A + B</code> PK(A) = PK(B) Unique values <code>dj.U('x') &amp; A</code> (x,) Global aggregate <code>dj.U().aggr(A, ...)</code> ()"},{"location":"reference/specs/semantic-matching/","title":"Semantic Matching - Specification","text":""},{"location":"reference/specs/semantic-matching/#overview","title":"Overview","text":"<p>This document specifies semantic matching for binary operators in DataJoint 2.0. Semantic matching ensures that attributes are only matched when they share both the same name and the same lineage (origin), preventing accidental matches on unrelated attributes that happen to share names. This replaces the name-based matching rules from pre-2.0 versions.</p>"},{"location":"reference/specs/semantic-matching/#goals","title":"Goals","text":"<ol> <li>Prevent incorrect matches on attributes that share names but represent different entities</li> <li>Enable valid operations that were previously blocked due to overly restrictive rules</li> <li>Maintain backward compatibility for well-designed schemas</li> <li>Provide clear error messages when semantic conflicts are detected</li> </ol>"},{"location":"reference/specs/semantic-matching/#user-guide","title":"User Guide","text":""},{"location":"reference/specs/semantic-matching/#quick-start","title":"Quick Start","text":"<p>Semantic matching is enabled by default in DataJoint 2.0. For most well-designed schemas, no changes are required.</p>"},{"location":"reference/specs/semantic-matching/#when-you-might-see-errors","title":"When You Might See Errors","text":"<pre><code># Two tables with generic 'id' attribute\nclass Student(dj.Manual):\n    definition = \"\"\"\n    id : int64\n    ---\n    name : varchar(100)\n    \"\"\"\n\nclass Course(dj.Manual):\n    definition = \"\"\"\n    id : int64\n    ---\n    title : varchar(100)\n    \"\"\"\n\n# This will raise an error because 'id' has different lineages\nStudent() * Course()  # DataJointError!\n</code></pre>"},{"location":"reference/specs/semantic-matching/#how-to-resolve","title":"How to Resolve","text":"<p>Option 1: Rename attributes using projection <pre><code>Student() * Course().proj(course_id='id')  # OK\n</code></pre></p> <p>Option 2: Bypass semantic check (use with caution) <pre><code>Student().join(Course(), semantic_check=False)  # OK, but be careful!\n</code></pre></p> <p>Option 3: Use descriptive names (best practice) <pre><code>class Student(dj.Manual):\n    definition = \"\"\"\n    student_id : int64\n    ---\n    name : varchar(100)\n    \"\"\"\n</code></pre></p>"},{"location":"reference/specs/semantic-matching/#rebuilding-lineage-for-existing-schemas","title":"Rebuilding Lineage for Existing Schemas","text":"<p>If you have existing schemas created before DataJoint 2.0, rebuild their lineage tables:</p> <pre><code>import datajoint as dj\n\n# Connect and get your schema\nschema = dj.Schema('my_database')\n\n# Rebuild lineage (do this once per schema)\nschema.rebuild_lineage()\n\n# Restart Python kernel to pick up changes\n</code></pre> <p>Important: If your schema references tables in other schemas, rebuild those upstream schemas first.</p>"},{"location":"reference/specs/semantic-matching/#api-reference","title":"API Reference","text":""},{"location":"reference/specs/semantic-matching/#schema-methods","title":"Schema Methods","text":""},{"location":"reference/specs/semantic-matching/#schemarebuild_lineage","title":"<code>schema.rebuild_lineage()</code>","text":"<p>Rebuild the <code>~lineage</code> table for all tables in this schema.</p> <pre><code>schema.rebuild_lineage()\n</code></pre> <p>Description: Recomputes lineage for all attributes by querying FK relationships from the database's <code>information_schema</code>. Use this to restore lineage for schemas that predate the lineage system or after corruption.</p> <p>Requirements:</p> <ul> <li>Schema must exist</li> <li>Upstream schemas (referenced via cross-schema FKs) must have their lineage rebuilt first</li> </ul> <p>Side Effects:</p> <ul> <li>Creates <code>~lineage</code> table if it doesn't exist</li> <li>Deletes and repopulates all lineage entries for tables in the schema</li> </ul> <p>Post-Action: Restart Python kernel and reimport to pick up new lineage information.</p>"},{"location":"reference/specs/semantic-matching/#schemalineage_table_exists","title":"<code>schema.lineage_table_exists</code>","text":"<p>Property indicating whether the <code>~lineage</code> table exists in this schema.</p> <pre><code>if schema.lineage_table_exists:\n    print(\"Lineage tracking is enabled\")\n</code></pre> <p>Returns: <code>bool</code> - <code>True</code> if <code>~lineage</code> table exists, <code>False</code> otherwise.</p>"},{"location":"reference/specs/semantic-matching/#schemalineage","title":"<code>schema.lineage</code>","text":"<p>Property returning all lineage entries for the schema.</p> <pre><code>schema.lineage\n# {'myschema.session.session_id': 'myschema.session.session_id',\n#  'myschema.trial.session_id': 'myschema.session.session_id',\n#  'myschema.trial.trial_num': 'myschema.trial.trial_num'}\n</code></pre> <p>Returns: <code>dict</code> - Maps <code>'schema.table.attribute'</code> to its lineage origin</p>"},{"location":"reference/specs/semantic-matching/#join-methods","title":"Join Methods","text":""},{"location":"reference/specs/semantic-matching/#exprjoinother-semantic_checktrue","title":"<code>expr.join(other, semantic_check=True)</code>","text":"<p>Join two expressions with optional semantic checking.</p> <pre><code>result = A.join(B)                        # semantic_check=True (default)\nresult = A.join(B, semantic_check=False)  # bypass semantic check\n</code></pre> <p>Parameters:</p> <ul> <li><code>other</code>: Another query expression to join with</li> <li><code>semantic_check</code> (bool): If <code>True</code> (default), raise error on non-homologous namesakes. If <code>False</code>, perform natural join without lineage checking.</li> </ul> <p>Raises: <code>DataJointError</code> if <code>semantic_check=True</code> and namesake attributes have different lineages.</p>"},{"location":"reference/specs/semantic-matching/#exprrestrictother-semantic_checktrue","title":"<code>expr.restrict(other, semantic_check=True)</code>","text":"<p>Restrict expression with optional semantic checking.</p> <pre><code>result = A.restrict(B)                        # semantic_check=True (default)\nresult = A.restrict(B, semantic_check=False)  # bypass semantic check\n</code></pre> <p>Parameters:</p> <ul> <li><code>other</code>: Restriction condition (expression, dict, string, etc.)</li> <li><code>semantic_check</code> (bool): If <code>True</code> (default), raise error on non-homologous namesakes when restricting by another expression. If <code>False</code>, no lineage checking.</li> </ul> <p>Raises: <code>DataJointError</code> if <code>semantic_check=True</code> and namesake attributes have different lineages.</p>"},{"location":"reference/specs/semantic-matching/#operators","title":"Operators","text":""},{"location":"reference/specs/semantic-matching/#a-b-join","title":"<code>A * B</code> (Join)","text":"<p>Equivalent to <code>A.join(B, semantic_check=True)</code>.</p>"},{"location":"reference/specs/semantic-matching/#a-b-restriction","title":"<code>A &amp; B</code> (Restriction)","text":"<p>Equivalent to <code>A.restrict(B, semantic_check=True)</code>.</p>"},{"location":"reference/specs/semantic-matching/#a-b-anti-restriction","title":"<code>A - B</code> (Anti-restriction)","text":"<p>Restriction with negation. Semantic checking applies.</p> <p>To bypass semantic checking: <code>A.restrict(dj.Not(B), semantic_check=False)</code></p>"},{"location":"reference/specs/semantic-matching/#a-b-union","title":"<code>A + B</code> (Union)","text":"<p>Union of expressions. Requires all namesake attributes to have matching lineage.</p>"},{"location":"reference/specs/semantic-matching/#universal-set-dju","title":"Universal Set (<code>dj.U</code>)","text":""},{"location":"reference/specs/semantic-matching/#valid-operations","title":"Valid Operations","text":"<pre><code>dj.U('a', 'b') &amp; A           # Restriction: promotes a, b to PK\ndj.U('a', 'b').aggr(A, ...)  # Aggregation: groups by a, b\ndj.U() &amp; A                   # Distinct primary keys of A\n</code></pre>"},{"location":"reference/specs/semantic-matching/#invalid-operations","title":"Invalid Operations","text":"<pre><code>dj.U('a', 'b') - A   # DataJointError: produces infinite set\ndj.U('a', 'b') * A   # DataJointError: use &amp; instead\n</code></pre> <p>For conceptual background on lineage, terminology, and matching rules, see Semantic Matching (Explanation).</p>"},{"location":"reference/specs/semantic-matching/#implementation-details","title":"Implementation Details","text":""},{"location":"reference/specs/semantic-matching/#lineage-table","title":"<code>~lineage</code> Table","text":"<p>Each schema has a hidden <code>~lineage</code> table storing lineage information:</p> <pre><code>CREATE TABLE `schema_name`.`~lineage` (\n    table_name VARCHAR(64) NOT NULL,\n    attribute_name VARCHAR(64) NOT NULL,\n    lineage VARCHAR(255) NOT NULL,\n    PRIMARY KEY (table_name, attribute_name)\n)\n</code></pre>"},{"location":"reference/specs/semantic-matching/#lineage-population","title":"Lineage Population","text":"<p>At table declaration:</p> <ol> <li>Delete any existing lineage entries for the table</li> <li>For FK attributes: copy lineage from parent (with warning if parent lineage missing)</li> <li>For native PK attributes: set lineage to <code>schema.table.attribute</code></li> <li>Native secondary attributes: no entry (lineage = None)</li> </ol> <p>At table drop:</p> <ul> <li>Delete all lineage entries for the table</li> </ul>"},{"location":"reference/specs/semantic-matching/#missing-lineage-handling","title":"Missing Lineage Handling","text":"<p>If <code>~lineage</code> table doesn't exist:</p> <ul> <li>Warning issued during semantic check</li> <li>Semantic checking disabled (join proceeds as natural join)</li> </ul> <p>If parent lineage missing during declaration:</p> <ul> <li>Warning issued</li> <li>Parent attribute used as origin</li> <li>Recommend rebuilding lineage after parent schema is fixed</li> </ul>"},{"location":"reference/specs/semantic-matching/#headings-lineage_available-property","title":"Heading's <code>lineage_available</code> Property","text":"<p>The <code>Heading</code> class tracks whether lineage information is available:</p> <pre><code>heading.lineage_available  # True if ~lineage table exists for this schema\n</code></pre> <p>This property is:</p> <ul> <li>Set when heading is loaded from database</li> <li>Propagated through projections, joins, and other operations</li> <li>Used by <code>assert_join_compatibility</code> to decide whether to perform semantic checking</li> </ul>"},{"location":"reference/specs/semantic-matching/#error-messages","title":"Error Messages","text":""},{"location":"reference/specs/semantic-matching/#non-homologous-namesakes","title":"Non-Homologous Namesakes","text":"<pre><code>DataJointError: Cannot join on attribute `id`: different lineages\n(university.student.id vs university.course.id).\nUse .proj() to rename one of the attributes.\n</code></pre>"},{"location":"reference/specs/semantic-matching/#missing-lineage-warning","title":"Missing Lineage Warning","text":"<pre><code>WARNING: Semantic check disabled: ~lineage table not found.\nTo enable semantic matching, rebuild lineage with: schema.rebuild_lineage()\n</code></pre>"},{"location":"reference/specs/semantic-matching/#parent-lineage-missing-warning","title":"Parent Lineage Missing Warning","text":"<pre><code>WARNING: Lineage for `parent_db`.`parent_table`.`attr` not found\n(parent schema's ~lineage table may be missing or incomplete).\nUsing it as origin. Once the parent schema's lineage is rebuilt,\nrun schema.rebuild_lineage() on this schema to correct the lineage.\n</code></pre>"},{"location":"reference/specs/table-declaration/","title":"DataJoint Table Declaration Specification","text":""},{"location":"reference/specs/table-declaration/#overview","title":"Overview","text":"<p>This document specifies the table declaration mechanism in DataJoint Python. Table declarations define the schema structure using a domain-specific language (DSL) embedded in Python class definitions.</p>"},{"location":"reference/specs/table-declaration/#1-table-class-structure","title":"1. Table Class Structure","text":""},{"location":"reference/specs/table-declaration/#11-basic-declaration-pattern","title":"1.1 Basic Declaration Pattern","text":"<pre><code>@schema\nclass TableName(dj.Manual):\n    definition = \"\"\"\n    # table comment\n    primary_attr : int32\n    ---\n    secondary_attr : float64\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/table-declaration/#12-table-tiers","title":"1.2 Table Tiers","text":"Tier Base Class Table Prefix Purpose Manual <code>dj.Manual</code> (none) User-entered data Lookup <code>dj.Lookup</code> <code>#</code> Reference/enumeration data Imported <code>dj.Imported</code> <code>_</code> Data from external sources Computed <code>dj.Computed</code> <code>__</code> Derived from other tables Part <code>dj.Part</code> <code>master__</code> Detail records of master table"},{"location":"reference/specs/table-declaration/#13-class-naming-rules","title":"1.3 Class Naming Rules","text":"<ul> <li>Format: Strict CamelCase (e.g., <code>MyTable</code>, <code>ProcessedData</code>)</li> <li>Pattern: <code>^[A-Z][A-Za-z0-9]*$</code></li> <li>Conversion: CamelCase to snake_case for SQL table name</li> <li>Examples:<ul> <li><code>SessionTrial</code> -&gt; <code>session_trial</code></li> <li><code>ProcessedEMG</code> -&gt; <code>processed_emg</code></li> </ul> </li> </ul>"},{"location":"reference/specs/table-declaration/#14-table-name-constraints","title":"1.4 Table Name Constraints","text":"<ul> <li>Maximum length: 64 characters (MySQL limit)</li> <li>Final name: prefix + snake_case(class_name)</li> <li>Validation: Checked at declaration time</li> </ul>"},{"location":"reference/specs/table-declaration/#2-definition-string-grammar","title":"2. Definition String Grammar","text":""},{"location":"reference/specs/table-declaration/#21-overall-structure","title":"2.1 Overall Structure","text":"<pre><code>[table_comment]\nprimary_key_section\n---\nsecondary_section\n</code></pre>"},{"location":"reference/specs/table-declaration/#22-table-comment-optional","title":"2.2 Table Comment (Optional)","text":"<pre><code># Free-form description of the table purpose\n</code></pre> <ul> <li>Must be first non-empty line if present</li> <li>Starts with <code>#</code></li> <li>Cannot start with <code>#:</code></li> <li>Stored in MySQL table COMMENT</li> </ul>"},{"location":"reference/specs/table-declaration/#23-primary-key-separator","title":"2.3 Primary Key Separator","text":"<pre><code>---\n</code></pre> <ul> <li>Three or more dashes</li> <li>Separates primary key attributes (above) from secondary attributes (below)</li> <li>Required if table has secondary attributes</li> </ul>"},{"location":"reference/specs/table-declaration/#24-line-types","title":"2.4 Line Types","text":"<p>Each non-empty, non-comment line is one of:</p> <ol> <li>Attribute definition</li> <li>Foreign key reference</li> <li>Index declaration</li> </ol>"},{"location":"reference/specs/table-declaration/#25-singleton-tables-empty-primary-keys","title":"2.5 Singleton Tables (Empty Primary Keys)","text":"<p>New in 2.1</p> <p>Singleton tables were introduced in DataJoint 2.1.</p> <p>A singleton table can hold at most one row. It is declared with no attributes in the primary key section:</p> <pre><code>@schema\nclass Config(dj.Lookup):\n    definition = \"\"\"\n    # Global configuration\n    ---\n    setting1 : varchar(100)\n    setting2 : int32\n    \"\"\"\n</code></pre> <p>Behavior:</p> Operation Result Insert Works without specifying a key Second insert Raises <code>DuplicateError</code> <code>fetch1()</code> Returns the single row <code>heading.primary_key</code> Returns <code>[]</code> (empty) <p>Use cases:</p> <ul> <li>Global configuration settings</li> <li>Pipeline parameters</li> <li>Summary statistics</li> <li>State tracking</li> </ul> <p>Implementation:</p> <p>Internally, singleton tables use a hidden <code>_singleton</code> attribute of type <code>bool</code> as the primary key. This attribute is:</p> <ul> <li>Automatically created and populated</li> <li>Excluded from <code>heading.attributes</code></li> <li>Excluded from <code>fetch()</code> results</li> <li>Excluded from join matching</li> </ul>"},{"location":"reference/specs/table-declaration/#3-attribute-definition","title":"3. Attribute Definition","text":""},{"location":"reference/specs/table-declaration/#31-syntax","title":"3.1 Syntax","text":"<pre><code>attribute_name [= default_value] : type [# comment]\n</code></pre>"},{"location":"reference/specs/table-declaration/#32-components","title":"3.2 Components","text":"Component Required Description <code>attribute_name</code> Yes Identifier for the column <code>default_value</code> No Default value (before colon) <code>type</code> Yes Data type specification <code>comment</code> No Documentation (after <code>#</code>)"},{"location":"reference/specs/table-declaration/#33-attribute-name-rules","title":"3.3 Attribute Name Rules","text":"<ul> <li>Pattern: <code>^[a-z_][a-z0-9_]*$</code></li> <li>Start: Lowercase letter or underscore</li> <li>Contains: Lowercase letters, digits, underscores</li> <li>Convention: snake_case</li> </ul>"},{"location":"reference/specs/table-declaration/#34-hidden-attributes","title":"3.4 Hidden Attributes","text":"<p>Attributes with names starting with underscore (<code>_</code>) are hidden:</p> <pre><code>definition = \"\"\"\nsession_id : int32\n---\nresult : float64\n_job_start_time : datetime(3)   # hidden\n_job_duration : float32         # hidden\n\"\"\"\n</code></pre> <p>Behavior:</p> Context Hidden Attributes <code>heading.attributes</code> Excluded <code>heading._attributes</code> Included Default table display Excluded <code>to_dicts()</code> / <code>to_pandas()</code> Excluded unless explicitly projected Join matching (namesakes) Excluded Dict restrictions Excluded (silently ignored) String restrictions Included (passed to SQL) <p>Accessing hidden attributes:</p> <pre><code># Visible attributes only (default)\nresults = MyTable.to_dicts()\n\n# Explicitly include hidden attributes\nresults = MyTable.proj('result', '_job_start_time').to_dicts()\n\n# Or with fetch1 for single row\nrow = (MyTable &amp; key).fetch1('result', '_job_start_time')\n\n# String restriction works with hidden attributes\nMyTable &amp; \"_job_start_time &gt; '2024-01-01'\"\n\n# Dict restriction IGNORES hidden attributes\nMyTable &amp; {'_job_start_time': some_date}  # no effect\n</code></pre> <p>Use cases:</p> <ul> <li>Job metadata (<code>_job_start_time</code>, <code>_job_duration</code>, <code>_job_version</code>)</li> <li>Internal tracking fields</li> <li>Attributes that should not participate in automatic joins</li> </ul>"},{"location":"reference/specs/table-declaration/#35-examples","title":"3.5 Examples","text":"<pre><code>definition = \"\"\"\n# Experimental session with subject and timing info\nsession_id : int32                          # auto-assigned\n---\nsubject_name : varchar(100)                 # subject identifier\ntrial_number = 1 : int32                    # default to 1\nscore = null : float32                      # nullable\ntimestamp = CURRENT_TIMESTAMP : datetime    # auto-timestamp\nnotes = '' : varchar(4000)                  # empty default\n\"\"\"\n</code></pre>"},{"location":"reference/specs/table-declaration/#4-type-system","title":"4. Type System","text":""},{"location":"reference/specs/table-declaration/#41-core-types","title":"4.1 Core Types","text":"<p>Scientist-friendly type names with guaranteed semantics:</p> Type SQL Mapping Size Description <code>int8</code> <code>tinyint</code> 1 byte 8-bit signed integer <code>int16</code> <code>tinyint unsigned</code> 1 byte 8-bit unsigned integer <code>int16</code> <code>smallint</code> 2 bytes 16-bit signed integer <code>int32</code> <code>smallint unsigned</code> 2 bytes 16-bit unsigned integer <code>int32</code> <code>int</code> 4 bytes 32-bit signed integer <code>int64</code> <code>int unsigned</code> 4 bytes 32-bit unsigned integer <code>int64</code> <code>bigint</code> 8 bytes 64-bit signed integer <code>int64</code> <code>bigint unsigned</code> 8 bytes 64-bit unsigned integer <code>float32</code> <code>float</code> 4 bytes 32-bit IEEE 754 float <code>float64</code> <code>double</code> 8 bytes 64-bit IEEE 754 float <code>bool</code> <code>tinyint</code> 1 byte Boolean (0 or 1) <code>uuid</code> <code>binary(16)</code> 16 bytes UUID stored as binary <code>bytes</code> <code>longblob</code> Variable Binary data (up to 4GB)"},{"location":"reference/specs/table-declaration/#42-string-types","title":"4.2 String Types","text":"Type SQL Mapping Description <code>char(N)</code> <code>char(N)</code> Fixed-length string <code>varchar(N)</code> <code>varchar(N)</code> Variable-length string (max N) <code>enum('a','b',...)</code> <code>enum(...)</code> Enumerated values"},{"location":"reference/specs/table-declaration/#43-temporal-types","title":"4.3 Temporal Types","text":"Type SQL Mapping Description <code>date</code> <code>date</code> Date (YYYY-MM-DD) <code>datetime</code> <code>datetime</code> Date and time <code>datetime(N)</code> <code>datetime(N)</code> With fractional seconds (0-6)"},{"location":"reference/specs/table-declaration/#44-other-types","title":"4.4 Other Types","text":"Type SQL Mapping Description <code>json</code> <code>json</code> JSON document <code>decimal(P,S)</code> <code>decimal(P,S)</code> Fixed-point decimal"},{"location":"reference/specs/table-declaration/#45-native-sql-types-passthrough","title":"4.5 Native SQL Types (Passthrough)","text":"<p>These SQL types are accepted but generate a warning recommending core types:</p> <ul> <li>Integer variants: <code>tinyint</code>, <code>smallint</code>, <code>mediumint</code>, <code>bigint</code>, <code>integer</code>, <code>serial</code></li> <li>Float variants: <code>float</code>, <code>double</code>, <code>real</code> (with size specifiers)</li> <li>Text variants: <code>tinytext</code>, <code>mediumtext</code>, <code>longtext</code></li> <li>Blob variants: <code>tinyblob</code>, <code>smallblob</code>, <code>mediumblob</code>, <code>longblob</code></li> <li>Temporal: <code>time</code>, <code>timestamp</code>, <code>year</code></li> <li>Numeric: <code>numeric(P,S)</code></li> </ul>"},{"location":"reference/specs/table-declaration/#46-codec-types","title":"4.6 Codec Types","text":"<p>Format: <code>&lt;codec_name&gt;</code> or <code>&lt;codec_name@store&gt;</code></p> Codec In-table dtype In-store dtype Purpose <code>&lt;blob&gt;</code> <code>bytes</code> <code>&lt;hash&gt;</code> Serialized Python objects <code>&lt;hash&gt;</code> N/A (in-store only) <code>json</code> Hash-addressed deduped storage <code>&lt;attach&gt;</code> <code>bytes</code> <code>&lt;hash&gt;</code> File attachments with filename <code>&lt;filepath&gt;</code> N/A (in-store only) <code>json</code> Reference to managed file <code>&lt;object&gt;</code> N/A (in-store only) <code>json</code> Object storage (Zarr, HDF5) <p>In-store storage syntax: - <code>&lt;blob@&gt;</code> - default store - <code>&lt;blob@store_name&gt;</code> - named store</p>"},{"location":"reference/specs/table-declaration/#47-type-reconstruction","title":"4.7 Type Reconstruction","text":"<p>Core types and codecs are stored in the SQL COMMENT field for reconstruction:</p> <pre><code>COMMENT ':float32:user comment here'\nCOMMENT ':&lt;blob@store&gt;:user comment'\n</code></pre>"},{"location":"reference/specs/table-declaration/#5-default-values","title":"5. Default Values","text":""},{"location":"reference/specs/table-declaration/#51-syntax","title":"5.1 Syntax","text":"<pre><code>attribute_name = default_value : type\n</code></pre>"},{"location":"reference/specs/table-declaration/#52-literal-types","title":"5.2 Literal Types","text":"Value Meaning SQL <code>null</code> Nullable attribute <code>DEFAULT NULL</code> <code>CURRENT_TIMESTAMP</code> Server timestamp <code>DEFAULT CURRENT_TIMESTAMP</code> <code>\"string\"</code> or <code>'string'</code> String literal <code>DEFAULT \"string\"</code> <code>123</code> Numeric literal <code>DEFAULT 123</code> <code>true</code>/<code>false</code> Boolean <code>DEFAULT 1</code>/<code>DEFAULT 0</code>"},{"location":"reference/specs/table-declaration/#53-constant-literals","title":"5.3 Constant Literals","text":"<p>These values are used without quotes in SQL: - <code>NULL</code> - <code>CURRENT_TIMESTAMP</code></p>"},{"location":"reference/specs/table-declaration/#54-nullable-attributes","title":"5.4 Nullable Attributes","text":"<pre><code>score = null : float32\n</code></pre> <ul> <li>The special default <code>null</code> (case-insensitive) makes the attribute nullable</li> <li>Nullable attributes can be omitted from INSERT</li> <li>Primary key attributes CANNOT be nullable</li> </ul>"},{"location":"reference/specs/table-declaration/#55-blobjson-default-restrictions","title":"5.5 Blob/JSON Default Restrictions","text":"<p>Blob and JSON attributes can only have <code>null</code> as default:</p> <pre><code># Valid\ndata = null : &lt;blob&gt;\n\n# Invalid - raises DataJointError\ndata = '' : &lt;blob&gt;\n</code></pre>"},{"location":"reference/specs/table-declaration/#6-foreign-key-references","title":"6. Foreign Key References","text":""},{"location":"reference/specs/table-declaration/#61-syntax","title":"6.1 Syntax","text":"<pre><code>-&gt; [options] ReferencedTable\n</code></pre>"},{"location":"reference/specs/table-declaration/#62-options","title":"6.2 Options","text":"Option Effect <code>nullable</code> All inherited attributes become nullable <code>unique</code> Creates UNIQUE INDEX on FK attributes <p>Options are comma-separated in brackets: <pre><code>-&gt; [nullable, unique] ParentTable\n</code></pre></p>"},{"location":"reference/specs/table-declaration/#63-attribute-inheritance","title":"6.3 Attribute Inheritance","text":"<p>Foreign keys automatically inherit all primary key attributes from the referenced table:</p> <pre><code># Parent\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : int32\n    ---\n    name : varchar(100)\n    \"\"\"\n\n# Child - inherits subject_id\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_id : int32\n    ---\n    session_date : date\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/table-declaration/#64-position-rules","title":"6.4 Position Rules","text":"Position Effect Before <code>---</code> FK attributes become part of primary key After <code>---</code> FK attributes are secondary"},{"location":"reference/specs/table-declaration/#65-nullable-foreign-keys","title":"6.5 Nullable Foreign Keys","text":"<pre><code>-&gt; [nullable] OptionalParent\n</code></pre> <ul> <li>Only allowed after <code>---</code> (secondary)</li> <li>Primary key FKs cannot be nullable</li> <li>Creates optional relationship</li> </ul>"},{"location":"reference/specs/table-declaration/#66-unique-foreign-keys","title":"6.6 Unique Foreign Keys","text":"<pre><code>-&gt; [unique] ParentTable\n</code></pre> <ul> <li>Creates UNIQUE INDEX on inherited attributes</li> <li>Enforces one-to-one relationship from child perspective</li> </ul>"},{"location":"reference/specs/table-declaration/#67-nullable-unique-foreign-keys","title":"6.7 Nullable Unique Foreign Keys","text":"<pre><code>-&gt; [nullable, unique] ParentTable\n</code></pre> <ul> <li>Combines nullable and unique constraints</li> <li>Multiple rows can have NULL values (SQL standard: NULLs are not considered equal in UNIQUE constraints)</li> <li>At most one row per non-NULL parent reference</li> <li>Use case: optional one-to-one relationships where the child may not reference any parent</li> </ul>"},{"location":"reference/specs/table-declaration/#68-projections-in-foreign-keys","title":"6.8 Projections in Foreign Keys","text":"<pre><code>-&gt; Parent.proj(alias='original_name')\n</code></pre> <ul> <li>Reference same table multiple times with different attribute names</li> <li>Useful for self-referential or multi-reference patterns</li> </ul>"},{"location":"reference/specs/table-declaration/#68-referential-actions","title":"6.8 Referential Actions","text":"<p>All foreign keys use: - <code>ON UPDATE CASCADE</code> - Parent key changes propagate - <code>ON DELETE RESTRICT</code> - Cannot delete parent with children</p>"},{"location":"reference/specs/table-declaration/#69-lineage-tracking","title":"6.9 Lineage Tracking","text":"<p>Foreign key relationships are recorded in the <code>~lineage</code> table:</p> <pre><code>{\n    'child_attr': ('parent_schema.parent_table', 'parent_attr')\n}\n</code></pre> <p>Used for semantic attribute matching in queries.</p>"},{"location":"reference/specs/table-declaration/#7-index-declarations","title":"7. Index Declarations","text":""},{"location":"reference/specs/table-declaration/#71-syntax","title":"7.1 Syntax","text":"<pre><code>index(attr1, attr2, ...)\nunique index(attr1, attr2, ...)\n</code></pre>"},{"location":"reference/specs/table-declaration/#72-examples","title":"7.2 Examples","text":"<pre><code>definition = \"\"\"\n# User contact information\nuser_id : int32\n---\nfirst_name : varchar(50)\nlast_name : varchar(50)\nemail : varchar(100)\nindex(last_name, first_name)\nunique index(email)\n\"\"\"\n</code></pre>"},{"location":"reference/specs/table-declaration/#73-computed-expressions","title":"7.3 Computed Expressions","text":"<p>Indexes can include SQL expressions:</p> <pre><code>index(last_name, (YEAR(birth_date)))\n</code></pre>"},{"location":"reference/specs/table-declaration/#74-limitations","title":"7.4 Limitations","text":"<ul> <li>Cannot be altered after table creation (via <code>table.alter()</code>)</li> <li>Must reference existing attributes</li> </ul>"},{"location":"reference/specs/table-declaration/#8-part-tables","title":"8. Part Tables","text":""},{"location":"reference/specs/table-declaration/#81-declaration","title":"8.1 Declaration","text":"<pre><code>@schema\nclass Master(dj.Manual):\n    definition = \"\"\"\n    master_id : int32\n    \"\"\"\n\n    class Detail(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        detail_id : int32\n        ---\n        value : float32\n        \"\"\"\n</code></pre>"},{"location":"reference/specs/table-declaration/#82-naming","title":"8.2 Naming","text":"<ul> <li>SQL name: <code>master_table__part_name</code></li> <li>Example: <code>experiment__trial</code></li> </ul>"},{"location":"reference/specs/table-declaration/#83-master-reference","title":"8.3 Master Reference","text":"<p>Within Part definition, use: - <code>-&gt; master</code> (lowercase keyword) - <code>-&gt; MasterClassName</code> (class name)</p>"},{"location":"reference/specs/table-declaration/#84-constraints","title":"8.4 Constraints","text":"<ul> <li>Parts must reference their master</li> <li>Cannot delete Part records directly (use master)</li> <li>Cannot drop Part table directly (use master)</li> <li>Part inherits master's primary key</li> </ul>"},{"location":"reference/specs/table-declaration/#9-auto-populated-tables","title":"9. Auto-Populated Tables","text":""},{"location":"reference/specs/table-declaration/#91-classes","title":"9.1 Classes","text":"<ul> <li><code>dj.Imported</code> - Data from external sources</li> <li><code>dj.Computed</code> - Derived from other DataJoint tables</li> </ul>"},{"location":"reference/specs/table-declaration/#92-primary-key-constraint","title":"9.2 Primary Key Constraint","text":"<p>All primary key attributes must come from foreign key references.</p> <p>Valid: <pre><code>class Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    -&gt; Parameter\n    ---\n    result : float64\n    \"\"\"\n</code></pre></p> <p>Invalid (by default): <pre><code>class Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    analysis_id : int32    # ERROR: non-FK primary key\n    ---\n    result : float64\n    \"\"\"\n</code></pre></p> <p>Override: <pre><code>dj.config['jobs.allow_new_pk_fields_in_computed_tables'] = True\n</code></pre></p>"},{"location":"reference/specs/table-declaration/#93-job-metadata","title":"9.3 Job Metadata","text":"<p>When <code>config['jobs.add_job_metadata'] = True</code>, auto-populated tables receive:</p> Column Type Description <code>_job_start_time</code> <code>datetime(3)</code> Job start timestamp <code>_job_duration</code> <code>float64</code> Duration in seconds <code>_job_version</code> <code>varchar(64)</code> Code version"},{"location":"reference/specs/table-declaration/#10-validation","title":"10. Validation","text":""},{"location":"reference/specs/table-declaration/#101-parse-time-checks","title":"10.1 Parse-Time Checks","text":"Check Error Unknown type <code>DataJointError: Unsupported attribute type</code> Invalid attribute name <code>DataJointError: Declaration error</code> Comment starts with <code>:</code> <code>DataJointError: comment must not start with colon</code> Non-null blob default <code>DataJointError: default value for blob can only be NULL</code>"},{"location":"reference/specs/table-declaration/#102-declaration-time-checks","title":"10.2 Declaration-Time Checks","text":"Check Error Table name &gt; 64 chars <code>DataJointError: Table name exceeds max length</code> No primary key <code>DataJointError: Table must have a primary key</code> Nullable primary key attr <code>DataJointError: Primary key attributes cannot be nullable</code> Invalid CamelCase <code>DataJointError: Invalid table name</code> FK resolution failure <code>DataJointError: Foreign key reference could not be resolved</code>"},{"location":"reference/specs/table-declaration/#103-insert-time-validation","title":"10.3 Insert-Time Validation","text":"<p>The <code>table.validate()</code> method checks: - Required fields present - NULL constraints satisfied - Primary key completeness - Codec validation (if defined) - UUID format - JSON serializability</p>"},{"location":"reference/specs/table-declaration/#11-sql-generation","title":"11. SQL Generation","text":""},{"location":"reference/specs/table-declaration/#111-create-table-template","title":"11.1 CREATE TABLE Template","text":"<pre><code>CREATE TABLE `schema`.`table_name` (\n    `attr1` TYPE1 NOT NULL COMMENT \"...\",\n    `attr2` TYPE2 DEFAULT NULL COMMENT \"...\",\n    PRIMARY KEY (`pk1`, `pk2`),\n    FOREIGN KEY (`fk_attr`) REFERENCES `parent` (`pk`)\n        ON UPDATE CASCADE ON DELETE RESTRICT,\n    INDEX (`idx_attr`),\n    UNIQUE INDEX (`uniq_attr`)\n) ENGINE=InnoDB COMMENT=\"table comment\"\n</code></pre>"},{"location":"reference/specs/table-declaration/#112-type-comment-encoding","title":"11.2 Type Comment Encoding","text":"<p>Core types and codecs are preserved in comments:</p> <pre><code>`value` float NOT NULL COMMENT \":float32:measurement value\"\n`data` longblob DEFAULT NULL COMMENT \":&lt;blob&gt;:serialized data\"\n`archive` json DEFAULT NULL COMMENT \":&lt;blob@cold&gt;:in-store data\"\n</code></pre>"},{"location":"reference/specs/table-declaration/#12-implementation-files","title":"12. Implementation Files","text":"File Purpose <code>declare.py</code> Definition parsing, SQL generation <code>heading.py</code> Attribute metadata, type reconstruction <code>table.py</code> Base Table class, declaration interface <code>user_tables.py</code> Tier classes (Manual, Computed, etc.) <code>schemas.py</code> Schema binding, table decoration <code>codecs.py</code> Codec registry and resolution <code>lineage.py</code> Attribute lineage tracking"},{"location":"reference/specs/table-declaration/#13-future-considerations","title":"13. Future Considerations","text":"<p>Potential improvements identified for the declaration system:</p> <ol> <li>Better error messages with suggestions and context</li> <li>Import-time validation via <code>__init_subclass__</code></li> <li>Parser alternatives (regex-based for simpler grammar)</li> <li>SQL dialect abstraction for multi-database support</li> <li>Extended constraints (CHECK, custom validation)</li> <li>Migration support for schema evolution</li> <li>Definition caching for performance</li> <li>IDE tooling support via structured intermediate representation</li> </ol>"},{"location":"reference/specs/type-system/","title":"Type System Specification","text":""},{"location":"reference/specs/type-system/#overview","title":"Overview","text":"<p>New in 2.1</p> <p>PostgreSQL type mappings were added in DataJoint 2.1. See Database Backends for backend configuration.</p> <p>This document defines a three-layer type architecture:</p> <ol> <li>Native database types - Backend-specific (<code>FLOAT</code>, <code>TINYINT UNSIGNED</code>, <code>LONGBLOB</code>). Discouraged for direct use.</li> <li>Core DataJoint types - Standardized across backends, scientist-friendly (<code>int32</code>, <code>float32</code>, <code>bool</code>, <code>json</code>).</li> <li>Codec Types - Programmatic types with <code>encode()</code>/<code>decode()</code> semantics. Composable.</li> </ol> Layer Description Examples 3. Codec Types Programmatic types with <code>encode()</code>/<code>decode()</code> semantics <code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>, <code>&lt;object@&gt;</code>, <code>&lt;hash@&gt;</code>, <code>&lt;filepath@&gt;</code>, user-defined 2. Core DataJoint Standardized, scientist-friendly types (preferred) <code>int32</code>, <code>float64</code>, <code>varchar(n)</code>, <code>bool</code>, <code>datetime</code>, <code>json</code>, <code>bytes</code> 1. Native Database Backend-specific types (discouraged) <code>INT</code>, <code>FLOAT</code>, <code>TINYINT UNSIGNED</code>, <code>LONGBLOB</code> <p>Codec types resolve through core types to native types: <code>&lt;blob&gt;</code> \u2192 <code>bytes</code> \u2192 <code>LONGBLOB</code>.</p> <p>Syntax distinction: - Core types: <code>int32</code>, <code>float64</code>, <code>varchar(255)</code> - no brackets - Codec types: <code>&lt;blob&gt;</code>, <code>&lt;object@store&gt;</code>, <code>&lt;filepath@main&gt;</code> - angle brackets - The <code>@</code> character indicates store (object storage vs in-table)</p>"},{"location":"reference/specs/type-system/#core-datajoint-types-layer-2","title":"Core DataJoint Types (Layer 2)","text":"<p>Core types provide a standardized, scientist-friendly interface that works identically across MySQL and PostgreSQL backends. Users should prefer these over native database types.</p> <p>All core types are recorded in field comments using <code>:type:</code> syntax for reconstruction.</p>"},{"location":"reference/specs/type-system/#numeric-types","title":"Numeric Types","text":"Core Type Description MySQL PostgreSQL <code>int8</code> 8-bit signed <code>TINYINT</code> <code>SMALLINT</code> <code>int16</code> 16-bit signed <code>SMALLINT</code> <code>SMALLINT</code> <code>int32</code> 32-bit signed <code>INT</code> <code>INTEGER</code> <code>int64</code> 64-bit signed <code>BIGINT</code> <code>BIGINT</code> <code>float32</code> 32-bit float <code>FLOAT</code> <code>REAL</code> <code>float64</code> 64-bit float <code>DOUBLE</code> <code>DOUBLE PRECISION</code> <code>decimal(n,f)</code> Fixed-point <code>DECIMAL(n,f)</code> <code>NUMERIC(n,f)</code>"},{"location":"reference/specs/type-system/#string-types","title":"String Types","text":"Core Type Description MySQL PostgreSQL <code>char(n)</code> Fixed-length <code>CHAR(n)</code> <code>CHAR(n)</code> <code>varchar(n)</code> Variable-length <code>VARCHAR(n)</code> <code>VARCHAR(n)</code> <p>Encoding: All strings use UTF-8 (<code>utf8mb4</code> in MySQL, <code>UTF8</code> in PostgreSQL). See Encoding and Collation Policy for details.</p>"},{"location":"reference/specs/type-system/#boolean","title":"Boolean","text":"Core Type Description MySQL PostgreSQL <code>bool</code> True/False <code>TINYINT</code> <code>BOOLEAN</code>"},{"location":"reference/specs/type-system/#datetime-types","title":"Date/Time Types","text":"Core Type Description MySQL PostgreSQL <code>date</code> Date only <code>DATE</code> <code>DATE</code> <code>datetime</code> Date and time <code>DATETIME</code> <code>TIMESTAMP</code> <p>Timezone policy: All <code>datetime</code> values should be stored as UTC. Timezone conversion is a presentation concern handled by the application layer, not the database. This ensures: - Reproducible computations regardless of server or client timezone settings - Simple arithmetic on temporal values (no DST ambiguity) - Portable data across systems and regions</p> <p>Use <code>CURRENT_TIMESTAMP</code> for auto-populated creation times: <pre><code>created_at : datetime = CURRENT_TIMESTAMP\n</code></pre></p>"},{"location":"reference/specs/type-system/#binary-types","title":"Binary Types","text":"<p>The core <code>bytes</code> type stores raw bytes without any serialization. Use the <code>&lt;blob&gt;</code> codec for serialized Python objects.</p> Core Type Description MySQL PostgreSQL <code>bytes</code> Raw bytes <code>LONGBLOB</code> <code>BYTEA</code>"},{"location":"reference/specs/type-system/#other-types","title":"Other Types","text":"Core Type Description MySQL PostgreSQL <code>json</code> JSON document <code>JSON</code> <code>JSONB</code> <code>uuid</code> UUID <code>BINARY(16)</code> <code>UUID</code> <code>enum(...)</code> Enumeration <code>ENUM(...)</code> <code>CREATE TYPE ... AS ENUM</code>"},{"location":"reference/specs/type-system/#native-passthrough-types","title":"Native Passthrough Types","text":"<p>Users may use native database types directly (e.g., <code>int</code>, <code>float</code>, <code>mediumint</code>, <code>tinyblob</code>), but these are discouraged and will generate a warning. Native types lack explicit size information, are not recorded in field comments, and may have portability issues across database backends.</p> <p>Prefer core DataJoint types over native types:</p> Native (discouraged) Core DataJoint (preferred) <code>int</code> <code>int32</code> <code>float</code> <code>float32</code> or <code>float64</code> <code>double</code> <code>float64</code> <code>tinyint</code> <code>int8</code> <code>smallint</code> <code>int16</code> <code>bigint</code> <code>int64</code> <code>tinyint unsigned</code> <code>int16</code> (use larger signed type) <code>smallint unsigned</code> <code>int32</code> (use larger signed type) <code>int unsigned</code> <code>int64</code> (use larger signed type) <code>bigint unsigned</code> <code>decimal(20,0)</code> (use decimal for full range)"},{"location":"reference/specs/type-system/#type-modifiers-policy","title":"Type Modifiers Policy","text":"<p>DataJoint table definitions have their own syntax for constraints and metadata. SQL type modifiers are not allowed in type specifications because they conflict with DataJoint's declarative syntax:</p> Modifier Status DataJoint Alternative <code>NOT NULL</code> / <code>NULL</code> \u274c Not allowed Use <code>= NULL</code> for nullable; omit default for required <code>DEFAULT value</code> \u274c Not allowed Use <code>= value</code> syntax before the type <code>PRIMARY KEY</code> \u274c Not allowed Position above <code>---</code> line <code>UNIQUE</code> \u274c Not allowed Use DataJoint index syntax <code>COMMENT 'text'</code> \u274c Not allowed Use <code># comment</code> syntax <code>CHARACTER SET</code> \u274c Not allowed Database-level configuration <code>COLLATE</code> \u274c Not allowed Database-level configuration <code>AUTO_INCREMENT</code> \u26a0\ufe0f Discouraged Allowed with native types only, generates warning <code>UNSIGNED</code> \u26a0\ufe0f Discouraged MySQL-specific, not portable (use larger signed type) <p>Nullability and defaults: DataJoint handles nullability through the default value syntax. An attribute is nullable if and only if its default is <code>NULL</code>:</p> <pre><code># Required (NOT NULL, no default)\nname : varchar(100)\n\n# Nullable (default is NULL)\nnickname = NULL : varchar(100)\n\n# Required with default value\nstatus = \"active\" : varchar(20)\n</code></pre> <p>Auto-increment policy: DataJoint discourages <code>AUTO_INCREMENT</code> / <code>SERIAL</code> because: - Breaks reproducibility (IDs depend on insertion order) - Makes pipelines non-deterministic - Complicates data migration and replication - Primary keys should be meaningful, not arbitrary</p> <p>If required, use native types: <code>int auto_increment</code> or <code>serial</code> (with warning).</p>"},{"location":"reference/specs/type-system/#encoding-and-collation-policy","title":"Encoding and Collation Policy","text":"<p>Character encoding and collation are database-level configuration, not part of type definitions. This ensures consistent behavior across all tables and simplifies portability.</p> <p>Configuration (in <code>dj.config</code> or <code>datajoint.json</code>): <pre><code>{\n    \"database.charset\": \"utf8mb4\",\n    \"database.collation\": \"utf8mb4_bin\"\n}\n</code></pre></p> <p>Defaults:</p> Setting MySQL PostgreSQL Charset <code>utf8mb4</code> <code>UTF8</code> Collation <code>utf8mb4_bin</code> <code>C</code> <p>Policy: - UTF-8 required: DataJoint validates charset is UTF-8 compatible at connection time - Case-sensitive by default: Binary collation (<code>utf8mb4_bin</code> / <code>C</code>) ensures predictable comparisons - No per-column overrides: <code>CHARACTER SET</code> and <code>COLLATE</code> are rejected in type definitions - Like timezone: Encoding is infrastructure configuration, not part of the data model</p>"},{"location":"reference/specs/type-system/#codec-types-layer-3","title":"Codec Types (Layer 3)","text":"<p>Codec types provide <code>encode()</code>/<code>decode()</code> semantics on top of core types. They are composable and can be built-in or user-defined.</p>"},{"location":"reference/specs/type-system/#storage-mode-convention","title":"Storage Mode: <code>@</code> Convention","text":"<p>The <code>@</code> character in codec syntax indicates object store (vs in-table):</p> <ul> <li>No <code>@</code>: In-table storage (database column) - e.g., <code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code></li> <li><code>@</code> present: Object store - e.g., <code>&lt;blob@&gt;</code>, <code>&lt;attach@store&gt;</code></li> <li><code>@</code> alone: Use default store - e.g., <code>&lt;blob@&gt;</code></li> <li><code>@name</code>: Use named store - e.g., <code>&lt;blob@cold&gt;</code></li> </ul> <p>Some codecs support both modes (<code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>), others are store-only (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>, <code>&lt;hash@&gt;</code>, <code>&lt;filepath@&gt;</code>).</p>"},{"location":"reference/specs/type-system/#codec-base-class","title":"Codec Base Class","text":"<p>Codecs inherit from <code>dj.Codec</code> and auto-register when their class is defined. See the Codec API Specification for complete details on creating custom codecs.</p> <pre><code>class GraphCodec(dj.Codec):\n    \"\"\"Auto-registered as &lt;graph&gt;.\"\"\"\n    name = \"graph\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n\n    def encode(self, graph, *, key=None, store_name=None):\n        return {'nodes': list(graph.nodes()), 'edges': list(graph.edges())}\n\n    def decode(self, stored, *, key=None):\n        import networkx as nx\n        G = nx.Graph()\n        G.add_nodes_from(stored['nodes'])\n        G.add_edges_from(stored['edges'])\n        return G\n</code></pre>"},{"location":"reference/specs/type-system/#codec-resolution-and-chaining","title":"Codec Resolution and Chaining","text":"<p>Codecs resolve to core types through chaining. The <code>get_dtype(is_store)</code> method returns the appropriate dtype based on storage mode:</p> Codec <code>is_store</code> Resolution Chain SQL Type <code>&lt;blob&gt;</code> <code>False</code> <code>\"bytes\"</code> <code>LONGBLOB</code>/<code>BYTEA</code> <code>&lt;blob@&gt;</code> <code>True</code> <code>\"&lt;hash&gt;\"</code> \u2192 <code>\"json\"</code> <code>JSON</code>/<code>JSONB</code> <code>&lt;blob@cold&gt;</code> <code>True</code> <code>\"&lt;hash&gt;\"</code> \u2192 <code>\"json\"</code> (store=cold) <code>JSON</code>/<code>JSONB</code> <code>&lt;attach&gt;</code> <code>False</code> <code>\"bytes\"</code> <code>LONGBLOB</code>/<code>BYTEA</code> <code>&lt;attach@&gt;</code> <code>True</code> <code>\"&lt;hash&gt;\"</code> \u2192 <code>\"json\"</code> <code>JSON</code>/<code>JSONB</code> <code>&lt;object@&gt;</code> <code>True</code> <code>\"json\"</code> <code>JSON</code>/<code>JSONB</code> <code>&lt;npy@&gt;</code> <code>True</code> <code>\"json\"</code> <code>JSON</code>/<code>JSONB</code> <code>&lt;object&gt;</code> <code>False</code> ERROR (store only) \u2014 <code>&lt;npy&gt;</code> <code>False</code> ERROR (store only) \u2014 <code>&lt;hash@&gt;</code> <code>True</code> <code>\"json\"</code> <code>JSON</code>/<code>JSONB</code> <code>&lt;filepath@&gt;</code> <code>True</code> <code>\"json\"</code> <code>JSON</code>/<code>JSONB</code>"},{"location":"reference/specs/type-system/#object-objectstore-schema-addressed-storage","title":"<code>&lt;object@&gt;</code> / <code>&lt;object@store&gt;</code> - Schema-Addressed Storage","text":"<p>Built-in codec. Store only.</p> <p>Schema-addressed OAS storage for complex, multi-part objects (files, folders, Zarr arrays, HDF5):</p> <ul> <li>Schema-addressed: Path mirrors database structure: <code>{schema}/{table}/{pk}/{attribute}/</code></li> <li>Complex objects: Can store directory structures with multiple files (e.g., Zarr arrays)</li> <li>One-to-one relationship with table row</li> <li>Deleted when row is deleted</li> <li>Returns <code>ObjectRef</code> for lazy access</li> <li>Supports direct writes (Zarr, HDF5) via fsspec</li> <li>dtype: <code>json</code> (stores path, store name, metadata)</li> </ul> <pre><code>class Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Recording\n    ---\n    results : &lt;object@&gt;         # default store\n    archive : &lt;object@cold&gt;     # specific store\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/type-system/#implementation","title":"Implementation","text":"<pre><code>class ObjectCodec(SchemaCodec):\n    \"\"\"Schema-addressed OAS storage. Store only.\"\"\"\n    name = \"object\"\n\n    # get_dtype inherited from SchemaCodec\n\n    def encode(self, value, *, key=None, store_name=None) -&gt; dict:\n        schema, table, field, pk = self._extract_context(key)\n        path, _ = self._build_path(schema, table, field, pk)\n        backend = self._get_backend(store_name)\n        backend.put(path, value)\n        return {\"path\": path, \"store\": store_name, ...}\n\n    def decode(self, stored: dict, *, key=None) -&gt; ObjectRef:\n        backend = self._get_backend(stored[\"store\"])\n        return ObjectRef.from_json(stored, backend=backend)\n</code></pre>"},{"location":"reference/specs/type-system/#hash-hashstore-hash-addressed-storage","title":"<code>&lt;hash@&gt;</code> / <code>&lt;hash@store&gt;</code> - Hash-Addressed Storage","text":"<p>Built-in codec. Store only.</p> <p>Hash-addressed storage with deduplication for individual, atomic objects:</p> <ul> <li>Hash-addressed: Path derived from content hash: <code>_hash/{hash[:2]}/{hash[2:4]}/{hash}</code></li> <li>Individual/atomic objects only: Stores single files or serialized blobs (not directory structures)</li> <li>Cannot handle complex multi-part objects like Zarr arrays\u2014use <code>&lt;object@&gt;</code> for those</li> <li>Per-project scope: content is shared across all schemas in a project (not per-schema)</li> <li>Many-to-one: multiple rows (even across schemas) can reference same content</li> <li>Reference counted for garbage collection</li> <li>Deduplication: identical content stored once across the entire project</li> <li>dtype: <code>json</code> (stores hash, store name, size, metadata)</li> </ul> <pre><code>store_root/\n\u251c\u2500\u2500 {schema}/{table}/{pk}/     # schema-addressed storage\n\u2502   \u2514\u2500\u2500 {attribute}/\n\u2502\n\u2514\u2500\u2500 _hash/                     # hash-addressed storage\n    \u2514\u2500\u2500 {hash[:2]}/{hash[2:4]}/{hash}\n</code></pre>"},{"location":"reference/specs/type-system/#implementation_1","title":"Implementation","text":"<pre><code>class HashCodec(dj.Codec):\n    \"\"\"Hash-addressed storage. Store only.\"\"\"\n    name = \"hash\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        if not is_store:\n            raise DataJointError(\"&lt;hash&gt; requires @ (store only)\")\n        return \"json\"\n\n    def encode(self, data: bytes, *, key=None, store_name=None) -&gt; dict:\n        \"\"\"Store content, return metadata as JSON.\"\"\"\n        hash_id = hashlib.md5(data).hexdigest()  # 32-char hex\n        store = get_store(store_name or dj.config['stores']['default'])\n        path = f\"_hash/{hash_id[:2]}/{hash_id[2:4]}/{hash_id}\"\n\n        if not store.exists(path):\n            store.put(path, data)\n\n        # Metadata stored in JSON column (no separate registry)\n        return {\"hash\": hash_id, \"store\": store_name, \"size\": len(data)}\n\n    def decode(self, stored: dict, *, key=None) -&gt; bytes:\n        \"\"\"Retrieve content by hash.\"\"\"\n        store = get_store(stored[\"store\"])\n        path = f\"_hash/{stored['hash'][:2]}/{stored['hash'][2:4]}/{stored['hash']}\"\n        return store.get(path)\n</code></pre>"},{"location":"reference/specs/type-system/#database-column","title":"Database Column","text":"<p>The <code>&lt;hash@&gt;</code> type stores JSON metadata:</p> <pre><code>-- content column (MySQL)\nfeatures JSON NOT NULL\n-- Contains: {\"hash\": \"abc123...\", \"store\": \"main\", \"size\": 12345}\n\n-- content column (PostgreSQL)\nfeatures JSONB NOT NULL\n</code></pre>"},{"location":"reference/specs/type-system/#filepathstore-portable-external-reference","title":"<code>&lt;filepath@store&gt;</code> - Portable External Reference","text":"<p>Built-in codec. In-store only (store required).</p> <p>Relative path references within configured stores:</p> <ul> <li>Relative paths: paths within a configured store (portable across environments)</li> <li>Store-aware: resolves paths against configured store backend</li> <li>Returns <code>ObjectRef</code> for lazy access via fsspec</li> <li>Stores optional checksum for verification</li> <li>dtype: <code>json</code> (stores path, store name, checksum, metadata)</li> </ul> <p>Key benefit: Portability. The path is relative to the store, so pipelines can be moved between environments (dev \u2192 prod, cloud \u2192 local) by changing store configuration without updating data.</p> <pre><code>class RawData(dj.Manual):\n    definition = \"\"\"\n    session_id : int32\n    ---\n    recording : &lt;filepath@main&gt;      # relative path within 'main' store\n    \"\"\"\n\n# Insert - user provides relative path within the store\ntable.insert1({\n    'session_id': 1,\n    'recording': 'experiment_001/data.nwb'  # relative to main store root\n})\n\n# Fetch - returns ObjectRef (lazy)\nrow = (table &amp; 'session_id=1').fetch1()\nref = row['recording']           # ObjectRef\nref.download('/local/path')      # explicit download\nref.open()                       # fsspec streaming access\n</code></pre>"},{"location":"reference/specs/type-system/#when-to-use-filepathstore-vs-varchar","title":"When to Use <code>&lt;filepath@store&gt;</code> vs <code>varchar</code>","text":"Use Case Recommended Type Need ObjectRef/lazy access <code>&lt;filepath@store&gt;</code> Need portability (relative paths) <code>&lt;filepath@store&gt;</code> Want checksum verification <code>&lt;filepath@store&gt;</code> Just storing a URL string <code>varchar</code> External URLs you don't control <code>varchar</code> <p>For arbitrary URLs (S3, HTTP, etc.) where you don't need ObjectRef semantics, just use <code>varchar</code>. A string is simpler and more transparent.</p>"},{"location":"reference/specs/type-system/#implementation_2","title":"Implementation","text":"<pre><code>class FilepathCodec(dj.Codec):\n    \"\"\"Store-relative file references. External only.\"\"\"\n    name = \"filepath\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        if not is_store:\n            raise DataJointError(\"&lt;filepath&gt; requires @store\")\n        return \"json\"\n\n    def encode(self, relative_path: str, *, key=None, store_name=None) -&gt; dict:\n        \"\"\"Register reference to file in store.\"\"\"\n        store = get_store(store_name)  # store_name required for filepath\n        return {'path': relative_path, 'store': store_name}\n\n    def decode(self, stored: dict, *, key=None) -&gt; ObjectRef:\n        \"\"\"Return ObjectRef for lazy access.\"\"\"\n        return ObjectRef(store=get_store(stored['store']), path=stored['path'])\n</code></pre>"},{"location":"reference/specs/type-system/#database-column_1","title":"Database Column","text":"<pre><code>-- filepath column (MySQL)\nrecording JSON NOT NULL\n-- Contains: {\"path\": \"experiment_001/data.nwb\", \"store\": \"main\", \"checksum\": \"...\", \"size\": ...}\n\n-- filepath column (PostgreSQL)\nrecording JSONB NOT NULL\n</code></pre>"},{"location":"reference/specs/type-system/#key-differences-from-legacy-filepathstore-now-filepathstore","title":"Key Differences from Legacy <code>filepath@store</code> (now <code>&lt;filepath@store&gt;</code>)","text":"Feature Legacy New Access Copy to local stage ObjectRef (lazy) Copying Automatic Explicit via <code>ref.download()</code> Streaming No Yes via <code>ref.open()</code> Paths Relative Relative (unchanged) Store param Required (<code>@store</code>) Required (<code>@store</code>)"},{"location":"reference/specs/type-system/#database-types","title":"Database Types","text":""},{"location":"reference/specs/type-system/#json-cross-database-json-type","title":"<code>json</code> - Cross-Database JSON Type","text":"<p>JSON storage compatible across MySQL and PostgreSQL:</p> <pre><code>-- MySQL\ncolumn_name JSON NOT NULL\n\n-- PostgreSQL (uses JSONB for better indexing)\ncolumn_name JSONB NOT NULL\n</code></pre> <p>The <code>json</code> database type: - Used as dtype by built-in codecs (<code>&lt;object@&gt;</code>, <code>&lt;hash@&gt;</code>, <code>&lt;filepath@store&gt;</code>) - Stores arbitrary JSON-serializable data - Automatically uses appropriate type for database backend - Supports JSON path queries where available</p>"},{"location":"reference/specs/type-system/#built-in-codecs","title":"Built-in Codecs","text":""},{"location":"reference/specs/type-system/#blob-blob-serialized-python-objects","title":"<code>&lt;blob&gt;</code> / <code>&lt;blob@&gt;</code> - Serialized Python Objects","text":"<p>Supports both in-table and in-store storage.</p> <p>Serializes Python objects using DataJoint's custom binary serialization format. The format uses protocol headers and type-specific encoding to serialize complex Python objects efficiently.</p> <p>Serialization format:</p> <ul> <li>Protocol headers:<ul> <li><code>mYm</code> \u2014 Original MATLAB-compatible format for numeric arrays, structs, cells (see mYm on MATLAB FileExchange and mym on GitHub)</li> <li><code>dj0</code> \u2014 Extended format supporting Python-specific types (UUID, Decimal, datetime, etc.)</li> </ul> </li> <li>Compression: Automatic zlib compression for data &gt; 1KB</li> <li>Type codes: Each Python type has a specific serialization code:<ul> <li><code>'A'</code> \u2014 NumPy arrays (numeric)</li> <li><code>'F'</code> \u2014 NumPy recarrays (structured arrays with fields)</li> <li><code>'\\x01'</code> \u2014 Tuples</li> <li><code>'\\x02'</code> \u2014 Lists</li> <li><code>'\\x03'</code> \u2014 Sets</li> <li><code>'\\x04'</code> \u2014 Dicts</li> <li><code>'\\x05'</code> \u2014 Strings (UTF-8)</li> <li><code>'\\x06'</code> \u2014 Bytes</li> <li><code>'\\x0a'</code> \u2014 Unbounded integers</li> <li><code>'\\x0b'</code> \u2014 Booleans</li> <li><code>'\\x0c'</code> \u2014 Complex numbers</li> <li><code>'\\x0d'</code> \u2014 Floats</li> <li><code>'d'</code> \u2014 Decimal</li> <li><code>'t'</code> \u2014 Datetime/date/time</li> <li><code>'u'</code> \u2014 UUID</li> <li><code>'S'</code> \u2014 MATLAB structs</li> <li><code>'C'</code> \u2014 MATLAB cell arrays</li> </ul> </li> </ul> <p>Version detection: The protocol header (<code>mYm\\0</code> or <code>dj0\\0</code>) is embedded at the start of the blob, enabling automatic format detection and backward compatibility.</p> <p>Storage modes:</p> <ul> <li><code>&lt;blob&gt;</code>: Stored in database (<code>bytes</code> \u2192 <code>LONGBLOB</code>/<code>BYTEA</code>)</li> <li><code>&lt;blob@&gt;</code>: Stored externally via <code>&lt;hash@&gt;</code> with MD5 deduplication</li> <li><code>&lt;blob@store&gt;</code>: Stored in specific named store</li> </ul> <pre><code>class BlobCodec(dj.Codec):\n    \"\"\"Serialized Python objects. Supports internal and external.\"\"\"\n    name = \"blob\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;hash&gt;\" if is_store else \"bytes\"\n\n    def encode(self, value, *, key=None, store_name=None) -&gt; bytes:\n        from . import blob\n        return blob.pack(value, compress=True)\n\n    def decode(self, stored, *, key=None) -&gt; Any:\n        from . import blob\n        return blob.unpack(stored)\n</code></pre> <p>Usage: <pre><code>class ProcessedData(dj.Computed):\n    definition = \"\"\"\n    -&gt; RawData\n    ---\n    small_result : &lt;blob&gt;          # in-table (in database)\n    large_result : &lt;blob@&gt;         # in-store (default store)\n    archive_result : &lt;blob@cold&gt;   # in-store (specific store)\n    \"\"\"\n</code></pre></p>"},{"location":"reference/specs/type-system/#attach-attach-file-attachments","title":"<code>&lt;attach&gt;</code> / <code>&lt;attach@&gt;</code> - File Attachments","text":"<p>Supports both in-table and in-store storage.</p> <p>Stores files with filename preserved. On fetch, extracts to configured download path.</p> <ul> <li><code>&lt;attach&gt;</code>: Stored in database (<code>bytes</code> \u2192 <code>LONGBLOB</code>/<code>BYTEA</code>)</li> <li><code>&lt;attach@&gt;</code>: Stored in object store via <code>&lt;hash@&gt;</code> with deduplication</li> <li><code>&lt;attach@store&gt;</code>: Stored in specific named store</li> </ul> <pre><code>class AttachCodec(dj.Codec):\n    \"\"\"File attachment with filename. Supports in-table and in-store.\"\"\"\n    name = \"attach\"\n\n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;hash&gt;\" if is_store else \"bytes\"\n\n    def encode(self, filepath, *, key=None, store_name=None) -&gt; bytes:\n        path = Path(filepath)\n        return path.name.encode() + b\"\\0\" + path.read_bytes()\n\n    def decode(self, stored, *, key=None) -&gt; str:\n        filename, contents = stored.split(b\"\\0\", 1)\n        filename = filename.decode()\n        download_path = Path(dj.config['download_path']) / filename\n        download_path.write_bytes(contents)\n        return str(download_path)\n</code></pre> <p>Usage: <pre><code>class Attachments(dj.Manual):\n    definition = \"\"\"\n    attachment_id : int32\n    ---\n    config : &lt;attach&gt;           # in-table (small file in DB)\n    data_file : &lt;attach@&gt;       # in-store (default store)\n    archive : &lt;attach@cold&gt;     # in-store (specific store)\n    \"\"\"\n</code></pre></p>"},{"location":"reference/specs/type-system/#user-defined-codecs","title":"User-Defined Codecs","text":"<p>Users can define custom codecs for domain-specific data. See the Codec API Specification for complete examples including:</p> <ul> <li>Simple serialization codecs</li> <li>In-store codecs</li> <li>JSON with schema validation</li> <li>Context-dependent encoding</li> <li>In-store-only codecs (Zarr, HDF5)</li> </ul>"},{"location":"reference/specs/type-system/#storage-comparison","title":"Storage Comparison","text":"Type get_dtype Resolves To Storage Location Dedup Returns <code>&lt;blob&gt;</code> <code>bytes</code> <code>LONGBLOB</code>/<code>BYTEA</code> Database No Python object <code>&lt;blob@&gt;</code> <code>&lt;hash&gt;</code> <code>json</code> <code>_hash/{hash}</code> Yes Python object <code>&lt;blob@s&gt;</code> <code>&lt;hash&gt;</code> <code>json</code> <code>_hash/{hash}</code> Yes Python object <code>&lt;attach&gt;</code> <code>bytes</code> <code>LONGBLOB</code>/<code>BYTEA</code> Database No Local file path <code>&lt;attach@&gt;</code> <code>&lt;hash&gt;</code> <code>json</code> <code>_hash/{hash}</code> Yes Local file path <code>&lt;attach@s&gt;</code> <code>&lt;hash&gt;</code> <code>json</code> <code>_hash/{hash}</code> Yes Local file path <code>&lt;object@&gt;</code> <code>json</code> <code>JSON</code>/<code>JSONB</code> <code>{schema}/{table}/{pk}/</code> No ObjectRef <code>&lt;object@s&gt;</code> <code>json</code> <code>JSON</code>/<code>JSONB</code> <code>{schema}/{table}/{pk}/</code> No ObjectRef <code>&lt;hash@&gt;</code> <code>json</code> <code>JSON</code>/<code>JSONB</code> <code>_hash/{hash}</code> Yes bytes <code>&lt;hash@s&gt;</code> <code>json</code> <code>JSON</code>/<code>JSONB</code> <code>_hash/{hash}</code> Yes bytes <code>&lt;filepath@s&gt;</code> <code>json</code> <code>JSON</code>/<code>JSONB</code> Configured store No ObjectRef"},{"location":"reference/specs/type-system/#garbage-collection-for-hash-storage","title":"Garbage Collection for Hash Storage","text":"<p>Hash metadata (hash, store, size) is stored directly in each table's JSON column - no separate registry table is needed. Garbage collection scans all tables to find referenced hashes:</p> <pre><code>def garbage_collect(store_name):\n    \"\"\"Remove hash-addressed data not referenced by any table.\"\"\"\n    # Scan store for all hash files\n    store = get_store(store_name)\n    all_hashes = set(store.list_hashes())  # from _hash/ directory\n\n    # Scan all tables for referenced hashes\n    referenced = set()\n    for schema in project.schemas:\n        for table in schema.tables:\n            for attr in table.heading.attributes:\n                if uses_hash_storage(attr):  # &lt;blob@&gt;, &lt;attach@&gt;, &lt;hash@&gt;\n                    for row in table:\n                        val = row.get(attr.name)\n                        if val and val.get('store') == store_name:\n                            referenced.add(val['hash'])\n\n    # Delete orphaned files\n    for hash_id in (all_hashes - referenced):\n        store.delete(hash_path(hash_id))\n</code></pre>"},{"location":"reference/specs/type-system/#built-in-codec-comparison","title":"Built-in Codec Comparison","text":"Feature <code>&lt;blob&gt;</code> <code>&lt;attach&gt;</code> <code>&lt;object@&gt;</code> <code>&lt;hash@&gt;</code> <code>&lt;filepath@&gt;</code> Storage modes Both Both External only External only External only Internal dtype <code>bytes</code> <code>bytes</code> N/A N/A N/A External dtype <code>&lt;hash@&gt;</code> <code>&lt;hash@&gt;</code> <code>json</code> <code>json</code> <code>json</code> Addressing Hash Hash Primary key Hash Relative path Deduplication Yes (external) Yes (external) No Yes No Structure Single blob Single file Files, folders Single blob Any Returns Python object Local path ObjectRef bytes ObjectRef GC Ref counted Ref counted With row Ref counted User managed <p>When to use each: - <code>&lt;blob&gt;</code>: Serialized Python objects (NumPy arrays, dicts). Use <code>&lt;blob@&gt;</code> for large/duplicated data - <code>&lt;attach&gt;</code>: File attachments with filename preserved. Use <code>&lt;attach@&gt;</code> for large files - <code>&lt;object@&gt;</code>: Large/complex file structures (Zarr, HDF5) where DataJoint controls organization - <code>&lt;hash@&gt;</code>: Raw bytes with deduplication (typically used via <code>&lt;blob@&gt;</code> or <code>&lt;attach@&gt;</code>) - <code>&lt;filepath@store&gt;</code>: Portable references to externally-managed files - <code>varchar</code>: Arbitrary URLs/paths where ObjectRef semantics aren't needed</p>"},{"location":"reference/specs/type-system/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Three-layer architecture:<ul> <li>Layer 1: Native database types (backend-specific, discouraged)</li> <li>Layer 2: Core DataJoint types (standardized, scientist-friendly)</li> <li>Layer 3: Codec types (encode/decode, composable)</li> </ul> </li> <li>Core types are scientist-friendly: <code>float32</code>, <code>int8</code>, <code>bool</code>, <code>bytes</code> instead of <code>FLOAT</code>, <code>TINYINT</code>, <code>LONGBLOB</code></li> <li>Codecs use angle brackets: <code>&lt;blob&gt;</code>, <code>&lt;object@store&gt;</code>, <code>&lt;filepath@main&gt;</code> - distinguishes from core types</li> <li><code>@</code> indicates in-store storage: No <code>@</code> = database, <code>@</code> present = object store</li> <li><code>get_dtype(is_store)</code> method: Codecs resolve dtype at declaration time based on storage mode</li> <li>Codecs are composable: <code>&lt;blob@&gt;</code> uses <code>&lt;hash@&gt;</code>, which uses <code>json</code></li> <li>Built-in in-store codecs use JSON dtype: Stores metadata (path, hash, store name, etc.)</li> <li>Two OAS regions: object (PK-addressed) and hash (hash-addressed) within managed stores</li> <li>Filepath for portability: <code>&lt;filepath@store&gt;</code> uses relative paths within stores for environment portability</li> <li>No <code>uri</code> type: For arbitrary URLs, use <code>varchar</code>\u2014simpler and more transparent</li> <li>Naming conventions:     - <code>@</code> = in-store storage (object store)     - No <code>@</code> = in-table storage (database)     - <code>@</code> alone = default store     - <code>@name</code> = named store</li> <li>Dual-mode codecs: <code>&lt;blob&gt;</code> and <code>&lt;attach&gt;</code> support both in-table and in-store storage</li> <li>In-store-only codecs: <code>&lt;object@&gt;</code>, <code>&lt;hash@&gt;</code>, <code>&lt;filepath@&gt;</code> require <code>@</code></li> <li>Transparent access: Codecs return Python objects or file paths</li> <li>Lazy access: <code>&lt;object@&gt;</code> and <code>&lt;filepath@store&gt;</code> return ObjectRef</li> <li>MD5 for content hashing: See Hash Algorithm Choice below</li> <li>No separate registry: Hash metadata stored in JSON columns, not a separate table</li> <li>Auto-registration via <code>__init_subclass__</code>: Codecs register automatically when subclassed\u2014no decorator needed. Use <code>register=False</code> for abstract bases. Requires Python 3.10+.</li> </ol>"},{"location":"reference/specs/type-system/#hash-algorithm-choice","title":"Hash Algorithm Choice","text":"<p>Content-addressed storage uses MD5 (128-bit, 32-char hex) rather than SHA256 (256-bit, 64-char hex).</p> <p>Rationale:</p> <ol> <li> <p>Practical collision resistance is sufficient: The birthday bound for MD5 is ~2^64 operations    before 50% collision probability. No scientific project will store anywhere near 10^19 files.    For content deduplication (not cryptographic verification), MD5 provides adequate uniqueness.</p> </li> <li> <p>Storage efficiency: 32-char hashes vs 64-char hashes in every JSON metadata field.    With millions of records, this halves the storage overhead for hash identifiers.</p> </li> <li> <p>Performance: MD5 is ~2-3x faster than SHA256 for large files. While both are fast,    the difference is measurable when hashing large scientific datasets.</p> </li> <li> <p>Legacy compatibility: DataJoint's existing <code>uuid_from_buffer()</code> function uses MD5.    The new system changes only the storage format (hex string in JSON vs binary UUID),    not the underlying hash algorithm. This simplifies migration.</p> </li> <li> <p>Consistency with existing codebase: Internal functions use MD5 for query caching.</p> </li> </ol> <p>Why not SHA256?</p> <p>SHA256 is the modern standard for content-addressable storage (Git, Docker, IPFS). However: - These systems prioritize cryptographic security against adversarial collision attacks - Scientific data pipelines face no adversarial threat model - The practical benefits (storage, speed, compatibility) outweigh theoretical security gains</p> <p>Note: If cryptographic verification is ever needed (e.g., for compliance or reproducibility audits), SHA256 checksums can be computed on-demand without changing the storage addressing scheme.</p>"},{"location":"reference/specs/type-system/#migration-from-legacy-types","title":"Migration from Legacy Types","text":"Legacy New Equivalent <code>longblob</code> (auto-serialized) <code>&lt;blob&gt;</code> <code>blob@store</code> <code>&lt;blob@store&gt;</code> <code>attach</code> <code>&lt;attach&gt;</code> <code>attach@store</code> <code>&lt;attach@store&gt;</code> <code>filepath@store</code> (copy-based) <code>&lt;filepath@store&gt;</code> (ObjectRef-based)"},{"location":"reference/specs/type-system/#migration-from-legacy-external_-stores","title":"Migration from Legacy <code>~external_*</code> Stores","text":"<p>Legacy external storage used per-schema <code>~external_{store}</code> tables with UUID references. Migration to the new JSON-based hash storage requires:</p> <pre><code>def migrate_external_store(schema, store_name):\n    \"\"\"\n    Migrate legacy ~external_{store} to new HashRegistry.\n\n    1. Read all entries from ~external_{store}\n    2. For each entry:\n       - Fetch content from legacy location\n       - Compute MD5 hash\n       - Copy to _hash/{hash}/ if not exists\n       - Update table column to new hash format\n    3. After all schemas migrated, drop ~external_{store} tables\n    \"\"\"\n    external_table = schema.external[store_name]\n\n    for entry in external_table:\n        legacy_uuid = entry['hash']\n\n        # Fetch content from legacy location\n        content = external_table.get(legacy_uuid)\n\n        # Compute new content hash\n        hash_id = hashlib.md5(content).hexdigest()\n\n        # Store in new location if not exists\n        new_path = f\"_hash/{hash_id[:2]}/{hash_id[2:4]}/{hash_id}\"\n        store = get_store(store_name)\n        if not store.exists(new_path):\n            store.put(new_path, content)\n\n        # Update referencing tables: convert UUID column to JSON with hash metadata\n        # The JSON column stores {\"hash\": hash_id, \"store\": store_name, \"size\": len(content)}\n        # ... update all tables that reference this UUID ...\n\n    # After migration complete for all schemas:\n    # DROP TABLE `{schema}`.`~external_{store}`\n</code></pre> <p>Migration considerations: - Legacy UUIDs were based on MD5 content hash stored as <code>binary(16)</code> (UUID format) - New system uses <code>char(32)</code> MD5 hex strings stored in JSON - The hash algorithm is unchanged (MD5), only the storage format differs - Migration can be done incrementally per schema - Backward compatibility layer can read both formats during transition</p>"},{"location":"reference/specs/type-system/#open-questions","title":"Open Questions","text":"<ol> <li>How long should the backward compatibility layer support legacy <code>~external_*</code> format?</li> <li>Should <code>&lt;hash@&gt;</code> (without store name) use a default store or require explicit store name?</li> </ol>"},{"location":"reference/specs/virtual-schemas/","title":"Virtual Schemas Specification","text":""},{"location":"reference/specs/virtual-schemas/#overview","title":"Overview","text":"<p>Virtual schemas provide a way to access existing database schemas without the original Python source code. This is useful for:</p> <ul> <li>Exploring schemas created by other users</li> <li>Accessing legacy schemas</li> <li>Quick data inspection and queries</li> <li>Schema migration and maintenance</li> </ul>"},{"location":"reference/specs/virtual-schemas/#1-schema-module-convention","title":"1. Schema-Module Convention","text":"<p>DataJoint maintains a 1:1 mapping between database schemas and Python modules:</p> Database Python Schema Module Table Class <p>This convention reduces conceptual complexity: modules are schemas, classes are tables.</p> <p>When you define tables in Python: <pre><code># lab.py module\nimport datajoint as dj\nschema = dj.Schema('lab')\n\n@schema\nclass Subject(dj.Manual):  # Subject class \u2192 `lab`.`subject` table\n    ...\n\n@schema\nclass Session(dj.Manual):  # Session class \u2192 `lab`.`session` table\n    ...\n</code></pre></p> <p>Virtual schemas recreate this mapping when the Python source isn't available: <pre><code># Creates module-like object with table classes\nlab = dj.virtual_schema('lab')\nlab.Subject  # Subject class for `lab`.`subject`\nlab.Session  # Session class for `lab`.`session`\n</code></pre></p>"},{"location":"reference/specs/virtual-schemas/#2-schema-introspection-api","title":"2. Schema Introspection API","text":""},{"location":"reference/specs/virtual-schemas/#21-direct-table-access","title":"2.1 Direct Table Access","text":"<p>Access individual tables by name using bracket notation:</p> <pre><code>schema = dj.Schema('my_schema')\n\n# By CamelCase class name\nexperiment = schema['Experiment']\n\n# By snake_case SQL name\nexperiment = schema['experiment']\n\n# Query the table\nexperiment.fetch()\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#22-get_table-method","title":"2.2 <code>get_table()</code> Method","text":"<p>Explicit method for table access:</p> <pre><code>table = schema.get_table('Experiment')\ntable = schema.get_table('experiment')  # also works\n</code></pre> <p>Parameters: - <code>name</code> (str): Table name in CamelCase or snake_case</p> <p>Returns: <code>FreeTable</code> instance</p> <p>Raises: <code>DataJointError</code> if table doesn't exist</p>"},{"location":"reference/specs/virtual-schemas/#23-iteration","title":"2.3 Iteration","text":"<p>Iterate over all tables in dependency order:</p> <pre><code>for table in schema:\n    print(table.full_table_name, len(table))\n</code></pre> <p>Tables are yielded as <code>FreeTable</code> instances in topological order (dependencies before dependents).</p>"},{"location":"reference/specs/virtual-schemas/#24-containment-check","title":"2.4 Containment Check","text":"<p>Check if a table exists:</p> <pre><code>if 'Experiment' in schema:\n    print(\"Table exists\")\n\nif 'nonexistent' not in schema:\n    print(\"Table doesn't exist\")\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#3-virtual-schema-function","title":"3. Virtual Schema Function","text":""},{"location":"reference/specs/virtual-schemas/#31-djvirtual_schema","title":"3.1 <code>dj.virtual_schema()</code>","text":"<p>The recommended way to access existing schemas as modules:</p> <pre><code>lab = dj.virtual_schema('my_lab_schema')\n\n# Access tables as attributes (classes)\nlab.Subject.fetch()\nlab.Session &amp; \"subject_id='M001'\"\n\n# Full query algebra supported\n(lab.Session * lab.Subject).fetch()\n</code></pre> <p>This maintains the module-class convention: <code>lab</code> behaves like a Python module with table classes as attributes.</p> <p>Parameters:</p> Parameter Type Default Description <code>schema_name</code> str required Database schema name <code>connection</code> Connection None Database connection (uses default) <code>create_schema</code> bool False Create schema if missing <code>create_tables</code> bool False Allow new table declarations <code>add_objects</code> dict None Additional objects for namespace <p>Returns: <code>VirtualModule</code> instance</p>"},{"location":"reference/specs/virtual-schemas/#32-virtualmodule-class","title":"3.2 VirtualModule Class","text":"<p>The underlying class (prefer <code>virtual_schema()</code> function):</p> <pre><code>module = dj.VirtualModule('lab', 'my_lab_schema')\nmodule.Subject.fetch()\n</code></pre> <p>The first argument is the module display name, second is the schema name.</p>"},{"location":"reference/specs/virtual-schemas/#33-accessing-the-schema-object","title":"3.3 Accessing the Schema Object","text":"<p>Virtual modules expose the underlying Schema:</p> <pre><code>lab = dj.virtual_schema('my_lab_schema')\nlab.schema.database  # 'my_lab_schema'\nlab.schema.list_tables()  # ['subject', 'session', ...]\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#4-table-class-generation","title":"4. Table Class Generation","text":""},{"location":"reference/specs/virtual-schemas/#41-make_classes","title":"4.1 <code>make_classes()</code>","text":"<p>Create Python classes for all tables in a schema:</p> <pre><code>schema = dj.Schema('existing_schema')\nschema.make_classes()\n\n# Now table classes are available in local namespace\nSubject.fetch()\nSession &amp; \"date &gt; '2024-01-01'\"\n</code></pre> <p>Parameters: - <code>into</code> (dict, optional): Namespace to populate. Defaults to caller's locals.</p>"},{"location":"reference/specs/virtual-schemas/#42-generated-class-types","title":"4.2 Generated Class Types","text":"<p>Classes are created based on table naming conventions:</p> Table Name Pattern Generated Class <code>subject</code> <code>dj.Manual</code> <code>#lookup_table</code> <code>dj.Lookup</code> <code>_imported_table</code> <code>dj.Imported</code> <code>__computed_table</code> <code>dj.Computed</code> <code>master__part</code> <code>dj.Part</code>"},{"location":"reference/specs/virtual-schemas/#43-part-table-handling","title":"4.3 Part Table Handling","text":"<p>Part tables are attached to their master classes:</p> <pre><code>lab = dj.virtual_schema('my_lab')\n\n# Part tables are nested attributes\nlab.Session.Trial.fetch()  # Session.Trial is a Part table\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#5-use-cases","title":"5. Use Cases","text":""},{"location":"reference/specs/virtual-schemas/#51-data-exploration","title":"5.1 Data Exploration","text":"<pre><code># Quick exploration of unknown schema\nlab = dj.virtual_schema('collaborator_lab')\n\n# List all tables\nprint(lab.schema.list_tables())\n\n# Check table structure\nprint(lab.Subject.describe())\n\n# Preview data\nlab.Subject.fetch(limit=5)\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#52-cross-schema-queries","title":"5.2 Cross-Schema Queries","text":"<pre><code>my_schema = dj.Schema('my_analysis')\nexternal = dj.virtual_schema('external_lab')\n\n# Reference external tables in queries\n@my_schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; external.Session\n    ---\n    result : float\n    \"\"\"\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#53-schema-migration","title":"5.3 Schema Migration","text":"<pre><code>old = dj.virtual_schema('old_schema')\nnew = dj.Schema('new_schema')\n\n# Copy data in topological order (iteration yields dependencies first)\nfor table in old:\n    new_table = new.get_table(table.table_name)\n    # Server-side INSERT...SELECT (no client-side data transfer)\n    new_table.insert(table)\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#54-garbage-collection","title":"5.4 Garbage Collection","text":"<pre><code>from datajoint.gc import scan_hash_references\n\nschema = dj.Schema('my_schema')\n\n# Scan all tables for hash references\nrefs = scan_hash_references(schema, verbose=True)\n</code></pre>"},{"location":"reference/specs/virtual-schemas/#6-comparison-of-methods","title":"6. Comparison of Methods","text":"Method Use Case Returns <code>schema['Name']</code> Quick single table access <code>FreeTable</code> <code>schema.get_table('name')</code> Explicit table access <code>FreeTable</code> <code>for t in schema</code> Iterate all tables <code>FreeTable</code> generator <code>'Name' in schema</code> Check existence <code>bool</code> <code>dj.virtual_schema(name)</code> Module-like access <code>VirtualModule</code> <code>make_classes()</code> Populate namespace None (side effect)"},{"location":"reference/specs/virtual-schemas/#7-implementation-reference","title":"7. Implementation Reference","text":"File Purpose <code>schemas.py</code> Schema, VirtualModule, virtual_schema <code>table.py</code> FreeTable class <code>gc.py</code> Uses get_table() for scanning"},{"location":"reference/specs/virtual-schemas/#8-error-messages","title":"8. Error Messages","text":"Error Cause Solution \"Table does not exist\" <code>get_table()</code> on missing table Check table name spelling \"Schema must be activated\" Operations on unactivated schema Call <code>schema.activate(name)</code> \"Schema does not exist\" Schema name not in database Check schema name, create if needed"},{"location":"tutorials/","title":"Tutorials","text":"<p>Learn DataJoint by building real pipelines.</p> <p>These tutorials guide you through building data pipelines step by step. Each tutorial is a Jupyter notebook that you can run interactively. Start with the basics and progress to domain-specific and advanced topics.</p>"},{"location":"tutorials/#quick-start","title":"Quick Start","text":"<p>Install DataJoint:</p> <pre><code>pip install datajoint\n</code></pre> <p>Configure database credentials in your project (see Configuration):</p> <pre><code># Create datajoint.json for non-sensitive settings\necho '{\"database\": {\"host\": \"localhost\", \"port\": 3306}}' &gt; datajoint.json\n\n# Create secrets directory for credentials\nmkdir -p .secrets\necho \"root\" &gt; .secrets/database.user\necho \"password\" &gt; .secrets/database.password\n</code></pre> <p>Define and populate a simple pipeline:</p> <pre><code>import datajoint as dj\n\nschema = dj.Schema('my_pipeline')\n\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : int32\n    ---\n    name : varchar(100)\n    date_of_birth : date\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int16\n    ---\n    session_date : date\n    \"\"\"\n\n@schema\nclass SessionAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    result : float64\n    \"\"\"\n\n    def make(self, key):\n        # Compute result for this session\n        self.insert1({**key, 'result': 42.0})\n\n# Insert data\nSubject.insert1({'subject_id': 1, 'name': 'M001', 'date_of_birth': '2026-01-15'})\nSession.insert1({'subject_id': 1, 'session_idx': 1, 'session_date': '2026-01-06'})\n\n# Run computations\nSessionAnalysis.populate()\n</code></pre> <p>Continue learning with the structured tutorials below.</p>"},{"location":"tutorials/#learning-paths","title":"Learning Paths","text":"<p>Choose your learning path based on your goals:</p>"},{"location":"tutorials/#new-to-datajoint","title":"\ud83c\udf31 New to DataJoint","text":"<p>Goal: Understand core concepts and build your first pipeline</p> <p>Path:</p> <ol> <li>First Pipeline \u2014 30 min \u2014 Tables, queries, four core operations</li> <li>Schema Design \u2014 45 min \u2014 Primary keys, relationships, table tiers</li> <li>Data Entry \u2014 30 min \u2014 Inserting and managing data</li> <li>Queries \u2014 45 min \u2014 Operators, restrictions, projections</li> <li>Try an example: University Database \u2014 Complete pipeline with realistic data</li> </ol> <p>Next: Read Relational Workflow Model to understand the conceptual foundation.</p>"},{"location":"tutorials/#building-production-pipelines","title":"\ud83d\ude80 Building Production Pipelines","text":"<p>Goal: Create automated, scalable data processing workflows</p> <p>Prerequisites: Complete basics above or have equivalent experience</p> <p>Path:</p> <ol> <li>Computation \u2014 Automated processing with Imported/Computed tables</li> <li>Object Storage \u2014 Handle large data (arrays, files, images)</li> <li>Distributed Computing \u2014 Multi-worker parallel execution</li> <li>Practice: Fractal Pipeline or Blob Detection</li> </ol> <p>Next:</p> <ul> <li>Run Computations \u2014 populate() usage patterns</li> <li>Distributed Computing \u2014 Cluster deployment</li> <li>Handle Errors \u2014 Job management and recovery</li> </ul>"},{"location":"tutorials/#domain-specific-applications","title":"\ud83e\uddea Domain-Specific Applications","text":"<p>Goal: Build scientific data pipelines for your field</p> <p>Prerequisites: Complete basics, understand computation model</p> <p>Production Software: DataJoint Elements</p> <p>Standard pipelines for neurophysiology experiments, actively used in many labs worldwide. These are not tutorials\u2014they are production-ready modular pipelines for calcium imaging, electrophysiology, array ephys, optogenetics, and more.</p> <p>Learning tutorials (neuroscience):</p> <ul> <li>Calcium Imaging \u2014 Import movies, segment cells, extract traces</li> <li>Electrophysiology \u2014 Import recordings, spike detection, waveforms</li> <li>Allen CCF \u2014 Hierarchical brain atlas ontology</li> </ul> <p>General patterns:</p> <ul> <li>Hotel Reservations \u2014 Booking systems with resource management</li> <li>Languages &amp; Proficiency \u2014 Many-to-many relationships</li> </ul>"},{"location":"tutorials/#extending-datajoint","title":"\ud83d\udd27 Extending DataJoint","text":"<p>Goal: Customize DataJoint for specialized needs</p> <p>Prerequisites: Proficient with basics and production pipelines</p> <p>Path:</p> <ol> <li>Custom Codecs \u2014 Create domain-specific data types</li> <li>JSON Data Type \u2014 Semi-structured data patterns</li> <li>SQL Comparison \u2014 Understand DataJoint's query algebra</li> </ol> <p>Next:</p> <ul> <li>Codec API \u2014 Complete codec specification</li> <li>Create Custom Codec \u2014 Step-by-step codec development</li> </ul>"},{"location":"tutorials/#basics","title":"Basics","text":"<p>Core concepts for getting started with DataJoint:</p> <ol> <li>First Pipeline \u2014 Tables, queries, and the four core operations</li> <li>Schema Design \u2014 Primary keys, relationships, and table tiers</li> <li>Data Entry \u2014 Inserting and managing data</li> <li>Queries \u2014 Operators and fetching results</li> <li>Computation \u2014 Imported and Computed tables</li> <li>Object Storage \u2014 Blobs, attachments, and object stores</li> </ol>"},{"location":"tutorials/#examples","title":"Examples","text":"<p>Complete pipelines demonstrating DataJoint patterns:</p> <ul> <li>University Database \u2014 Academic records with students, courses, and grades</li> <li>Hotel Reservations \u2014 Booking system with rooms, guests, and reservations</li> <li>Languages &amp; Proficiency \u2014 Language skills tracking with many-to-many relationships</li> <li>Fractal Pipeline \u2014 Iterative computation and parameter sweeps</li> <li>Blob Detection \u2014 Image processing with automated computation</li> </ul>"},{"location":"tutorials/#domain-tutorials","title":"Domain Tutorials","text":"<p>Real-world scientific pipelines:</p> <ul> <li>Calcium Imaging \u2014 Import TIFF movies, segment cells, extract fluorescence traces</li> <li>Electrophysiology \u2014 Import recordings, detect spikes, extract waveforms</li> <li>Electrophysiology with Object Storage \u2014 Neural data with <code>&lt;npy@&gt;</code> lazy loading</li> <li>Allen CCF \u2014 Brain atlas with hierarchical region ontology</li> </ul>"},{"location":"tutorials/#advanced-topics","title":"Advanced Topics","text":"<p>Extending DataJoint for specialized use cases:</p> <ul> <li>SQL Comparison \u2014 DataJoint for SQL users</li> <li>JSON Data Type \u2014 Semi-structured data in tables</li> <li>Distributed Computing \u2014 Multi-process and cluster workflows</li> <li>Custom Codecs \u2014 Extending the type system</li> </ul>"},{"location":"tutorials/#running-the-tutorials","title":"Running the Tutorials","text":"<pre><code># Clone the repository\ngit clone https://github.com/datajoint/datajoint-docs.git\ncd datajoint-docs\n\n# Start the tutorial environment\ndocker compose up -d\n\n# Launch Jupyter\njupyter lab src/tutorials/\n</code></pre> <p>All tutorials use a local MySQL database that resets between sessions.</p>"},{"location":"tutorials/advanced/custom-codecs/","title":"Custom Codecs","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\n\nschema = dj.Schema('tutorial_codecs')\n</pre> import datajoint as dj import numpy as np  schema = dj.Schema('tutorial_codecs') <pre>[2026-01-27 15:28:22] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>import networkx as nx\n\nclass GraphCodec(dj.Codec):\n    \"\"\"Store NetworkX graphs.\"\"\"\n    \n    name = \"graph\"  # Use as &lt;graph&gt;\n    \n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n    \n    def encode(self, value, *, key=None, store_name=None):\n        return {'nodes': list(value.nodes(data=True)), 'edges': list(value.edges(data=True))}\n    \n    def decode(self, stored, *, key=None):\n        g = nx.Graph()\n        g.add_nodes_from(stored['nodes'])\n        g.add_edges_from(stored['edges'])\n        return g\n    \n    def validate(self, value):\n        if not isinstance(value, nx.Graph):\n            raise TypeError(f\"Expected nx.Graph\")\n</pre> import networkx as nx  class GraphCodec(dj.Codec):     \"\"\"Store NetworkX graphs.\"\"\"          name = \"graph\"  # Use as           def get_dtype(self, is_store: bool) -&gt; str:         return \"\"          def encode(self, value, *, key=None, store_name=None):         return {'nodes': list(value.nodes(data=True)), 'edges': list(value.edges(data=True))}          def decode(self, stored, *, key=None):         g = nx.Graph()         g.add_nodes_from(stored['nodes'])         g.add_edges_from(stored['edges'])         return g          def validate(self, value):         if not isinstance(value, nx.Graph):             raise TypeError(f\"Expected nx.Graph\") In\u00a0[3]: Copied! <pre>@schema\nclass Connectivity(dj.Manual):\n    definition = \"\"\"\n    conn_id : int32\n    ---\n    network : &lt;graph&gt;\n    \"\"\"\n</pre> @schema class Connectivity(dj.Manual):     definition = \"\"\"     conn_id : int32     ---     network :      \"\"\" In\u00a0[4]: Copied! <pre># Create and insert\ng = nx.Graph()\ng.add_edges_from([(1, 2), (2, 3), (1, 3)])\nConnectivity.insert1({'conn_id': 1, 'network': g})\n\n# Fetch\nresult = (Connectivity &amp; {'conn_id': 1}).fetch1('network')\nprint(f\"Type: {type(result)}\")\nprint(f\"Edges: {list(result.edges())}\")\n</pre> # Create and insert g = nx.Graph() g.add_edges_from([(1, 2), (2, 3), (1, 3)]) Connectivity.insert1({'conn_id': 1, 'network': g})  # Fetch result = (Connectivity &amp; {'conn_id': 1}).fetch1('network') print(f\"Type: {type(result)}\") print(f\"Edges: {list(result.edges())}\") <pre>Type: &lt;class 'networkx.classes.graph.Graph'&gt;\nEdges: [(1, 2), (1, 3), (2, 3)]\n</pre> In\u00a0[5]: Copied! <pre>from dataclasses import dataclass\n\n@dataclass\nclass SpikeTrain:\n    times: np.ndarray\n    unit_id: int\n    quality: str\n\nclass SpikeTrainCodec(dj.Codec):\n    name = \"spike_train\"\n    \n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"\n    \n    def encode(self, value, *, key=None, store_name=None):\n        return {'times': value.times, 'unit_id': value.unit_id, 'quality': value.quality}\n    \n    def decode(self, stored, *, key=None):\n        return SpikeTrain(times=stored['times'], unit_id=stored['unit_id'], quality=stored['quality'])\n</pre> from dataclasses import dataclass  @dataclass class SpikeTrain:     times: np.ndarray     unit_id: int     quality: str  class SpikeTrainCodec(dj.Codec):     name = \"spike_train\"          def get_dtype(self, is_store: bool) -&gt; str:         return \"\"          def encode(self, value, *, key=None, store_name=None):         return {'times': value.times, 'unit_id': value.unit_id, 'quality': value.quality}          def decode(self, stored, *, key=None):         return SpikeTrain(times=stored['times'], unit_id=stored['unit_id'], quality=stored['quality']) In\u00a0[6]: Copied! <pre>@schema\nclass Unit(dj.Manual):\n    definition = \"\"\"\n    unit_id : int32\n    ---\n    spikes : &lt;spike_train&gt;\n    \"\"\"\n\ntrain = SpikeTrain(times=np.sort(np.random.uniform(0, 100, 50)), unit_id=1, quality='good')\nUnit.insert1({'unit_id': 1, 'spikes': train})\n\nresult = (Unit &amp; {'unit_id': 1}).fetch1('spikes')\nprint(f\"Type: {type(result)}, Spikes: {len(result.times)}\")\n</pre> @schema class Unit(dj.Manual):     definition = \"\"\"     unit_id : int32     ---     spikes :      \"\"\"  train = SpikeTrain(times=np.sort(np.random.uniform(0, 100, 50)), unit_id=1, quality='good') Unit.insert1({'unit_id': 1, 'spikes': train})  result = (Unit &amp; {'unit_id': 1}).fetch1('spikes') print(f\"Type: {type(result)}, Spikes: {len(result.times)}\") <pre>Type: &lt;class '__main__.SpikeTrain'&gt;, Spikes: 50\n</pre> In\u00a0[7]: Copied! <pre>schema.drop(prompt=False)\n</pre> schema.drop(prompt=False)"},{"location":"tutorials/advanced/custom-codecs/#custom-codecs","title":"Custom Codecs\u00b6","text":"<p>This tutorial covers extending DataJoint's type system. You'll learn:</p> <ul> <li>Codec basics \u2014 Encoding and decoding</li> <li>Creating codecs \u2014 Domain-specific types</li> <li>Codec chaining \u2014 Composing codecs</li> </ul>"},{"location":"tutorials/advanced/custom-codecs/#creating-a-custom-codec","title":"Creating a Custom Codec\u00b6","text":""},{"location":"tutorials/advanced/custom-codecs/#codec-structure","title":"Codec Structure\u00b6","text":"<pre>class MyCodec(dj.Codec):\n    name = \"mytype\"  # Use as &lt;mytype&gt;\n    \n    def get_dtype(self, is_store: bool) -&gt; str:\n        return \"&lt;blob&gt;\"  # Storage type\n    \n    def encode(self, value, *, key=None, store_name=None):\n        return serializable_data\n    \n    def decode(self, stored, *, key=None):\n        return python_object\n    \n    def validate(self, value):  # Optional\n        pass\n</pre>"},{"location":"tutorials/advanced/custom-codecs/#example-spike-train","title":"Example: Spike Train\u00b6","text":""},{"location":"tutorials/advanced/distributed/","title":"Distributed Computing","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nimport time\n\nschema = dj.Schema('tutorial_distributed')\n\n# Clean up from previous runs\nschema.drop(prompt=False)\nschema = dj.Schema('tutorial_distributed')\n</pre> import datajoint as dj import numpy as np import time  schema = dj.Schema('tutorial_distributed')  # Clean up from previous runs schema.drop(prompt=False) schema = dj.Schema('tutorial_distributed') <pre>[2026-01-27 15:28:25] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Experiment(dj.Manual):\n    definition = \"\"\"\n    exp_id : int\n    ---\n    n_samples : int\n    \"\"\"\n\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Experiment\n    ---\n    result : float64\n    compute_time : float32\n    \"\"\"\n\n    def make(self, key):\n        start = time.time()\n        n = (Experiment &amp; key).fetch1('n_samples')\n        result = float(np.mean(np.random.randn(n) ** 2))\n        time.sleep(0.1)\n        self.insert1({**key, 'result': result, 'compute_time': time.time() - start})\n</pre> @schema class Experiment(dj.Manual):     definition = \"\"\"     exp_id : int     ---     n_samples : int     \"\"\"  @schema class Analysis(dj.Computed):     definition = \"\"\"     -&gt; Experiment     ---     result : float64     compute_time : float32     \"\"\"      def make(self, key):         start = time.time()         n = (Experiment &amp; key).fetch1('n_samples')         result = float(np.mean(np.random.randn(n) ** 2))         time.sleep(0.1)         self.insert1({**key, 'result': result, 'compute_time': time.time() - start}) <pre>[2026-01-27 15:28:25][WARNING]: Native type 'int' is used in attribute 'exp_id'. Consider using a core DataJoint type for better portability.\n</pre> <pre>[2026-01-27 15:28:25][WARNING]: Native type 'int' is used in attribute 'n_samples'. Consider using a core DataJoint type for better portability.\n</pre> In\u00a0[3]: Copied! <pre>Experiment.insert([{'exp_id': i, 'n_samples': 10000} for i in range(20)])\nprint(f\"To compute: {len(Analysis.key_source - Analysis)}\")\n</pre> Experiment.insert([{'exp_id': i, 'n_samples': 10000} for i in range(20)]) print(f\"To compute: {len(Analysis.key_source - Analysis)}\") <pre>To compute: 20\n</pre> In\u00a0[4]: Copied! <pre># Distributed mode\nAnalysis.populate(reserve_jobs=True, max_calls=5, display_progress=True)\n</pre> # Distributed mode Analysis.populate(reserve_jobs=True, max_calls=5, display_progress=True) <pre>[2026-01-27 15:28:25][WARNING]: Native type 'integer' is used in attribute 'exp_id'. Consider using a core DataJoint type for better portability.\n</pre> <pre>\rAnalysis:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\rAnalysis:  20%|\u2588\u2588        | 1/5 [00:00&lt;00:00,  7.36it/s]</pre> <pre>\rAnalysis:  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:00&lt;00:00,  8.06it/s]</pre> <pre>\rAnalysis:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:00&lt;00:00,  8.28it/s]</pre> <pre>\rAnalysis:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:00&lt;00:00,  8.26it/s]</pre> <pre>\rAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00,  8.31it/s]</pre> <pre>\rAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00,  8.18it/s]</pre> <pre>\n</pre> Out[4]: <pre>{'success_count': 5, 'error_list': []}</pre> In\u00a0[5]: Copied! <pre># Refresh job queue\nresult = Analysis.jobs.refresh()\nprint(f\"Added: {result['added']}\")\n\n# Check status\nfor status, count in Analysis.jobs.progress().items():\n    print(f\"{status}: {count}\")\n</pre> # Refresh job queue result = Analysis.jobs.refresh() print(f\"Added: {result['added']}\")  # Check status for status, count in Analysis.jobs.progress().items():     print(f\"{status}: {count}\") <pre>Added: 0\npending: 15\nreserved: 0\nsuccess: 0\nerror: 0\nignore: 0\ntotal: 15\n</pre> In\u00a0[6]: Copied! <pre># Complete remaining jobs with distributed coordination\nAnalysis.populate(reserve_jobs=True, display_progress=True)\nprint(f\"Computed: {len(Analysis())}\")\n</pre> # Complete remaining jobs with distributed coordination Analysis.populate(reserve_jobs=True, display_progress=True) print(f\"Computed: {len(Analysis())}\") <pre>\rAnalysis:   0%|          | 0/15 [00:00&lt;?, ?it/s]</pre> <pre>\rAnalysis:   7%|\u258b         | 1/15 [00:00&lt;00:01,  8.51it/s]</pre> <pre>\rAnalysis:  13%|\u2588\u258e        | 2/15 [00:00&lt;00:01,  8.27it/s]</pre> <pre>\rAnalysis:  20%|\u2588\u2588        | 3/15 [00:00&lt;00:01,  8.27it/s]</pre> <pre>\rAnalysis:  27%|\u2588\u2588\u258b       | 4/15 [00:00&lt;00:01,  8.21it/s]</pre> <pre>\rAnalysis:  33%|\u2588\u2588\u2588\u258e      | 5/15 [00:00&lt;00:01,  8.15it/s]</pre> <pre>\rAnalysis:  40%|\u2588\u2588\u2588\u2588      | 6/15 [00:00&lt;00:01,  8.09it/s]</pre> <pre>\rAnalysis:  47%|\u2588\u2588\u2588\u2588\u258b     | 7/15 [00:00&lt;00:00,  8.09it/s]</pre> <pre>\rAnalysis:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 8/15 [00:00&lt;00:00,  8.06it/s]</pre> <pre>\rAnalysis:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 9/15 [00:01&lt;00:00,  8.05it/s]</pre> <pre>\rAnalysis:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 10/15 [00:01&lt;00:00,  8.06it/s]</pre> <pre>\rAnalysis:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 11/15 [00:01&lt;00:00,  8.08it/s]</pre> <pre>\rAnalysis:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 12/15 [00:01&lt;00:00,  8.11it/s]</pre> <pre>\rAnalysis:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 13/15 [00:01&lt;00:00,  8.11it/s]</pre> <pre>\rAnalysis:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 14/15 [00:01&lt;00:00,  8.08it/s]</pre> <pre>\rAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:01&lt;00:00,  8.17it/s]</pre> <pre>\rAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:01&lt;00:00,  8.13it/s]</pre> <pre>Computed: 20\n</pre> <pre>\n</pre> In\u00a0[7]: Copied! <pre># View errors\nprint(f\"Errors: {len(Analysis.jobs.errors)}\")\n\n# Retry failed jobs\nAnalysis.jobs.errors.delete()\nAnalysis.populate(reserve_jobs=True, suppress_errors=True)\n</pre> # View errors print(f\"Errors: {len(Analysis.jobs.errors)}\")  # Retry failed jobs Analysis.jobs.errors.delete() Analysis.populate(reserve_jobs=True, suppress_errors=True) <pre>Errors: 0\n</pre> Out[7]: <pre>{'success_count': 0, 'error_list': []}</pre> In\u00a0[8]: Copied! <pre>schema.drop(prompt=False)\n</pre> schema.drop(prompt=False)"},{"location":"tutorials/advanced/distributed/#distributed-computing","title":"Distributed Computing\u00b6","text":"<p>This tutorial covers running computations across multiple workers. You'll learn:</p> <ul> <li>Jobs 2.0 \u2014 DataJoint's job coordination system</li> <li>Multi-process \u2014 Parallel workers on one machine</li> <li>Multi-machine \u2014 Cluster-scale computation</li> <li>Error handling \u2014 Recovery and monitoring</li> </ul>"},{"location":"tutorials/advanced/distributed/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/advanced/distributed/#direct-vs-distributed-mode","title":"Direct vs Distributed Mode\u00b6","text":"<p>Direct mode (default): No coordination, suitable for single worker.</p> <p>Distributed mode (<code>reserve_jobs=True</code>): Workers coordinate via jobs table.</p>"},{"location":"tutorials/advanced/distributed/#the-jobs-table","title":"The Jobs Table\u00b6","text":""},{"location":"tutorials/advanced/distributed/#multi-process-and-multi-machine","title":"Multi-Process and Multi-Machine\u00b6","text":"<p>The <code>processes=N</code> parameter spawns multiple worker processes on one machine. However, this requires table classes to be defined in importable Python modules (not notebooks), because multiprocessing needs to pickle and transfer the class definitions to worker processes.</p> <p>For production use, define your tables in a module and run workers as scripts:</p> <pre># pipeline.py - Define your tables\nimport datajoint as dj\nschema = dj.Schema('my_pipeline')\n\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"...\"\"\"\n    def make(self, key): ...\n</pre> <pre># worker.py - Run workers\nfrom pipeline import Analysis\n\n# Single machine, 4 processes\nAnalysis.populate(reserve_jobs=True, processes=4)\n\n# Or run this script on multiple machines\nwhile True:\n    result = Analysis.populate(reserve_jobs=True, max_calls=100, suppress_errors=True)\n    if result['success_count'] == 0:\n        break\n</pre> <p>In this notebook, we'll demonstrate distributed coordination with a single process:</p>"},{"location":"tutorials/advanced/distributed/#error-handling","title":"Error Handling\u00b6","text":""},{"location":"tutorials/advanced/distributed/#quick-reference","title":"Quick Reference\u00b6","text":"Option Description <code>reserve_jobs=True</code> Enable coordination <code>processes=N</code> N worker processes <code>max_calls=N</code> Limit jobs per run <code>suppress_errors=True</code> Continue on errors"},{"location":"tutorials/advanced/json-type/","title":"JSON Data Type","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\n\nschema = dj.Schema('tutorial_json')\n</pre> import datajoint as dj  schema = dj.Schema('tutorial_json') <pre>[2026-01-27 15:28:33] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Equipment(dj.Manual):\n    definition = \"\"\"\n    equipment_id : int32\n    ---\n    name : varchar(100)\n    specs=null : json    # flexible specifications\n    \"\"\"\n</pre> @schema class Equipment(dj.Manual):     definition = \"\"\"     equipment_id : int32     ---     name : varchar(100)     specs=null : json    # flexible specifications     \"\"\" In\u00a0[3]: Copied! <pre>Equipment.insert([\n    {\n        'equipment_id': 1,\n        'name': 'Microscope A',\n        'specs': {\n            'magnification': [10, 40, 100],\n            'camera': {'model': 'XR500', 'resolution': [2048, 2048]},\n            'calibrated': True,\n        },\n    },\n    {\n        'equipment_id': 2,\n        'name': 'Electrode Array',\n        'specs': {\n            'channels': 64,\n            'impedance_kohm': 0.5,\n            'material': 'tungsten',\n        },\n    },\n    {\n        'equipment_id': 3,\n        'name': 'Pending Setup',\n        'specs': None,  # null allowed\n    },\n])\n</pre> Equipment.insert([     {         'equipment_id': 1,         'name': 'Microscope A',         'specs': {             'magnification': [10, 40, 100],             'camera': {'model': 'XR500', 'resolution': [2048, 2048]},             'calibrated': True,         },     },     {         'equipment_id': 2,         'name': 'Electrode Array',         'specs': {             'channels': 64,             'impedance_kohm': 0.5,             'material': 'tungsten',         },     },     {         'equipment_id': 3,         'name': 'Pending Setup',         'specs': None,  # null allowed     }, ]) In\u00a0[4]: Copied! <pre>Equipment()\n</pre> Equipment() Out[4]: None <p>equipment_id</p> <p>name</p> <p>specs</p> flexible specifications 1 Microscope A json2 Electrode Array json3 Pending Setup json <p>Total: 3</p> In\u00a0[5]: Copied! <pre># Fetch all\nfor row in Equipment.to_dicts():\n    print(f\"{row['name']}: {row['specs']}\")\n</pre> # Fetch all for row in Equipment.to_dicts():     print(f\"{row['name']}: {row['specs']}\") <pre>Microscope A: {'camera': {'model': 'XR500', 'resolution': [2048, 2048]}, 'calibrated': True, 'magnification': [10, 40, 100]}\nElectrode Array: {'channels': 64, 'material': 'tungsten', 'impedance_kohm': 0.5}\nPending Setup: None\n</pre> In\u00a0[6]: Copied! <pre># Fetch one and access nested fields\nmicroscope = (Equipment &amp; {'equipment_id': 1}).fetch1()\nspecs = microscope['specs']\n\nprint(f\"Camera model: {specs['camera']['model']}\")\nprint(f\"Magnifications: {specs['magnification']}\")\n</pre> # Fetch one and access nested fields microscope = (Equipment &amp; {'equipment_id': 1}).fetch1() specs = microscope['specs']  print(f\"Camera model: {specs['camera']['model']}\") print(f\"Magnifications: {specs['magnification']}\") <pre>Camera model: XR500\nMagnifications: [10, 40, 100]\n</pre> In\u00a0[7]: Copied! <pre># Find calibrated equipment\ncalibrated = [\n    e for e in Equipment.to_dicts()\n    if e['specs'] and e['specs'].get('calibrated')\n]\nprint(\"Calibrated:\", [e['name'] for e in calibrated])\n\n# Find equipment with &gt;32 channels\nmulti_channel = [\n    e for e in Equipment.to_dicts()\n    if e['specs'] and e['specs'].get('channels', 0) &gt; 32\n]\nprint(\"Multi-channel:\", [e['name'] for e in multi_channel])\n</pre> # Find calibrated equipment calibrated = [     e for e in Equipment.to_dicts()     if e['specs'] and e['specs'].get('calibrated') ] print(\"Calibrated:\", [e['name'] for e in calibrated])  # Find equipment with &gt;32 channels multi_channel = [     e for e in Equipment.to_dicts()     if e['specs'] and e['specs'].get('channels', 0) &gt; 32 ] print(\"Multi-channel:\", [e['name'] for e in multi_channel]) <pre>Calibrated: ['Microscope A']\nMulti-channel: ['Electrode Array']\n</pre> In\u00a0[8]: Copied! <pre># Fetch, modify, delete, reinsert\npending = (Equipment &amp; {'equipment_id': 3}).fetch1()\npending['specs'] = {'type': 'behavioral', 'sensors': ['IR', 'pressure']}\n\n(Equipment &amp; {'equipment_id': 3}).delete(prompt=False)\nEquipment.insert1(pending)\n\n(Equipment &amp; {'equipment_id': 3}).fetch1()\n</pre> # Fetch, modify, delete, reinsert pending = (Equipment &amp; {'equipment_id': 3}).fetch1() pending['specs'] = {'type': 'behavioral', 'sensors': ['IR', 'pressure']}  (Equipment &amp; {'equipment_id': 3}).delete(prompt=False) Equipment.insert1(pending)  (Equipment &amp; {'equipment_id': 3}).fetch1() <pre>[2026-01-27 15:28:33] Deleting 1 rows from \"tutorial_json\".\"equipment\"\n</pre> Out[8]: <pre>{'equipment_id': 3,\n 'name': 'Pending Setup',\n 'specs': {'type': 'behavioral', 'sensors': ['IR', 'pressure']}}</pre> In\u00a0[9]: Copied! <pre>schema.drop(prompt=False)\n</pre> schema.drop(prompt=False)"},{"location":"tutorials/advanced/json-type/#json-data-type","title":"JSON Data Type\u00b6","text":"<p>The <code>json</code> type stores semi-structured data as complete objects. This tutorial covers:</p> <ul> <li>When to use JSON vs normalized tables</li> <li>Defining, inserting, and fetching JSON data</li> </ul>"},{"location":"tutorials/advanced/json-type/#when-to-use-json","title":"When to Use JSON\u00b6","text":"<p>Good for:</p> <ul> <li>Evolving schemas not yet finalized</li> <li>Heterogeneous data with varying fields per entry</li> <li>Metadata and configuration storage</li> <li>Preserving structure from external APIs</li> </ul> <p>Prefer normalized tables when:</p> <ul> <li>Structure is consistent across entries</li> <li>You need database-level filtering on specific fields</li> <li>Referential integrity matters</li> </ul> <p>JSON fields are stored and retrieved as complete objects. Filter on JSON content in Python after fetching.</p>"},{"location":"tutorials/advanced/json-type/#table-definition","title":"Table Definition\u00b6","text":""},{"location":"tutorials/advanced/json-type/#inserting-json-data","title":"Inserting JSON Data\u00b6","text":"<p>Pass Python dicts for JSON fields. Structure can vary between entries:</p>"},{"location":"tutorials/advanced/json-type/#viewing-data","title":"Viewing Data\u00b6","text":"<p>JSON displays as <code>json</code> in previews (like blobs):</p>"},{"location":"tutorials/advanced/json-type/#fetching-json-data","title":"Fetching JSON Data\u00b6","text":"<p>JSON deserializes to Python dicts on fetch:</p>"},{"location":"tutorials/advanced/json-type/#filtering-on-json-content","title":"Filtering on JSON Content\u00b6","text":"<p>Fetch then filter in Python:</p>"},{"location":"tutorials/advanced/json-type/#updating-json-data","title":"Updating JSON Data\u00b6","text":""},{"location":"tutorials/advanced/json-type/#design-guidelines","title":"Design Guidelines\u00b6","text":"JSON Normalized Tables Flexible schema Fixed schema Filter in Python Filter in SQL No type enforcement Type-safe No referential integrity FK constraints <p>Tips:</p> <ul> <li>Use JSON for metadata, configs, external API responses</li> <li>If you frequently filter on a field, normalize it to a column</li> <li>Document expected JSON structure in comments</li> </ul>"},{"location":"tutorials/advanced/json-type/#cleanup","title":"Cleanup\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/","title":"DataJoint for SQL Users","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\n\nschema = dj.Schema('tutorial_sql_comparison')\n</pre> import datajoint as dj  schema = dj.Schema('tutorial_sql_comparison') <pre>[2026-01-27 15:28:36] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Researcher(dj.Manual):\n    definition = \"\"\"\n    researcher_id : int32\n    ---\n    name : varchar(100)\n    email : varchar(100)\n    \"\"\"\n\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : int32\n    ---\n    species : varchar(32)\n    sex : enum('M', 'F', 'unknown')\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_date : date\n    ---\n    -&gt; Researcher\n    notes : varchar(255)\n    \"\"\"\n</pre> @schema class Researcher(dj.Manual):     definition = \"\"\"     researcher_id : int32     ---     name : varchar(100)     email : varchar(100)     \"\"\"  @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : int32     ---     species : varchar(32)     sex : enum('M', 'F', 'unknown')     \"\"\"  @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_date : date     ---     -&gt; Researcher     notes : varchar(255)     \"\"\" In\u00a0[3]: Copied! <pre>Researcher.insert([\n    {'researcher_id': 1, 'name': 'Alice Chen', 'email': 'alice@lab.org'},\n    {'researcher_id': 2, 'name': 'Bob Smith', 'email': 'bob@lab.org'},\n])\n\nSubject.insert([\n    {'subject_id': 1, 'species': 'mouse', 'sex': 'M'},\n    {'subject_id': 2, 'species': 'mouse', 'sex': 'F'},\n    {'subject_id': 3, 'species': 'rat', 'sex': 'M'},\n])\n\nSession.insert([\n    {'subject_id': 1, 'session_date': '2024-06-01',\n     'researcher_id': 1, 'notes': 'First session'},\n    {'subject_id': 1, 'session_date': '2024-06-15',\n     'researcher_id': 1, 'notes': 'Follow-up'},\n    {'subject_id': 2, 'session_date': '2024-06-10',\n     'researcher_id': 2, 'notes': 'Initial'},\n    {'subject_id': 3, 'session_date': '2024-06-20',\n     'researcher_id': 2, 'notes': 'Rat study'},\n])\n</pre> Researcher.insert([     {'researcher_id': 1, 'name': 'Alice Chen', 'email': 'alice@lab.org'},     {'researcher_id': 2, 'name': 'Bob Smith', 'email': 'bob@lab.org'}, ])  Subject.insert([     {'subject_id': 1, 'species': 'mouse', 'sex': 'M'},     {'subject_id': 2, 'species': 'mouse', 'sex': 'F'},     {'subject_id': 3, 'species': 'rat', 'sex': 'M'}, ])  Session.insert([     {'subject_id': 1, 'session_date': '2024-06-01',      'researcher_id': 1, 'notes': 'First session'},     {'subject_id': 1, 'session_date': '2024-06-15',      'researcher_id': 1, 'notes': 'Follow-up'},     {'subject_id': 2, 'session_date': '2024-06-10',      'researcher_id': 2, 'notes': 'Initial'},     {'subject_id': 3, 'session_date': '2024-06-20',      'researcher_id': 2, 'notes': 'Rat study'}, ]) In\u00a0[4]: Copied! <pre># SQL: SELECT * FROM Subject\nSubject()\n</pre> # SQL: SELECT * FROM Subject Subject() Out[4]: None <p>subject_id</p> <p>species</p> <p>sex</p> 1 mouse M2 mouse F3 rat M <p>Total: 3</p> In\u00a0[5]: Copied! <pre># SQL: SELECT * FROM Subject WHERE sex = 'M'\nSubject &amp; {'sex': 'M'}\n</pre> # SQL: SELECT * FROM Subject WHERE sex = 'M' Subject &amp; {'sex': 'M'} Out[5]: None <p>subject_id</p> <p>species</p> <p>sex</p> 1 mouse M3 rat M <p>Total: 2</p> In\u00a0[6]: Copied! <pre># SQL: SELECT * FROM Subject WHERE species = 'mouse' AND sex = 'F'\nSubject &amp; {'species': 'mouse', 'sex': 'F'}\n</pre> # SQL: SELECT * FROM Subject WHERE species = 'mouse' AND sex = 'F' Subject &amp; {'species': 'mouse', 'sex': 'F'} Out[6]: None <p>subject_id</p> <p>species</p> <p>sex</p> 2 mouse F <p>Total: 1</p> In\u00a0[7]: Copied! <pre># SQL: SELECT * FROM Session WHERE session_date &gt; '2024-06-10'\nSession &amp; \"session_date &gt; '2024-06-10'\"\n</pre> # SQL: SELECT * FROM Session WHERE session_date &gt; '2024-06-10' Session &amp; \"session_date &gt; '2024-06-10'\" Out[7]: None <p>subject_id</p> None <p>session_date</p> <p>researcher_id</p> None <p>notes</p> 1 2024-06-15 1 Follow-up3 2024-06-20 2 Rat study <p>Total: 2</p> In\u00a0[8]: Copied! <pre># SQL: SELECT * FROM Session \n#      WHERE subject_id IN (SELECT subject_id FROM Subject WHERE species = 'mouse')\n\n# DataJoint: restrict Session by a query on Subject\nmice = Subject &amp; {'species': 'mouse'}\nSession &amp; mice\n</pre> # SQL: SELECT * FROM Session  #      WHERE subject_id IN (SELECT subject_id FROM Subject WHERE species = 'mouse')  # DataJoint: restrict Session by a query on Subject mice = Subject &amp; {'species': 'mouse'} Session &amp; mice Out[8]: None <p>subject_id</p> None <p>session_date</p> <p>researcher_id</p> None <p>notes</p> 1 2024-06-01 1 First session1 2024-06-15 1 Follow-up2 2024-06-10 2 Initial <p>Total: 3</p> In\u00a0[9]: Copied! <pre># Restrict by Alice's sessions (finds subjects she worked with)\nalice_sessions = Session &amp; (Researcher &amp; {'name': 'Alice Chen'})\nSubject &amp; alice_sessions\n</pre> # Restrict by Alice's sessions (finds subjects she worked with) alice_sessions = Session &amp; (Researcher &amp; {'name': 'Alice Chen'}) Subject &amp; alice_sessions Out[9]: None <p>subject_id</p> <p>species</p> <p>sex</p> 1 mouse M <p>Total: 1</p> In\u00a0[10]: Copied! <pre># All these operations use semantic matching on subject_id:\n\n# Join: combines Session and Subject on subject_id\nSession * Subject\n\n# Restriction: filters Session to rows matching the Subject query  \nSession &amp; (Subject &amp; {'species': 'mouse'})\n\n# Anti-restriction: Session rows NOT matching any Subject (none here, all subjects exist)\nSession - Subject\n</pre> # All these operations use semantic matching on subject_id:  # Join: combines Session and Subject on subject_id Session * Subject  # Restriction: filters Session to rows matching the Subject query   Session &amp; (Subject &amp; {'species': 'mouse'})  # Anti-restriction: Session rows NOT matching any Subject (none here, all subjects exist) Session - Subject Out[10]: None <p>subject_id</p> None <p>session_date</p> <p>researcher_id</p> None <p>notes</p> <p>Total: 0</p> In\u00a0[11]: Copied! <pre># SQL: SELECT name, email FROM Researcher\nResearcher.proj('name', 'email')\n</pre> # SQL: SELECT name, email FROM Researcher Researcher.proj('name', 'email') Out[11]: None <p>researcher_id</p> <p>name</p> <p>email</p> 1 Alice Chen alice@lab.org2 Bob Smith bob@lab.org <p>Total: 2</p> In\u00a0[12]: Copied! <pre># SQL: SELECT subject_id, species AS animal_type FROM Subject\nSubject.proj(animal_type='species')\n</pre> # SQL: SELECT subject_id, species AS animal_type FROM Subject Subject.proj(animal_type='species') Out[12]: None <p>subject_id</p> <p>species</p> <p>sex</p> 1 bytes bytes2 bytes bytes3 bytes bytes <p>Total: 3</p> In\u00a0[13]: Copied! <pre># SQL: SELECT * FROM Session JOIN Subject USING (subject_id)\nSession * Subject\n</pre> # SQL: SELECT * FROM Session JOIN Subject USING (subject_id) Session * Subject Out[13]: <p>subject_id</p> None <p>session_date</p> <p>researcher_id</p> None <p>notes</p> <p>species</p> <p>sex</p> 1 2024-06-01 1 First session mouse M1 2024-06-15 1 Follow-up mouse M2 2024-06-10 2 Initial mouse F3 2024-06-20 2 Rat study rat M <p>Total: 4</p> In\u00a0[14]: Copied! <pre># SQL: SELECT session_date, name, species \n#      FROM Session \n#      JOIN Subject USING (subject_id) \n#      JOIN Researcher USING (researcher_id)\n(Session * Subject * Researcher).proj('session_date', 'name', 'species')\n</pre> # SQL: SELECT session_date, name, species  #      FROM Session  #      JOIN Subject USING (subject_id)  #      JOIN Researcher USING (researcher_id) (Session * Subject * Researcher).proj('session_date', 'name', 'species') Out[14]: <p>subject_id</p> None <p>session_date</p> <p>species</p> <p>name</p> 1 2024-06-01 mouse Alice Chen1 2024-06-15 mouse Alice Chen2 2024-06-10 mouse Bob Smith3 2024-06-20 rat Bob Smith <p>Total: 4</p> In\u00a0[15]: Copied! <pre># SQL: SELECT subject_id, COUNT(*) as num_sessions \n#      FROM Session GROUP BY subject_id\nSubject.aggr(Session, num_sessions='count(*)')\n</pre> # SQL: SELECT subject_id, COUNT(*) as num_sessions  #      FROM Session GROUP BY subject_id Subject.aggr(Session, num_sessions='count(*)') Out[15]: <p>subject_id</p> <p>num_sessions</p> calculated attribute 1 22 13 1 <p>Total: 3</p> In\u00a0[16]: Copied! <pre># SQL: SELECT researcher_id, name, COUNT(*) as num_sessions\n#      FROM Researcher JOIN Session USING (researcher_id)\n#      GROUP BY researcher_id\nResearcher.aggr(Session, num_sessions='count(*)')\n</pre> # SQL: SELECT researcher_id, name, COUNT(*) as num_sessions #      FROM Researcher JOIN Session USING (researcher_id) #      GROUP BY researcher_id Researcher.aggr(Session, num_sessions='count(*)') Out[16]: <p>researcher_id</p> <p>num_sessions</p> calculated attribute 1 22 2 <p>Total: 2</p> In\u00a0[17]: Copied! <pre># SQL: SELECT AVG(...), COUNT(*) FROM Session (no grouping)\ndj.U().aggr(Session, total_sessions='count(*)')\n</pre> # SQL: SELECT AVG(...), COUNT(*) FROM Session (no grouping) dj.U().aggr(Session, total_sessions='count(*)') Out[17]: <p>total_sessions</p> calculated attribute 4 <p>Total: 1</p> In\u00a0[18]: Copied! <pre># SQL: SELECT * FROM Subject \n#      WHERE subject_id NOT IN (SELECT subject_id FROM Session)\n# (Subjects with no sessions)\nSubject - Session\n</pre> # SQL: SELECT * FROM Subject  #      WHERE subject_id NOT IN (SELECT subject_id FROM Session) # (Subjects with no sessions) Subject - Session Out[18]: None <p>subject_id</p> <p>species</p> <p>sex</p> <p>Total: 0</p> In\u00a0[19]: Copied! <pre># SQL: SELECT r.name, COUNT(*) as mouse_sessions\n#      FROM Researcher r\n#      JOIN Session s USING (researcher_id)\n#      JOIN Subject sub USING (subject_id)\n#      WHERE sub.species = 'mouse'\n#      GROUP BY r.researcher_id\n\nResearcher.aggr(\n    Session * (Subject &amp; {'species': 'mouse'}),\n    mouse_sessions='count(*)'\n)\n</pre> # SQL: SELECT r.name, COUNT(*) as mouse_sessions #      FROM Researcher r #      JOIN Session s USING (researcher_id) #      JOIN Subject sub USING (subject_id) #      WHERE sub.species = 'mouse' #      GROUP BY r.researcher_id  Researcher.aggr(     Session * (Subject &amp; {'species': 'mouse'}),     mouse_sessions='count(*)' ) Out[19]: <p>researcher_id</p> <p>mouse_sessions</p> calculated attribute 1 22 1 <p>Total: 2</p> In\u00a0[20]: Copied! <pre>@schema\nclass SessionAnalysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session           # depends on Session\n    ---\n    day_of_week : varchar(10)\n    \"\"\"\n    \n    def make(self, key):\n        # Fetch only from upstream tables (Session and its dependencies)\n        date = (Session &amp; key).fetch1('session_date')\n        self.insert1({**key, 'day_of_week': date.strftime('%A')})\n</pre> @schema class SessionAnalysis(dj.Computed):     definition = \"\"\"     -&gt; Session           # depends on Session     ---     day_of_week : varchar(10)     \"\"\"          def make(self, key):         # Fetch only from upstream tables (Session and its dependencies)         date = (Session &amp; key).fetch1('session_date')         self.insert1({**key, 'day_of_week': date.strftime('%A')}) In\u00a0[21]: Copied! <pre># Automatically compute for all sessions\nSessionAnalysis.populate(display_progress=True)\nSessionAnalysis()\n</pre> # Automatically compute for all sessions SessionAnalysis.populate(display_progress=True) SessionAnalysis() <pre>\rSessionAnalysis:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>\rSessionAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 144.34it/s]</pre> <pre>\n</pre> Out[21]: None <p>subject_id</p> None <p>session_date</p> None <p>day_of_week</p> 1 2024-06-01 Saturday1 2024-06-15 Saturday2 2024-06-10 Monday3 2024-06-20 Thursday <p>Total: 4</p> <p>In SQL, you'd need triggers, stored procedures, or external scheduling to achieve this.</p> In\u00a0[22]: Copied! <pre># Deleting a session would delete its computed analysis\n# (Session &amp; {'subject_id': 1, 'session_date': '2024-06-01'}).delete()  # Uncomment to try\n</pre> # Deleting a session would delete its computed analysis # (Session &amp; {'subject_id': 1, 'session_date': '2024-06-01'}).delete()  # Uncomment to try In\u00a0[23]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[23]: <ul> <li>Green = Manual (input)</li> <li>Red = Computed (derived)</li> <li>Arrows show dependency/execution order</li> </ul> In\u00a0[24]: Copied! <pre># See the generated SQL for any query\nquery = Session * Subject &amp; {'species': 'mouse'}\nprint(query.make_sql())\n</pre> # See the generated SQL for any query query = Session * Subject &amp; {'species': 'mouse'} print(query.make_sql()) <pre>SELECT \"subject_id\",\"session_date\",\"researcher_id\",\"notes\",\"species\",\"sex\" FROM \"tutorial_sql_comparison\".\"session\" JOIN \"tutorial_sql_comparison\".\"subject\" USING (\"subject_id\") WHERE ( (\"species\"='mouse'))\n</pre> In\u00a0[25]: Copied! <pre># Execute raw SQL when needed\n# result = dj.conn().query('SELECT * FROM ...')\n</pre> # Execute raw SQL when needed # result = dj.conn().query('SELECT * FROM ...') In\u00a0[26]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/advanced/sql-comparison/#datajoint-for-sql-users","title":"DataJoint for SQL Users\u00b6","text":"<p>This tutorial maps SQL concepts to DataJoint for users with relational database experience. You'll see:</p> <ul> <li>How DataJoint syntax corresponds to SQL</li> <li>What DataJoint adds beyond standard SQL</li> <li>When to use each approach</li> </ul> <p>Prerequisites: Familiarity with SQL (SELECT, JOIN, WHERE, GROUP BY).</p>"},{"location":"tutorials/advanced/sql-comparison/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#schema-definition","title":"Schema Definition\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#sql","title":"SQL\u00b6","text":"<pre>CREATE TABLE Researcher (\n    researcher_id INT NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(100),\n    PRIMARY KEY (researcher_id)\n);\n\nCREATE TABLE Subject (\n    subject_id INT NOT NULL,\n    species VARCHAR(32) NOT NULL,\n    sex ENUM('M', 'F', 'unknown'),\n    PRIMARY KEY (subject_id)\n);\n\nCREATE TABLE Session (\n    subject_id INT NOT NULL,\n    session_date DATE NOT NULL,\n    researcher_id INT NOT NULL,\n    notes VARCHAR(255),\n    PRIMARY KEY (subject_id, session_date),\n    FOREIGN KEY (subject_id) REFERENCES Subject(subject_id),\n    FOREIGN KEY (researcher_id) REFERENCES Researcher(researcher_id)\n);\n</pre>"},{"location":"tutorials/advanced/sql-comparison/#datajoint","title":"DataJoint\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#key-differences","title":"Key Differences\u00b6","text":"Aspect SQL DataJoint Primary key <code>PRIMARY KEY (...)</code> Above <code>---</code> line Foreign key <code>FOREIGN KEY ... REFERENCES</code> <code>-&gt; TableName</code> Types <code>INT</code>, <code>VARCHAR(n)</code> <code>int32</code>, <code>varchar(n)</code> Table metadata None Table tier (<code>Manual</code>, <code>Computed</code>, etc.)"},{"location":"tutorials/advanced/sql-comparison/#insert-sample-data","title":"Insert Sample Data\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#query-comparison","title":"Query Comparison\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#select-from-table","title":"SELECT * FROM table\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#where-restriction","title":"WHERE \u2014 Restriction (<code>&amp;</code>)\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#restriction-by-query-subqueries","title":"Restriction by Query (Subqueries)\u00b6","text":"<p>In SQL, you often use subqueries to filter based on another table:</p> <pre>-- Sessions with mice only\nSELECT * FROM Session \nWHERE subject_id IN (SELECT subject_id FROM Subject WHERE species = 'mouse')\n</pre> <p>In DataJoint, you simply restrict by another query \u2014 no special subquery syntax needed:</p>"},{"location":"tutorials/advanced/sql-comparison/#semantic-matching","title":"Semantic Matching\u00b6","text":"<p>A fundamental difference between SQL and DataJoint is semantic matching \u2014 the principle that attributes acquire meaning through foreign key relationships, and all binary operators use this meaning to determine how tables combine.</p>"},{"location":"tutorials/advanced/sql-comparison/#the-problem-with-sql","title":"The Problem with SQL\u00b6","text":"<p>SQL requires you to explicitly specify how tables connect:</p> <pre>SELECT * FROM Session \nJOIN Subject ON Session.subject_id = Subject.subject_id;\n</pre> <p>This is verbose and error-prone. Nothing prevents you from joining on unrelated columns that happen to share a name, or accidentally creating a Cartesian product when tables have no common columns.</p> <p>Experienced SQL programmers learn to always join through foreign key relationships. DataJoint makes this the default and enforced behavior.</p>"},{"location":"tutorials/advanced/sql-comparison/#how-semantic-matching-works","title":"How Semantic Matching Works\u00b6","text":"<p>In DataJoint, when you declare a foreign key with <code>-&gt; Subject</code>, the <code>subject_id</code> attribute in your table inherits its meaning from the <code>Subject</code> table. This meaning propagates through the foreign key graph.</p> <p>Semantic matching means: all binary operators (<code>*</code>, <code>&amp;</code>, <code>-</code>, <code>+</code>, <code>.aggr()</code>) match attributes based on shared meaning \u2014 those connected through foreign keys. If two tables have no semantically matching attributes, the operation raises an error rather than silently producing incorrect results.</p>"},{"location":"tutorials/advanced/sql-comparison/#one-join-operator-instead-of-many","title":"One Join Operator Instead of Many\u00b6","text":"<p>SQL has multiple join types (<code>INNER</code>, <code>LEFT</code>, <code>RIGHT</code>, <code>FULL OUTER</code>, <code>CROSS</code>) because it must handle arbitrary column matching. DataJoint's single join operator (<code>*</code>) is sufficient because semantic matching is more restrictive than SQL's natural joins:</p> <ul> <li>SQL natural joins match on all columns with the same name \u2014 which can accidentally match unrelated columns</li> <li>DataJoint semantic joins match only on attributes connected through foreign keys \u2014 and raise an error if you attempt to join on attributes that shouldn't be joined</li> </ul> <p>This catches errors at query time rather than producing silently incorrect results.</p>"},{"location":"tutorials/advanced/sql-comparison/#algebraic-closure","title":"Algebraic Closure\u00b6","text":"<p>In standard SQL, query results are just \"bags of rows\" \u2014 they don't have a defined entity type. You cannot know what kind of thing each row represents without external context.</p> <p>DataJoint achieves algebraic closure: every query result is a valid entity set with a well-defined entity type. You always know what kind of entity the result represents, identified by a specific primary key. This means:</p> <ol> <li>Every operator returns a valid relation \u2014 not just rows, but a set of entities of a known type</li> <li>Operators compose indefinitely \u2014 you can chain any sequence of operations</li> <li>Results remain queryable \u2014 a query result can be used as an operand in further operations</li> </ol> <p>The entity type (and its primary key) is determined by precise rules based on the operator and the functional dependencies between operands. See the Primary Keys specification for details.</p>"},{"location":"tutorials/advanced/sql-comparison/#select-columns-projection-proj","title":"SELECT columns \u2014 Projection (<code>.proj()</code>)\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#join","title":"JOIN\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#group-by-aggregation-aggr","title":"GROUP BY \u2014 Aggregation (<code>.aggr()</code>)\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#not-in-negative-restriction-","title":"NOT IN \u2014 Negative Restriction (<code>-</code>)\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#combined-example","title":"Combined Example\u00b6","text":""},{"location":"tutorials/advanced/sql-comparison/#operator-reference","title":"Operator Reference\u00b6","text":"SQL DataJoint Notes <code>SELECT *</code> <code>Table()</code> Display table <code>SELECT cols</code> <code>.proj('col1', 'col2')</code> Projection <code>SELECT col AS alias</code> <code>.proj(alias='col')</code> Rename <code>WHERE condition</code> <code>&amp; {'col': value}</code> or <code>&amp; 'expr'</code> Restriction <code>JOIN ... USING</code> <code>Table1 * Table2</code> Natural join <code>GROUP BY ... AGG()</code> <code>.aggr(Table, alias='agg()')</code> Aggregation <code>NOT IN (subquery)</code> <code>Table1 - Table2</code> Anti-restriction <code>UNION</code> <code>Table1 + Table2</code> Union"},{"location":"tutorials/advanced/sql-comparison/#what-datajoint-adds","title":"What DataJoint Adds\u00b6","text":"<p>DataJoint is not just \"Python syntax for SQL.\" It adds:</p>"},{"location":"tutorials/advanced/sql-comparison/#1-table-tiers","title":"1. Table Tiers\u00b6","text":"<p>Tables are classified by their role in the workflow:</p> Tier Purpose SQL Equivalent <code>Lookup</code> Reference data, parameters Regular table <code>Manual</code> User-entered data Regular table <code>Imported</code> Data from external files Regular table + trigger <code>Computed</code> Derived results Materialized view + trigger"},{"location":"tutorials/advanced/sql-comparison/#2-automatic-computation","title":"2. Automatic Computation\u00b6","text":"<p>Computed tables have a <code>make()</code> method that runs automatically. An important principle: <code>make()</code> should only fetch data from upstream tables \u2014 those declared as dependencies in the table definition.</p>"},{"location":"tutorials/advanced/sql-comparison/#3-cascading-deletes","title":"3. Cascading Deletes\u00b6","text":"<p>DataJoint enforces referential integrity with automatic cascading:</p>"},{"location":"tutorials/advanced/sql-comparison/#4-schema-as-workflow","title":"4. Schema as Workflow\u00b6","text":"<p>The diagram shows the computational workflow, not just relationships:</p>"},{"location":"tutorials/advanced/sql-comparison/#5-object-storage-integration","title":"5. Object Storage Integration\u00b6","text":"<p>Store large objects (arrays, files) with relational semantics:</p> <pre>class Recording(dj.Imported):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    raw_data : &lt;blob&gt;       # NumPy array stored in database\n    video : &lt;blob@storage&gt;  # Large file stored externally\n    \"\"\"\n</pre> <p>SQL has no standard way to handle this.</p>"},{"location":"tutorials/advanced/sql-comparison/#when-to-use-raw-sql","title":"When to Use Raw SQL\u00b6","text":"<p>DataJoint generates SQL under the hood. Sometimes raw SQL is useful:</p>"},{"location":"tutorials/advanced/sql-comparison/#summary","title":"Summary\u00b6","text":"Feature SQL DataJoint Query language SQL strings Python operators Schema definition DDL Python classes Foreign keys Manual declaration <code>-&gt;</code> syntax Table purpose Implicit Explicit tiers Automatic computation Triggers/procedures <code>populate()</code> Large objects BLOBs (limited) Codec system Workflow visualization None <code>dj.Diagram()</code> <p>DataJoint uses SQL databases (MySQL/PostgreSQL) underneath but provides:</p> <ul> <li>Pythonic syntax for queries</li> <li>Workflow semantics for scientific pipelines</li> <li>Automatic computation via <code>populate()</code></li> <li>Object storage for large scientific data</li> </ul>"},{"location":"tutorials/basics/01-first-pipeline/","title":"A Simple Pipeline","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\n\nschema = dj.Schema('tutorial_first_pipeline')\n</pre> import datajoint as dj  schema = dj.Schema('tutorial_first_pipeline') <pre>[2026-01-27 15:28:43] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Researcher(dj.Manual):\n    definition = \"\"\"\n    researcher_id : int32\n    ---\n    researcher_name : varchar(100)\n    email : varchar(100)\n    \"\"\"\n</pre> @schema class Researcher(dj.Manual):     definition = \"\"\"     researcher_id : int32     ---     researcher_name : varchar(100)     email : varchar(100)     \"\"\" In\u00a0[3]: Copied! <pre>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : int32\n    ---\n    species : varchar(32)\n    date_of_birth : date\n    sex : enum('M', 'F', 'unknown')\n    \"\"\"\n</pre> @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : int32     ---     species : varchar(32)     date_of_birth : date     sex : enum('M', 'F', 'unknown')     \"\"\" In\u00a0[4]: Copied! <pre>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_date : date\n    ---\n    -&gt; Researcher\n    session_notes : varchar(255)\n    \"\"\"\n</pre> @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_date : date     ---     -&gt; Researcher     session_notes : varchar(255)     \"\"\" <p>The <code>-&gt; Subject</code> in the primary key means:</p> <ul> <li><code>subject_id</code> is automatically included in Session's primary key</li> <li>Combined with <code>session_date</code>, each session is uniquely identified</li> <li>You cannot create a session for a non-existent subject</li> </ul> <p>The <code>-&gt; Researcher</code> below the line is a non-primary dependency\u2014it records who ran the session but isn't part of the unique identifier.</p> In\u00a0[5]: Copied! <pre>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    recording_id : int16\n    ---\n    duration : decimal(5,1)     # recording duration (seconds)\n    quality : enum('good', 'fair', 'poor')\n    \"\"\"\n</pre> @schema class Recording(dj.Manual):     definition = \"\"\"     -&gt; Session     recording_id : int16     ---     duration : decimal(5,1)     # recording duration (seconds)     quality : enum('good', 'fair', 'poor')     \"\"\" In\u00a0[6]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[6]: <p>Reading the diagram:</p> <ul> <li>Green boxes = Manual tables (you enter data)</li> <li>Solid lines = Primary key dependencies (part of identity)</li> <li>Dashed lines = Non-primary dependencies (references)</li> </ul> In\u00a0[7]: Copied! <pre># First: tables with no dependencies\nResearcher.insert([\n    {'researcher_id': 1, 'researcher_name': 'Alice Chen',\n     'email': 'alice@lab.org'},\n    {'researcher_id': 2, 'researcher_name': 'Bob Smith',\n     'email': 'bob@lab.org'},\n])\n\nSubject.insert([\n    {'subject_id': 1, 'species': 'mouse',\n     'date_of_birth': '2024-01-15', 'sex': 'M'},\n    {'subject_id': 2, 'species': 'mouse',\n     'date_of_birth': '2024-01-20', 'sex': 'F'},\n    {'subject_id': 3, 'species': 'mouse',\n     'date_of_birth': '2024-02-01', 'sex': 'M'},\n])\n</pre> # First: tables with no dependencies Researcher.insert([     {'researcher_id': 1, 'researcher_name': 'Alice Chen',      'email': 'alice@lab.org'},     {'researcher_id': 2, 'researcher_name': 'Bob Smith',      'email': 'bob@lab.org'}, ])  Subject.insert([     {'subject_id': 1, 'species': 'mouse',      'date_of_birth': '2024-01-15', 'sex': 'M'},     {'subject_id': 2, 'species': 'mouse',      'date_of_birth': '2024-01-20', 'sex': 'F'},     {'subject_id': 3, 'species': 'mouse',      'date_of_birth': '2024-02-01', 'sex': 'M'}, ]) In\u00a0[8]: Copied! <pre># Then: tables that depend on others\nSession.insert([\n    {'subject_id': 1, 'session_date': '2024-06-01',\n     'researcher_id': 1, 'session_notes': 'First session'},\n    {'subject_id': 1, 'session_date': '2024-06-15',\n     'researcher_id': 1, 'session_notes': 'Follow-up'},\n    {'subject_id': 2, 'session_date': '2024-06-10',\n     'researcher_id': 2, 'session_notes': 'Initial recording'},\n])\n</pre> # Then: tables that depend on others Session.insert([     {'subject_id': 1, 'session_date': '2024-06-01',      'researcher_id': 1, 'session_notes': 'First session'},     {'subject_id': 1, 'session_date': '2024-06-15',      'researcher_id': 1, 'session_notes': 'Follow-up'},     {'subject_id': 2, 'session_date': '2024-06-10',      'researcher_id': 2, 'session_notes': 'Initial recording'}, ]) In\u00a0[9]: Copied! <pre># Finally: tables at the bottom of the hierarchy\nRecording.insert([\n    {'subject_id': 1, 'session_date': '2024-06-01', 'recording_id': 1,\n     'duration': 300.5, 'quality': 'good'},\n    {'subject_id': 1, 'session_date': '2024-06-01', 'recording_id': 2,\n     'duration': 450.0, 'quality': 'good'},\n    {'subject_id': 1, 'session_date': '2024-06-15', 'recording_id': 1,\n     'duration': 600.0, 'quality': 'fair'},\n    {'subject_id': 2, 'session_date': '2024-06-10', 'recording_id': 1,\n     'duration': 350.0, 'quality': 'good'},\n])\n</pre> # Finally: tables at the bottom of the hierarchy Recording.insert([     {'subject_id': 1, 'session_date': '2024-06-01', 'recording_id': 1,      'duration': 300.5, 'quality': 'good'},     {'subject_id': 1, 'session_date': '2024-06-01', 'recording_id': 2,      'duration': 450.0, 'quality': 'good'},     {'subject_id': 1, 'session_date': '2024-06-15', 'recording_id': 1,      'duration': 600.0, 'quality': 'fair'},     {'subject_id': 2, 'session_date': '2024-06-10', 'recording_id': 1,      'duration': 350.0, 'quality': 'good'}, ]) In\u00a0[10]: Copied! <pre>Subject()\n</pre> Subject() Out[10]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> 1 mouse 2024-01-15 M2 mouse 2024-01-20 F3 mouse 2024-02-01 M <p>Total: 3</p> In\u00a0[11]: Copied! <pre>Recording()\n</pre> Recording() Out[11]: None <p>subject_id</p> None <p>session_date</p> None <p>recording_id</p> <p>duration</p> recording duration (seconds) <p>quality</p> 1 2024-06-01 1 300.5 good1 2024-06-01 2 450.0 good1 2024-06-15 1 600.0 fair2 2024-06-10 1 350.0 good <p>Total: 4</p> In\u00a0[12]: Copied! <pre># Subjects that are male\nSubject &amp; {'sex': 'M'}\n</pre> # Subjects that are male Subject &amp; {'sex': 'M'} Out[12]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> 1 mouse 2024-01-15 M3 mouse 2024-02-01 M <p>Total: 2</p> In\u00a0[13]: Copied! <pre># Recordings with good quality\nRecording &amp; {'quality': 'good'}\n</pre> # Recordings with good quality Recording &amp; {'quality': 'good'} Out[13]: None <p>subject_id</p> None <p>session_date</p> None <p>recording_id</p> <p>duration</p> recording duration (seconds) <p>quality</p> 1 2024-06-01 1 300.5 good1 2024-06-01 2 450.0 good2 2024-06-10 1 350.0 good <p>Total: 3</p> In\u00a0[14]: Copied! <pre># Sessions for subject 1\nSession &amp; {'subject_id': 1}\n</pre> # Sessions for subject 1 Session &amp; {'subject_id': 1} Out[14]: None <p>subject_id</p> None <p>session_date</p> <p>researcher_id</p> None <p>session_notes</p> 1 2024-06-01 1 First session1 2024-06-15 1 Follow-up <p>Total: 2</p> In\u00a0[15]: Copied! <pre># Just names and emails\nResearcher.proj('researcher_name', 'email')\n</pre> # Just names and emails Researcher.proj('researcher_name', 'email') Out[15]: None <p>researcher_id</p> <p>researcher_name</p> <p>email</p> 1 Alice Chen alice@lab.org2 Bob Smith bob@lab.org <p>Total: 2</p> In\u00a0[16]: Copied! <pre># Compute duration in minutes\nRecording.proj(duration_min='duration / 60')\n</pre> # Compute duration in minutes Recording.proj(duration_min='duration / 60') Out[16]: None <p>subject_id</p> None <p>session_date</p> None <p>recording_id</p> <p>duration</p> recording duration (seconds) <p>quality</p> 1 2024-06-01 1 bytes bytes1 2024-06-01 2 bytes bytes1 2024-06-15 1 bytes bytes2 2024-06-10 1 bytes bytes <p>Total: 4</p> In\u00a0[17]: Copied! <pre># Sessions with subject info\nSession * Subject\n</pre> # Sessions with subject info Session * Subject Out[17]: <p>subject_id</p> None <p>session_date</p> <p>researcher_id</p> None <p>session_notes</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> 1 2024-06-01 1 First session mouse 2024-01-15 M1 2024-06-15 1 Follow-up mouse 2024-01-15 M2 2024-06-10 2 Initial recording mouse 2024-01-20 F <p>Total: 3</p> In\u00a0[18]: Copied! <pre># Full recording details with subject and researcher\n(Recording * Session * Subject * Researcher).proj(\n    'researcher_name', 'species', 'duration', 'quality'\n)\n</pre> # Full recording details with subject and researcher (Recording * Session * Subject * Researcher).proj(     'researcher_name', 'species', 'duration', 'quality' ) Out[18]: <p>subject_id</p> None <p>session_date</p> None <p>recording_id</p> <p>duration</p> recording duration (seconds) <p>quality</p> <p>species</p> <p>researcher_name</p> 1 2024-06-01 1 300.5 good mouse Alice Chen1 2024-06-01 2 450.0 good mouse Alice Chen1 2024-06-15 1 600.0 fair mouse Alice Chen2 2024-06-10 1 350.0 good mouse Bob Smith <p>Total: 4</p> In\u00a0[19]: Copied! <pre># Count recordings per session\nSession.aggr(Recording, num_recordings='count(*)')\n</pre> # Count recordings per session Session.aggr(Recording, num_recordings='count(*)') Out[19]: <p>subject_id</p> None <p>session_date</p> <p>num_recordings</p> calculated attribute 1 2024-06-01 21 2024-06-15 12 2024-06-10 1 <p>Total: 3</p> In\u00a0[20]: Copied! <pre># Total recording time per subject\nSubject.aggr(Recording, total_duration='sum(duration)')\n</pre> # Total recording time per subject Subject.aggr(Recording, total_duration='sum(duration)') Out[20]: <p>subject_id</p> <p>total_duration</p> calculated attribute 1 1350.52 350.03 None <p>Total: 3</p> In\u00a0[21]: Copied! <pre># Average duration across all recordings\ndj.U().aggr(Recording, avg_duration='avg(duration)')\n</pre> # Average duration across all recordings dj.U().aggr(Recording, avg_duration='avg(duration)') Out[21]: <p>avg_duration</p> calculated attribute 425.1250000000000000 <p>Total: 1</p> In\u00a0[22]: Copied! <pre># Good-quality recordings for male subjects, with researcher name\n(\n    Recording \n    &amp; {'quality': 'good'} \n    &amp; (Subject &amp; {'sex': 'M'})\n) * Session * Researcher.proj('researcher_name')\n</pre> # Good-quality recordings for male subjects, with researcher name (     Recording      &amp; {'quality': 'good'}      &amp; (Subject &amp; {'sex': 'M'}) ) * Session * Researcher.proj('researcher_name') Out[22]: <p>subject_id</p> None <p>session_date</p> None <p>recording_id</p> <p>duration</p> recording duration (seconds) <p>quality</p> <p>researcher_id</p> None <p>session_notes</p> <p>researcher_name</p> 1 2024-06-01 1 300.5 good 1 First session Alice Chen1 2024-06-01 2 450.0 good 1 First session Alice Chen <p>Total: 2</p> In\u00a0[23]: Copied! <pre># Count of good recordings per researcher\nResearcher.aggr(\n    Session * (Recording &amp; {'quality': 'good'}),\n    good_recordings='count(*)'\n)\n</pre> # Count of good recordings per researcher Researcher.aggr(     Session * (Recording &amp; {'quality': 'good'}),     good_recordings='count(*)' ) Out[23]: <p>researcher_id</p> <p>good_recordings</p> calculated attribute 1 22 1 <p>Total: 2</p> In\u00a0[24]: Copied! <pre># Fetch as list of dicts\nSubject.to_dicts()\n</pre> # Fetch as list of dicts Subject.to_dicts() Out[24]: <pre>[{'subject_id': 1,\n  'species': 'mouse',\n  'date_of_birth': datetime.date(2024, 1, 15),\n  'sex': 'M'},\n {'subject_id': 2,\n  'species': 'mouse',\n  'date_of_birth': datetime.date(2024, 1, 20),\n  'sex': 'F'},\n {'subject_id': 3,\n  'species': 'mouse',\n  'date_of_birth': datetime.date(2024, 2, 1),\n  'sex': 'M'}]</pre> In\u00a0[25]: Copied! <pre># Fetch specific attributes as arrays\ndurations = (Recording &amp; {'quality': 'good'}).to_arrays('duration')\nprint(f\"Good recording durations: {durations}\")\n</pre> # Fetch specific attributes as arrays durations = (Recording &amp; {'quality': 'good'}).to_arrays('duration') print(f\"Good recording durations: {durations}\") <pre>Good recording durations: [Decimal('300.5') Decimal('450.0') Decimal('350.0')]\n</pre> In\u00a0[26]: Copied! <pre># Fetch one row\none_subject = (Subject &amp; {'subject_id': 1}).fetch1()\nprint(f\"Subject 1: {one_subject}\")\n</pre> # Fetch one row one_subject = (Subject &amp; {'subject_id': 1}).fetch1() print(f\"Subject 1: {one_subject}\") <pre>Subject 1: {'subject_id': 1, 'species': 'mouse', 'date_of_birth': datetime.date(2024, 1, 15), 'sex': 'M'}\n</pre> In\u00a0[27]: Copied! <pre># This would delete the session AND all its recordings\n# (Session &amp; {'subject_id': 2, 'session_date': '2024-06-10'}).delete()\n\n# Uncomment to try (will prompt for confirmation)\n</pre> # This would delete the session AND all its recordings # (Session &amp; {'subject_id': 2, 'session_date': '2024-06-10'}).delete()  # Uncomment to try (will prompt for confirmation) In\u00a0[28]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/basics/01-first-pipeline/#a-simple-pipeline","title":"A Simple Pipeline\u00b6","text":"<p>This tutorial introduces DataJoint by building a simple research lab database. You'll learn to:</p> <ul> <li>Define tables with primary keys and dependencies</li> <li>Insert and query data</li> <li>Use the four core operations: restriction, projection, join, aggregation</li> <li>Understand the schema diagram</li> </ul> <p>We'll work with Manual tables only\u2014tables where you enter data directly. Later tutorials introduce automated computation.</p> <p>Database Backend: All tutorials work identically on MySQL and PostgreSQL (PostgreSQL support added in DataJoint Python 2.1). The examples shown here were executed on PostgreSQL, but you can follow along using either backend.</p> <p>For complete working examples, see:</p> <ul> <li>University Database \u2014 Academic records with complex queries</li> <li>Blob Detection \u2014 Image processing with computation</li> </ul>"},{"location":"tutorials/basics/01-first-pipeline/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/basics/01-first-pipeline/#the-domain-a-research-lab","title":"The Domain: A Research Lab\u00b6","text":"<p>We'll model a research lab that:</p> <ul> <li>Has researchers who conduct experiments</li> <li>Works with subjects (e.g., mice)</li> <li>Runs sessions where data is collected</li> <li>Collects recordings during each session</li> </ul> <pre>\nflowchart TD\n    Researcher --&gt; Session\n    Subject --&gt; Session\n    Session --&gt; Recording\n</pre>"},{"location":"tutorials/basics/01-first-pipeline/#defining-tables","title":"Defining Tables\u00b6","text":"<p>Each table is a Python class. The <code>definition</code> string specifies:</p> <ul> <li>Primary key (above <code>---</code>) \u2014 uniquely identifies each row</li> <li>Attributes (below <code>---</code>) \u2014 additional data for each row</li> <li>Dependencies (<code>-&gt;</code>) \u2014 references to other tables</li> </ul>"},{"location":"tutorials/basics/01-first-pipeline/#dependencies","title":"Dependencies\u00b6","text":"<p>A <code>Session</code> involves one researcher and one subject. The <code>-&gt;</code> syntax creates a dependency (foreign key):</p>"},{"location":"tutorials/basics/01-first-pipeline/#schema-diagram","title":"Schema Diagram\u00b6","text":"<p>The diagram shows tables and their dependencies:</p>"},{"location":"tutorials/basics/01-first-pipeline/#inserting-data","title":"Inserting Data\u00b6","text":"<p>Data must be inserted in dependency order\u2014you can't reference something that doesn't exist.</p>"},{"location":"tutorials/basics/01-first-pipeline/#viewing-data","title":"Viewing Data\u00b6","text":"<p>Display a table by calling it:</p>"},{"location":"tutorials/basics/01-first-pipeline/#the-four-core-operations","title":"The Four Core Operations\u00b6","text":"<p>DataJoint queries use four fundamental operations. These compose to answer any question about your data.</p>"},{"location":"tutorials/basics/01-first-pipeline/#1-restriction-filter-rows","title":"1. Restriction (<code>&amp;</code>) \u2014 Filter rows\u00b6","text":"<p>Keep only rows matching a condition:</p>"},{"location":"tutorials/basics/01-first-pipeline/#2-projection-proj-select-columns","title":"2. Projection (<code>.proj()</code>) \u2014 Select columns\u00b6","text":"<p>Choose which attributes to return, or compute new ones:</p>"},{"location":"tutorials/basics/01-first-pipeline/#3-join-combine-tables","title":"3. Join (<code>*</code>) \u2014 Combine tables\u00b6","text":"<p>Merge data from related tables:</p>"},{"location":"tutorials/basics/01-first-pipeline/#4-aggregation-aggr-summarize-groups","title":"4. Aggregation (<code>.aggr()</code>) \u2014 Summarize groups\u00b6","text":"<p>Compute statistics across groups of rows:</p>"},{"location":"tutorials/basics/01-first-pipeline/#combining-operations","title":"Combining Operations\u00b6","text":"<p>Operations chain together to answer complex questions:</p>"},{"location":"tutorials/basics/01-first-pipeline/#fetching-data","title":"Fetching Data\u00b6","text":"<p>To get data into Python, use fetch methods:</p>"},{"location":"tutorials/basics/01-first-pipeline/#deleting-data","title":"Deleting Data\u00b6","text":"<p>Deleting respects dependencies\u2014downstream data is deleted automatically:</p>"},{"location":"tutorials/basics/01-first-pipeline/#summary","title":"Summary\u00b6","text":"<p>You've learned the fundamentals of DataJoint:</p> Concept Description Tables Python classes with a <code>definition</code> string Primary key Above <code>---</code>, uniquely identifies rows Dependencies <code>-&gt;</code> creates foreign keys Restriction <code>&amp;</code> filters rows Projection <code>.proj()</code> selects/computes columns Join <code>*</code> combines tables Aggregation <code>.aggr()</code> summarizes groups"},{"location":"tutorials/basics/01-first-pipeline/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Schema Design \u2014 Primary keys, relationships, table tiers</li> <li>Queries \u2014 Advanced query patterns</li> <li>Computation \u2014 Automated processing with Imported/Computed tables</li> </ul>"},{"location":"tutorials/basics/01-first-pipeline/#complete-examples","title":"Complete Examples\u00b6","text":"<ul> <li>University Database \u2014 Complex queries on academic records</li> <li>Blob Detection \u2014 Image processing pipeline with computation</li> </ul>"},{"location":"tutorials/basics/02-schema-design/","title":"Schema Design","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\n\nschema = dj.Schema('tutorial_design')\n</pre> import datajoint as dj  schema = dj.Schema('tutorial_design') <pre>[2026-01-27 15:28:46] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"\n    # Research laboratory\n    lab_id : varchar(16)            # short identifier (e.g., 'tolias')\n    ---\n    lab_name : varchar(100)\n    institution : varchar(100)\n    created_at = CURRENT_TIMESTAMP : datetime   # when record was created\n    \"\"\"\n\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    # Experimental subject\n    subject_id : varchar(16)\n    ---\n    -&gt; Lab\n    species : varchar(50)\n    date_of_birth : date\n    sex : enum('M', 'F', 'U')\n    \"\"\"\n</pre> @schema class Lab(dj.Manual):     definition = \"\"\"     # Research laboratory     lab_id : varchar(16)            # short identifier (e.g., 'tolias')     ---     lab_name : varchar(100)     institution : varchar(100)     created_at = CURRENT_TIMESTAMP : datetime   # when record was created     \"\"\"  @schema class Subject(dj.Manual):     definition = \"\"\"     # Experimental subject     subject_id : varchar(16)     ---     -&gt; Lab     species : varchar(50)     date_of_birth : date     sex : enum('M', 'F', 'U')     \"\"\" In\u00a0[3]: Copied! <pre>@schema\nclass TaskType(dj.Lookup):\n    definition = \"\"\"\n    # Types of behavioral tasks\n    task_type : varchar(32)\n    ---\n    description : varchar(255)\n    \"\"\"\n    contents = [\n        {'task_type': 'go_nogo', 'description': 'Go/No-Go discrimination task'},\n        {'task_type': '2afc', 'description': 'Two-alternative forced choice'},\n        {'task_type': 'foraging', 'description': 'Foraging/exploration task'},\n    ]\n\n@schema\nclass SessionStatus(dj.Lookup):\n    definition = \"\"\"\n    # Session status codes\n    status : varchar(16)\n    \"\"\"\n    contents = [\n        {'status': 'scheduled'},\n        {'status': 'in_progress'},\n        {'status': 'completed'},\n        {'status': 'aborted'},\n    ]\n</pre> @schema class TaskType(dj.Lookup):     definition = \"\"\"     # Types of behavioral tasks     task_type : varchar(32)     ---     description : varchar(255)     \"\"\"     contents = [         {'task_type': 'go_nogo', 'description': 'Go/No-Go discrimination task'},         {'task_type': '2afc', 'description': 'Two-alternative forced choice'},         {'task_type': 'foraging', 'description': 'Foraging/exploration task'},     ]  @schema class SessionStatus(dj.Lookup):     definition = \"\"\"     # Session status codes     status : varchar(16)     \"\"\"     contents = [         {'status': 'scheduled'},         {'status': 'in_progress'},         {'status': 'completed'},         {'status': 'aborted'},     ] In\u00a0[4]: Copied! <pre># Lookup tables are automatically populated\nTaskType()\n</pre> # Lookup tables are automatically populated TaskType() Out[4]: Types of behavioral tasks <p>task_type</p> <p>description</p> 2afc Two-alternative forced choiceforaging Foraging/exploration taskgo_nogo Go/No-Go discrimination task <p>Total: 3</p> In\u00a0[5]: Copied! <pre>@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    # Experimental session\n    -&gt; Subject\n    session_idx : int32            # session number for this subject\n    ---\n    -&gt; TaskType\n    -&gt; SessionStatus\n    session_date : date\n    session_notes = '' : varchar(1000)\n    task_params = NULL : json       # task-specific parameters (nullable)\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        # Individual trial within a session\n        -&gt; master\n        trial_idx : int32\n        ---\n        stimulus : varchar(50)\n        response : varchar(50)\n        correct : bool\n        reaction_time : decimal(3,2)  # seconds\n        \"\"\"\n</pre> @schema class Session(dj.Manual):     definition = \"\"\"     # Experimental session     -&gt; Subject     session_idx : int32            # session number for this subject     ---     -&gt; TaskType     -&gt; SessionStatus     session_date : date     session_notes = '' : varchar(1000)     task_params = NULL : json       # task-specific parameters (nullable)     \"\"\"      class Trial(dj.Part):         definition = \"\"\"         # Individual trial within a session         -&gt; master         trial_idx : int32         ---         stimulus : varchar(50)         response : varchar(50)         correct : bool         reaction_time : decimal(3,2)  # seconds         \"\"\" <p>The primary key of <code>Session</code> is <code>(subject_id, session_idx)</code> \u2014 a composite key. This means:</p> <ul> <li>Each subject can have multiple sessions (1, 2, 3, ...)</li> <li>Session 1 for subject A is different from session 1 for subject B</li> </ul> In\u00a0[6]: Copied! <pre># Let's insert some data to see how foreign keys work\nLab.insert1({\n    'lab_id': 'tolias',\n    'lab_name': 'Tolias Lab',\n    'institution': 'Baylor College of Medicine'\n})\n# Note: created_at is auto-populated with CURRENT_TIMESTAMP\n\nSubject.insert1({\n    'subject_id': 'M001',\n    'lab_id': 'tolias',\n    'species': 'Mus musculus',\n    'date_of_birth': '2026-01-15',\n    'sex': 'M'\n})\n\nSubject()\n</pre> # Let's insert some data to see how foreign keys work Lab.insert1({     'lab_id': 'tolias',     'lab_name': 'Tolias Lab',     'institution': 'Baylor College of Medicine' }) # Note: created_at is auto-populated with CURRENT_TIMESTAMP  Subject.insert1({     'subject_id': 'M001',     'lab_id': 'tolias',     'species': 'Mus musculus',     'date_of_birth': '2026-01-15',     'sex': 'M' })  Subject() Out[6]: Experimental subject <p>subject_id</p> <p>lab_id</p> None <p>species</p> <p>date_of_birth</p> <p>sex</p> M001 tolias Mus musculus 2026-01-15 M <p>Total: 1</p> In\u00a0[7]: Copied! <pre># Insert sessions for this subject\nSession.insert([\n    {'subject_id': 'M001', 'session_idx': 1, 'task_type': 'go_nogo', \n     'status': 'completed', 'session_date': '2026-01-06',\n     'task_params': {'go_probability': 0.5, 'timeout_sec': 2.0}},\n    {'subject_id': 'M001', 'session_idx': 2, 'task_type': 'go_nogo',\n     'status': 'completed', 'session_date': '2026-01-07',\n     'task_params': {'go_probability': 0.7, 'timeout_sec': 1.5}},\n    {'subject_id': 'M001', 'session_idx': 3, 'task_type': '2afc',\n     'status': 'in_progress', 'session_date': '2026-01-08',\n     'task_params': None},  # NULL - no parameters for this session\n])\n\nSession()\n</pre> # Insert sessions for this subject Session.insert([     {'subject_id': 'M001', 'session_idx': 1, 'task_type': 'go_nogo',       'status': 'completed', 'session_date': '2026-01-06',      'task_params': {'go_probability': 0.5, 'timeout_sec': 2.0}},     {'subject_id': 'M001', 'session_idx': 2, 'task_type': 'go_nogo',      'status': 'completed', 'session_date': '2026-01-07',      'task_params': {'go_probability': 0.7, 'timeout_sec': 1.5}},     {'subject_id': 'M001', 'session_idx': 3, 'task_type': '2afc',      'status': 'in_progress', 'session_date': '2026-01-08',      'task_params': None},  # NULL - no parameters for this session ])  Session() Out[7]: Experimental session <p>subject_id</p> None <p>session_idx</p> session number for this subject <p>task_type</p> None <p>status</p> None <p>session_date</p> <p>session_notes</p> <p>task_params</p> task-specific parameters (nullable) M001 1 go_nogo completed 2026-01-06 jsonM001 2 go_nogo completed 2026-01-07 jsonM001 3 2afc in_progress 2026-01-08 json <p>Total: 3</p> In\u00a0[8]: Copied! <pre># This would fail - referential integrity prevents invalid foreign keys\ntry:\n    Session.insert1({'subject_id': 'INVALID', 'session_idx': 1, \n                     'task_type': 'go_nogo', 'status': 'completed',\n                     'session_date': '2026-01-06'})\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n    print(\"Cannot insert session for non-existent subject!\")\n</pre> # This would fail - referential integrity prevents invalid foreign keys try:     Session.insert1({'subject_id': 'INVALID', 'session_idx': 1,                       'task_type': 'go_nogo', 'status': 'completed',                      'session_date': '2026-01-06'}) except Exception as e:     print(f\"Error: {type(e).__name__}\")     print(\"Cannot insert session for non-existent subject!\") <pre>Error: IntegrityError\nCannot insert session for non-existent subject!\n</pre> In\u00a0[9]: Copied! <pre># Access the part table\nSession.Trial()\n</pre> # Access the part table Session.Trial() Out[9]: Individual trial within a session <p>subject_id</p> None <p>session_idx</p> None <p>trial_idx</p> <p>stimulus</p> <p>response</p> <p>correct</p> <p>reaction_time</p> seconds <p>Total: 0</p> In\u00a0[10]: Copied! <pre>@schema\nclass SessionSummary(dj.Computed):\n    definition = \"\"\"\n    # Summary statistics for a session\n    -&gt; Session\n    ---\n    num_trials : int32\n    num_correct : int32\n    accuracy : float32\n    mean_reaction_time : float32\n    \"\"\"\n    \n    def make(self, key):\n        correct_vals, rt_vals = (Session.Trial &amp; key).to_arrays('correct', 'reaction_time')\n        rt_vals = rt_vals.astype(float)  # convert decimal to float\n        n_trials = len(correct_vals)\n        n_correct = sum(correct_vals) if n_trials else 0\n\n        self.insert1({\n            **key,\n            'num_trials': n_trials,\n            'num_correct': n_correct,\n            'accuracy': n_correct / n_trials if n_trials else 0.0,\n            'mean_reaction_time': sum(rt_vals) / n_trials if n_trials else 0.0\n        })\n</pre> @schema class SessionSummary(dj.Computed):     definition = \"\"\"     # Summary statistics for a session     -&gt; Session     ---     num_trials : int32     num_correct : int32     accuracy : float32     mean_reaction_time : float32     \"\"\"          def make(self, key):         correct_vals, rt_vals = (Session.Trial &amp; key).to_arrays('correct', 'reaction_time')         rt_vals = rt_vals.astype(float)  # convert decimal to float         n_trials = len(correct_vals)         n_correct = sum(correct_vals) if n_trials else 0          self.insert1({             **key,             'num_trials': n_trials,             'num_correct': n_correct,             'accuracy': n_correct / n_trials if n_trials else 0.0,             'mean_reaction_time': sum(rt_vals) / n_trials if n_trials else 0.0         }) In\u00a0[11]: Copied! <pre>@schema\nclass Experimenter(dj.Manual):\n    definition = \"\"\"\n    # Lab member who runs experiments\n    experimenter_id : uuid          # anonymized identifier\n    ---\n    full_name : varchar(100)\n    email = '' : varchar(100)\n    \"\"\"\n\n@schema\nclass SessionExperimenter(dj.Manual):\n    definition = \"\"\"\n    # Links sessions to experimenters (optional)\n    -&gt; Session\n    ---\n    -&gt; [nullable] Experimenter      # experimenter may be unknown\n    \"\"\"\n</pre> @schema class Experimenter(dj.Manual):     definition = \"\"\"     # Lab member who runs experiments     experimenter_id : uuid          # anonymized identifier     ---     full_name : varchar(100)     email = '' : varchar(100)     \"\"\"  @schema class SessionExperimenter(dj.Manual):     definition = \"\"\"     # Links sessions to experimenters (optional)     -&gt; Session     ---     -&gt; [nullable] Experimenter      # experimenter may be unknown     \"\"\" In\u00a0[12]: Copied! <pre>@schema\nclass Protocol(dj.Lookup):\n    definition = \"\"\"\n    # Experimental protocols\n    protocol_id : varchar(32)\n    ---\n    protocol_name : varchar(100)\n    version : varchar(16)\n    \"\"\"\n    contents = [\n        {'protocol_id': 'iacuc_2024_01', 'protocol_name': 'Mouse Behavior', 'version': '1.0'},\n        {'protocol_id': 'iacuc_2024_02', 'protocol_name': 'Imaging Protocol', 'version': '2.1'},\n    ]\n\n@schema \nclass SubjectProtocol(dj.Manual):\n    definition = \"\"\"\n    # Protocols assigned to subjects (many-to-many)\n    -&gt; Subject\n    -&gt; Protocol\n    ---\n    assignment_date : date\n    \"\"\"\n</pre> @schema class Protocol(dj.Lookup):     definition = \"\"\"     # Experimental protocols     protocol_id : varchar(32)     ---     protocol_name : varchar(100)     version : varchar(16)     \"\"\"     contents = [         {'protocol_id': 'iacuc_2024_01', 'protocol_name': 'Mouse Behavior', 'version': '1.0'},         {'protocol_id': 'iacuc_2024_02', 'protocol_name': 'Imaging Protocol', 'version': '2.1'},     ]  @schema  class SubjectProtocol(dj.Manual):     definition = \"\"\"     # Protocols assigned to subjects (many-to-many)     -&gt; Subject     -&gt; Protocol     ---     assignment_date : date     \"\"\" In\u00a0[13]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[13]: In\u00a0[14]: Copied! <pre># Insert trials for the first session\nimport random\nrandom.seed(42)\n\ntrials = []\nfor i in range(20):\n    correct = random.random() &gt; 0.3\n    trials.append({\n        'subject_id': 'M001',\n        'session_idx': 1,\n        'trial_idx': i + 1,\n        'stimulus': random.choice(['left', 'right']),\n        'response': random.choice(['go', 'nogo']),\n        'correct': correct,\n        'reaction_time': random.uniform(0.2, 0.8)\n    })\n\nSession.Trial.insert(trials, skip_duplicates=True)\nprint(f\"Inserted {len(Session.Trial())} trials\")\n</pre> # Insert trials for the first session import random random.seed(42)  trials = [] for i in range(20):     correct = random.random() &gt; 0.3     trials.append({         'subject_id': 'M001',         'session_idx': 1,         'trial_idx': i + 1,         'stimulus': random.choice(['left', 'right']),         'response': random.choice(['go', 'nogo']),         'correct': correct,         'reaction_time': random.uniform(0.2, 0.8)     })  Session.Trial.insert(trials, skip_duplicates=True) print(f\"Inserted {len(Session.Trial())} trials\") <pre>Inserted 20 trials\n</pre> In\u00a0[15]: Copied! <pre># Populate the computed summary\nSessionSummary.populate(display_progress=True)\nSessionSummary()\n</pre> # Populate the computed summary SessionSummary.populate(display_progress=True) SessionSummary() <pre>\rSessionSummary:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>\rSessionSummary: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 103.46it/s]</pre> <pre>\n</pre> Out[15]: Summary statistics for a session <p>subject_id</p> None <p>session_idx</p> None <p>num_trials</p> <p>num_correct</p> <p>accuracy</p> <p>mean_reaction_time</p> M001 1 20 14 0.7 0.5365M001 2 0 0 0.0 0.0M001 3 0 0 0.0 0.0 <p>Total: 3</p> In\u00a0[16]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/basics/02-schema-design/#schema-design","title":"Schema Design\u00b6","text":"<p>This tutorial covers how to design DataJoint schemas effectively. You'll learn:</p> <ul> <li>Table tiers \u2014 Manual, Lookup, Imported, and Computed tables</li> <li>Primary keys \u2014 Uniquely identifying entities</li> <li>Foreign keys \u2014 Creating dependencies between tables</li> <li>Relationship patterns \u2014 One-to-many, one-to-one, and many-to-many</li> </ul> <p>We'll build a schema for a neuroscience experiment tracking subjects, sessions, and trials.</p>"},{"location":"tutorials/basics/02-schema-design/#table-tiers","title":"Table Tiers\u00b6","text":"<p>DataJoint has four table tiers, each serving a different purpose:</p> Tier Class Purpose Data Entry Manual <code>dj.Manual</code> Core experimental data Inserted by operators or instruments Lookup <code>dj.Lookup</code> Reference/configuration data Pre-populated, rarely changes Imported <code>dj.Imported</code> Data from external files Auto-populated via <code>make()</code> Computed <code>dj.Computed</code> Derived/processed data Auto-populated via <code>make()</code> <p>Manual tables are not necessarily populated by hand\u2014they contain data entered into the pipeline by operators, instruments, or ingestion scripts using <code>insert</code> commands. In contrast, Imported and Computed tables are auto-populated by calling the <code>.populate()</code> method, which invokes the <code>make()</code> callback for each missing entry.</p>"},{"location":"tutorials/basics/02-schema-design/#manual-tables","title":"Manual Tables\u00b6","text":"<p>Manual tables store data that is inserted directly\u2014the starting point of your pipeline.</p>"},{"location":"tutorials/basics/02-schema-design/#lookup-tables","title":"Lookup Tables\u00b6","text":"<p>Lookup tables store reference data that rarely changes. Use the <code>contents</code> attribute to pre-populate them.</p>"},{"location":"tutorials/basics/02-schema-design/#primary-keys","title":"Primary Keys\u00b6","text":"<p>The primary key uniquely identifies each row. Attributes above the <code>---</code> line form the primary key.</p>"},{"location":"tutorials/basics/02-schema-design/#design-principles","title":"Design Principles\u00b6","text":"<ol> <li>Entity integrity \u2014 Each row represents exactly one real-world entity</li> <li>No duplicates \u2014 The primary key prevents inserting the same entity twice</li> <li>Minimal \u2014 Include only attributes necessary for uniqueness</li> </ol>"},{"location":"tutorials/basics/02-schema-design/#natural-vs-surrogate-keys","title":"Natural vs Surrogate Keys\u00b6","text":"<ul> <li><p>Natural key: An identifier used outside the database to refer to entities in the real world. Requires a real-world mechanism to establish and maintain the association (e.g., ear tags, cage labels, barcodes). Example: <code>subject_id = 'M001'</code> where M001 is printed on the animal's cage.</p> </li> <li><p>Surrogate key: An identifier used only inside the database, with minimal or no exposure to end users. Users don't search by surrogate keys or use them in conversation. Example: internal record IDs, auto-generated UUIDs for system tracking.</p> </li> </ul> <p>DataJoint works well with both. Natural keys make data more interpretable and enable identification of physical entities. Surrogate keys are appropriate when entities exist only within the system or when natural identifiers shouldn't be stored (e.g., privacy).</p>"},{"location":"tutorials/basics/02-schema-design/#foreign-keys","title":"Foreign Keys\u00b6","text":"<p>The <code>-&gt;</code> syntax creates a foreign key dependency. Foreign keys:</p> <ol> <li>Import attributes \u2014 Primary key attributes are inherited from the parent</li> <li>Enforce referential integrity \u2014 Can't insert a session for a non-existent subject</li> <li>Enable cascading deletes \u2014 Deleting a subject removes all its sessions</li> <li>Define workflow \u2014 The parent must exist before the child</li> </ol>"},{"location":"tutorials/basics/02-schema-design/#relationship-patterns","title":"Relationship Patterns\u00b6","text":""},{"location":"tutorials/basics/02-schema-design/#one-to-many-hierarchical","title":"One-to-Many (Hierarchical)\u00b6","text":"<p>When a foreign key is part of the primary key, it creates a one-to-many relationship:</p> <ul> <li>One subject \u2192 many sessions</li> <li>One session \u2192 many trials</li> </ul>"},{"location":"tutorials/basics/02-schema-design/#master-part-compositional-integrity","title":"Master-Part (Compositional Integrity)\u00b6","text":"<p>A part table provides compositional integrity: master and parts are inserted and deleted as an atomic unit. Part tables:</p> <ul> <li>Reference the master with <code>-&gt; master</code></li> <li>Are inserted together with the master atomically</li> <li>Are deleted when the master is deleted</li> <li>Can be one-to-many or one-to-one with the master</li> <li>A master can have multiple part tables, which may reference each other</li> </ul> <p>We defined <code>Session.Trial</code> as a part table because trials belong to their session:</p> <ul> <li>A session and all its trials should be entered together</li> <li>Deleting a session removes all its trials</li> <li>Downstream computations can assume all trials are present once the session exists</li> </ul> <p>Use part tables when components must be complete before processing can begin.</p>"},{"location":"tutorials/basics/02-schema-design/#one-to-one-extension","title":"One-to-One (Extension)\u00b6","text":"<p>When the child's primary key exactly matches the parent's, it creates a one-to-one relationship. This is useful for:</p> <ul> <li>Extending a table with optional or computed data</li> <li>Separating computed results from source data</li> </ul> <p><code>SessionSummary</code> below has a one-to-one relationship with <code>Session</code>\u2014each session has exactly one summary.</p>"},{"location":"tutorials/basics/02-schema-design/#foreign-key-modifiers","title":"Foreign Key Modifiers\u00b6","text":"<p>Foreign keys support modifiers that change their behavior:</p> Modifier Syntax Effect Nullable <code>-&gt; [nullable] Table</code> Makes FK optional (can be NULL) Unique <code>-&gt; [unique] Table</code> Enforces one-to-one from child side Both <code>-&gt; [nullable, unique] Table</code> Optional one-to-one relationship <p>Rules:</p> <ul> <li><code>[nullable]</code> only allowed for secondary FKs (below <code>---</code>)</li> <li><code>[unique]</code> allowed for both primary and secondary FKs</li> <li>Primary key attributes cannot be nullable</li> </ul> <p>Nullable unique behavior: Multiple rows can have NULL values because SQL's UNIQUE constraint does not consider NULLs equal to each other.</p>"},{"location":"tutorials/basics/02-schema-design/#many-to-many-association-tables","title":"Many-to-Many (Association Tables)\u00b6","text":"<p>For many-to-many relationships, create an association table with foreign keys to both parents:</p>"},{"location":"tutorials/basics/02-schema-design/#view-the-schema","title":"View the Schema\u00b6","text":"<p>DataJoint can visualize the schema as a diagram:</p>"},{"location":"tutorials/basics/02-schema-design/#reading-the-diagram","title":"Reading the Diagram\u00b6","text":"<p>DataJoint diagrams show tables as nodes and foreign keys as edges. The notation conveys relationship semantics at a glance.</p> <p>Line Styles:</p> Line Style Relationship Meaning \u2501\u2501\u2501 Thick solid Extension FK is entire PK (one-to-one) \u2500\u2500\u2500 Thin solid Containment FK in PK with other fields (one-to-many) \u2504\u2504\u2504 Dashed Reference FK in secondary attributes (one-to-many) <p>Visual Indicators:</p> Indicator Meaning Underlined name Introduces new dimension (new PK attributes) Non-underlined name Inherits all dimensions (PK entirely from FKs) Green Manual table Gray Lookup table Red Computed table Blue Imported table Orange dots Renamed foreign keys (via <code>.proj()</code>) <p>Key principle: Solid lines mean the parent's identity becomes part of the child's identity. Dashed lines mean the child maintains independent identity.</p> <p>Note: Diagrams do NOT show <code>[nullable]</code> or <code>[unique]</code> modifiers\u2014check table definitions for these constraints.</p> <p>See How to Read Diagrams for diagram operations and comparison to ER notation.</p>"},{"location":"tutorials/basics/02-schema-design/#insert-test-data-and-populate","title":"Insert Test Data and Populate\u00b6","text":""},{"location":"tutorials/basics/02-schema-design/#best-practices","title":"Best Practices\u00b6","text":""},{"location":"tutorials/basics/02-schema-design/#1-choose-meaningful-primary-keys","title":"1. Choose Meaningful Primary Keys\u00b6","text":"<ul> <li>Use natural identifiers when possible (<code>subject_id = 'M001'</code>)</li> <li>Keep keys minimal but sufficient for uniqueness</li> </ul>"},{"location":"tutorials/basics/02-schema-design/#2-use-appropriate-table-tiers","title":"2. Use Appropriate Table Tiers\u00b6","text":"<ul> <li>Manual: Data entered by operators or instruments</li> <li>Lookup: Configuration, parameters, reference data</li> <li>Imported: Data read from files (recordings, images)</li> <li>Computed: Derived analyses and summaries</li> </ul>"},{"location":"tutorials/basics/02-schema-design/#3-normalize-your-data","title":"3. Normalize Your Data\u00b6","text":"<ul> <li>Don't repeat information across rows</li> <li>Create separate tables for distinct entities</li> <li>Use foreign keys to link related data</li> </ul>"},{"location":"tutorials/basics/02-schema-design/#4-use-core-datajoint-types","title":"4. Use Core DataJoint Types\u00b6","text":"<p>DataJoint has a three-layer type architecture (see Type System Specification/):</p> <ol> <li><p>Native database types (Layer 1): Backend-specific types like <code>INT</code>, <code>FLOAT</code>, <code>TINYINT UNSIGNED</code>. These are discouraged but allowed for backward compatibility.</p> </li> <li><p>Core DataJoint types (Layer 2): Standardized, scientist-friendly types that work identically across MySQL and PostgreSQL. Always prefer these.</p> </li> <li><p>Codec types (Layer 3): Types with <code>encode()</code>/<code>decode()</code> semantics like <code>&lt;blob&gt;</code>, <code>&lt;attach&gt;</code>, <code>&lt;object@&gt;</code>.</p> </li> </ol> <p>Core types used in this tutorial:</p> Type Description Example <code>int16</code>, <code>int32</code>, <code>int32</code> Sized integers <code>session_idx : int32</code> <code>float32</code>, <code>float64</code> Sized floats <code>reaction_time : float32</code> <code>varchar(n)</code> Variable-length string <code>name : varchar(100)</code> <code>bool</code> Boolean <code>correct : bool</code> <code>date</code> Date only <code>date_of_birth : date</code> <code>datetime</code> Date and time (UTC) <code>created_at : datetime</code> <code>enum(...)</code> Enumeration <code>sex : enum('M', 'F', 'U')</code> <code>json</code> JSON document <code>task_params : json</code> <code>uuid</code> Universally unique ID <code>experimenter_id : uuid</code> <p>Why native types are allowed but discouraged:</p> <p>Native types (like <code>int</code>, <code>float</code>, <code>tinyint</code>) are passed through to the database but generate a warning at declaration time. They are discouraged because:</p> <ul> <li>They lack explicit size information</li> <li>They are not portable across database backends</li> <li>They are not recorded in field metadata for reconstruction</li> </ul> <p>If you see a warning like <code>\"Native type 'int' used; consider 'int32' instead\"</code>, update your definition to use the corresponding core type.</p>"},{"location":"tutorials/basics/02-schema-design/#5-document-your-tables","title":"5. Document Your Tables\u00b6","text":"<ul> <li>Add comments after <code>#</code> in definitions</li> <li>Document units in attribute comments</li> </ul>"},{"location":"tutorials/basics/02-schema-design/#key-concepts-recap","title":"Key Concepts Recap\u00b6","text":"Concept Description Primary Key Attributes above <code>---</code> that uniquely identify rows Secondary Attributes Attributes below <code>---</code> that store additional data Foreign Key (<code>-&gt;</code>) Reference to another table, imports its primary key One-to-Many FK in primary key: parent has many children One-to-One FK is entire primary key: exactly one child per parent Master-Part Compositional integrity: master and parts inserted/deleted atomically Nullable FK <code>[nullable]</code> makes the reference optional Unique FK <code>[unique]</code> enforces one-to-one from child side Lookup Table Pre-populated reference data"},{"location":"tutorials/basics/02-schema-design/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Data Entry \u2014 Inserting, updating, and deleting data</li> <li>Queries \u2014 Filtering, joining, and projecting</li> <li>Computation \u2014 Building computational pipelines</li> </ul>"},{"location":"tutorials/basics/03-data-entry/","title":"Data Entry","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\n\nschema = dj.Schema('tutorial_data_entry')\n</pre> import datajoint as dj import numpy as np  schema = dj.Schema('tutorial_data_entry') <pre>[2026-01-27 15:28:52] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre># Define tables for this tutorial\n@schema\nclass Lab(dj.Manual):\n    definition = \"\"\"\n    lab_id : varchar(16)\n    ---\n    lab_name : varchar(100)\n    \"\"\"\n\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    -&gt; Lab\n    species : varchar(50)\n    date_of_birth : date\n    notes = '' : varchar(1000)\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int32\n    ---\n    session_date : date\n    duration : decimal(4,1)       # minutes\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32\n        ---\n        outcome : enum('hit', 'miss', 'false_alarm', 'correct_reject')\n        reaction_time : decimal(3,2)  # seconds\n        \"\"\"\n\n@schema\nclass ProcessedData(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    hit_rate : float32\n    \"\"\"\n    \n    def make(self, key):\n        outcomes = (Session.Trial &amp; key).to_arrays('outcome')\n        n_trials = len(outcomes)\n        hit_rate = np.sum(outcomes == 'hit') / n_trials if n_trials else 0.0\n        self.insert1({**key, 'hit_rate': hit_rate})\n</pre> # Define tables for this tutorial @schema class Lab(dj.Manual):     definition = \"\"\"     lab_id : varchar(16)     ---     lab_name : varchar(100)     \"\"\"  @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : varchar(16)     ---     -&gt; Lab     species : varchar(50)     date_of_birth : date     notes = '' : varchar(1000)     \"\"\"  @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_idx : int32     ---     session_date : date     duration : decimal(4,1)       # minutes     \"\"\"      class Trial(dj.Part):         definition = \"\"\"         -&gt; master         trial_idx : int32         ---         outcome : enum('hit', 'miss', 'false_alarm', 'correct_reject')         reaction_time : decimal(3,2)  # seconds         \"\"\"  @schema class ProcessedData(dj.Computed):     definition = \"\"\"     -&gt; Session     ---     hit_rate : float32     \"\"\"          def make(self, key):         outcomes = (Session.Trial &amp; key).to_arrays('outcome')         n_trials = len(outcomes)         hit_rate = np.sum(outcomes == 'hit') / n_trials if n_trials else 0.0         self.insert1({**key, 'hit_rate': hit_rate}) In\u00a0[3]: Copied! <pre># Insert a single row\nLab.insert1({'lab_id': 'tolias', 'lab_name': 'Tolias Lab'})\n\nSubject.insert1({\n    'subject_id': 'M001',\n    'lab_id': 'tolias',\n    'species': 'Mus musculus',\n    'date_of_birth': '2026-01-15'\n})\n\nSubject()\n</pre> # Insert a single row Lab.insert1({'lab_id': 'tolias', 'lab_name': 'Tolias Lab'})  Subject.insert1({     'subject_id': 'M001',     'lab_id': 'tolias',     'species': 'Mus musculus',     'date_of_birth': '2026-01-15' })  Subject() Out[3]: None <p>subject_id</p> <p>lab_id</p> None <p>species</p> <p>date_of_birth</p> <p>notes</p> M001 tolias Mus musculus 2026-01-15 <p>Total: 1</p> In\u00a0[4]: Copied! <pre># Insert multiple rows as a list of dictionaries\nSubject.insert([\n    {\n        'subject_id': 'M002',\n        'lab_id': 'tolias',\n        'species': 'Mus musculus',\n        'date_of_birth': '2026-02-01'\n    },\n    {\n        'subject_id': 'M003',\n        'lab_id': 'tolias',\n        'species': 'Mus musculus',\n        'date_of_birth': '2026-02-15'\n    },\n])\n\nSubject()\n</pre> # Insert multiple rows as a list of dictionaries Subject.insert([     {         'subject_id': 'M002',         'lab_id': 'tolias',         'species': 'Mus musculus',         'date_of_birth': '2026-02-01'     },     {         'subject_id': 'M003',         'lab_id': 'tolias',         'species': 'Mus musculus',         'date_of_birth': '2026-02-15'     }, ])  Subject() Out[4]: None <p>subject_id</p> <p>lab_id</p> None <p>species</p> <p>date_of_birth</p> <p>notes</p> M001 tolias Mus musculus 2026-01-15 M002 tolias Mus musculus 2026-02-01 M003 tolias Mus musculus 2026-02-15 <p>Total: 3</p> In\u00a0[5]: Copied! <pre># Insert from pandas DataFrame\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'subject_id': ['M004', 'M005'],\n    'lab_id': ['tolias', 'tolias'],\n    'species': ['Mus musculus', 'Mus musculus'],\n    'date_of_birth': ['2026-03-01', '2026-03-15']\n})\n\nSubject.insert(df)\nprint(f\"Total subjects: {len(Subject())}\")\n</pre> # Insert from pandas DataFrame import pandas as pd  df = pd.DataFrame({     'subject_id': ['M004', 'M005'],     'lab_id': ['tolias', 'tolias'],     'species': ['Mus musculus', 'Mus musculus'],     'date_of_birth': ['2026-03-01', '2026-03-15'] })  Subject.insert(df) print(f\"Total subjects: {len(Subject())}\") <pre>Total subjects: 5\n</pre> In\u00a0[6]: Copied! <pre># This will raise an error - duplicate primary key\ntry:\n    Subject.insert1({'subject_id': 'M001', 'lab_id': 'tolias', \n                     'species': 'Mus musculus', 'date_of_birth': '2026-01-15'})\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n    print(\"Cannot insert duplicate primary key!\")\n</pre> # This will raise an error - duplicate primary key try:     Subject.insert1({'subject_id': 'M001', 'lab_id': 'tolias',                       'species': 'Mus musculus', 'date_of_birth': '2026-01-15'}) except Exception as e:     print(f\"Error: {type(e).__name__}\")     print(\"Cannot insert duplicate primary key!\") <pre>Error: DuplicateError\nCannot insert duplicate primary key!\n</pre> <p>Use <code>skip_duplicates=True</code> to silently skip rows with existing keys:</p> In\u00a0[7]: Copied! <pre># Skip duplicates - existing row unchanged\nSubject.insert1(\n    {'subject_id': 'M001', 'lab_id': 'tolias', 'species': 'Mus musculus', 'date_of_birth': '2026-01-15'},\n    skip_duplicates=True\n)\nprint(\"Insert completed (duplicate skipped)\")\n</pre> # Skip duplicates - existing row unchanged Subject.insert1(     {'subject_id': 'M001', 'lab_id': 'tolias', 'species': 'Mus musculus', 'date_of_birth': '2026-01-15'},     skip_duplicates=True ) print(\"Insert completed (duplicate skipped)\") <pre>Insert completed (duplicate skipped)\n</pre> <p>Note: <code>replace=True</code> is also available but has the same caveats as <code>update1()</code>\u2014it bypasses immutability and can break provenance. Use sparingly for corrections only.</p> In\u00a0[8]: Copied! <pre>try:\n    Subject.insert1({'subject_id': 'M006', 'lab_id': 'tolias', \n                     'species': 'Mus musculus', 'date_of_birth': '2026-04-01',\n                     'unknown_field': 'some value'})  # Unknown field!\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n    print(\"Field 'unknown_field' not in table!\")\n</pre> try:     Subject.insert1({'subject_id': 'M006', 'lab_id': 'tolias',                       'species': 'Mus musculus', 'date_of_birth': '2026-04-01',                      'unknown_field': 'some value'})  # Unknown field! except Exception as e:     print(f\"Error: {type(e).__name__}\")     print(\"Field 'unknown_field' not in table!\") <pre>Error: KeyError\nField 'unknown_field' not in table!\n</pre> In\u00a0[9]: Copied! <pre># Use ignore_extra_fields=True to silently ignore unknown fields\nSubject.insert1(\n    {'subject_id': 'M006', 'lab_id': 'tolias', 'species': 'Mus musculus',\n     'date_of_birth': '2026-04-01', 'unknown_field': 'ignored'},\n    ignore_extra_fields=True\n)\nprint(f\"Total subjects: {len(Subject())}\")\n</pre> # Use ignore_extra_fields=True to silently ignore unknown fields Subject.insert1(     {'subject_id': 'M006', 'lab_id': 'tolias', 'species': 'Mus musculus',      'date_of_birth': '2026-04-01', 'unknown_field': 'ignored'},     ignore_extra_fields=True ) print(f\"Total subjects: {len(Subject())}\") <pre>Total subjects: 6\n</pre> In\u00a0[10]: Copied! <pre># Use a transaction to ensure master and parts are inserted atomically\nwith dj.conn().transaction:\n    Session.insert1({\n        'subject_id': 'M001',\n        'session_idx': 1,\n        'session_date': '2026-01-06',\n        'duration': 45.5\n    })\n    Session.Trial.insert([\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 1,\n         'outcome': 'hit', 'reaction_time': 0.35},\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 2,\n         'outcome': 'miss', 'reaction_time': 0.82},\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 3,\n         'outcome': 'hit', 'reaction_time': 0.41},\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 4,\n         'outcome': 'false_alarm', 'reaction_time': 0.28},\n        {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 5,\n         'outcome': 'hit', 'reaction_time': 0.39},\n    ])\n\n# Both master and parts committed together, or neither if error occurred\nSession.Trial()\n</pre> # Use a transaction to ensure master and parts are inserted atomically with dj.conn().transaction:     Session.insert1({         'subject_id': 'M001',         'session_idx': 1,         'session_date': '2026-01-06',         'duration': 45.5     })     Session.Trial.insert([         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 1,          'outcome': 'hit', 'reaction_time': 0.35},         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 2,          'outcome': 'miss', 'reaction_time': 0.82},         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 3,          'outcome': 'hit', 'reaction_time': 0.41},         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 4,          'outcome': 'false_alarm', 'reaction_time': 0.28},         {'subject_id': 'M001', 'session_idx': 1, 'trial_idx': 5,          'outcome': 'hit', 'reaction_time': 0.39},     ])  # Both master and parts committed together, or neither if error occurred Session.Trial() Out[10]: None <p>subject_id</p> None <p>session_idx</p> None <p>trial_idx</p> <p>outcome</p> <p>reaction_time</p> seconds M001 1 1 hit 0.35M001 1 2 miss 0.82M001 1 3 hit 0.41M001 1 4 false_alarm 0.28M001 1 5 hit 0.39 <p>Total: 5</p> In\u00a0[11]: Copied! <pre># Update a single row - must provide all primary key values\nSubject.update1({'subject_id': 'M001', 'notes': 'Primary subject for behavioral study'})\n\n(Subject &amp; {'subject_id': 'M001'}).fetch1()\n</pre> # Update a single row - must provide all primary key values Subject.update1({'subject_id': 'M001', 'notes': 'Primary subject for behavioral study'})  (Subject &amp; {'subject_id': 'M001'}).fetch1() Out[11]: <pre>{'subject_id': 'M001',\n 'lab_id': 'tolias',\n 'species': 'Mus musculus',\n 'date_of_birth': datetime.date(2026, 1, 15),\n 'notes': 'Primary subject for behavioral study'}</pre> In\u00a0[12]: Copied! <pre># Update multiple attributes at once\nSubject.update1({\n    'subject_id': 'M002',\n    'notes': 'Control group',\n    'species': 'Mus musculus (C57BL/6)'  # More specific\n})\n\n(Subject &amp; {'subject_id': 'M002'}).fetch1()\n</pre> # Update multiple attributes at once Subject.update1({     'subject_id': 'M002',     'notes': 'Control group',     'species': 'Mus musculus (C57BL/6)'  # More specific })  (Subject &amp; {'subject_id': 'M002'}).fetch1() Out[12]: <pre>{'subject_id': 'M002',\n 'lab_id': 'tolias',\n 'species': 'Mus musculus (C57BL/6)',\n 'date_of_birth': datetime.date(2026, 2, 1),\n 'notes': 'Control group'}</pre> In\u00a0[13]: Copied! <pre># Error: incomplete primary key\ntry:\n    Subject.update1({'notes': 'Missing subject_id!'})\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n    print(\"Primary key must be complete\")\n</pre> # Error: incomplete primary key try:     Subject.update1({'notes': 'Missing subject_id!'}) except Exception as e:     print(f\"Error: {type(e).__name__}\")     print(\"Primary key must be complete\") <pre>Error: DataJointError\nPrimary key must be complete\n</pre> In\u00a0[14]: Copied! <pre># Error: cannot update restricted table\ntry:\n    (Subject &amp; {'subject_id': 'M001'}).update1({'subject_id': 'M001', 'notes': 'test'})\nexcept Exception as e:\n    print(f\"Error: {type(e).__name__}\")\n    print(\"Cannot update restricted table\")\n</pre> # Error: cannot update restricted table try:     (Subject &amp; {'subject_id': 'M001'}).update1({'subject_id': 'M001', 'notes': 'test'}) except Exception as e:     print(f\"Error: {type(e).__name__}\")     print(\"Cannot update restricted table\") <pre>Error: DataJointError\nCannot update restricted table\n</pre> In\u00a0[15]: Copied! <pre># Reset notes to default (empty string)\nSubject.update1({'subject_id': 'M003', 'notes': None})\n\n(Subject &amp; {'subject_id': 'M003'}).fetch1()\n</pre> # Reset notes to default (empty string) Subject.update1({'subject_id': 'M003', 'notes': None})  (Subject &amp; {'subject_id': 'M003'}).fetch1() Out[15]: <pre>{'subject_id': 'M003',\n 'lab_id': 'tolias',\n 'species': 'Mus musculus',\n 'date_of_birth': datetime.date(2026, 2, 15),\n 'notes': ''}</pre> In\u00a0[16]: Copied! <pre># First, let's see what we have\nprint(f\"Sessions: {len(Session())}\")\nprint(f\"Trials: {len(Session.Trial())}\")\n\n# Populate computed table\nProcessedData.populate()\nprint(f\"ProcessedData: {len(ProcessedData())}\")\n</pre> # First, let's see what we have print(f\"Sessions: {len(Session())}\") print(f\"Trials: {len(Session.Trial())}\")  # Populate computed table ProcessedData.populate() print(f\"ProcessedData: {len(ProcessedData())}\") <pre>Sessions: 1\nTrials: 5\nProcessedData: 1\n</pre> In\u00a0[17]: Copied! <pre># Delete a session - cascades to Trial and ProcessedData\n(Session &amp; {'subject_id': 'M001', 'session_idx': 1}).delete(prompt=False)\n\nprint(f\"After delete:\")\nprint(f\"Sessions: {len(Session())}\")\nprint(f\"Trials: {len(Session.Trial())}\")\nprint(f\"ProcessedData: {len(ProcessedData())}\")\n</pre> # Delete a session - cascades to Trial and ProcessedData (Session &amp; {'subject_id': 'M001', 'session_idx': 1}).delete(prompt=False)  print(f\"After delete:\") print(f\"Sessions: {len(Session())}\") print(f\"Trials: {len(Session.Trial())}\") print(f\"ProcessedData: {len(ProcessedData())}\") <pre>[2026-01-27 15:28:52] Deleting 5 rows from \"tutorial_data_entry\".\"session__trial\"\n</pre> <pre>[2026-01-27 15:28:52] Deleting 1 rows from \"tutorial_data_entry\".\"__processed_data\"\n</pre> <pre>[2026-01-27 15:28:52] Deleting 1 rows from \"tutorial_data_entry\".\"session\"\n</pre> <pre>After delete:\nSessions: 0\nTrials: 0\nProcessedData: 0\n</pre> In\u00a0[18]: Copied! <pre># Add more data for demonstration\nwith dj.conn().transaction:\n    Session.insert1({\n        'subject_id': 'M002',\n        'session_idx': 1,\n        'session_date': '2026-01-07',\n        'duration': 30.0\n    })\n    Session.Trial.insert([\n        {'subject_id': 'M002', 'session_idx': 1, 'trial_idx': 1,\n         'outcome': 'hit', 'reaction_time': 0.40},\n        {'subject_id': 'M002', 'session_idx': 1, 'trial_idx': 2,\n         'outcome': 'hit', 'reaction_time': 0.38},\n    ])\n\n# Delete with prompt=False (no confirmation prompt)\n(Session &amp; {'subject_id': 'M002', 'session_idx': 1}).delete(prompt=False)\n</pre> # Add more data for demonstration with dj.conn().transaction:     Session.insert1({         'subject_id': 'M002',         'session_idx': 1,         'session_date': '2026-01-07',         'duration': 30.0     })     Session.Trial.insert([         {'subject_id': 'M002', 'session_idx': 1, 'trial_idx': 1,          'outcome': 'hit', 'reaction_time': 0.40},         {'subject_id': 'M002', 'session_idx': 1, 'trial_idx': 2,          'outcome': 'hit', 'reaction_time': 0.38},     ])  # Delete with prompt=False (no confirmation prompt) (Session &amp; {'subject_id': 'M002', 'session_idx': 1}).delete(prompt=False) <pre>[2026-01-27 15:28:52] Deleting 2 rows from \"tutorial_data_entry\".\"session__trial\"\n</pre> <pre>[2026-01-27 15:28:52] Deleting 1 rows from \"tutorial_data_entry\".\"session\"\n</pre> Out[18]: <pre>1</pre> In\u00a0[19]: Copied! <pre># Add a session with trials (using transaction for compositional integrity)\nwith dj.conn().transaction:\n    Session.insert1({\n        'subject_id': 'M003',\n        'session_idx': 1,\n        'session_date': '2026-01-08',\n        'duration': 40.0\n    })\n    Session.Trial.insert([\n        {'subject_id': 'M003', 'session_idx': 1, 'trial_idx': 1,\n         'outcome': 'hit', 'reaction_time': 0.35},\n        {'subject_id': 'M003', 'session_idx': 1, 'trial_idx': 2,\n         'outcome': 'miss', 'reaction_time': 0.50},\n    ])\n\n# Compute results\nProcessedData.populate()\nprint(\"Before correction:\", ProcessedData.fetch1())\n</pre> # Add a session with trials (using transaction for compositional integrity) with dj.conn().transaction:     Session.insert1({         'subject_id': 'M003',         'session_idx': 1,         'session_date': '2026-01-08',         'duration': 40.0     })     Session.Trial.insert([         {'subject_id': 'M003', 'session_idx': 1, 'trial_idx': 1,          'outcome': 'hit', 'reaction_time': 0.35},         {'subject_id': 'M003', 'session_idx': 1, 'trial_idx': 2,          'outcome': 'miss', 'reaction_time': 0.50},     ])  # Compute results ProcessedData.populate() print(\"Before correction:\", ProcessedData.fetch1()) <pre>Before correction: {'subject_id': 'M003', 'session_idx': 1, 'hit_rate': 0.5}\n</pre> In\u00a0[20]: Copied! <pre># Suppose we discovered trial 2 was actually a 'hit' not 'miss'\n# WRONG: Updating the trial would leave ProcessedData stale!\n# Session.Trial.update1({...})  # DON'T DO THIS\n\n# CORRECT: Delete, reinsert, recompute\nkey = {'subject_id': 'M003', 'session_idx': 1}\n\n# 1. Delete cascades to ProcessedData\n(Session &amp; key).delete(prompt=False)\n\n# 2. Reinsert with corrected data (using transaction)\nwith dj.conn().transaction:\n    Session.insert1({**key, 'session_date': '2026-01-08', 'duration': 40.0})\n    Session.Trial.insert([\n        {**key, 'trial_idx': 1, 'outcome': 'hit', 'reaction_time': 0.35},\n        {**key, 'trial_idx': 2, 'outcome': 'hit', 'reaction_time': 0.50},\n    ])\n\n# 3. Recompute\nProcessedData.populate()\nprint(\"After correction:\", ProcessedData.fetch1())\n</pre> # Suppose we discovered trial 2 was actually a 'hit' not 'miss' # WRONG: Updating the trial would leave ProcessedData stale! # Session.Trial.update1({...})  # DON'T DO THIS  # CORRECT: Delete, reinsert, recompute key = {'subject_id': 'M003', 'session_idx': 1}  # 1. Delete cascades to ProcessedData (Session &amp; key).delete(prompt=False)  # 2. Reinsert with corrected data (using transaction) with dj.conn().transaction:     Session.insert1({**key, 'session_date': '2026-01-08', 'duration': 40.0})     Session.Trial.insert([         {**key, 'trial_idx': 1, 'outcome': 'hit', 'reaction_time': 0.35},         {**key, 'trial_idx': 2, 'outcome': 'hit', 'reaction_time': 0.50},     ])  # 3. Recompute ProcessedData.populate() print(\"After correction:\", ProcessedData.fetch1()) <pre>[2026-01-27 15:28:52] Deleting 2 rows from \"tutorial_data_entry\".\"session__trial\"\n</pre> <pre>[2026-01-27 15:28:52] Deleting 1 rows from \"tutorial_data_entry\".\"__processed_data\"\n</pre> <pre>[2026-01-27 15:28:52] Deleting 1 rows from \"tutorial_data_entry\".\"session\"\n</pre> <pre>After correction: {'subject_id': 'M003', 'session_idx': 1, 'hit_rate': 1.0}\n</pre> In\u00a0[21]: Copied! <pre># Validate rows before inserting\nrows_to_insert = [\n    {'subject_id': 'M007', 'lab_id': 'tolias', 'species': 'Mus musculus', 'date_of_birth': '2026-05-01'},\n    {'subject_id': 'M008', 'lab_id': 'tolias', 'species': 'Mus musculus', 'date_of_birth': '2026-05-15'},\n]\n\nresult = Subject.validate(rows_to_insert)\n\nif result:\n    Subject.insert(rows_to_insert)\n    print(f\"Inserted {len(rows_to_insert)} rows\")\nelse:\n    print(\"Validation failed:\")\n    print(result.summary())\n</pre> # Validate rows before inserting rows_to_insert = [     {'subject_id': 'M007', 'lab_id': 'tolias', 'species': 'Mus musculus', 'date_of_birth': '2026-05-01'},     {'subject_id': 'M008', 'lab_id': 'tolias', 'species': 'Mus musculus', 'date_of_birth': '2026-05-15'}, ]  result = Subject.validate(rows_to_insert)  if result:     Subject.insert(rows_to_insert)     print(f\"Inserted {len(rows_to_insert)} rows\") else:     print(\"Validation failed:\")     print(result.summary()) <pre>Inserted 2 rows\n</pre> In\u00a0[22]: Copied! <pre># Example of validation failure\nbad_rows = [\n    {'subject_id': 'M009', 'species': 'Mus musculus', 'date_of_birth': '2026-05-20'},  # Missing lab_id!\n]\n\nresult = Subject.validate(bad_rows)\n\nif not result:\n    print(\"Validation failed!\")\n    for error in result.errors:\n        print(f\"  {error}\")\n</pre> # Example of validation failure bad_rows = [     {'subject_id': 'M009', 'species': 'Mus musculus', 'date_of_birth': '2026-05-20'},  # Missing lab_id! ]  result = Subject.validate(bad_rows)  if not result:     print(\"Validation failed!\")     for error in result.errors:         print(f\"  {error}\") <pre>Validation failed!\n  (0, 'lab_id', \"Required field 'lab_id' is missing\")\n</pre> In\u00a0[23]: Copied! <pre># Atomic transaction - all inserts succeed or none do\nwith dj.conn().transaction:\n    Session.insert1({\n        'subject_id': 'M007',\n        'session_idx': 1,\n        'session_date': '2026-01-10',\n        'duration': 35.0\n    })\n    Session.Trial.insert([\n        {'subject_id': 'M007', 'session_idx': 1, 'trial_idx': 1,\n         'outcome': 'hit', 'reaction_time': 0.33},\n        {'subject_id': 'M007', 'session_idx': 1, 'trial_idx': 2,\n         'outcome': 'miss', 'reaction_time': 0.45},\n    ])\n\nprint(f\"Session inserted with {len(Session.Trial &amp; {'subject_id': 'M007'})} trials\")\n</pre> # Atomic transaction - all inserts succeed or none do with dj.conn().transaction:     Session.insert1({         'subject_id': 'M007',         'session_idx': 1,         'session_date': '2026-01-10',         'duration': 35.0     })     Session.Trial.insert([         {'subject_id': 'M007', 'session_idx': 1, 'trial_idx': 1,          'outcome': 'hit', 'reaction_time': 0.33},         {'subject_id': 'M007', 'session_idx': 1, 'trial_idx': 2,          'outcome': 'miss', 'reaction_time': 0.45},     ])  print(f\"Session inserted with {len(Session.Trial &amp; {'subject_id': 'M007'})} trials\") <pre>Session inserted with 2 trials\n</pre> In\u00a0[24]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/basics/03-data-entry/#data-entry","title":"Data Entry\u00b6","text":"<p>This tutorial covers how to manipulate data in DataJoint tables. You'll learn:</p> <ul> <li>Insert \u2014 Adding rows to tables</li> <li>Update \u2014 Modifying existing rows (for corrections)</li> <li>Delete \u2014 Removing rows with cascading</li> <li>Validation \u2014 Checking data before insertion</li> </ul> <p>DataJoint is designed around insert and delete as the primary operations. Updates are intentionally limited to surgical corrections.</p>"},{"location":"tutorials/basics/03-data-entry/#insert-operations","title":"Insert Operations\u00b6","text":""},{"location":"tutorials/basics/03-data-entry/#insert1-single-row","title":"<code>insert1()</code> \u2014 Single Row\u00b6","text":"<p>Use <code>insert1()</code> to add a single row as a dictionary:</p>"},{"location":"tutorials/basics/03-data-entry/#insert-multiple-rows","title":"<code>insert()</code> \u2014 Multiple Rows\u00b6","text":"<p>Use <code>insert()</code> to add multiple rows at once. This is more efficient than calling <code>insert1()</code> in a loop.</p>"},{"location":"tutorials/basics/03-data-entry/#accepted-input-formats","title":"Accepted Input Formats\u00b6","text":"<p><code>insert()</code> accepts several formats:</p> Format Example List of dicts <code>[{'id': 1, 'name': 'A'}, ...]</code> pandas DataFrame <code>pd.DataFrame({'id': [1, 2], 'name': ['A', 'B']})</code> numpy structured array <code>np.array([(1, 'A')], dtype=[('id', int), ('name', 'U10')])</code> QueryExpression <code>OtherTable.proj(...)</code> (INSERT...SELECT)"},{"location":"tutorials/basics/03-data-entry/#handling-duplicates","title":"Handling Duplicates\u00b6","text":"<p>By default, inserting a row with an existing primary key raises an error:</p>"},{"location":"tutorials/basics/03-data-entry/#extra-fields","title":"Extra Fields\u00b6","text":"<p>By default, inserting a row with fields not in the table raises an error:</p>"},{"location":"tutorials/basics/03-data-entry/#master-part-tables-and-transactions","title":"Master-Part Tables and Transactions\u00b6","text":"<p>Compositional integrity means that a master and all its parts must be inserted (or deleted) as an atomic unit. This ensures downstream computations see complete data.</p> <ul> <li>Auto-populated tables (Computed, Imported) enforce this automatically\u2014<code>make()</code> runs in a transaction</li> <li>Manual tables require explicit transactions to maintain compositional integrity</li> </ul>"},{"location":"tutorials/basics/03-data-entry/#inserting-master-with-parts","title":"Inserting Master with Parts\u00b6","text":""},{"location":"tutorials/basics/03-data-entry/#update-operations","title":"Update Operations\u00b6","text":"<p>DataJoint provides only <code>update1()</code> for modifying single rows. This is intentional\u2014updates bypass the normal workflow and should be used sparingly for corrective operations.</p>"},{"location":"tutorials/basics/03-data-entry/#when-to-use-updates","title":"When to Use Updates\u00b6","text":"<p>Appropriate uses:</p> <ul> <li>Fixing data entry errors (typos, wrong values)</li> <li>Adding notes or metadata after the fact</li> <li>Administrative corrections</li> </ul> <p>Inappropriate uses (use delete + insert + populate instead):</p> <ul> <li>Regular workflow operations</li> <li>Changes that should trigger recomputation</li> </ul>"},{"location":"tutorials/basics/03-data-entry/#update-requirements","title":"Update Requirements\u00b6","text":"<ol> <li>Complete primary key: All PK attributes must be provided</li> <li>Exactly one match: Must match exactly one existing row</li> <li>No restrictions: Cannot call on a restricted table</li> </ol>"},{"location":"tutorials/basics/03-data-entry/#reset-to-default","title":"Reset to Default\u00b6","text":"<p>Setting an attribute to <code>None</code> resets it to its default value:</p>"},{"location":"tutorials/basics/03-data-entry/#delete-operations","title":"Delete Operations\u00b6","text":""},{"location":"tutorials/basics/03-data-entry/#cascading-deletes","title":"Cascading Deletes\u00b6","text":"<p>Deleting a row automatically cascades to all dependent tables. This maintains referential integrity across the pipeline.</p>"},{"location":"tutorials/basics/03-data-entry/#prompt-behavior","title":"Prompt Behavior\u00b6","text":"<p>The <code>prompt</code> parameter controls whether <code>delete()</code> asks for confirmation. When <code>prompt=None</code> (default), the behavior is determined by <code>dj.config['safemode']</code>:</p> <pre># Uses config['safemode'] setting (default)\n(Table &amp; condition).delete()\n\n# Explicitly skip confirmation\n(Table &amp; condition).delete(prompt=False)\n\n# Explicitly require confirmation\n(Table &amp; condition).delete(prompt=True)\n</pre>"},{"location":"tutorials/basics/03-data-entry/#the-recomputation-pattern","title":"The Recomputation Pattern\u00b6","text":"<p>When source data needs to change, the correct pattern is delete \u2192 insert \u2192 populate. This ensures all derived data remains consistent:</p>"},{"location":"tutorials/basics/03-data-entry/#validation","title":"Validation\u00b6","text":"<p>Use <code>validate()</code> to check data before insertion:</p>"},{"location":"tutorials/basics/03-data-entry/#transactions","title":"Transactions\u00b6","text":"<p>Single operations are atomic by default. Use explicit transactions for:</p> <ol> <li>Master-part inserts \u2014 Maintain compositional integrity</li> <li>Multi-table operations \u2014 All succeed or all fail</li> <li>Complex workflows \u2014 Coordinate related changes</li> </ol>"},{"location":"tutorials/basics/03-data-entry/#best-practices","title":"Best Practices\u00b6","text":""},{"location":"tutorials/basics/03-data-entry/#1-prefer-insertdelete-over-update","title":"1. Prefer Insert/Delete Over Update\u00b6","text":"<p>When source data changes, delete and reinsert rather than updating. Updates and <code>replace=True</code> bypass immutability and break provenance:</p> <pre># Good: Delete and reinsert\n(Trial &amp; key).delete(prompt=False)\nTrial.insert1(corrected_trial)\nDerivedTable.populate()\n\n# Avoid: Update that leaves derived data stale\nTrial.update1({**key, 'value': new_value})\n</pre>"},{"location":"tutorials/basics/03-data-entry/#2-use-transactions-for-master-part-inserts","title":"2. Use Transactions for Master-Part Inserts\u00b6","text":"<pre># Ensures compositional integrity\nwith dj.conn().transaction:\n    Session.insert1(session_data)\n    Session.Trial.insert(trials)\n</pre>"},{"location":"tutorials/basics/03-data-entry/#3-batch-inserts-for-performance","title":"3. Batch Inserts for Performance\u00b6","text":"<pre># Good: Single insert call\nSubject.insert(all_rows)\n\n# Slow: Loop of insert1 calls\nfor row in all_rows:\n    Subject.insert1(row)  # Creates many transactions\n</pre>"},{"location":"tutorials/basics/03-data-entry/#4-validate-before-insert","title":"4. Validate Before Insert\u00b6","text":"<pre>result = Subject.validate(rows)\nif not result:\n    raise ValueError(result.summary())\nSubject.insert(rows)\n</pre>"},{"location":"tutorials/basics/03-data-entry/#5-configure-safe-mode-for-production","title":"5. Configure Safe Mode for Production\u00b6","text":"<pre># In production scripts, explicitly control prompt behavior\n(Subject &amp; condition).delete(prompt=False)  # No confirmation\n\n# Or configure globally via settings\ndj.config['safemode'] = True  # Require confirmation by default\n</pre>"},{"location":"tutorials/basics/03-data-entry/#quick-reference","title":"Quick Reference\u00b6","text":"Operation Method Use Case Insert one <code>insert1(row)</code> Adding single entity Insert many <code>insert(rows)</code> Bulk data loading Update one <code>update1(row)</code> Surgical corrections only Delete <code>delete()</code> Removing entities (cascades) Delete quick <code>delete_quick()</code> Internal cleanup (no cascade) Validate <code>validate(rows)</code> Pre-insert check <p>See the Data Manipulation Specification for complete details.</p>"},{"location":"tutorials/basics/03-data-entry/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Queries \u2014 Filtering, joining, and projecting data</li> <li>Computation \u2014 Building computational pipelines</li> </ul>"},{"location":"tutorials/basics/04-queries/","title":"Queries","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\n\nschema = dj.Schema('tutorial_queries')\n</pre> import datajoint as dj import numpy as np  schema = dj.Schema('tutorial_queries') <pre>[2026-01-27 15:28:55] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre># Define tables for this tutorial\n@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species : varchar(50)\n    date_of_birth : date\n    sex : enum('M', 'F', 'U')\n    weight : decimal(4,1)        # grams\n    \"\"\"\n\n@schema\nclass Experimenter(dj.Manual):\n    definition = \"\"\"\n    experimenter_id : varchar(16)\n    ---\n    full_name : varchar(100)\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int32\n    ---\n    -&gt; Experimenter\n    session_date : date\n    duration : decimal(4,1)      # minutes\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32\n        ---\n        stimulus : varchar(50)\n        response : varchar(50)\n        correct : bool\n        reaction_time : decimal(3,2)  # seconds\n        \"\"\"\n</pre> # Define tables for this tutorial @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : varchar(16)     ---     species : varchar(50)     date_of_birth : date     sex : enum('M', 'F', 'U')     weight : decimal(4,1)        # grams     \"\"\"  @schema class Experimenter(dj.Manual):     definition = \"\"\"     experimenter_id : varchar(16)     ---     full_name : varchar(100)     \"\"\"  @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_idx : int32     ---     -&gt; Experimenter     session_date : date     duration : decimal(4,1)      # minutes     \"\"\"      class Trial(dj.Part):         definition = \"\"\"         -&gt; master         trial_idx : int32         ---         stimulus : varchar(50)         response : varchar(50)         correct : bool         reaction_time : decimal(3,2)  # seconds         \"\"\" In\u00a0[3]: Copied! <pre># Insert sample data\nimport random\nrandom.seed(42)\n\nExperimenter.insert([\n    {'experimenter_id': 'alice', 'full_name': 'Alice Smith'},\n    {'experimenter_id': 'bob', 'full_name': 'Bob Jones'},\n])\n\nsubjects = [\n    {'subject_id': 'M001', 'species': 'Mus musculus',\n     'date_of_birth': '2026-01-15', 'sex': 'M', 'weight': 25.3},\n    {'subject_id': 'M002', 'species': 'Mus musculus',\n     'date_of_birth': '2026-02-01', 'sex': 'F', 'weight': 22.1},\n    {'subject_id': 'M003', 'species': 'Mus musculus',\n     'date_of_birth': '2026-02-15', 'sex': 'M', 'weight': 26.8},\n    {'subject_id': 'R001', 'species': 'Rattus norvegicus',\n     'date_of_birth': '2024-01-01', 'sex': 'F', 'weight': 280.5},\n]\nSubject.insert(subjects)\n\n# Insert sessions\nsessions = [\n    {'subject_id': 'M001', 'session_idx': 1, 'experimenter_id': 'alice',\n     'session_date': '2026-01-06', 'duration': 45.0},\n    {'subject_id': 'M001', 'session_idx': 2, 'experimenter_id': 'alice',\n     'session_date': '2026-01-07', 'duration': 50.0},\n    {'subject_id': 'M002', 'session_idx': 1, 'experimenter_id': 'bob',\n     'session_date': '2026-01-06', 'duration': 40.0},\n    {'subject_id': 'M002', 'session_idx': 2, 'experimenter_id': 'bob',\n     'session_date': '2026-01-08', 'duration': 55.0},\n    {'subject_id': 'M003', 'session_idx': 1, 'experimenter_id': 'alice',\n     'session_date': '2026-01-07', 'duration': 35.0},\n]\nSession.insert(sessions)\n\n# Insert trials\ntrials = []\nfor s in sessions:\n    for i in range(10):\n        trials.append({\n            'subject_id': s['subject_id'],\n            'session_idx': s['session_idx'],\n            'trial_idx': i + 1,\n            'stimulus': random.choice(['left', 'right']),\n            'response': random.choice(['left', 'right']),\n            'correct': random.random() &gt; 0.3,\n            'reaction_time': random.uniform(0.2, 0.8)\n        })\nSession.Trial.insert(trials)\n\nprint(f\"Subjects: {len(Subject())}, Sessions: {len(Session())}, \"\n      f\"Trials: {len(Session.Trial())}\")\n</pre> # Insert sample data import random random.seed(42)  Experimenter.insert([     {'experimenter_id': 'alice', 'full_name': 'Alice Smith'},     {'experimenter_id': 'bob', 'full_name': 'Bob Jones'}, ])  subjects = [     {'subject_id': 'M001', 'species': 'Mus musculus',      'date_of_birth': '2026-01-15', 'sex': 'M', 'weight': 25.3},     {'subject_id': 'M002', 'species': 'Mus musculus',      'date_of_birth': '2026-02-01', 'sex': 'F', 'weight': 22.1},     {'subject_id': 'M003', 'species': 'Mus musculus',      'date_of_birth': '2026-02-15', 'sex': 'M', 'weight': 26.8},     {'subject_id': 'R001', 'species': 'Rattus norvegicus',      'date_of_birth': '2024-01-01', 'sex': 'F', 'weight': 280.5}, ] Subject.insert(subjects)  # Insert sessions sessions = [     {'subject_id': 'M001', 'session_idx': 1, 'experimenter_id': 'alice',      'session_date': '2026-01-06', 'duration': 45.0},     {'subject_id': 'M001', 'session_idx': 2, 'experimenter_id': 'alice',      'session_date': '2026-01-07', 'duration': 50.0},     {'subject_id': 'M002', 'session_idx': 1, 'experimenter_id': 'bob',      'session_date': '2026-01-06', 'duration': 40.0},     {'subject_id': 'M002', 'session_idx': 2, 'experimenter_id': 'bob',      'session_date': '2026-01-08', 'duration': 55.0},     {'subject_id': 'M003', 'session_idx': 1, 'experimenter_id': 'alice',      'session_date': '2026-01-07', 'duration': 35.0}, ] Session.insert(sessions)  # Insert trials trials = [] for s in sessions:     for i in range(10):         trials.append({             'subject_id': s['subject_id'],             'session_idx': s['session_idx'],             'trial_idx': i + 1,             'stimulus': random.choice(['left', 'right']),             'response': random.choice(['left', 'right']),             'correct': random.random() &gt; 0.3,             'reaction_time': random.uniform(0.2, 0.8)         }) Session.Trial.insert(trials)  print(f\"Subjects: {len(Subject())}, Sessions: {len(Session())}, \"       f\"Trials: {len(Session.Trial())}\") <pre>Subjects: 4, Sessions: 5, Trials: 50\n</pre> In\u00a0[4]: Copied! <pre># Simple comparison\nSubject &amp; \"weight &gt; 25\"\n</pre> # Simple comparison Subject &amp; \"weight &gt; 25\" Out[4]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus 2026-01-15 M 25.3M003 Mus musculus 2026-02-15 M 26.8R001 Rattus norvegicus 2024-01-01 F 280.5 <p>Total: 3</p> In\u00a0[5]: Copied! <pre># Date comparison\nSession &amp; \"session_date &gt; '2026-01-06'\"\n</pre> # Date comparison Session &amp; \"session_date &gt; '2026-01-06'\" Out[5]: None <p>subject_id</p> None <p>session_idx</p> <p>experimenter_id</p> None <p>session_date</p> <p>duration</p> minutes M001 2 alice 2026-01-07 50.0M002 2 bob 2026-01-08 55.0M003 1 alice 2026-01-07 35.0 <p>Total: 3</p> In\u00a0[6]: Copied! <pre># Multiple conditions with AND\nSubject &amp; \"sex = 'M' AND weight &gt; 25\"\n</pre> # Multiple conditions with AND Subject &amp; \"sex = 'M' AND weight &gt; 25\" Out[6]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus 2026-01-15 M 25.3M003 Mus musculus 2026-02-15 M 26.8 <p>Total: 2</p> In\u00a0[7]: Copied! <pre># Single attribute\nSubject &amp; {'sex': 'F'}\n</pre> # Single attribute Subject &amp; {'sex': 'F'} Out[7]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M002 Mus musculus 2026-02-01 F 22.1R001 Rattus norvegicus 2024-01-01 F 280.5 <p>Total: 2</p> In\u00a0[8]: Copied! <pre># Multiple attributes (AND)\nSession &amp; {'subject_id': 'M001', 'session_idx': 1}\n</pre> # Multiple attributes (AND) Session &amp; {'subject_id': 'M001', 'session_idx': 1} Out[8]: None <p>subject_id</p> None <p>session_idx</p> <p>experimenter_id</p> None <p>session_date</p> <p>duration</p> minutes M001 1 alice 2026-01-06 45.0 <p>Total: 1</p> In\u00a0[9]: Copied! <pre># Subjects that have at least one session\nSubject &amp; Session\n</pre> # Subjects that have at least one session Subject &amp; Session Out[9]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus 2026-01-15 M 25.3M002 Mus musculus 2026-02-01 F 22.1M003 Mus musculus 2026-02-15 M 26.8 <p>Total: 3</p> In\u00a0[10]: Copied! <pre># Subjects without any sessions (R001 has no sessions)\nSubject - Session\n</pre> # Subjects without any sessions (R001 has no sessions) Subject - Session Out[10]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams R001 Rattus norvegicus 2024-01-01 F 280.5 <p>Total: 1</p> In\u00a0[11]: Copied! <pre># Either of these subjects\nSubject &amp; [{'subject_id': 'M001'}, {'subject_id': 'M002'}]\n</pre> # Either of these subjects Subject &amp; [{'subject_id': 'M001'}, {'subject_id': 'M002'}] Out[11]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus 2026-01-15 M 25.3M002 Mus musculus 2026-02-01 F 22.1 <p>Total: 2</p> In\u00a0[12]: Copied! <pre># These are equivalent\nresult1 = Subject &amp; \"sex = 'M'\" &amp; \"weight &gt; 25\"\nresult2 = (Subject &amp; \"sex = 'M'\") &amp; \"weight &gt; 25\"\n\nprint(f\"Result 1: {len(result1)} rows\")\nprint(f\"Result 2: {len(result2)} rows\")\n</pre> # These are equivalent result1 = Subject &amp; \"sex = 'M'\" &amp; \"weight &gt; 25\" result2 = (Subject &amp; \"sex = 'M'\") &amp; \"weight &gt; 25\"  print(f\"Result 1: {len(result1)} rows\") print(f\"Result 2: {len(result2)} rows\") <pre>Result 1: 2 rows\nResult 2: 2 rows\n</pre> In\u00a0[13]: Copied! <pre># Top 2 heaviest subjects\nSubject &amp; dj.Top(limit=2, order_by='weight DESC')\n</pre> # Top 2 heaviest subjects Subject &amp; dj.Top(limit=2, order_by='weight DESC') Out[13]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams R001 Rattus norvegicus 2024-01-01 F 280.5M003 Mus musculus 2026-02-15 M 26.8 <p>Total: 2</p> In\u00a0[14]: Copied! <pre># Skip first 2, then get next 2 (pagination)\nSubject &amp; dj.Top(limit=2, order_by='weight DESC', offset=2)\n</pre> # Skip first 2, then get next 2 (pagination) Subject &amp; dj.Top(limit=2, order_by='weight DESC', offset=2) Out[14]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus 2026-01-15 M 25.3M002 Mus musculus 2026-02-01 F 22.1 <p>Total: 2</p> In\u00a0[15]: Copied! <pre># Combine with other restrictions\n(Subject &amp; \"sex = 'M'\") &amp; dj.Top(limit=1, order_by='weight DESC')\n</pre> # Combine with other restrictions (Subject &amp; \"sex = 'M'\") &amp; dj.Top(limit=1, order_by='weight DESC') Out[15]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M003 Mus musculus 2026-02-15 M 26.8 <p>Total: 1</p> <p>When to use <code>dj.Top</code> vs fetch-time <code>order_by</code>/<code>limit</code>:</p> <ul> <li>Use <code>dj.Top</code> when the limited result needs to be joined or restricted further</li> <li>Use fetch-time parameters (<code>to_dicts(order_by=..., limit=...)</code>) for final output</li> </ul> <p>Note: Some databases (including MySQL 8.0) don't support LIMIT in certain subquery contexts. If you encounter this limitation, fetch the keys first and use them as a restriction:</p> In\u00a0[16]: Copied! <pre># Get trials only from the 2 longest sessions\n# Workaround: fetch keys first, then use as restriction\nlongest_session_keys = (Session &amp; dj.Top(limit=2, order_by='duration DESC')).keys()\nSession.Trial &amp; longest_session_keys\n</pre> # Get trials only from the 2 longest sessions # Workaround: fetch keys first, then use as restriction longest_session_keys = (Session &amp; dj.Top(limit=2, order_by='duration DESC')).keys() Session.Trial &amp; longest_session_keys Out[16]: None <p>subject_id</p> None <p>session_idx</p> None <p>trial_idx</p> <p>stimulus</p> <p>response</p> <p>correct</p> <p>reaction_time</p> seconds M001 2 1 right left True 0.23M001 2 2 left right True 0.71M001 2 3 left right False 0.58M001 2 4 right left True 0.33M001 2 5 right left True 0.30M001 2 6 left left True 0.36M001 2 7 left right True 0.67M001 2 8 left left True 0.44M001 2 9 left left True 0.54M001 2 10 right left True 0.44M002 2 1 right left False 0.66M002 2 2 left right True 0.27 <p>...</p> <p>Total: 20</p> In\u00a0[17]: Copied! <pre># Primary key only (no arguments)\nSubject.proj()\n</pre> # Primary key only (no arguments) Subject.proj() Out[17]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 bytes bytes bytes bytesM002 bytes bytes bytes bytesM003 bytes bytes bytes bytesR001 bytes bytes bytes bytes <p>Total: 4</p> In\u00a0[18]: Copied! <pre># Primary key + specific attributes\nSubject.proj('species', 'sex')\n</pre> # Primary key + specific attributes Subject.proj('species', 'sex') Out[18]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus bytes M bytesM002 Mus musculus bytes F bytesM003 Mus musculus bytes M bytesR001 Rattus norvegicus bytes F bytes <p>Total: 4</p> In\u00a0[19]: Copied! <pre># All attributes (using ellipsis)\nSubject.proj(...)\n</pre> # All attributes (using ellipsis) Subject.proj(...) Out[19]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus 2026-01-15 M 25.3M002 Mus musculus 2026-02-01 F 22.1M003 Mus musculus 2026-02-15 M 26.8R001 Rattus norvegicus 2024-01-01 F 280.5 <p>Total: 4</p> In\u00a0[20]: Copied! <pre># All except specific attributes\nSubject.proj(..., '-weight')\n</pre> # All except specific attributes Subject.proj(..., '-weight') Out[20]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus 2026-01-15 M bytesM002 Mus musculus 2026-02-01 F bytesM003 Mus musculus 2026-02-15 M bytesR001 Rattus norvegicus 2024-01-01 F bytes <p>Total: 4</p> In\u00a0[21]: Copied! <pre># Rename 'species' to 'animal_species'\nSubject.proj(animal_species='species')\n</pre> # Rename 'species' to 'animal_species' Subject.proj(animal_species='species') Out[21]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 bytes bytes bytes bytesM002 bytes bytes bytes bytesM003 bytes bytes bytes bytesR001 bytes bytes bytes bytes <p>Total: 4</p> In\u00a0[22]: Copied! <pre># Arithmetic computation\nSubject.proj('species', weight_kg='weight / 1000')\n</pre> # Arithmetic computation Subject.proj('species', weight_kg='weight / 1000') Out[22]: None <p>subject_id</p> <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 Mus musculus bytes bytes bytesM002 Mus musculus bytes bytes bytesM003 Mus musculus bytes bytes bytesR001 Rattus norvegicus bytes bytes bytes <p>Total: 4</p> In\u00a0[23]: Copied! <pre># Date functions\nSession.proj('session_date', year='YEAR(session_date)', month='MONTH(session_date)')\n</pre> # Date functions Session.proj('session_date', year='YEAR(session_date)', month='MONTH(session_date)') Out[23]: None <p>subject_id</p> None <p>session_idx</p> <p>experimenter_id</p> None <p>session_date</p> <p>duration</p> minutes M001 1 bytes 2026-01-06 bytesM001 2 bytes 2026-01-07 bytesM002 1 bytes 2026-01-06 bytesM002 2 bytes 2026-01-08 bytesM003 1 bytes 2026-01-07 bytes <p>Total: 5</p> In\u00a0[24]: Copied! <pre># Join Subject and Session on subject_id\nSubject * Session\n</pre> # Join Subject and Session on subject_id Subject * Session Out[24]: <p>subject_id</p> None <p>session_idx</p> <p>experimenter_id</p> None <p>session_date</p> <p>duration</p> minutes <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 1 alice 2026-01-06 45.0 Mus musculus 2026-01-15 M 25.3M001 2 alice 2026-01-07 50.0 Mus musculus 2026-01-15 M 25.3M002 1 bob 2026-01-06 40.0 Mus musculus 2026-02-01 F 22.1M002 2 bob 2026-01-08 55.0 Mus musculus 2026-02-01 F 22.1M003 1 alice 2026-01-07 35.0 Mus musculus 2026-02-15 M 26.8 <p>Total: 5</p> In\u00a0[25]: Copied! <pre># Join then restrict\n(Subject * Session) &amp; \"sex = 'M'\"\n</pre> # Join then restrict (Subject * Session) &amp; \"sex = 'M'\" Out[25]: <p>subject_id</p> None <p>session_idx</p> <p>experimenter_id</p> None <p>session_date</p> <p>duration</p> minutes <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 1 alice 2026-01-06 45.0 Mus musculus 2026-01-15 M 25.3M001 2 alice 2026-01-07 50.0 Mus musculus 2026-01-15 M 25.3M003 1 alice 2026-01-07 35.0 Mus musculus 2026-02-15 M 26.8 <p>Total: 3</p> In\u00a0[26]: Copied! <pre># Restrict then join (equivalent result)\n(Subject &amp; \"sex = 'M'\") * Session\n</pre> # Restrict then join (equivalent result) (Subject &amp; \"sex = 'M'\") * Session Out[26]: <p>subject_id</p> None <p>session_idx</p> <p>experimenter_id</p> None <p>session_date</p> <p>duration</p> minutes <p>species</p> <p>date_of_birth</p> <p>sex</p> <p>weight</p> grams M001 1 alice 2026-01-06 45.0 Mus musculus 2026-01-15 M 25.3M001 2 alice 2026-01-07 50.0 Mus musculus 2026-01-15 M 25.3M003 1 alice 2026-01-07 35.0 Mus musculus 2026-02-15 M 26.8 <p>Total: 3</p> In\u00a0[27]: Copied! <pre># Three-way join\n(Subject * Session * Experimenter).proj('species', 'session_date', 'full_name')\n</pre> # Three-way join (Subject * Session * Experimenter).proj('species', 'session_date', 'full_name') Out[27]: <p>subject_id</p> None <p>session_idx</p> <p>session_date</p> <p>species</p> <p>full_name</p> M001 1 2026-01-06 Mus musculus Alice SmithM001 2 2026-01-07 Mus musculus Alice SmithM002 1 2026-01-06 Mus musculus Bob JonesM002 2 2026-01-08 Mus musculus Bob JonesM003 1 2026-01-07 Mus musculus Alice Smith <p>Total: 5</p> In\u00a0[28]: Copied! <pre># Session contains experimenter_id (FK to Experimenter)\n# extend adds Experimenter's attributes while keeping all Sessions\nSession.extend(Experimenter)\n</pre> # Session contains experimenter_id (FK to Experimenter) # extend adds Experimenter's attributes while keeping all Sessions Session.extend(Experimenter) Out[28]: <p>subject_id</p> None <p>session_idx</p> <p>experimenter_id</p> None <p>session_date</p> <p>duration</p> minutes <p>full_name</p> M001 1 alice 2026-01-06 45.0 Alice SmithM001 2 alice 2026-01-07 50.0 Alice SmithM002 1 bob 2026-01-06 40.0 Bob JonesM002 2 bob 2026-01-08 55.0 Bob JonesM003 1 alice 2026-01-07 35.0 Alice Smith <p>Total: 5</p> <p>Why extend instead of join?</p> <p>A regular join (<code>*</code>) would exclude sessions if their experimenter wasn't in the Experimenter table. Extend preserves all sessions, filling in NULL for missing experimenter data. This is essential when you want to add optional attributes without filtering your results.</p> In\u00a0[29]: Copied! <pre># Count trials per session\nSession.aggr(Session.Trial, n_trials='count(*)')\n</pre> # Count trials per session Session.aggr(Session.Trial, n_trials='count(*)') Out[29]: <p>subject_id</p> None <p>session_idx</p> <p>n_trials</p> calculated attribute M001 1 10M001 2 10M002 1 10M002 2 10M003 1 10 <p>Total: 5</p> In\u00a0[30]: Copied! <pre># Multiple aggregates\nSession.aggr(\n    Session.Trial,\n    n_trials='count(*)',\n    n_correct='sum(CASE WHEN correct THEN 1 ELSE 0 END)',\n    avg_rt='avg(reaction_time)'\n)\n</pre> # Multiple aggregates Session.aggr(     Session.Trial,     n_trials='count(*)',     n_correct='sum(CASE WHEN correct THEN 1 ELSE 0 END)',     avg_rt='avg(reaction_time)' ) Out[30]: <p>subject_id</p> None <p>session_idx</p> <p>n_trials</p> calculated attribute <p>n_correct</p> calculated attribute <p>avg_rt</p> calculated attribute M001 1 10 8 0.50800000000000000000M001 2 10 9 0.46000000000000000000M002 1 10 7 0.45900000000000000000M002 2 10 6 0.50300000000000000000M003 1 10 6 0.51100000000000000000 <p>Total: 5</p> In\u00a0[31]: Copied! <pre># Count sessions per subject\nSubject.aggr(Session, n_sessions='count(*)')\n</pre> # Count sessions per subject Subject.aggr(Session, n_sessions='count(*)') Out[31]: <p>subject_id</p> <p>n_sessions</p> calculated attribute M001 2M002 2M003 1R001 1 <p>Total: 4</p> In\u00a0[32]: Copied! <pre># All subjects, including those without sessions (n_sessions=0)\n# count(session_idx) returns 0 for NULLs, unlike count(*)\nSubject.aggr(Session, n_sessions='count(session_idx)')\n</pre> # All subjects, including those without sessions (n_sessions=0) # count(session_idx) returns 0 for NULLs, unlike count(*) Subject.aggr(Session, n_sessions='count(session_idx)') Out[32]: <p>subject_id</p> <p>n_sessions</p> calculated attribute M001 2M002 2M003 1R001 0 <p>Total: 4</p> In\u00a0[33]: Copied! <pre># Only subjects that have at least one session (exclude those without matches)\nSubject.aggr(Session, n_sessions='count(session_idx)', exclude_nonmatching=True)\n</pre> # Only subjects that have at least one session (exclude those without matches) Subject.aggr(Session, n_sessions='count(session_idx)', exclude_nonmatching=True) Out[33]: <p>subject_id</p> None <p>n_sessions</p> calculated attribute M001 2M002 2M003 1 <p>Total: 3</p> In\u00a0[34]: Copied! <pre># Group by session_date (not a primary key in any table)\ndj.U('session_date').aggr(Session, n_sessions='count(*)', total_duration='sum(duration)')\n</pre> # Group by session_date (not a primary key in any table) dj.U('session_date').aggr(Session, n_sessions='count(*)', total_duration='sum(duration)') Out[34]: <p>session_date</p> <p>n_sessions</p> calculated attribute <p>total_duration</p> calculated attribute 2026-01-06 2 85.02026-01-07 2 85.02026-01-08 1 55.0 <p>Total: 3</p> In\u00a0[35]: Copied! <pre># Universal aggregation: dj.U() with no attributes produces one row\n# This aggregates against the singleton \"universe\"\ndj.U().aggr(Session, total_sessions='count(*)', avg_duration='avg(duration)')\n</pre> # Universal aggregation: dj.U() with no attributes produces one row # This aggregates against the singleton \"universe\" dj.U().aggr(Session, total_sessions='count(*)', avg_duration='avg(duration)') Out[35]: <p>total_sessions</p> calculated attribute <p>avg_duration</p> calculated attribute 5 45.0000000000000000 <p>Total: 1</p> In\u00a0[36]: Copied! <pre># Group by experimenter_id (a foreign key in Session, not part of Session's PK)\n# Without dj.U(), we couldn't aggregate sessions by experimenter\ndj.U('experimenter_id').aggr(Session, n_sessions='count(*)')\n</pre> # Group by experimenter_id (a foreign key in Session, not part of Session's PK) # Without dj.U(), we couldn't aggregate sessions by experimenter dj.U('experimenter_id').aggr(Session, n_sessions='count(*)') Out[36]: <p>experimenter_id</p> None <p>n_sessions</p> calculated attribute alice 3bob 2 <p>Total: 2</p> In\u00a0[37]: Copied! <pre># Unique values\ndj.U('species') &amp; Subject\n</pre> # Unique values dj.U('species') &amp; Subject Out[37]: <p>species</p> Mus musculusRattus norvegicus <p>Total: 2</p> In\u00a0[38]: Copied! <pre># Get all rows as list of dicts\nrows = Subject.to_dicts()\nrows[:2]\n</pre> # Get all rows as list of dicts rows = Subject.to_dicts() rows[:2] Out[38]: <pre>[{'subject_id': 'M001',\n  'species': 'Mus musculus',\n  'date_of_birth': datetime.date(2026, 1, 15),\n  'sex': 'M',\n  'weight': Decimal('25.3')},\n {'subject_id': 'M002',\n  'species': 'Mus musculus',\n  'date_of_birth': datetime.date(2026, 2, 1),\n  'sex': 'F',\n  'weight': Decimal('22.1')}]</pre> In\u00a0[39]: Copied! <pre># Get as pandas DataFrame (primary key as index)\ndf = Subject.to_pandas()\ndf\n</pre> # Get as pandas DataFrame (primary key as index) df = Subject.to_pandas() df Out[39]: species date_of_birth sex weight subject_id M001 Mus musculus 2026-01-15 M 25.3 M002 Mus musculus 2026-02-01 F 22.1 M003 Mus musculus 2026-02-15 M 26.8 R001 Rattus norvegicus 2024-01-01 F 280.5 In\u00a0[40]: Copied! <pre># Structured array (all columns)\narr = Subject.to_arrays()\narr\n</pre> # Structured array (all columns) arr = Subject.to_arrays() arr Out[40]: <pre>array([('M001', 'Mus musculus', datetime.date(2026, 1, 15), 'M', Decimal('25.3')),\n       ('M002', 'Mus musculus', datetime.date(2026, 2, 1), 'F', Decimal('22.1')),\n       ('M003', 'Mus musculus', datetime.date(2026, 2, 15), 'M', Decimal('26.8')),\n       ('R001', 'Rattus norvegicus', datetime.date(2024, 1, 1), 'F', Decimal('280.5'))],\n      dtype=[('subject_id', 'O'), ('species', 'O'), ('date_of_birth', 'O'), ('sex', 'O'), ('weight', 'O')])</pre> In\u00a0[41]: Copied! <pre># Specific columns as separate arrays\nspecies, weights = Subject.to_arrays('species', 'weight')\nprint(f\"Species: {species}\")\nprint(f\"Weights: {weights}\")\n</pre> # Specific columns as separate arrays species, weights = Subject.to_arrays('species', 'weight') print(f\"Species: {species}\") print(f\"Weights: {weights}\") <pre>Species: ['Mus musculus' 'Mus musculus' 'Mus musculus' 'Rattus norvegicus']\nWeights: [Decimal('25.3') Decimal('22.1') Decimal('26.8') Decimal('280.5')]\n</pre> In\u00a0[42]: Copied! <pre># Get primary keys for iteration\nkeys = Session.keys()\nkeys[:3]\n</pre> # Get primary keys for iteration keys = Session.keys() keys[:3] Out[42]: <pre>[{'subject_id': 'M001', 'session_idx': 1},\n {'subject_id': 'M001', 'session_idx': 2},\n {'subject_id': 'M002', 'session_idx': 1}]</pre> In\u00a0[43]: Copied! <pre># Fetch one row (raises error if not exactly 1)\nrow = (Subject &amp; {'subject_id': 'M001'}).fetch1()\nrow\n</pre> # Fetch one row (raises error if not exactly 1) row = (Subject &amp; {'subject_id': 'M001'}).fetch1() row Out[43]: <pre>{'subject_id': 'M001',\n 'species': 'Mus musculus',\n 'date_of_birth': datetime.date(2026, 1, 15),\n 'sex': 'M',\n 'weight': Decimal('25.3')}</pre> In\u00a0[44]: Copied! <pre># Fetch specific attributes from one row\nspecies, weight = (Subject &amp; {'subject_id': 'M001'}).fetch1('species', 'weight')\nprint(f\"{species}: {weight}g\")\n</pre> # Fetch specific attributes from one row species, weight = (Subject &amp; {'subject_id': 'M001'}).fetch1('species', 'weight') print(f\"{species}: {weight}g\") <pre>Mus musculus: 25.3g\n</pre> In\u00a0[45]: Copied! <pre># Sort by weight descending, get top 2\nSubject.to_dicts(order_by='weight DESC', limit=2)\n</pre> # Sort by weight descending, get top 2 Subject.to_dicts(order_by='weight DESC', limit=2) Out[45]: <pre>[{'subject_id': 'R001',\n  'species': 'Rattus norvegicus',\n  'date_of_birth': datetime.date(2024, 1, 1),\n  'sex': 'F',\n  'weight': Decimal('280.5')},\n {'subject_id': 'M003',\n  'species': 'Mus musculus',\n  'date_of_birth': datetime.date(2026, 2, 15),\n  'sex': 'M',\n  'weight': Decimal('26.8')}]</pre> In\u00a0[46]: Copied! <pre># Sort by primary key\nSubject.to_dicts(order_by='KEY')\n</pre> # Sort by primary key Subject.to_dicts(order_by='KEY') Out[46]: <pre>[{'subject_id': 'M001',\n  'species': 'Mus musculus',\n  'date_of_birth': datetime.date(2026, 1, 15),\n  'sex': 'M',\n  'weight': Decimal('25.3')},\n {'subject_id': 'M002',\n  'species': 'Mus musculus',\n  'date_of_birth': datetime.date(2026, 2, 1),\n  'sex': 'F',\n  'weight': Decimal('22.1')},\n {'subject_id': 'M003',\n  'species': 'Mus musculus',\n  'date_of_birth': datetime.date(2026, 2, 15),\n  'sex': 'M',\n  'weight': Decimal('26.8')},\n {'subject_id': 'R001',\n  'species': 'Rattus norvegicus',\n  'date_of_birth': datetime.date(2024, 1, 1),\n  'sex': 'F',\n  'weight': Decimal('280.5')}]</pre> In\u00a0[47]: Copied! <pre># Stream rows (single database cursor)\nfor row in Subject:\n    print(f\"{row['subject_id']}: {row['species']}\")\n</pre> # Stream rows (single database cursor) for row in Subject:     print(f\"{row['subject_id']}: {row['species']}\") <pre>M001: Mus musculus\nM002: Mus musculus\nM003: Mus musculus\nR001: Rattus norvegicus\n</pre> In\u00a0[48]: Copied! <pre># Build a complex query step by step\nmale_mice = Subject &amp; \"sex = 'M'\" &amp; \"species LIKE '%musculus%'\"\nsessions_with_subject = male_mice * Session\nalice_sessions = sessions_with_subject &amp; {'experimenter_id': 'alice'}\nresult = alice_sessions.proj('session_date', 'duration', 'weight')\n\nresult\n</pre> # Build a complex query step by step male_mice = Subject &amp; \"sex = 'M'\" &amp; \"species LIKE '%musculus%'\" sessions_with_subject = male_mice * Session alice_sessions = sessions_with_subject &amp; {'experimenter_id': 'alice'} result = alice_sessions.proj('session_date', 'duration', 'weight')  result Out[48]: <p>subject_id</p> None <p>session_idx</p> <p>session_date</p> <p>duration</p> minutes <p>weight</p> grams M001 1 2026-01-06 45.0 25.3M001 2 2026-01-07 50.0 25.3M003 1 2026-01-07 35.0 26.8 <p>Total: 3</p> In\u00a0[49]: Copied! <pre># Or as a single expression\n((Subject &amp; \"sex = 'M'\" &amp; \"species LIKE '%musculus%'\") \n * Session \n &amp; {'experimenter_id': 'alice'}\n).proj('session_date', 'duration', 'weight')\n</pre> # Or as a single expression ((Subject &amp; \"sex = 'M'\" &amp; \"species LIKE '%musculus%'\")   * Session   &amp; {'experimenter_id': 'alice'} ).proj('session_date', 'duration', 'weight') Out[49]: <p>subject_id</p> None <p>session_idx</p> <p>session_date</p> <p>duration</p> minutes <p>weight</p> grams M001 1 2026-01-06 45.0 25.3M001 2 2026-01-07 50.0 25.3M003 1 2026-01-07 35.0 26.8 <p>Total: 3</p> In\u00a0[50]: Copied! <pre># Without parentheses: join happens first\n# Subject * Session &amp; condition  means  (Subject * Session) &amp; condition\n\n# With parentheses: explicit order\nresult1 = (Subject &amp; \"sex = 'M'\") * Session   # Restrict then join\nresult2 = Subject * (Session &amp; \"duration &gt; 40\")  # Restrict then join\n\nprint(f\"Result 1: {len(result1)} rows\")\nprint(f\"Result 2: {len(result2)} rows\")\n</pre> # Without parentheses: join happens first # Subject * Session &amp; condition  means  (Subject * Session) &amp; condition  # With parentheses: explicit order result1 = (Subject &amp; \"sex = 'M'\") * Session   # Restrict then join result2 = Subject * (Session &amp; \"duration &gt; 40\")  # Restrict then join  print(f\"Result 1: {len(result1)} rows\") print(f\"Result 2: {len(result2)} rows\") <pre>Result 1: 3 rows\nResult 2: 3 rows\n</pre> In\u00a0[51]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/basics/04-queries/#queries","title":"Queries\u00b6","text":"<p>This tutorial covers how to query data in DataJoint. You'll learn:</p> <ul> <li>Restriction (<code>&amp;</code>, <code>-</code>) \u2014 Filtering rows</li> <li>Top (<code>dj.Top</code>) \u2014 Limiting and ordering results</li> <li>Projection (<code>.proj()</code>) \u2014 Selecting and computing columns</li> <li>Join (<code>*</code>) \u2014 Combining tables</li> <li>Extension (<code>.extend()</code>) \u2014 Adding optional attributes</li> <li>Aggregation (<code>.aggr()</code>) \u2014 Grouping and summarizing</li> <li>Fetching \u2014 Retrieving data in various formats</li> </ul> <p>DataJoint queries are lazy\u2014they build SQL expressions that execute only when you fetch data.</p>"},{"location":"tutorials/basics/04-queries/#restriction-and-","title":"Restriction (<code>&amp;</code> and <code>-</code>)\u00b6","text":"<p>Restriction filters rows based on conditions. Use <code>&amp;</code> to select matching rows, <code>-</code> to exclude them.</p>"},{"location":"tutorials/basics/04-queries/#string-conditions","title":"String Conditions\u00b6","text":"<p>SQL expressions using attribute names:</p>"},{"location":"tutorials/basics/04-queries/#dictionary-conditions","title":"Dictionary Conditions\u00b6","text":"<p>Dictionaries specify exact matches:</p>"},{"location":"tutorials/basics/04-queries/#restriction-by-query-expression","title":"Restriction by Query Expression\u00b6","text":"<p>Restrict by another query expression. DataJoint uses semantic matching: attributes with the same name are matched only if they share the same origin through foreign key lineage. This prevents accidental matches on unrelated attributes that happen to share names (like generic <code>id</code> columns in unrelated tables).</p> <p>See Semantic Matching for the full specification.</p>"},{"location":"tutorials/basics/04-queries/#collection-conditions-or","title":"Collection Conditions (OR)\u00b6","text":"<p>Lists create OR conditions:</p>"},{"location":"tutorials/basics/04-queries/#chaining-restrictions","title":"Chaining Restrictions\u00b6","text":"<p>Sequential restrictions combine with AND:</p>"},{"location":"tutorials/basics/04-queries/#top-restriction-djtop","title":"Top Restriction (<code>dj.Top</code>)\u00b6","text":"<p><code>dj.Top</code> is a special restriction that limits and orders query results. Unlike fetch-time <code>order_by</code> and <code>limit</code>, <code>dj.Top</code> applies within the query itself, making it composable with other operators.</p> <pre>query &amp; dj.Top(limit=N, order_by='attr DESC', offset=M)\n</pre> <p>This is useful when you need the \"top N\" rows as part of a larger query\u2014for example, the 5 highest-scoring trials per session.</p>"},{"location":"tutorials/basics/04-queries/#projection-proj","title":"Projection (<code>.proj()</code>)\u00b6","text":"<p>Projection selects, renames, or computes attributes.</p>"},{"location":"tutorials/basics/04-queries/#selecting-attributes","title":"Selecting Attributes\u00b6","text":""},{"location":"tutorials/basics/04-queries/#renaming-attributes","title":"Renaming Attributes\u00b6","text":""},{"location":"tutorials/basics/04-queries/#computed-attributes","title":"Computed Attributes\u00b6","text":""},{"location":"tutorials/basics/04-queries/#join","title":"Join (<code>*</code>)\u00b6","text":"<p>Join combines tables on shared attributes. Unlike SQL, which offers many join variants (INNER, LEFT, RIGHT, FULL, CROSS, NATURAL), DataJoint provides one rigorous join operator with strict semantic rules.</p> <p>The <code>*</code> operator:</p> <ul> <li>Matches only semantically compatible attributes (same name AND same origin via foreign key lineage)</li> <li>Produces a result with a valid primary key determined by functional dependencies</li> <li>Follows clear algebraic properties</li> </ul> <p>This simplicity makes DataJoint queries unambiguous and composable.</p>"},{"location":"tutorials/basics/04-queries/#primary-keys-in-join-results","title":"Primary Keys in Join Results\u00b6","text":"<p>Every query result has a valid primary key. For joins, the result's primary key depends on functional dependencies between the operands:</p> Condition Result Primary Key <code>A \u2192 B</code> (A determines B) PK(A) <code>B \u2192 A</code> (B determines A) PK(B) Both PK(A) Neither PK(A) \u222a PK(B) <p>\"A determines B\" means all of B's primary key attributes exist in A (as primary or secondary attributes).</p> <p>In our example:</p> <ul> <li><code>Session</code> has PK: <code>(subject_id, session_idx)</code></li> <li><code>Trial</code> has PK: <code>(subject_id, session_idx, trial_idx)</code></li> </ul> <p>Since Session's PK is a subset of Trial's PK, <code>Session \u2192 Trial</code>. The join <code>Session * Trial</code> has the same primary key as Session.</p> <p>See the Query Algebra Specification for the complete functional dependency rules.</p>"},{"location":"tutorials/basics/04-queries/#extension-extend","title":"Extension (<code>.extend()</code>)\u00b6","text":"<p>Sometimes you want to add attributes from a related table without losing rows that lack matching entries. The extend operator is a specialized join for this purpose.</p> <p><code>A.extend(B)</code> is equivalent to a left join: it preserves all rows from A, adding B's attributes where matches exist (with NULL where they don't).</p> <p>Requirement: A must \"determine\" B\u2014all of B's primary key attributes must exist in A. This ensures the result maintains A's entity identity.</p>"},{"location":"tutorials/basics/04-queries/#aggregation-aggr","title":"Aggregation (<code>.aggr()</code>)\u00b6","text":"<p>DataJoint aggregation operates entity-to-entity: you aggregate one entity type with respect to another. This differs fundamentally from SQL's <code>GROUP BY</code>, which groups by arbitrary attribute sets.</p> <p>In DataJoint:</p> <pre>Session.aggr(Trial, n_trials='count(*)')\n</pre> <p>This reads: \"For each Session entity, aggregate its associated Trial entities.\"</p> <p>The equivalent SQL would be:</p> <pre>SELECT session.*, COUNT(*) as n_trials\nFROM session\nJOIN trial USING (subject_id, session_idx)\nGROUP BY session.subject_id, session.session_idx\n</pre> <p>The key insight: aggregation always groups by the primary key of the left operand. This enforces meaningful groupings\u2014you aggregate over well-defined entities, not arbitrary attribute combinations.</p>"},{"location":"tutorials/basics/04-queries/#the-exclude_nonmatching-parameter","title":"The <code>exclude_nonmatching</code> Parameter\u00b6","text":"<p>By default, aggregation keeps all entities from the grouping table, even those without matches. This ensures you see zeros rather than missing rows.</p> <p>However, <code>count(*)</code> counts the NULL-joined row as 1. To correctly count 0 for entities without matches, use <code>count(pk_attribute)</code> which excludes NULLs:</p>"},{"location":"tutorials/basics/04-queries/#universal-set-dju","title":"Universal Set (<code>dj.U()</code>)\u00b6","text":"<p>What if you need to aggregate but there's no appropriate entity to group by? DataJoint provides <code>dj.U()</code> (the \"universal set\") for these cases.</p> <p><code>dj.U()</code> (no attributes) represents the singleton entity\u2014the \"one universe.\" Aggregating against it produces a single row with global statistics.</p> <p><code>dj.U('attr1', 'attr2')</code> creates an ad-hoc grouping entity from the specified attributes. This enables aggregation when no table exists with those attributes as its primary key.</p> <p>For example, suppose you want to count sessions by <code>session_date</code>, but no table has <code>session_date</code> as its primary key. You can use <code>dj.U('session_date')</code> to create the grouping:</p>"},{"location":"tutorials/basics/04-queries/#fetching-data","title":"Fetching Data\u00b6","text":"<p>DataJoint 2.0 provides explicit methods for different output formats.</p>"},{"location":"tutorials/basics/04-queries/#to_dicts-list-of-dictionaries","title":"<code>to_dicts()</code> \u2014 List of Dictionaries\u00b6","text":""},{"location":"tutorials/basics/04-queries/#to_pandas-dataframe","title":"<code>to_pandas()</code> \u2014 DataFrame\u00b6","text":""},{"location":"tutorials/basics/04-queries/#to_arrays-numpy-arrays","title":"<code>to_arrays()</code> \u2014 NumPy Arrays\u00b6","text":""},{"location":"tutorials/basics/04-queries/#keys-primary-keys","title":"<code>keys()</code> \u2014 Primary Keys\u00b6","text":""},{"location":"tutorials/basics/04-queries/#fetch1-single-row","title":"<code>fetch1()</code> \u2014 Single Row\u00b6","text":""},{"location":"tutorials/basics/04-queries/#ordering-and-limiting","title":"Ordering and Limiting\u00b6","text":""},{"location":"tutorials/basics/04-queries/#lazy-iteration","title":"Lazy Iteration\u00b6","text":"<p>Iterating directly over a table streams rows efficiently:</p>"},{"location":"tutorials/basics/04-queries/#query-composition","title":"Query Composition\u00b6","text":"<p>Queries are composable and immutable. Build complex queries step by step:</p>"},{"location":"tutorials/basics/04-queries/#operator-precedence","title":"Operator Precedence\u00b6","text":"<p>Python operator precedence applies:</p> <ol> <li><code>*</code> (join) \u2014 highest</li> <li><code>+</code>, <code>-</code> (union, anti-restriction)</li> <li><code>&amp;</code> (restriction) \u2014 lowest</li> </ol> <p>Use parentheses for clarity:</p>"},{"location":"tutorials/basics/04-queries/#quick-reference","title":"Quick Reference\u00b6","text":""},{"location":"tutorials/basics/04-queries/#operators","title":"Operators\u00b6","text":"Operation Syntax Description Restrict <code>A &amp; cond</code> Select matching rows Anti-restrict <code>A - cond</code> Select non-matching rows Top <code>A &amp; dj.Top(limit, order_by)</code> Limit/order results Project <code>A.proj(...)</code> Select/compute columns Join <code>A * B</code> Combine tables Extend <code>A.extend(B)</code> Add B's attributes, keep all A rows Aggregate <code>A.aggr(B, ...)</code> Group and summarize Union <code>A + B</code> Combine entity sets"},{"location":"tutorials/basics/04-queries/#fetch-methods","title":"Fetch Methods\u00b6","text":"Method Returns Use Case <code>to_dicts()</code> <code>list[dict]</code> JSON, iteration <code>to_pandas()</code> <code>DataFrame</code> Data analysis <code>to_arrays()</code> <code>np.ndarray</code> Numeric computation <code>to_arrays('a', 'b')</code> <code>tuple[array, ...]</code> Specific columns <code>keys()</code> <code>list[dict]</code> Primary keys <code>fetch1()</code> <code>dict</code> Single row <p>See the Query Algebra Specification and Fetch API for complete details.</p>"},{"location":"tutorials/basics/04-queries/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Computation \u2014 Building computational pipelines</li> </ul>"},{"location":"tutorials/basics/05-computation/","title":"Computation","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\n\nschema = dj.Schema('tutorial_computation')\n</pre> import datajoint as dj import numpy as np  schema = dj.Schema('tutorial_computation') <pre>[2026-01-27 15:28:58] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Subject(dj.Manual):\n    definition = \"\"\"\n    subject_id : varchar(16)\n    ---\n    species : varchar(50)\n    \"\"\"\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Subject\n    session_idx : int32\n    ---\n    session_date : date\n    \"\"\"\n\n    class Trial(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32\n        ---\n        stimulus : varchar(50)\n        response : varchar(50)\n        correct : bool\n        reaction_time : decimal(3,2)  # seconds\n        \"\"\"\n\n@schema\nclass AnalysisMethod(dj.Lookup):\n    definition = \"\"\"\n    method_name : varchar(32)\n    ---\n    description : varchar(255)\n    \"\"\"\n    contents = [\n        {'method_name': 'basic', 'description': 'Simple accuracy calculation'},\n        {'method_name': 'weighted', 'description': 'Reaction-time weighted accuracy'},\n    ]\n</pre> @schema class Subject(dj.Manual):     definition = \"\"\"     subject_id : varchar(16)     ---     species : varchar(50)     \"\"\"  @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Subject     session_idx : int32     ---     session_date : date     \"\"\"      class Trial(dj.Part):         definition = \"\"\"         -&gt; master         trial_idx : int32         ---         stimulus : varchar(50)         response : varchar(50)         correct : bool         reaction_time : decimal(3,2)  # seconds         \"\"\"  @schema class AnalysisMethod(dj.Lookup):     definition = \"\"\"     method_name : varchar(32)     ---     description : varchar(255)     \"\"\"     contents = [         {'method_name': 'basic', 'description': 'Simple accuracy calculation'},         {'method_name': 'weighted', 'description': 'Reaction-time weighted accuracy'},     ] In\u00a0[3]: Copied! <pre># Insert sample data\nimport random\nrandom.seed(42)\n\nSubject.insert([\n    {'subject_id': 'M001', 'species': 'Mus musculus'},\n    {'subject_id': 'M002', 'species': 'Mus musculus'},\n])\n\nsessions = [\n    {'subject_id': 'M001', 'session_idx': 1, 'session_date': '2026-01-06'},\n    {'subject_id': 'M001', 'session_idx': 2, 'session_date': '2026-01-07'},\n    {'subject_id': 'M002', 'session_idx': 1, 'session_date': '2026-01-06'},\n]\nSession.insert(sessions)\n\n# Insert trials for each session\ntrials = []\nfor s in sessions:\n    for i in range(15):\n        trials.append({\n            'subject_id': s['subject_id'],\n            'session_idx': s['session_idx'],\n            'trial_idx': i + 1,\n            'stimulus': random.choice(['left', 'right']),\n            'response': random.choice(['left', 'right']),\n            'correct': random.random() &gt; 0.3,\n            'reaction_time': random.uniform(0.2, 0.8)\n        })\nSession.Trial.insert(trials)\n\nprint(f\"Subjects: {len(Subject())}, Sessions: {len(Session())}, \"\n      f\"Trials: {len(Session.Trial())}\")\n</pre> # Insert sample data import random random.seed(42)  Subject.insert([     {'subject_id': 'M001', 'species': 'Mus musculus'},     {'subject_id': 'M002', 'species': 'Mus musculus'}, ])  sessions = [     {'subject_id': 'M001', 'session_idx': 1, 'session_date': '2026-01-06'},     {'subject_id': 'M001', 'session_idx': 2, 'session_date': '2026-01-07'},     {'subject_id': 'M002', 'session_idx': 1, 'session_date': '2026-01-06'}, ] Session.insert(sessions)  # Insert trials for each session trials = [] for s in sessions:     for i in range(15):         trials.append({             'subject_id': s['subject_id'],             'session_idx': s['session_idx'],             'trial_idx': i + 1,             'stimulus': random.choice(['left', 'right']),             'response': random.choice(['left', 'right']),             'correct': random.random() &gt; 0.3,             'reaction_time': random.uniform(0.2, 0.8)         }) Session.Trial.insert(trials)  print(f\"Subjects: {len(Subject())}, Sessions: {len(Session())}, \"       f\"Trials: {len(Session.Trial())}\") <pre>Subjects: 2, Sessions: 3, Trials: 45\n</pre> In\u00a0[4]: Copied! <pre>@schema\nclass SessionSummary(dj.Computed):\n    definition = \"\"\"\n    # Summary statistics for each session\n    -&gt; Session\n    ---\n    n_trials : int32\n    n_correct : int32\n    accuracy : float32\n    mean_rt : float32               # mean reaction time (seconds)\n    \"\"\"\n\n    def make(self, key):\n        # Fetch trial data for this session (convert decimal to float)\n        correct, rt = (Session.Trial &amp; key).to_arrays('correct', 'reaction_time')\n        rt = rt.astype(float)\n        \n        n_trials = len(correct)\n        n_correct = sum(correct) if n_trials else 0\n        \n        # Insert computed result\n        self.insert1({\n            **key,\n            'n_trials': n_trials,\n            'n_correct': n_correct,\n            'accuracy': n_correct / n_trials if n_trials else 0.0,\n            'mean_rt': np.mean(rt) if n_trials else 0.0\n        })\n</pre> @schema class SessionSummary(dj.Computed):     definition = \"\"\"     # Summary statistics for each session     -&gt; Session     ---     n_trials : int32     n_correct : int32     accuracy : float32     mean_rt : float32               # mean reaction time (seconds)     \"\"\"      def make(self, key):         # Fetch trial data for this session (convert decimal to float)         correct, rt = (Session.Trial &amp; key).to_arrays('correct', 'reaction_time')         rt = rt.astype(float)                  n_trials = len(correct)         n_correct = sum(correct) if n_trials else 0                  # Insert computed result         self.insert1({             **key,             'n_trials': n_trials,             'n_correct': n_correct,             'accuracy': n_correct / n_trials if n_trials else 0.0,             'mean_rt': np.mean(rt) if n_trials else 0.0         }) In\u00a0[5]: Copied! <pre># Check what needs computing\nprint(f\"Entries to compute: {len(SessionSummary.key_source - SessionSummary)}\")\n\n# Run the computation\nSessionSummary.populate(display_progress=True)\n\n# View results\nSessionSummary()\n</pre> # Check what needs computing print(f\"Entries to compute: {len(SessionSummary.key_source - SessionSummary)}\")  # Run the computation SessionSummary.populate(display_progress=True)  # View results SessionSummary() <pre>Entries to compute: 3\n</pre> <pre>\rSessionSummary:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>\rSessionSummary: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 131.44it/s]</pre> <pre>\n</pre> Out[5]: Summary statistics for each session <p>subject_id</p> None <p>session_idx</p> None <p>n_trials</p> <p>n_correct</p> <p>accuracy</p> <p>mean_rt</p> mean reaction time (seconds) M001 1 15 12 0.8 0.482M001 2 15 12 0.8 0.46933332M002 1 15 9 0.6 0.49266666 <p>Total: 3</p> In\u00a0[6]: Copied! <pre># SessionSummary.key_source is automatically Session\n# (the table referenced in the primary key)\nprint(\"Key source:\")\nSessionSummary.key_source\n</pre> # SessionSummary.key_source is automatically Session # (the table referenced in the primary key) print(\"Key source:\") SessionSummary.key_source <pre>Key source:\n</pre> Out[6]: None <p>subject_id</p> None <p>session_idx</p> <p>session_date</p> M001 1 bytesM001 2 bytesM002 1 bytes <p>Total: 3</p> In\u00a0[7]: Copied! <pre>@schema\nclass SessionAnalysis(dj.Computed):\n    definition = \"\"\"\n    # Analysis with configurable method\n    -&gt; Session\n    -&gt; AnalysisMethod\n    ---\n    score : float32\n    \"\"\"\n\n    def make(self, key):\n        # Fetch trial data (convert decimal to float for computation)\n        correct, rt = (Session.Trial &amp; key).to_arrays('correct', 'reaction_time')\n        rt = rt.astype(float)\n        \n        # Apply method-specific analysis\n        if key['method_name'] == 'basic':\n            score = sum(correct) / len(correct) if len(correct) else 0.0\n        elif key['method_name'] == 'weighted':\n            # Weight correct trials by inverse reaction time\n            weights = 1.0 / rt\n            score = sum(correct * weights) / sum(weights) if len(correct) else 0.0\n        else:\n            score = 0.0\n        \n        self.insert1({**key, 'score': score})\n</pre> @schema class SessionAnalysis(dj.Computed):     definition = \"\"\"     # Analysis with configurable method     -&gt; Session     -&gt; AnalysisMethod     ---     score : float32     \"\"\"      def make(self, key):         # Fetch trial data (convert decimal to float for computation)         correct, rt = (Session.Trial &amp; key).to_arrays('correct', 'reaction_time')         rt = rt.astype(float)                  # Apply method-specific analysis         if key['method_name'] == 'basic':             score = sum(correct) / len(correct) if len(correct) else 0.0         elif key['method_name'] == 'weighted':             # Weight correct trials by inverse reaction time             weights = 1.0 / rt             score = sum(correct * weights) / sum(weights) if len(correct) else 0.0         else:             score = 0.0                  self.insert1({**key, 'score': score}) In\u00a0[8]: Copied! <pre># Key source is Session * AnalysisMethod (all combinations)\nprint(f\"Key source has {len(SessionAnalysis.key_source)} entries\")\nprint(f\"  = {len(Session())} sessions x {len(AnalysisMethod())} methods\")\n\nSessionAnalysis.populate(display_progress=True)\nSessionAnalysis()\n</pre> # Key source is Session * AnalysisMethod (all combinations) print(f\"Key source has {len(SessionAnalysis.key_source)} entries\") print(f\"  = {len(Session())} sessions x {len(AnalysisMethod())} methods\")  SessionAnalysis.populate(display_progress=True) SessionAnalysis() <pre>Key source has 6 entries\n  = 3 sessions x 2 methods\n</pre> <pre>\rSessionAnalysis:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>\rSessionAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;00:00, 434.91it/s]</pre> <pre>\n</pre> Out[8]: Analysis with configurable method <p>subject_id</p> None <p>session_idx</p> None <p>method_name</p> None <p>score</p> M001 1 basic 0.8M001 1 weighted 0.80586994M001 2 basic 0.8M001 2 weighted 0.7651669M002 1 basic 0.6M002 1 weighted 0.5117927 <p>Total: 6</p> In\u00a0[9]: Copied! <pre>@schema\nclass TrialAnalysis(dj.Computed):\n    definition = \"\"\"\n    # Per-trial analysis results\n    -&gt; Session\n    ---\n    n_analyzed : int32\n    \"\"\"\n\n    class TrialResult(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        trial_idx : int32\n        ---\n        rt_percentile : float32     # reaction time percentile within session\n        is_fast : bool              # below median reaction time\n        \"\"\"\n\n    def make(self, key):\n        # Fetch trial data\n        trial_data = (Session.Trial &amp; key).to_dicts()\n        \n        if not trial_data:\n            self.insert1({**key, 'n_analyzed': 0})\n            return\n        \n        # Calculate percentiles (convert decimal to float)\n        rts = [float(t['reaction_time']) for t in trial_data]\n        median_rt = np.median(rts)\n        \n        # Insert master entry\n        self.insert1({**key, 'n_analyzed': len(trial_data)})\n        \n        # Insert part entries\n        parts = []\n        for t in trial_data:\n            rt = float(t['reaction_time'])\n            percentile = sum(r &lt;= rt for r in rts) / len(rts) * 100\n            parts.append({\n                **key,\n                'trial_idx': t['trial_idx'],\n                'rt_percentile': float(percentile),\n                'is_fast': rt &lt; median_rt\n            })\n        \n        self.TrialResult.insert(parts)\n</pre> @schema class TrialAnalysis(dj.Computed):     definition = \"\"\"     # Per-trial analysis results     -&gt; Session     ---     n_analyzed : int32     \"\"\"      class TrialResult(dj.Part):         definition = \"\"\"         -&gt; master         trial_idx : int32         ---         rt_percentile : float32     # reaction time percentile within session         is_fast : bool              # below median reaction time         \"\"\"      def make(self, key):         # Fetch trial data         trial_data = (Session.Trial &amp; key).to_dicts()                  if not trial_data:             self.insert1({**key, 'n_analyzed': 0})             return                  # Calculate percentiles (convert decimal to float)         rts = [float(t['reaction_time']) for t in trial_data]         median_rt = np.median(rts)                  # Insert master entry         self.insert1({**key, 'n_analyzed': len(trial_data)})                  # Insert part entries         parts = []         for t in trial_data:             rt = float(t['reaction_time'])             percentile = sum(r &lt;= rt for r in rts) / len(rts) * 100             parts.append({                 **key,                 'trial_idx': t['trial_idx'],                 'rt_percentile': float(percentile),                 'is_fast': rt &lt; median_rt             })                  self.TrialResult.insert(parts) In\u00a0[10]: Copied! <pre>TrialAnalysis.populate(display_progress=True)\n\nprint(\"Master table:\")\nprint(TrialAnalysis())\n\nprint(\"\\nPart table (first session):\")\nprint((TrialAnalysis.TrialResult &amp; {'subject_id': 'M001', 'session_idx': 1}))\n</pre> TrialAnalysis.populate(display_progress=True)  print(\"Master table:\") print(TrialAnalysis())  print(\"\\nPart table (first session):\") print((TrialAnalysis.TrialResult &amp; {'subject_id': 'M001', 'session_idx': 1})) <pre>\rTrialAnalysis:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>\rTrialAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 244.30it/s]</pre> <pre>Master table:\n*subject_id    *session_idx   n_analyzed    \n+------------+ +------------+ +------------+\nM001           1              15            \nM001           2              15            \nM002           1              15            \n (Total: 3)\n\n\nPart table (first session):\n*subject_id    *session_idx   *trial_idx    rt_percentile  is_fast    \n+------------+ +------------+ +-----------+ +------------+ +---------+\nM001           1              1             33.333332      True       \nM001           1              2             100.0          False      \nM001           1              3             13.333333      True       \nM001           1              4             73.333336      False      \nM001           1              5             40.0           True       \nM001           1              6             53.333332      False      \nM001           1              7             53.333332      False      \nM001           1              8             93.333336      False      \nM001           1              9             60.0           False      \nM001           1              10            80.0           False      \nM001           1              11            6.6666665      True       \nM001           1              12            93.333336      False      \n   ...\n (Total: 15)\n\n</pre> <pre>\n</pre> In\u00a0[11]: Copied! <pre>@schema\nclass SubjectSummary(dj.Computed):\n    definition = \"\"\"\n    # Summary across all sessions for a subject\n    -&gt; Subject\n    ---\n    n_sessions : int32\n    total_trials : int64\n    overall_accuracy : float32\n    \"\"\"\n\n    def make(self, key):\n        # Fetch from SessionSummary (another computed table)\n        summaries = (SessionSummary &amp; key).to_dicts()\n        \n        n_sessions = len(summaries)\n        total_trials = sum(s['n_trials'] for s in summaries)\n        total_correct = sum(s['n_correct'] for s in summaries)\n        \n        self.insert1({\n            **key,\n            'n_sessions': n_sessions,\n            'total_trials': total_trials,\n            'overall_accuracy': total_correct / total_trials if total_trials else 0.0\n        })\n</pre> @schema class SubjectSummary(dj.Computed):     definition = \"\"\"     # Summary across all sessions for a subject     -&gt; Subject     ---     n_sessions : int32     total_trials : int64     overall_accuracy : float32     \"\"\"      def make(self, key):         # Fetch from SessionSummary (another computed table)         summaries = (SessionSummary &amp; key).to_dicts()                  n_sessions = len(summaries)         total_trials = sum(s['n_trials'] for s in summaries)         total_correct = sum(s['n_correct'] for s in summaries)                  self.insert1({             **key,             'n_sessions': n_sessions,             'total_trials': total_trials,             'overall_accuracy': total_correct / total_trials if total_trials else 0.0         }) In\u00a0[12]: Copied! <pre># SubjectSummary depends on SessionSummary which is already populated\nSubjectSummary.populate(display_progress=True)\nSubjectSummary()\n</pre> # SubjectSummary depends on SessionSummary which is already populated SubjectSummary.populate(display_progress=True) SubjectSummary() <pre>\rSubjectSummary:   0%|          | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>\rSubjectSummary: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00, 599.87it/s]</pre> <pre>\n</pre> Out[12]: Summary across all sessions for a subject <p>subject_id</p> None <p>n_sessions</p> <p>total_trials</p> <p>overall_accuracy</p> M001 2 30 0.8M002 1 15 0.6 <p>Total: 2</p> In\u00a0[13]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[13]: In\u00a0[14]: Copied! <pre># Add a new session\nSession.insert1({'subject_id': 'M001', 'session_idx': 3, 'session_date': '2026-01-08'})\n\n# Add trials for the new session\nnew_trials = [\n    {'subject_id': 'M001', 'session_idx': 3, 'trial_idx': i + 1,\n     'stimulus': 'left', 'response': 'left', 'correct': True, 'reaction_time': 0.3}\n    for i in range(20)\n]\nSession.Trial.insert(new_trials)\n\n# Re-populate (only computes new entries)\nprint(\"Populating new session...\")\nSessionSummary.populate(display_progress=True)\nTrialAnalysis.populate(display_progress=True)\n\n# SubjectSummary needs to be recomputed for M001\n# Delete old entry first (cascading not needed here since no dependents)\n(SubjectSummary &amp; {'subject_id': 'M001'}).delete(prompt=False)\nSubjectSummary.populate(display_progress=True)\n\nprint(\"\\nUpdated SubjectSummary:\")\nSubjectSummary()\n</pre> # Add a new session Session.insert1({'subject_id': 'M001', 'session_idx': 3, 'session_date': '2026-01-08'})  # Add trials for the new session new_trials = [     {'subject_id': 'M001', 'session_idx': 3, 'trial_idx': i + 1,      'stimulus': 'left', 'response': 'left', 'correct': True, 'reaction_time': 0.3}     for i in range(20) ] Session.Trial.insert(new_trials)  # Re-populate (only computes new entries) print(\"Populating new session...\") SessionSummary.populate(display_progress=True) TrialAnalysis.populate(display_progress=True)  # SubjectSummary needs to be recomputed for M001 # Delete old entry first (cascading not needed here since no dependents) (SubjectSummary &amp; {'subject_id': 'M001'}).delete(prompt=False) SubjectSummary.populate(display_progress=True)  print(\"\\nUpdated SubjectSummary:\") SubjectSummary() <pre>Populating new session...\n</pre> <pre>\rSessionSummary:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rSessionSummary: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 402.25it/s]</pre> <pre>\n</pre> <pre>\rTrialAnalysis:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rTrialAnalysis: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 385.19it/s]</pre> <pre>[2026-01-27 15:28:59] Deleting 1 rows from \"tutorial_computation\".\"__subject_summary\"\n</pre> <pre>\n</pre> <pre>\rSubjectSummary:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rSubjectSummary: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 653.93it/s]</pre> <pre>\nUpdated SubjectSummary:\n</pre> <pre>\n</pre> Out[14]: Summary across all sessions for a subject <p>subject_id</p> None <p>n_sessions</p> <p>total_trials</p> <p>overall_accuracy</p> M001 3 50 0.88M002 1 15 0.6 <p>Total: 2</p> In\u00a0[15]: Copied! <pre># Populate only for a specific subject\nSessionAnalysis.populate(Subject &amp; {'subject_id': 'M001'})\n</pre> # Populate only for a specific subject SessionAnalysis.populate(Subject &amp; {'subject_id': 'M001'}) Out[15]: <pre>{'success_count': 2, 'error_list': []}</pre> In\u00a0[16]: Copied! <pre># Process at most 5 entries\nSessionAnalysis.populate(max_calls=5, display_progress=True)\n</pre> # Process at most 5 entries SessionAnalysis.populate(max_calls=5, display_progress=True) Out[16]: <pre>{'success_count': 0, 'error_list': []}</pre> In\u00a0[17]: Copied! <pre># Continue despite errors\nresult = SessionAnalysis.populate(suppress_errors=True)\nprint(f\"Success: {result.get('success', 0)}, Errors: {result.get('error', 0)}\")\n</pre> # Continue despite errors result = SessionAnalysis.populate(suppress_errors=True) print(f\"Success: {result.get('success', 0)}, Errors: {result.get('error', 0)}\") <pre>Success: 0, Errors: 0\n</pre> In\u00a0[18]: Copied! <pre># Check progress\nremaining, total = SessionAnalysis.progress()\nprint(f\"SessionAnalysis: {total - remaining}/{total} computed\")\n</pre> # Check progress remaining, total = SessionAnalysis.progress() print(f\"SessionAnalysis: {total - remaining}/{total} computed\") <pre>SessionAnalysis: 8/8 computed\n</pre> In\u00a0[19]: Copied! <pre>@schema\nclass QualityCheck(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    ---\n    passes_qc : bool\n    \"\"\"\n\n    @property\n    def key_source(self):\n        # Only process sessions with at least 10 trials\n        good_sessions = dj.U('subject_id', 'session_idx').aggr(\n            Session.Trial, n='count(*)'\n        ) &amp; 'n &gt;= 10'\n        return Session &amp; good_sessions\n\n    def make(self, key):\n        # Fetch summary stats\n        summary = (SessionSummary &amp; key).fetch1()\n        \n        # QC: accuracy &gt; 50% and mean RT &lt; 1 second\n        passes = summary['accuracy'] &gt; 0.5 and summary['mean_rt'] &lt; 1.0\n        \n        self.insert1({**key, 'passes_qc': passes})\n</pre> @schema class QualityCheck(dj.Computed):     definition = \"\"\"     -&gt; Session     ---     passes_qc : bool     \"\"\"      @property     def key_source(self):         # Only process sessions with at least 10 trials         good_sessions = dj.U('subject_id', 'session_idx').aggr(             Session.Trial, n='count(*)'         ) &amp; 'n &gt;= 10'         return Session &amp; good_sessions      def make(self, key):         # Fetch summary stats         summary = (SessionSummary &amp; key).fetch1()                  # QC: accuracy &gt; 50% and mean RT &lt; 1 second         passes = summary['accuracy'] &gt; 0.5 and summary['mean_rt'] &lt; 1.0                  self.insert1({**key, 'passes_qc': passes}) In\u00a0[20]: Copied! <pre>print(f\"Key source entries: {len(QualityCheck.key_source)}\")\nQualityCheck.populate(display_progress=True)\nQualityCheck()\n</pre> print(f\"Key source entries: {len(QualityCheck.key_source)}\") QualityCheck.populate(display_progress=True) QualityCheck() <pre>Key source entries: 4\n</pre> <pre>\rQualityCheck:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>\rQualityCheck: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 679.10it/s]</pre> <pre>\n</pre> Out[20]: None <p>subject_id</p> None <p>session_idx</p> None <p>passes_qc</p> M001 1 TrueM001 2 TrueM001 3 TrueM002 1 True <p>Total: 4</p> In\u00a0[21]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/basics/05-computation/#computation","title":"Computation\u00b6","text":"<p>This tutorial covers how to build computational pipelines with DataJoint. You'll learn:</p> <ul> <li>Computed tables \u2014 Automatic derivation from other tables</li> <li>Imported tables \u2014 Ingesting data from external files</li> <li>The <code>make()</code> method \u2014 Computing and inserting results</li> <li>Part tables \u2014 Storing detailed results</li> <li>Populate patterns \u2014 Running computations efficiently</li> </ul> <p>DataJoint's auto-populated tables (<code>Computed</code> and <code>Imported</code>) execute automatically based on their dependencies.</p>"},{"location":"tutorials/basics/05-computation/#manual-tables-source-data","title":"Manual Tables (Source Data)\u00b6","text":"<p>First, let's define the source tables that our computations will depend on:</p>"},{"location":"tutorials/basics/05-computation/#computed-tables","title":"Computed Tables\u00b6","text":"<p>A <code>Computed</code> table derives its data from other DataJoint tables. The <code>make()</code> method computes and inserts one entry at a time.</p>"},{"location":"tutorials/basics/05-computation/#basic-computed-table","title":"Basic Computed Table\u00b6","text":""},{"location":"tutorials/basics/05-computation/#running-computations-with-populate","title":"Running Computations with <code>populate()</code>\u00b6","text":"<p>The <code>populate()</code> method automatically finds entries that need computing and calls <code>make()</code> for each:</p>"},{"location":"tutorials/basics/05-computation/#key-source","title":"Key Source\u00b6","text":"<p>The <code>key_source</code> property defines which entries should be computed. By default, it's the join of all parent tables referenced in the primary key:</p>"},{"location":"tutorials/basics/05-computation/#multiple-dependencies","title":"Multiple Dependencies\u00b6","text":"<p>Computed tables can depend on multiple parent tables. The <code>key_source</code> is the join of all parents:</p>"},{"location":"tutorials/basics/05-computation/#computed-tables-with-part-tables","title":"Computed Tables with Part Tables\u00b6","text":"<p>Use part tables to store detailed results alongside summary data:</p>"},{"location":"tutorials/basics/05-computation/#cascading-computations","title":"Cascading Computations\u00b6","text":"<p>Computed tables can depend on other computed tables, creating a pipeline:</p>"},{"location":"tutorials/basics/05-computation/#view-the-pipeline","title":"View the Pipeline\u00b6","text":"<p>Visualize the dependency structure:</p>"},{"location":"tutorials/basics/05-computation/#recomputation-after-changes","title":"Recomputation After Changes\u00b6","text":"<p>When source data changes, delete the affected computed entries and re-populate:</p>"},{"location":"tutorials/basics/05-computation/#populate-options","title":"Populate Options\u00b6","text":""},{"location":"tutorials/basics/05-computation/#restrict-to-specific-entries","title":"Restrict to Specific Entries\u00b6","text":""},{"location":"tutorials/basics/05-computation/#limit-number-of-computations","title":"Limit Number of Computations\u00b6","text":""},{"location":"tutorials/basics/05-computation/#error-handling","title":"Error Handling\u00b6","text":""},{"location":"tutorials/basics/05-computation/#progress-tracking","title":"Progress Tracking\u00b6","text":""},{"location":"tutorials/basics/05-computation/#custom-key-source","title":"Custom Key Source\u00b6","text":"<p>Override <code>key_source</code> to customize which entries to compute:</p>"},{"location":"tutorials/basics/05-computation/#best-practices","title":"Best Practices\u00b6","text":""},{"location":"tutorials/basics/05-computation/#1-keep-make-simple-and-idempotent","title":"1. Keep <code>make()</code> Simple and Idempotent\u00b6","text":"<pre>def make(self, key):\n    # 1. Fetch source data\n    data = (SourceTable &amp; key).fetch1()\n    \n    # 2. Compute result\n    result = compute(data)\n    \n    # 3. Insert result\n    self.insert1({**key, **result})\n</pre>"},{"location":"tutorials/basics/05-computation/#2-use-part-tables-for-detailed-results","title":"2. Use Part Tables for Detailed Results\u00b6","text":"<p>Store summary in master, details in parts:</p> <pre>def make(self, key):\n    self.insert1({**key, 'summary': s})  # Master\n    self.Detail.insert(details)          # Parts\n</pre>"},{"location":"tutorials/basics/05-computation/#3-re-populate-after-data-changes","title":"3. Re-populate After Data Changes\u00b6","text":"<pre># Delete affected entries (cascades automatically)\n(SourceTable &amp; key).delete()\n\n# Reinsert corrected data\nSourceTable.insert1(corrected)\n\n# Re-populate\nComputedTable.populate()\n</pre>"},{"location":"tutorials/basics/05-computation/#4-use-lookup-tables-for-parameters","title":"4. Use Lookup Tables for Parameters\u00b6","text":"<pre>@schema\nclass Method(dj.Lookup):\n    definition = \"...\"\n    contents = [...]  # Pre-defined methods\n\n@schema\nclass Analysis(dj.Computed):\n    definition = \"\"\"\n    -&gt; Session\n    -&gt; Method   # Parameter combinations\n    ---\n    result : float64\n    \"\"\"\n</pre> <p>See the AutoPopulate Specification for complete details.</p>"},{"location":"tutorials/basics/05-computation/#quick-reference","title":"Quick Reference\u00b6","text":"Method Description <code>populate()</code> Compute all pending entries <code>populate(restriction)</code> Compute subset of entries <code>populate(max_calls=N)</code> Compute at most N entries <code>populate(display_progress=True)</code> Show progress bar <code>populate(suppress_errors=True)</code> Continue on errors <code>progress()</code> Check completion status <code>key_source</code> Entries that should be computed"},{"location":"tutorials/basics/06-object-storage/","title":"Object-Augmented Schemas","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\n\nschema = dj.Schema('tutorial_oas')\n\n# Clean slate: drop existing schema if re-running\nschema.drop(prompt=False)\nschema = dj.Schema('tutorial_oas')\n</pre> import datajoint as dj import numpy as np  schema = dj.Schema('tutorial_oas')  # Clean slate: drop existing schema if re-running schema.drop(prompt=False) schema = dj.Schema('tutorial_oas') <pre>[2026-01-27 15:29:05] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Recording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int32\n    ---\n    metadata : &lt;blob&gt;         # Dict, stored in database\n    waveform : &lt;blob&gt;         # NumPy array, stored in database\n    \"\"\"\n</pre> @schema class Recording(dj.Manual):     definition = \"\"\"     recording_id : int32     ---     metadata :          # Dict, stored in database     waveform :          # NumPy array, stored in database     \"\"\" In\u00a0[3]: Copied! <pre># Insert with blob data\nRecording.insert1({\n    'recording_id': 1,\n    'metadata': {'channels': 32, 'sample_rate': 30000, 'duration': 60.0},\n    'waveform': np.random.randn(32, 30000)  # 32 channels x 1 second\n})\n\nRecording()\n</pre> # Insert with blob data Recording.insert1({     'recording_id': 1,     'metadata': {'channels': 32, 'sample_rate': 30000, 'duration': 60.0},     'waveform': np.random.randn(32, 30000)  # 32 channels x 1 second })  Recording() Out[3]: None <p>recording_id</p> <p>metadata</p> Dict, stored in database <p>waveform</p> NumPy array, stored in database 1 &lt;blob&gt; &lt;blob&gt; <p>Total: 1</p> In\u00a0[4]: Copied! <pre># Fetch blob data\ndata = (Recording &amp; {'recording_id': 1}).fetch1()\nprint(f\"Metadata: {data['metadata']}\")\nprint(f\"Waveform shape: {data['waveform'].shape}\")\n</pre> # Fetch blob data data = (Recording &amp; {'recording_id': 1}).fetch1() print(f\"Metadata: {data['metadata']}\") print(f\"Waveform shape: {data['waveform'].shape}\") <pre>Metadata: {'channels': 32, 'sample_rate': 30000, 'duration': 60.0}\nWaveform shape: (32, 30000)\n</pre> In\u00a0[5]: Copied! <pre>@schema\nclass AnalysisResult(dj.Manual):\n    definition = \"\"\"\n    result_id : int32\n    ---\n    arrays : &lt;blob&gt;\n    nested_data : &lt;blob&gt;\n    \"\"\"\n\n# Store complex data structures\narrays = {'x': np.array([1, 2, 3]), 'y': np.array([4, 5, 6])}\nnested = {'arrays': [np.array([1, 2]), np.array([3, 4])], 'params': {'a': 1, 'b': 2}}\n\nAnalysisResult.insert1({\n    'result_id': 1,\n    'arrays': arrays,\n    'nested_data': nested\n})\n\n# Fetch back\nresult = (AnalysisResult &amp; {'result_id': 1}).fetch1()\nprint(f\"Arrays type: {type(result['arrays'])}\")\nprint(f\"Arrays keys: {result['arrays'].keys()}\")\n</pre> @schema class AnalysisResult(dj.Manual):     definition = \"\"\"     result_id : int32     ---     arrays :      nested_data :      \"\"\"  # Store complex data structures arrays = {'x': np.array([1, 2, 3]), 'y': np.array([4, 5, 6])} nested = {'arrays': [np.array([1, 2]), np.array([3, 4])], 'params': {'a': 1, 'b': 2}}  AnalysisResult.insert1({     'result_id': 1,     'arrays': arrays,     'nested_data': nested })  # Fetch back result = (AnalysisResult &amp; {'result_id': 1}).fetch1() print(f\"Arrays type: {type(result['arrays'])}\") print(f\"Arrays keys: {result['arrays'].keys()}\") <pre>Arrays type: &lt;class 'dict'&gt;\nArrays keys: dict_keys(['x', 'y'])\n</pre> In\u00a0[6]: Copied! <pre>import tempfile\nimport os\n\n# Create a store for this tutorial\nstore_path = tempfile.mkdtemp(prefix='dj_store_')\n\n# Configure a named store for this tutorial\ndj.config.stores['tutorial'] = {\n    'protocol': 'file',\n    'location': store_path\n}\n\nprint(f\"Store configured at: {store_path}\")\n</pre> import tempfile import os  # Create a store for this tutorial store_path = tempfile.mkdtemp(prefix='dj_store_')  # Configure a named store for this tutorial dj.config.stores['tutorial'] = {     'protocol': 'file',     'location': store_path }  print(f\"Store configured at: {store_path}\") <pre>Store configured at: /var/folders/cn/dpwf5t7j3gd8gzyw2r7dhm8r0000gn/T/dj_store_sp2ddbn9\n</pre> In\u00a0[7]: Copied! <pre>@schema\nclass LargeRecording(dj.Manual):\n    definition = \"\"\"\n    recording_id : int32\n    ---\n    small_data : &lt;blob&gt;            # In database (small)\n    large_data : &lt;blob@tutorial&gt;   # In object storage (large)\n    \"\"\"\n</pre> @schema class LargeRecording(dj.Manual):     definition = \"\"\"     recording_id : int32     ---     small_data :             # In database (small)     large_data :    # In object storage (large)     \"\"\" In\u00a0[8]: Copied! <pre># Insert data - usage is identical regardless of storage\nsmall = np.random.randn(10, 10)\nlarge = np.random.randn(1000, 1000)  # ~8 MB array\n\nLargeRecording.insert1({\n    'recording_id': 1,\n    'small_data': small,\n    'large_data': large\n})\n\nLargeRecording()\n</pre> # Insert data - usage is identical regardless of storage small = np.random.randn(10, 10) large = np.random.randn(1000, 1000)  # ~8 MB array  LargeRecording.insert1({     'recording_id': 1,     'small_data': small,     'large_data': large })  LargeRecording() Out[8]: None <p>recording_id</p> <p>small_data</p> In database (small) <p>large_data</p> In object storage (large) 1 &lt;blob&gt; &lt;blob&gt; <p>Total: 1</p> In\u00a0[9]: Copied! <pre># Fetch is also identical - storage is transparent\ndata = (LargeRecording &amp; {'recording_id': 1}).fetch1()\nprint(f\"Small data shape: {data['small_data'].shape}\")\nprint(f\"Large data shape: {data['large_data'].shape}\")\n</pre> # Fetch is also identical - storage is transparent data = (LargeRecording &amp; {'recording_id': 1}).fetch1() print(f\"Small data shape: {data['small_data'].shape}\") print(f\"Large data shape: {data['large_data'].shape}\") <pre>Small data shape: (10, 10)\nLarge data shape: (1000, 1000)\n</pre> In\u00a0[10]: Copied! <pre># Objects are stored in the configured location\nfor root, dirs, files in os.walk(store_path):\n    for f in files:\n        path = os.path.join(root, f)\n        size = os.path.getsize(path)\n        print(f\"{os.path.relpath(path, store_path)}: {size:,} bytes\")\n</pre> # Objects are stored in the configured location for root, dirs, files in os.walk(store_path):     for f in files:         path = os.path.join(root, f)         size = os.path.getsize(path)         print(f\"{os.path.relpath(path, store_path)}: {size:,} bytes\") <pre>_hash/tutorial_oas/2ag53duaiyvpii3ar3dux3rmlq: 7,685,350 bytes\n</pre> In\u00a0[11]: Copied! <pre># Insert the same data twice\nshared_data = np.ones((500, 500))\n\nLargeRecording.insert([\n    {'recording_id': 2, 'small_data': small, 'large_data': shared_data},\n    {'recording_id': 3, 'small_data': small, 'large_data': shared_data},  # Same!\n])\n\nprint(f\"Rows in table: {len(LargeRecording())}\")\n\n# Deduplication: identical data stored once\nfiles = [f for _, _, fs in os.walk(store_path) for f in fs]\nprint(f\"Files in store: {len(files)}\")\n</pre> # Insert the same data twice shared_data = np.ones((500, 500))  LargeRecording.insert([     {'recording_id': 2, 'small_data': small, 'large_data': shared_data},     {'recording_id': 3, 'small_data': small, 'large_data': shared_data},  # Same! ])  print(f\"Rows in table: {len(LargeRecording())}\")  # Deduplication: identical data stored once files = [f for _, _, fs in os.walk(store_path) for f in fs] print(f\"Files in store: {len(files)}\") <pre>Rows in table: 3\nFiles in store: 2\n</pre> In\u00a0[12]: Copied! <pre>@schema\nclass ImagingSession(dj.Manual):\n    definition = \"\"\"\n    subject_id : int32\n    session_id : int32\n    ---\n    n_frames : int32\n    frame_rate : decimal(4,1)\n    frames : &lt;object@tutorial&gt;    # Zarr array stored at path derived from PK\n    \"\"\"\n</pre> @schema class ImagingSession(dj.Manual):     definition = \"\"\"     subject_id : int32     session_id : int32     ---     n_frames : int32     frame_rate : decimal(4,1)     frames :     # Zarr array stored at path derived from PK     \"\"\" In\u00a0[13]: Copied! <pre>import zarr\n\n# Simulate acquiring imaging data frame-by-frame\nn_frames = 100\nheight, width = 512, 512\n\nwith ImagingSession.staged_insert1 as staged:\n    # Set primary key values first\n    staged.rec['subject_id'] = 1\n    staged.rec['session_id'] = 1\n    \n    # Get storage handle for the object field\n    store = staged.store('frames', '.zarr')\n    \n    # Create Zarr array directly in object storage\n    z = zarr.open(store, mode='w', shape=(n_frames, height, width),\n                  chunks=(10, height, width), dtype='int32')\n    \n    # Write frames as they are \"acquired\"\n    for i in range(n_frames):\n        frame = np.random.randint(0, 4096, (height, width), dtype='int32')\n        z[i] = frame\n    \n    # Set remaining attributes\n    staged.rec['n_frames'] = n_frames\n    staged.rec['frame_rate'] = 30.0\n\n# Record is now inserted with metadata computed from the Zarr\nImagingSession()\n</pre> import zarr  # Simulate acquiring imaging data frame-by-frame n_frames = 100 height, width = 512, 512  with ImagingSession.staged_insert1 as staged:     # Set primary key values first     staged.rec['subject_id'] = 1     staged.rec['session_id'] = 1          # Get storage handle for the object field     store = staged.store('frames', '.zarr')          # Create Zarr array directly in object storage     z = zarr.open(store, mode='w', shape=(n_frames, height, width),                   chunks=(10, height, width), dtype='int32')          # Write frames as they are \"acquired\"     for i in range(n_frames):         frame = np.random.randint(0, 4096, (height, width), dtype='int32')         z[i] = frame          # Set remaining attributes     staged.rec['n_frames'] = n_frames     staged.rec['frame_rate'] = 30.0  # Record is now inserted with metadata computed from the Zarr ImagingSession() Out[13]: None <p>subject_id</p> <p>session_id</p> <p>n_frames</p> <p>frame_rate</p> <p>frames</p> Zarr array stored at path derived from PK 1 1 100 30.0 &lt;object&gt; <p>Total: 1</p> In\u00a0[14]: Copied! <pre># Fetch returns an ObjectRef for lazy access\nref = (ImagingSession &amp; {'subject_id': 1, 'session_id': 1}).fetch1('frames')\nprint(f\"Type: {type(ref).__name__}\")\nprint(f\"Path: {ref.path}\")\n\n# Open as Zarr array (data stays in object storage)\nz = zarr.open(ref.fsmap, mode='r')\nprint(f\"Shape: {z.shape}\")\nprint(f\"Chunks: {z.chunks}\")\nprint(f\"First frame mean: {z[0].mean():.1f}\")\n</pre> # Fetch returns an ObjectRef for lazy access ref = (ImagingSession &amp; {'subject_id': 1, 'session_id': 1}).fetch1('frames') print(f\"Type: {type(ref).__name__}\") print(f\"Path: {ref.path}\")  # Open as Zarr array (data stays in object storage) z = zarr.open(ref.fsmap, mode='r') print(f\"Shape: {z.shape}\") print(f\"Chunks: {z.chunks}\") print(f\"First frame mean: {z[0].mean():.1f}\") <pre>Type: ObjectRef\nPath: tutorial_oas/ImagingSession/subject_id=1/session_id=1/frames_Itka_HXm.zarr\nShape: (100, 512, 512)\nChunks: (10, 512, 512)\nFirst frame mean: 2048.7\n</pre> In\u00a0[15]: Copied! <pre>@schema\nclass Document(dj.Manual):\n    definition = \"\"\"\n    doc_id : int32\n    ---\n    report : &lt;attach@tutorial&gt;\n    \"\"\"\n</pre> @schema class Document(dj.Manual):     definition = \"\"\"     doc_id : int32     ---     report :      \"\"\" In\u00a0[16]: Copied! <pre># Create a sample file\nsample_file = os.path.join(tempfile.gettempdir(), 'analysis_report.txt')\nwith open(sample_file, 'w') as f:\n    f.write('Analysis Results\\n')\n    f.write('================\\n')\n    f.write('Accuracy: 95.2%\\n')\n\n# Insert using file path directly\nDocument.insert1({\n    'doc_id': 1,\n    'report': sample_file  # Just pass the path\n})\n\nDocument()\n</pre> # Create a sample file sample_file = os.path.join(tempfile.gettempdir(), 'analysis_report.txt') with open(sample_file, 'w') as f:     f.write('Analysis Results\\n')     f.write('================\\n')     f.write('Accuracy: 95.2%\\n')  # Insert using file path directly Document.insert1({     'doc_id': 1,     'report': sample_file  # Just pass the path })  Document() Out[16]: None <p>doc_id</p> <p>report</p> 1 &lt;attach&gt; <p>Total: 1</p> In\u00a0[17]: Copied! <pre># Fetch returns path to extracted file\ndoc_path = (Document &amp; {'doc_id': 1}).fetch1('report')\nprint(f\"Type: {type(doc_path)}\")\nprint(f\"Path: {doc_path}\")\n\n# Read the content\nwith open(doc_path, 'r') as f:\n    print(f\"Content:\\n{f.read()}\")\n</pre> # Fetch returns path to extracted file doc_path = (Document &amp; {'doc_id': 1}).fetch1('report') print(f\"Type: {type(doc_path)}\") print(f\"Path: {doc_path}\")  # Read the content with open(doc_path, 'r') as f:     print(f\"Content:\\n{f.read()}\") <pre>Type: &lt;class 'str'&gt;\nPath: analysis_report.txt\nContent:\nAnalysis Results\n================\nAccuracy: 95.2%\n\n</pre> In\u00a0[18]: Copied! <pre>@schema\nclass ProcessedRecording(dj.Computed):\n    definition = \"\"\"\n    -&gt; LargeRecording\n    ---\n    filtered : &lt;blob@tutorial&gt;     # Result in object storage\n    mean_value : float64\n    \"\"\"\n\n    def make(self, key):\n        # Fetch source data\n        data = (LargeRecording &amp; key).fetch1('large_data')\n        \n        # Process\n        from scipy.ndimage import gaussian_filter\n        filtered = gaussian_filter(data, sigma=2)\n        \n        self.insert1({\n            **key,\n            'filtered': filtered,\n            'mean_value': float(np.mean(filtered))\n        })\n</pre> @schema class ProcessedRecording(dj.Computed):     definition = \"\"\"     -&gt; LargeRecording     ---     filtered :      # Result in object storage     mean_value : float64     \"\"\"      def make(self, key):         # Fetch source data         data = (LargeRecording &amp; key).fetch1('large_data')                  # Process         from scipy.ndimage import gaussian_filter         filtered = gaussian_filter(data, sigma=2)                  self.insert1({             **key,             'filtered': filtered,             'mean_value': float(np.mean(filtered))         }) In\u00a0[19]: Copied! <pre>ProcessedRecording.populate(display_progress=True)\nProcessedRecording()\n</pre> ProcessedRecording.populate(display_progress=True) ProcessedRecording() <pre>\rProcessedRecording:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>\rProcessedRecording:  33%|\u2588\u2588\u2588\u258e      | 1/3 [00:00&lt;00:01,  1.73it/s]</pre> <pre>\rProcessedRecording: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  4.94it/s]</pre> <pre>\n</pre> Out[19]: None <p>recording_id</p> None <p>filtered</p> Result in object storage <p>mean_value</p> 1 &lt;blob&gt; 0.000130258423124673472 &lt;blob&gt; 1.00000000000000023 &lt;blob&gt; 1.0000000000000002 <p>Total: 3</p> In\u00a0[20]: Copied! <pre># Fetch only scalar metadata (fast)\nmeta = (ProcessedRecording &amp; {'recording_id': 1}).fetch1('mean_value')\nprint(f\"Mean value: {meta}\")\n</pre> # Fetch only scalar metadata (fast) meta = (ProcessedRecording &amp; {'recording_id': 1}).fetch1('mean_value') print(f\"Mean value: {meta}\") <pre>Mean value: 0.00013025842312467347\n</pre> In\u00a0[21]: Copied! <pre># Fetch large data only when needed\nfiltered = (ProcessedRecording &amp; {'recording_id': 1}).fetch1('filtered')\nprint(f\"Filtered shape: {filtered.shape}\")\n</pre> # Fetch large data only when needed filtered = (ProcessedRecording &amp; {'recording_id': 1}).fetch1('filtered') print(f\"Filtered shape: {filtered.shape}\") <pre>Filtered shape: (1000, 1000)\n</pre> In\u00a0[22]: Copied! <pre># Efficient: project to scalar columns before join\nresult = LargeRecording.proj('recording_id') * ProcessedRecording.proj('mean_value')\nresult\n</pre> # Efficient: project to scalar columns before join result = LargeRecording.proj('recording_id') * ProcessedRecording.proj('mean_value') result Out[22]: <p>recording_id</p> <p>mean_value</p> 1 0.000130258423124673472 1.00000000000000023 1.0000000000000002 <p>Total: 3</p> In\u00a0[23]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\nimport shutil\nshutil.rmtree(store_path, ignore_errors=True)\n</pre> # Cleanup schema.drop(prompt=False) import shutil shutil.rmtree(store_path, ignore_errors=True)"},{"location":"tutorials/basics/06-object-storage/#object-augmented-schemas","title":"Object-Augmented Schemas\u00b6","text":"<p>This tutorial covers DataJoint's Object-Augmented Schema (OAS) model. You'll learn:</p> <ul> <li>The OAS concept \u2014 Unified relational + object storage</li> <li>Blobs \u2014 Storing arrays and Python objects</li> <li>Object storage \u2014 Scaling to large datasets</li> <li>Staged insert \u2014 Writing directly to object storage (Zarr, HDF5)</li> <li>Attachments \u2014 Preserving file names and formats</li> <li>Codecs \u2014 How data is serialized and deserialized</li> </ul> <p>In an Object-Augmented Schema, the relational database and object storage operate as a single integrated system\u2014not as separate \"internal\" and \"external\" components.</p>"},{"location":"tutorials/basics/06-object-storage/#the-object-augmented-schema-model","title":"The Object-Augmented Schema Model\u00b6","text":"<p>Scientific data often combines:</p> <ul> <li>Structured metadata \u2014 Subjects, sessions, parameters (relational)</li> <li>Large data objects \u2014 Arrays, images, recordings (binary)</li> </ul> <p>DataJoint's OAS model manages both as a unified system:</p> <pre>\nblock-beta\n    columns 1\n    block:oas:1\n        columns 2\n        OAS[\"Object-Augmented Schema\"]:2\n        block:db:1\n            DB[\"Relational Database\"]\n            DB1[\"Metadata\"]\n            DB2[\"Keys\"]\n            DB3[\"Relationships\"]\n        end\n        block:os:1\n            OS[\"Object Storage (S3/File/etc)\"]\n            OS1[\"Large arrays\"]\n            OS2[\"Images/videos\"]\n            OS3[\"Recordings\"]\n        end\n    end\n</pre><p>From the user's perspective, this is one schema\u2014storage location is transparent.</p>"},{"location":"tutorials/basics/06-object-storage/#blob-attributes","title":"Blob Attributes\u00b6","text":"<p>Use <code>&lt;blob&gt;</code> to store arbitrary Python objects:</p>"},{"location":"tutorials/basics/06-object-storage/#what-can-be-stored-in-blobs","title":"What Can Be Stored in Blobs?\u00b6","text":"<p>The <code>&lt;blob&gt;</code> codec handles:</p> <ul> <li>NumPy arrays (any dtype, any shape)</li> <li>Python dicts, lists, tuples, sets</li> <li>Strings, bytes, integers, floats</li> <li>datetime objects and UUIDs</li> <li>Nested combinations of the above</li> </ul> <p>Note: Pandas DataFrames should be converted before storage (e.g., <code>df.to_dict()</code> or <code>df.to_records()</code>).</p>"},{"location":"tutorials/basics/06-object-storage/#object-storage-with","title":"Object Storage with <code>@</code>\u00b6","text":"<p>For large datasets, add <code>@</code> to route data to object storage. The schema remains unified\u2014only the physical storage location changes.</p>"},{"location":"tutorials/basics/06-object-storage/#configure-object-storage","title":"Configure Object Storage\u00b6","text":"<p>First, configure a store:</p>"},{"location":"tutorials/basics/06-object-storage/#using-object-storage","title":"Using Object Storage\u00b6","text":""},{"location":"tutorials/basics/06-object-storage/#hash-addressed-storage","title":"Hash-Addressed Storage\u00b6","text":"<p><code>&lt;blob@&gt;</code> uses hash-addressed storage. Data is identified by a Base32-encoded MD5 hash, enabling automatic deduplication\u2014identical data is stored only once:</p>"},{"location":"tutorials/basics/06-object-storage/#schema-addressed-storage-with-object","title":"Schema-Addressed Storage with <code>&lt;object@&gt;</code>\u00b6","text":"<p>While <code>&lt;blob@&gt;</code> uses hash-addressed storage with deduplication, <code>&lt;object@&gt;</code> uses schema-addressed storage where each row has its own dedicated storage path:</p> Aspect <code>&lt;blob@&gt;</code> <code>&lt;object@&gt;</code> Addressing By content hash By primary key Deduplication Yes No Deletion Garbage collected With row Use case Arrays, serialized objects Zarr, HDF5, multi-file outputs <p>Use <code>&lt;object@&gt;</code> when you need:</p> <ul> <li>Hierarchical formats like Zarr or HDF5</li> <li>Direct write access during data generation</li> <li>Each row to have its own isolated storage location</li> </ul>"},{"location":"tutorials/basics/06-object-storage/#staged-insert-for-direct-object-storage-writes","title":"Staged Insert for Direct Object Storage Writes\u00b6","text":"<p>For large datasets like multi-GB imaging recordings, copying data from local storage to object storage is inefficient. The <code>staged_insert1</code> context manager lets you write directly to object storage before finalizing the database insert:</p> <ol> <li>Set primary key values in <code>staged.rec</code></li> <li>Get a storage handle with <code>staged.store(field, extension)</code></li> <li>Write data directly (e.g., with Zarr)</li> <li>On successful exit, metadata is computed and the record is inserted</li> </ol>"},{"location":"tutorials/basics/06-object-storage/#benefits-of-staged-insert","title":"Benefits of Staged Insert\u00b6","text":"<ul> <li>No intermediate copies \u2014 Data flows directly to object storage</li> <li>Streaming writes \u2014 Write frame-by-frame as data is acquired</li> <li>Atomic transactions \u2014 If an error occurs, storage is cleaned up automatically</li> <li>Automatic metadata \u2014 File sizes and manifests are computed on finalize</li> </ul> <p>Use <code>staged_insert1</code> when:</p> <ul> <li>Data is too large to hold in memory</li> <li>You're generating data incrementally (e.g., during acquisition)</li> <li>You need direct control over storage format (Zarr chunks, HDF5 datasets)</li> </ul>"},{"location":"tutorials/basics/06-object-storage/#attachments","title":"Attachments\u00b6","text":"<p>Use <code>&lt;attach&gt;</code> to store files with their original names preserved:</p>"},{"location":"tutorials/basics/06-object-storage/#codec-summary","title":"Codec Summary\u00b6","text":"Codec Syntax Description <code>&lt;blob&gt;</code> In database Python objects, arrays <code>&lt;blob@&gt;</code> Default store Large objects, hash-addressed <code>&lt;blob@name&gt;</code> Named store Specific storage tier <code>&lt;attach&gt;</code> In database Files with names <code>&lt;attach@name&gt;</code> Named store Large files with names <code>&lt;object@name&gt;</code> Named store Path-addressed (Zarr, etc.) <code>&lt;filepath@name&gt;</code> Named store References to existing files"},{"location":"tutorials/basics/06-object-storage/#computed-tables-with-large-data","title":"Computed Tables with Large Data\u00b6","text":"<p>Computed tables commonly produce large results:</p>"},{"location":"tutorials/basics/06-object-storage/#efficient-data-access","title":"Efficient Data Access\u00b6","text":""},{"location":"tutorials/basics/06-object-storage/#fetch-only-what-you-need","title":"Fetch Only What You Need\u00b6","text":""},{"location":"tutorials/basics/06-object-storage/#project-away-large-columns-before-joins","title":"Project Away Large Columns Before Joins\u00b6","text":""},{"location":"tutorials/basics/06-object-storage/#best-practices","title":"Best Practices\u00b6","text":""},{"location":"tutorials/basics/06-object-storage/#1-choose-storage-based-on-size","title":"1. Choose Storage Based on Size\u00b6","text":"<pre># Small objects (&lt; 1 MB): no @\nparameters : &lt;blob&gt;\n\n# Large objects (&gt; 1 MB): use @\nraw_data : &lt;blob@&gt;\n</pre>"},{"location":"tutorials/basics/06-object-storage/#2-use-named-stores-for-different-tiers","title":"2. Use Named Stores for Different Tiers\u00b6","text":"<pre># Fast local storage for active data\nworking_data : &lt;blob@fast&gt;\n\n# Cold storage for archives\narchived_data : &lt;blob@archive&gt;\n</pre>"},{"location":"tutorials/basics/06-object-storage/#3-separate-queryable-metadata-from-large-data","title":"3. Separate Queryable Metadata from Large Data\u00b6","text":"<pre>@schema\nclass Experiment(dj.Manual):\n    definition = \"\"\"\n    exp_id : int32\n    ---\n    # Queryable metadata\n    date : date\n    duration : decimal(5,1)\n    n_trials : int32\n    # Large data\n    raw_data : &lt;blob@&gt;\n    \"\"\"\n</pre>"},{"location":"tutorials/basics/06-object-storage/#4-use-attachments-for-files","title":"4. Use Attachments for Files\u00b6","text":"<pre># Preserves filename\nvideo : &lt;attach@&gt;\nconfig_file : &lt;attach@&gt;\n</pre>"},{"location":"tutorials/basics/06-object-storage/#garbage-collection","title":"Garbage Collection\u00b6","text":"<p>Hash-addressed storage (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;hash@&gt;</code>) uses deduplication\u2014identical content is stored once. This means deleting a row doesn't automatically delete the stored content, since other rows might reference it.</p> <p>Use garbage collection to clean up orphaned content:</p> <pre>import datajoint as dj\n\n# Preview what would be deleted (dry run)\nstats = dj.gc.collect(dry_run=True)\nprint(f\"Orphaned items: {stats['orphaned']}\")\nprint(f\"Space to reclaim: {stats['orphaned_bytes'] / 1e6:.1f} MB\")\n\n# Actually delete orphaned content\nstats = dj.gc.collect()\nprint(f\"Deleted: {stats['deleted']} items\")\n</pre>"},{"location":"tutorials/basics/06-object-storage/#when-to-run-garbage-collection","title":"When to Run Garbage Collection\u00b6","text":"<ul> <li>After bulk deletions \u2014 Clean up storage after removing many rows</li> <li>Periodically \u2014 Schedule weekly/monthly cleanup jobs</li> <li>Before archiving \u2014 Reclaim space before backups</li> </ul>"},{"location":"tutorials/basics/06-object-storage/#key-points","title":"Key Points\u00b6","text":"<ul> <li>GC only affects hash-addressed types (<code>&lt;blob@&gt;</code>, <code>&lt;attach@&gt;</code>, <code>&lt;hash@&gt;</code>)</li> <li>Schema-addressed types (<code>&lt;object@&gt;</code>, <code>&lt;npy@&gt;</code>) are deleted with their rows</li> <li>Always use <code>dry_run=True</code> first to preview changes</li> <li>GC is safe\u2014it only deletes content with zero references</li> </ul> <p>See Clean Up Storage for detailed usage.</p>"},{"location":"tutorials/basics/06-object-storage/#quick-reference","title":"Quick Reference\u00b6","text":"Pattern Use Case <code>&lt;blob&gt;</code> Small Python objects <code>&lt;blob@&gt;</code> Large arrays with deduplication <code>&lt;blob@store&gt;</code> Large arrays in specific store <code>&lt;attach@store&gt;</code> Files preserving names <code>&lt;object@store&gt;</code> Schema-addressed data (Zarr, HDF5)"},{"location":"tutorials/basics/06-object-storage/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Configure Object Storage \u2014 Set up S3, MinIO, or filesystem stores</li> <li>Clean Up Storage \u2014 Garbage collection for hash-addressed storage</li> <li>Custom Codecs \u2014 Define domain-specific types</li> <li>Manage Large Data \u2014 Performance optimization</li> </ul>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/","title":"Allen Common Coordinate Framework (CCF)","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport urllib.request\n\nschema = dj.Schema('tutorial_allen_ccf')\n\nDATA_DIR = Path('./data')\nDATA_DIR.mkdir(exist_ok=True)\n</pre> import datajoint as dj import numpy as np import pandas as pd from pathlib import Path import urllib.request  schema = dj.Schema('tutorial_allen_ccf')  DATA_DIR = Path('./data') DATA_DIR.mkdir(exist_ok=True) <pre>[2026-01-27 15:29:38] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>ONTOLOGY_URL = (\n    \"http://api.brain-map.org/api/v2/data/query.csv?\"\n    \"criteria=model::Structure,rma::criteria,[ontology_id$eq1],\"\n    \"rma::options[order$eq%27structures.graph_order%27][num_rows$eqall]\"\n)\nONTOLOGY_FILE = DATA_DIR / 'allen_structure_graph.csv'\n\nif not ONTOLOGY_FILE.exists():\n    print(\"Downloading Allen brain structure ontology...\")\n    urllib.request.urlretrieve(ONTOLOGY_URL, ONTOLOGY_FILE)\n    print(f\"Downloaded to {ONTOLOGY_FILE}\")\nelse:\n    print(f\"Using cached {ONTOLOGY_FILE}\")\n\nontology = pd.read_csv(ONTOLOGY_FILE)\nprint(f\"Loaded {len(ontology)} brain regions\")\nontology.head()\n</pre> ONTOLOGY_URL = (     \"http://api.brain-map.org/api/v2/data/query.csv?\"     \"criteria=model::Structure,rma::criteria,[ontology_id$eq1],\"     \"rma::options[order$eq%27structures.graph_order%27][num_rows$eqall]\" ) ONTOLOGY_FILE = DATA_DIR / 'allen_structure_graph.csv'  if not ONTOLOGY_FILE.exists():     print(\"Downloading Allen brain structure ontology...\")     urllib.request.urlretrieve(ONTOLOGY_URL, ONTOLOGY_FILE)     print(f\"Downloaded to {ONTOLOGY_FILE}\") else:     print(f\"Using cached {ONTOLOGY_FILE}\")  ontology = pd.read_csv(ONTOLOGY_FILE) print(f\"Loaded {len(ontology)} brain regions\") ontology.head() <pre>Using cached data/allen_structure_graph.csv\nLoaded 1327 brain regions\n</pre> Out[2]: id atlas_id name acronym st_level ontology_id hemisphere_id weight parent_structure_id depth ... graph_order structure_id_path color_hex_triplet neuro_name_structure_id neuro_name_structure_id_path failed sphinx_id structure_name_facet failed_facet safe_name 0 997 -1.0 root root 0 1 3 8690 NaN 0 ... 0 /997/ FFFFFF NaN NaN f 1 385153371 734881840 root 1 8 0.0 Basic cell groups and regions grey 1 1 3 8690 997.0 1 ... 1 /997/8/ BFDAE3 NaN NaN f 2 2244697386 734881840 Basic cell groups and regions 2 567 70.0 Cerebrum CH 2 1 3 8690 8.0 2 ... 2 /997/8/567/ B0F0FF NaN NaN f 3 2878815794 734881840 Cerebrum 3 688 85.0 Cerebral cortex CTX 3 1 3 8690 567.0 3 ... 3 /997/8/567/688/ B0FFB8 NaN NaN f 4 3591311804 734881840 Cerebral cortex 4 695 86.0 Cortical plate CTXpl 4 1 3 8690 688.0 4 ... 4 /997/8/567/688/695/ 70FF70 NaN NaN f 5 3945900931 734881840 Cortical plate <p>5 rows \u00d7 21 columns</p> In\u00a0[3]: Copied! <pre>@schema\nclass CCF(dj.Manual):\n    definition = \"\"\"\n    # Common Coordinate Framework atlas\n    ccf_id : int32\n    ---\n    ccf_version : varchar(64)       # e.g., 'CCFv3'\n    ccf_resolution : float32        # voxel resolution in microns\n    ccf_description : varchar(255)\n    \"\"\"\n</pre> @schema class CCF(dj.Manual):     definition = \"\"\"     # Common Coordinate Framework atlas     ccf_id : int32     ---     ccf_version : varchar(64)       # e.g., 'CCFv3'     ccf_resolution : float32        # voxel resolution in microns     ccf_description : varchar(255)     \"\"\" In\u00a0[4]: Copied! <pre>@schema\nclass BrainRegion(dj.Imported):\n    definition = \"\"\"\n    # Brain region from Allen ontology\n    -&gt; CCF\n    region_id : int32               # Allen structure ID\n    ---\n    acronym : varchar(32)           # short name (e.g., 'VISp')\n    region_name : varchar(255)      # full name\n    color_hex : varchar(6)          # hex color code for visualization\n    structure_order : int32         # order in hierarchy\n    \"\"\"\n\n    def make(self, key):\n        # Load ontology and insert all regions for this CCF\n        ontology = pd.read_csv(ONTOLOGY_FILE)\n        \n        entries = [\n            {\n                **key,\n                'region_id': row['id'],\n                'acronym': row['acronym'],\n                'region_name': row['safe_name'],\n                'color_hex': row['color_hex_triplet'],\n                'structure_order': row['graph_order'],\n            }\n            for _, row in ontology.iterrows()\n        ]\n        \n        self.insert(entries)\n        print(f\"Inserted {len(entries)} brain regions\")\n</pre> @schema class BrainRegion(dj.Imported):     definition = \"\"\"     # Brain region from Allen ontology     -&gt; CCF     region_id : int32               # Allen structure ID     ---     acronym : varchar(32)           # short name (e.g., 'VISp')     region_name : varchar(255)      # full name     color_hex : varchar(6)          # hex color code for visualization     structure_order : int32         # order in hierarchy     \"\"\"      def make(self, key):         # Load ontology and insert all regions for this CCF         ontology = pd.read_csv(ONTOLOGY_FILE)                  entries = [             {                 **key,                 'region_id': row['id'],                 'acronym': row['acronym'],                 'region_name': row['safe_name'],                 'color_hex': row['color_hex_triplet'],                 'structure_order': row['graph_order'],             }             for _, row in ontology.iterrows()         ]                  self.insert(entries)         print(f\"Inserted {len(entries)} brain regions\") In\u00a0[5]: Copied! <pre>@schema\nclass RegionParent(dj.Imported):\n    definition = \"\"\"\n    # Hierarchical parent-child relationships\n    -&gt; BrainRegion\n    ---\n    -&gt; BrainRegion.proj(parent_id='region_id')   # parent region\n    depth : int16                                 # depth in hierarchy (root=0)\n    \"\"\"\n\n    def make(self, key):\n        ontology = pd.read_csv(ONTOLOGY_FILE)\n        \n        # Build parent mapping\n        parent_map = dict(zip(ontology['id'], ontology['parent_structure_id']))\n        \n        entries = []\n        for _, row in ontology.iterrows():\n            parent_id = row['parent_structure_id']\n            # Skip root (no parent) or if parent not in ontology\n            if pd.isna(parent_id):\n                parent_id = row['id']  # root points to itself\n            \n            entries.append({\n                **key,\n                'region_id': row['id'],\n                'parent_id': int(parent_id),\n                'depth': row['depth'],\n            })\n        \n        self.insert(entries)\n        print(f\"Inserted {len(entries)} parent relationships\")\n</pre> @schema class RegionParent(dj.Imported):     definition = \"\"\"     # Hierarchical parent-child relationships     -&gt; BrainRegion     ---     -&gt; BrainRegion.proj(parent_id='region_id')   # parent region     depth : int16                                 # depth in hierarchy (root=0)     \"\"\"      def make(self, key):         ontology = pd.read_csv(ONTOLOGY_FILE)                  # Build parent mapping         parent_map = dict(zip(ontology['id'], ontology['parent_structure_id']))                  entries = []         for _, row in ontology.iterrows():             parent_id = row['parent_structure_id']             # Skip root (no parent) or if parent not in ontology             if pd.isna(parent_id):                 parent_id = row['id']  # root points to itself                          entries.append({                 **key,                 'region_id': row['id'],                 'parent_id': int(parent_id),                 'depth': row['depth'],             })                  self.insert(entries)         print(f\"Inserted {len(entries)} parent relationships\") In\u00a0[6]: Copied! <pre>@schema\nclass Voxel(dj.Imported):\n    definition = \"\"\"\n    # Brain atlas voxels\n    -&gt; CCF\n    x : int32                       # AP axis (\u00b5m)\n    y : int32                       # DV axis (\u00b5m) \n    z : int32                       # ML axis (\u00b5m)\n    ---\n    -&gt; BrainRegion\n    index(y, z)                     # for efficient coronal slice queries\n    \"\"\"\n    \n    # Note: make() would load NRRD file and insert voxels\n    # Skipped in this tutorial due to data size\n</pre> @schema class Voxel(dj.Imported):     definition = \"\"\"     # Brain atlas voxels     -&gt; CCF     x : int32                       # AP axis (\u00b5m)     y : int32                       # DV axis (\u00b5m)      z : int32                       # ML axis (\u00b5m)     ---     -&gt; BrainRegion     index(y, z)                     # for efficient coronal slice queries     \"\"\"          # Note: make() would load NRRD file and insert voxels     # Skipped in this tutorial due to data size In\u00a0[7]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[7]: In\u00a0[8]: Copied! <pre># Insert CCF metadata\nCCF.insert1(\n    {\n        'ccf_id': 1,\n        'ccf_version': 'CCFv3',\n        'ccf_resolution': 25.0,\n        'ccf_description': 'Allen Mouse CCF v3 (25\u00b5m resolution)'\n    },\n    skip_duplicates=True\n)\n\nCCF()\n</pre> # Insert CCF metadata CCF.insert1(     {         'ccf_id': 1,         'ccf_version': 'CCFv3',         'ccf_resolution': 25.0,         'ccf_description': 'Allen Mouse CCF v3 (25\u00b5m resolution)'     },     skip_duplicates=True )  CCF() Out[8]: Common Coordinate Framework atlas <p>ccf_id</p> <p>ccf_version</p> e.g., 'CCFv3' <p>ccf_resolution</p> voxel resolution in microns <p>ccf_description</p> 1 CCFv3 25.0 Allen Mouse CCF v3 (25\u00b5m resolution) <p>Total: 1</p> In\u00a0[9]: Copied! <pre># Populate brain regions\nBrainRegion.populate(display_progress=True)\n</pre> # Populate brain regions BrainRegion.populate(display_progress=True) <pre>\rBrainRegion:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rBrainRegion: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 12.25it/s]</pre> <pre>Inserted 1327 brain regions\n</pre> <pre>\n</pre> Out[9]: <pre>{'success_count': 1, 'error_list': []}</pre> In\u00a0[10]: Copied! <pre># View sample regions\nBrainRegion() &amp; 'region_id &lt; 100'\n</pre> # View sample regions BrainRegion() &amp; 'region_id &lt; 100' Out[10]: Brain region from Allen ontology <p>ccf_id</p> None <p>region_id</p> Allen structure ID <p>acronym</p> short name (e.g., 'VISp') <p>region_name</p> full name <p>color_hex</p> hex color code for visualization <p>structure_order</p> order in hierarchy 1 1 TMv Tuberomammillary nucleus ventral part FF4C3E 7751 2 SSp-m6b Primary somatosensory area mouth layer 6b 188064 781 3 sec secondary fissure AAAAAA 13161 4 IC Inferior colliculus FF7AFF 8121 6 int internal capsule CCCCCC 12011 7 PSV Principal sensory nucleus of the trigeminal FFAE6F 8891 8 grey Basic cell groups and regions BFDAE3 11 9 SSp-tr6a Primary somatosensory area trunk layer 6a 188064 911 10 SCig Superior colliculus motor related intermediate gray layer FF90FF 8341 11 plf posterolateral fissure AAAAAA 13171 12 IF Interfascicular nucleus raphe FFA6FF 8691 14 im internal medullary lamina of the thalamus CCCCCC 1213 <p>...</p> <p>Total: 94</p> In\u00a0[11]: Copied! <pre># Populate parent relationships\nRegionParent.populate(display_progress=True)\n</pre> # Populate parent relationships RegionParent.populate(display_progress=True) <pre>\rRegionParent:   0%|          | 0/1327 [00:00&lt;?, ?it/s]</pre> <pre>\rRegionParent:   6%|\u258c         | 80/1327 [00:00&lt;00:01, 798.68it/s]</pre> <pre>Inserted 1327 parent relationships\n</pre> <pre>\rRegionParent:  20%|\u2588\u2588        | 271/1327 [00:00&lt;00:00, 1449.72it/s]</pre> <pre>\rRegionParent:  35%|\u2588\u2588\u2588\u258d      | 464/1327 [00:00&lt;00:00, 1667.27it/s]</pre> <pre>\rRegionParent:  49%|\u2588\u2588\u2588\u2588\u2589     | 656/1327 [00:00&lt;00:00, 1764.10it/s]</pre> <pre>\rRegionParent:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 851/1327 [00:00&lt;00:00, 1829.11it/s]</pre> <pre>\rRegionParent:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 1045/1327 [00:00&lt;00:00, 1866.21it/s]</pre> <pre>\rRegionParent:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 1236/1327 [00:00&lt;00:00, 1879.48it/s]</pre> <pre>\rRegionParent: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1327/1327 [00:00&lt;00:00, 1772.63it/s]</pre> <pre>\n</pre> Out[11]: <pre>{'success_count': 1, 'error_list': []}</pre> In\u00a0[12]: Copied! <pre># Find primary visual cortex\nBrainRegion &amp; {'acronym': 'VISp'}\n</pre> # Find primary visual cortex BrainRegion &amp; {'acronym': 'VISp'} Out[12]: Brain region from Allen ontology <p>ccf_id</p> None <p>region_id</p> Allen structure ID <p>acronym</p> short name (e.g., 'VISp') <p>region_name</p> full name <p>color_hex</p> hex color code for visualization <p>structure_order</p> order in hierarchy 1 385 VISp Primary visual area 08858C 185 <p>Total: 1</p> In\u00a0[13]: Copied! <pre># Get VISp region ID\nvisp = (BrainRegion &amp; {'acronym': 'VISp'}).fetch1()\nvisp_id = visp['region_id']\n\n# Find all children (direct descendants)\nchildren = BrainRegion * (RegionParent &amp; f'parent_id = {visp_id}' &amp; f'region_id != {visp_id}')\nchildren.proj('acronym', 'region_name')\n</pre> # Get VISp region ID visp = (BrainRegion &amp; {'acronym': 'VISp'}).fetch1() visp_id = visp['region_id']  # Find all children (direct descendants) children = BrainRegion * (RegionParent &amp; f'parent_id = {visp_id}' &amp; f'region_id != {visp_id}') children.proj('acronym', 'region_name') Out[13]: <p>ccf_id</p> None <p>region_id</p> Allen structure ID <p>acronym</p> short name (e.g., 'VISp') <p>region_name</p> full name 1 33 VISp6a Primary visual area layer 6a1 305 VISp6b Primary visual area layer 6b1 593 VISp1 Primary visual area layer 11 721 VISp4 Primary visual area layer 41 778 VISp5 Primary visual area layer 51 821 VISp2/3 Primary visual area layer 2/3 <p>Total: 6</p> In\u00a0[14]: Copied! <pre>def get_ancestors(region_acronym, ccf_id=1):\n    \"\"\"Get the path from a region to the root.\"\"\"\n    region = (BrainRegion &amp; {'acronym': region_acronym, 'ccf_id': ccf_id}).fetch1()\n    region_id = region['region_id']\n    \n    path = [region['acronym']]\n    \n    while True:\n        parent_id = (RegionParent &amp; {'ccf_id': ccf_id, 'region_id': region_id}).fetch1('parent_id')\n        if parent_id == region_id:  # reached root\n            break\n        parent = (BrainRegion &amp; {'ccf_id': ccf_id, 'region_id': parent_id}).fetch1()\n        path.append(parent['acronym'])\n        region_id = parent_id\n    \n    return ' \u2192 '.join(reversed(path))\n\n# Show path from VISp layer 1 to root\nprint(\"Path from VISp1 to root:\")\nprint(get_ancestors('VISp1'))\n</pre> def get_ancestors(region_acronym, ccf_id=1):     \"\"\"Get the path from a region to the root.\"\"\"     region = (BrainRegion &amp; {'acronym': region_acronym, 'ccf_id': ccf_id}).fetch1()     region_id = region['region_id']          path = [region['acronym']]          while True:         parent_id = (RegionParent &amp; {'ccf_id': ccf_id, 'region_id': region_id}).fetch1('parent_id')         if parent_id == region_id:  # reached root             break         parent = (BrainRegion &amp; {'ccf_id': ccf_id, 'region_id': parent_id}).fetch1()         path.append(parent['acronym'])         region_id = parent_id          return ' \u2192 '.join(reversed(path))  # Show path from VISp layer 1 to root print(\"Path from VISp1 to root:\") print(get_ancestors('VISp1')) <pre>Path from VISp1 to root:\nroot \u2192 grey \u2192 CH \u2192 CTX \u2192 CTXpl \u2192 Isocortex \u2192 VIS \u2192 VISp \u2192 VISp1\n</pre> In\u00a0[15]: Copied! <pre># Aggregate regions by depth in hierarchy\nimport matplotlib.pyplot as plt\n\ndepths = (RegionParent &amp; 'ccf_id = 1').to_arrays('depth')\nunique, counts = np.unique(depths, return_counts=True)\n\nplt.figure(figsize=(10, 4))\nplt.bar(unique, counts)\nplt.xlabel('Depth in Hierarchy')\nplt.ylabel('Number of Regions')\nplt.title('Brain Regions by Hierarchy Depth')\nplt.xticks(unique)\n</pre> # Aggregate regions by depth in hierarchy import matplotlib.pyplot as plt  depths = (RegionParent &amp; 'ccf_id = 1').to_arrays('depth') unique, counts = np.unique(depths, return_counts=True)  plt.figure(figsize=(10, 4)) plt.bar(unique, counts) plt.xlabel('Depth in Hierarchy') plt.ylabel('Number of Regions') plt.title('Brain Regions by Hierarchy Depth') plt.xticks(unique) Out[15]: <pre>([&lt;matplotlib.axis.XTick at 0x1a2b94080&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a296bdd0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b954c0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b5bb30&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b594c0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b591f0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b5a480&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b746e0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b75220&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b75fa0&gt;,\n  &lt;matplotlib.axis.XTick at 0x1a2b74710&gt;],\n [Text(0, 0, '0'),\n  Text(1, 0, '1'),\n  Text(2, 0, '2'),\n  Text(3, 0, '3'),\n  Text(4, 0, '4'),\n  Text(5, 0, '5'),\n  Text(6, 0, '6'),\n  Text(7, 0, '7'),\n  Text(8, 0, '8'),\n  Text(9, 0, '9'),\n  Text(10, 0, '10')])</pre> In\u00a0[16]: Copied! <pre># Find all visual-related regions\n(BrainRegion &amp; \"region_name LIKE '%Visual%'\").proj('acronym', 'region_name')\n</pre> # Find all visual-related regions (BrainRegion &amp; \"region_name LIKE '%Visual%'\").proj('acronym', 'region_name') Out[16]: Brain region from Allen ontology <p>ccf_id</p> None <p>region_id</p> Allen structure ID <p>acronym</p> short name (e.g., 'VISp') <p>region_name</p> full name <p>color_hex</p> hex color code for visualization <p>structure_order</p> order in hierarchy 1 457 VIS6a Visual areas layer 6a bytes bytes1 497 VIS6b Visual areas layer 6b bytes bytes1 561 VIS2/3 Visual areas layer 2/3 bytes bytes1 669 VIS Visual areas bytes bytes1 801 VIS1 Visual areas layer 1 bytes bytes1 913 VIS4 Visual areas layer 4 bytes bytes1 937 VIS5 Visual areas layer 5 bytes bytes <p>Total: 7</p> In\u00a0[17]: Copied! <pre>@schema\nclass RecordingSite(dj.Manual):\n    definition = \"\"\"\n    # Recording electrode location\n    recording_id : int32\n    ---\n    ap : float32                    # anterior-posterior (\u00b5m from bregma)\n    dv : float32                    # dorsal-ventral (\u00b5m from brain surface)\n    ml : float32                    # medial-lateral (\u00b5m from midline)\n    -&gt; BrainRegion                  # assigned brain region (includes ccf_id)\n    \"\"\"\n\n# Insert example recording sites\nRecordingSite.insert([\n    {'recording_id': 1, 'ccf_id': 1, 'ap': -3500, 'dv': 500, 'ml': 2500, \n     'region_id': (BrainRegion &amp; {'acronym': 'VISp'}).fetch1('region_id')},\n    {'recording_id': 2, 'ccf_id': 1, 'ap': -1800, 'dv': 1200, 'ml': 1500,\n     'region_id': (BrainRegion &amp; {'acronym': 'CA1'}).fetch1('region_id')},\n], skip_duplicates=True)\n\n# View recordings with region info\nRecordingSite * BrainRegion.proj('acronym', 'region_name')\n</pre> @schema class RecordingSite(dj.Manual):     definition = \"\"\"     # Recording electrode location     recording_id : int32     ---     ap : float32                    # anterior-posterior (\u00b5m from bregma)     dv : float32                    # dorsal-ventral (\u00b5m from brain surface)     ml : float32                    # medial-lateral (\u00b5m from midline)     -&gt; BrainRegion                  # assigned brain region (includes ccf_id)     \"\"\"  # Insert example recording sites RecordingSite.insert([     {'recording_id': 1, 'ccf_id': 1, 'ap': -3500, 'dv': 500, 'ml': 2500,       'region_id': (BrainRegion &amp; {'acronym': 'VISp'}).fetch1('region_id')},     {'recording_id': 2, 'ccf_id': 1, 'ap': -1800, 'dv': 1200, 'ml': 1500,      'region_id': (BrainRegion &amp; {'acronym': 'CA1'}).fetch1('region_id')}, ], skip_duplicates=True)  # View recordings with region info RecordingSite * BrainRegion.proj('acronym', 'region_name') Out[17]: <p>recording_id</p> <p>ap</p> anterior-posterior (\u00b5m from bregma) <p>dv</p> dorsal-ventral (\u00b5m from brain surface) <p>ml</p> medial-lateral (\u00b5m from midline) <p>ccf_id</p> None <p>region_id</p> None <p>acronym</p> short name (e.g., 'VISp') <p>region_name</p> full name 1 -3500.0 500.0 2500.0 1 385 VISp Primary visual area2 -1800.0 1200.0 1500.0 1 382 CA1 Field CA1 <p>Total: 2</p> In\u00a0[18]: Copied! <pre># Final schema diagram\ndj.Diagram(schema)\n</pre> # Final schema diagram dj.Diagram(schema) Out[18]: In\u00a0[19]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#allen-common-coordinate-framework-ccf","title":"Allen Common Coordinate Framework (CCF)\u00b6","text":"<p>This tutorial demonstrates how to model the Allen Mouse Brain Common Coordinate Framework in DataJoint. You'll learn to:</p> <ul> <li>Model hierarchical structures (brain region ontology)</li> <li>Use Part tables for large-scale voxel data</li> <li>Handle self-referential relationships (parent regions)</li> <li>Batch insert large datasets efficiently</li> </ul>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#the-allen-ccf","title":"The Allen CCF\u00b6","text":"<p>The CCF is a 3D reference atlas of the mouse brain, providing:</p> <ul> <li>Coordinate system with voxel resolution (10, 25, 50, or 100 \u00b5m)</li> <li>Hierarchical ontology of ~1300 brain regions</li> <li>Region boundaries for each voxel</li> </ul> <p>Reference:</p> <p>Wang Q, Ding SL, Li Y, et al. (2020). The Allen Mouse Brain Common Coordinate Framework: A 3D Reference Atlas. Cell, 181(4), 936-953.e20. DOI: 10.1016/j.cell.2020.04.007</p>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#data-sources","title":"Data Sources\u00b6","text":"<ul> <li>Ontology: structure_graph.csv</li> <li>Volume: Allen Institute Archive</li> </ul> <p>Note: This tutorial works with the ontology (small CSV). The full 3D volume requires ~100MB+ download.</p>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#download-brain-region-ontology","title":"Download Brain Region Ontology\u00b6","text":"<p>The ontology defines the hierarchical structure of brain regions.</p>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#schema-design","title":"Schema Design\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#ccf-master-table","title":"CCF Master Table\u00b6","text":"<p>The master table stores atlas metadata. Multiple CCF versions (different resolutions) can coexist.</p>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#brain-region-table","title":"Brain Region Table\u00b6","text":"<p>Each brain region has an ID, name, acronym, and color code for visualization.</p>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#hierarchical-parent-child-relationships","title":"Hierarchical Parent-Child Relationships\u00b6","text":"<p>Brain regions form a hierarchy (e.g., Visual Cortex \u2192 Primary Visual Area \u2192 Layer 1). We model this with a self-referential foreign key.</p>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#voxel-data-optional","title":"Voxel Data (Optional)\u00b6","text":"<p>For the full atlas, each voxel maps to a brain region. This is a large table (~10M+ rows for 10\u00b5m resolution).</p> <p>Design note: <code>CCF</code> is part of the primary key because a voxel's identity depends on which atlas it belongs to. The coordinate <code>(x=5000, y=3000, z=4000)</code> exists in every atlas version (10\u00b5m, 25\u00b5m, etc.) but represents different physical mappings. Without <code>ccf_id</code> in the primary key, you couldn't store voxels from multiple atlas resolutions.</p> <p>Note: We define the schema but don't populate it in this tutorial due to data size.</p>"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#view-schema","title":"View Schema\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#populate-the-database","title":"Populate the Database\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#querying-the-hierarchy","title":"Querying the Hierarchy\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#find-a-region-by-acronym","title":"Find a Region by Acronym\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#find-children-of-a-region","title":"Find Children of a Region\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#find-parent-path-ancestors","title":"Find Parent Path (Ancestors)\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#count-regions-by-depth","title":"Count Regions by Depth\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#search-regions-by-name","title":"Search Regions by Name\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#extending-the-schema","title":"Extending the Schema\u00b6","text":""},{"location":"tutorials/domain/allen-ccf/allen-ccf/#recording-locations","title":"Recording Locations\u00b6","text":"<p>A common use case is tracking where electrodes were placed during recordings.</p> <p>Design choice: Here we use <code>recording_id</code> alone as the primary key, with <code>BrainRegion</code> (which includes <code>ccf_id</code>) as a secondary attribute. This means each recording has exactly one canonical atlas registration.</p> <p>An alternative design would include <code>ccf_id</code> in the primary key:</p> <pre>recording_id : int32\n-&gt; CCF\n---\n...\n</pre> <p>This would allow the same recording to be registered to multiple atlas versions (e.g., comparing assignments in CCFv2 vs CCFv3). Choose based on your use case:</p> Design Primary Key Use Case Single registration <code>recording_id</code> One canonical atlas per lab Multi-atlas <code>(recording_id, ccf_id)</code> Compare across atlas versions"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#summary","title":"Summary\u00b6","text":"<p>This tutorial demonstrated DataJoint patterns for atlas and ontology data:</p> Pattern Example Purpose Hierarchical data <code>BrainRegion</code>, <code>RegionParent</code> Model tree structures Self-referential FK <code>parent_id \u2192 region_id</code> Parent-child relationships Batch insert <code>self.insert(entries)</code> Efficient bulk loading Secondary index <code>index(y, z)</code> Optimize spatial queries Linked tables <code>RecordingSite \u2192 BrainRegion</code> Reference atlas in experiments"},{"location":"tutorials/domain/allen-ccf/allen-ccf/#loading-full-atlas-data","title":"Loading Full Atlas Data\u00b6","text":"<p>To load the complete 3D volume:</p> <ol> <li>Download NRRD file from Allen Institute</li> <li>Install <code>pynrrd</code>: <code>pip install pynrrd</code></li> <li>Load and insert voxels (see Element Electrode Localization)</li> </ol>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/","title":"Calcium Imaging Pipeline","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom skimage import io\nfrom scipy import ndimage\n\nschema = dj.Schema('tutorial_calcium_imaging')\n\n# Data directory (relative to this notebook)\nDATA_DIR = Path('./data')\n</pre> import datajoint as dj import numpy as np import matplotlib.pyplot as plt from pathlib import Path from skimage import io from scipy import ndimage  schema = dj.Schema('tutorial_calcium_imaging')  # Data directory (relative to this notebook) DATA_DIR = Path('./data') <pre>[2026-01-27 15:29:46] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int32\n    ---\n    dob : date\n    sex : enum('M', 'F', 'unknown')\n    \"\"\"\n\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Mouse\n    session_date : date\n    ---\n    experimenter : varchar(100)\n    \"\"\"\n\n\n@schema\nclass Scan(dj.Manual):\n    definition = \"\"\"\n    -&gt; Session\n    scan_idx : int16\n    ---\n    depth : float32           # imaging depth (um)\n    wavelength : float32      # laser wavelength (nm)\n    laser_power : float32     # laser power (mW)\n    fps : float32             # frames per second\n    file_name : varchar(128)  # TIFF filename\n    \"\"\"\n</pre> @schema class Mouse(dj.Manual):     definition = \"\"\"     mouse_id : int32     ---     dob : date     sex : enum('M', 'F', 'unknown')     \"\"\"   @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Mouse     session_date : date     ---     experimenter : varchar(100)     \"\"\"   @schema class Scan(dj.Manual):     definition = \"\"\"     -&gt; Session     scan_idx : int16     ---     depth : float32           # imaging depth (um)     wavelength : float32      # laser wavelength (nm)     laser_power : float32     # laser power (mW)     fps : float32             # frames per second     file_name : varchar(128)  # TIFF filename     \"\"\" In\u00a0[3]: Copied! <pre># Insert mouse\nMouse.insert1(\n    {'mouse_id': 0, 'dob': '2017-03-01', 'sex': 'M'},\n    skip_duplicates=True\n)\n\n# Insert session\nSession.insert1(\n    {'mouse_id': 0, 'session_date': '2017-05-15', 'experimenter': 'Alice'},\n    skip_duplicates=True\n)\n\n# Insert scans (we have 3 TIFF files)\nScan.insert([\n    {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 1,\n     'depth': 150, 'wavelength': 920, 'laser_power': 26, 'fps': 15,\n     'file_name': 'example_scan_01.tif'},\n    {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 2,\n     'depth': 200, 'wavelength': 920, 'laser_power': 24, 'fps': 15,\n     'file_name': 'example_scan_02.tif'},\n    {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 3,\n     'depth': 250, 'wavelength': 920, 'laser_power': 25, 'fps': 15,\n     'file_name': 'example_scan_03.tif'},\n], skip_duplicates=True)\n\nScan()\n</pre> # Insert mouse Mouse.insert1(     {'mouse_id': 0, 'dob': '2017-03-01', 'sex': 'M'},     skip_duplicates=True )  # Insert session Session.insert1(     {'mouse_id': 0, 'session_date': '2017-05-15', 'experimenter': 'Alice'},     skip_duplicates=True )  # Insert scans (we have 3 TIFF files) Scan.insert([     {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 1,      'depth': 150, 'wavelength': 920, 'laser_power': 26, 'fps': 15,      'file_name': 'example_scan_01.tif'},     {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 2,      'depth': 200, 'wavelength': 920, 'laser_power': 24, 'fps': 15,      'file_name': 'example_scan_02.tif'},     {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 3,      'depth': 250, 'wavelength': 920, 'laser_power': 25, 'fps': 15,      'file_name': 'example_scan_03.tif'}, ], skip_duplicates=True)  Scan() Out[3]: None <p>mouse_id</p> None <p>session_date</p> None <p>scan_idx</p> <p>depth</p> imaging depth (um) <p>wavelength</p> laser wavelength (nm) <p>laser_power</p> laser power (mW) <p>fps</p> frames per second <p>file_name</p> TIFF filename 0 2017-05-15 1 150.0 920.0 26.0 15.0 example_scan_01.tif0 2017-05-15 2 200.0 920.0 24.0 15.0 example_scan_02.tif0 2017-05-15 3 250.0 920.0 25.0 15.0 example_scan_03.tif <p>Total: 3</p> In\u00a0[4]: Copied! <pre>@schema\nclass AverageFrame(dj.Imported):\n    definition = \"\"\"\n    -&gt; Scan\n    ---\n    average_frame : &lt;blob&gt;    # mean fluorescence across frames\n    \"\"\"\n\n    def make(self, key):\n        # Get filename from Scan table\n        file_name = (Scan &amp; key).fetch1('file_name')\n        file_path = DATA_DIR / file_name\n        \n        # Load TIFF and compute average\n        movie = io.imread(file_path)\n        avg_frame = movie.mean(axis=0)\n        \n        # Insert result\n        self.insert1({**key, 'average_frame': avg_frame})\n        print(f\"Processed {file_name}: {movie.shape[0]} frames\")\n</pre> @schema class AverageFrame(dj.Imported):     definition = \"\"\"     -&gt; Scan     ---     average_frame :     # mean fluorescence across frames     \"\"\"      def make(self, key):         # Get filename from Scan table         file_name = (Scan &amp; key).fetch1('file_name')         file_path = DATA_DIR / file_name                  # Load TIFF and compute average         movie = io.imread(file_path)         avg_frame = movie.mean(axis=0)                  # Insert result         self.insert1({**key, 'average_frame': avg_frame})         print(f\"Processed {file_name}: {movie.shape[0]} frames\") In\u00a0[5]: Copied! <pre># Populate computes all pending entries\nAverageFrame.populate(display_progress=True)\n</pre> # Populate computes all pending entries AverageFrame.populate(display_progress=True) <pre>\rAverageFrame:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>\rAverageFrame: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 54.89it/s]</pre> <pre>Processed example_scan_01.tif: 100 frames\nProcessed example_scan_02.tif: 100 frames\nProcessed example_scan_03.tif: 100 frames\n</pre> <pre>\n</pre> Out[5]: <pre>{'success_count': 3, 'error_list': []}</pre> In\u00a0[6]: Copied! <pre># Visualize average frames\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\nfor ax, key in zip(axes, AverageFrame.keys()):\n    avg = (AverageFrame &amp; key).fetch1('average_frame')\n    ax.imshow(avg, cmap='gray')\n    ax.set_title(f\"Scan {key['scan_idx']}\")\n    ax.axis('off')\nplt.tight_layout()\n</pre> # Visualize average frames fig, axes = plt.subplots(1, 3, figsize=(12, 4)) for ax, key in zip(axes, AverageFrame.keys()):     avg = (AverageFrame &amp; key).fetch1('average_frame')     ax.imshow(avg, cmap='gray')     ax.set_title(f\"Scan {key['scan_idx']}\")     ax.axis('off') plt.tight_layout() In\u00a0[7]: Copied! <pre>@schema\nclass SegmentationParam(dj.Lookup):\n    definition = \"\"\"\n    seg_param_id : int16\n    ---\n    threshold : float32    # intensity threshold\n    min_size : int32       # minimum blob size (pixels)\n    \"\"\"\n    \n    # Pre-populate with parameter sets to try\n    contents = [\n        {'seg_param_id': 1, 'threshold': 50.0, 'min_size': 50},\n        {'seg_param_id': 2, 'threshold': 60.0, 'min_size': 50},\n    ]\n\nSegmentationParam()\n</pre> @schema class SegmentationParam(dj.Lookup):     definition = \"\"\"     seg_param_id : int16     ---     threshold : float32    # intensity threshold     min_size : int32       # minimum blob size (pixels)     \"\"\"          # Pre-populate with parameter sets to try     contents = [         {'seg_param_id': 1, 'threshold': 50.0, 'min_size': 50},         {'seg_param_id': 2, 'threshold': 60.0, 'min_size': 50},     ]  SegmentationParam() Out[7]: None <p>seg_param_id</p> <p>threshold</p> intensity threshold <p>min_size</p> minimum blob size (pixels) 1 50.0 502 60.0 50 <p>Total: 2</p> In\u00a0[8]: Copied! <pre>@schema\nclass Segmentation(dj.Computed):\n    definition = \"\"\"\n    -&gt; AverageFrame\n    -&gt; SegmentationParam\n    ---\n    num_rois : int16          # number of detected ROIs\n    segmented_mask : &lt;blob&gt;   # labeled mask image\n    \"\"\"\n\n    class Roi(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        roi_idx : int16\n        ---\n        mask : &lt;blob&gt;         # binary mask for this ROI\n        center_x : float32    # ROI center x coordinate\n        center_y : float32    # ROI center y coordinate\n        \"\"\"\n\n    def make(self, key):\n        # Fetch inputs\n        avg_frame = (AverageFrame &amp; key).fetch1('average_frame')\n        threshold, min_size = (SegmentationParam &amp; key).fetch1('threshold', 'min_size')\n        \n        # Threshold to get binary mask\n        binary_mask = avg_frame &gt; threshold\n        \n        # Label connected components\n        labeled, num_labels = ndimage.label(binary_mask)\n        \n        # Filter by size and extract ROIs\n        roi_masks = []\n        for i in range(1, num_labels + 1):  # 0 is background\n            roi_mask = (labeled == i)\n            if roi_mask.sum() &gt;= min_size:\n                roi_masks.append(roi_mask)\n        \n        # Re-label the filtered mask\n        final_mask = np.zeros_like(labeled)\n        for i, mask in enumerate(roi_masks, 1):\n            final_mask[mask] = i\n        \n        # Insert master entry\n        self.insert1({\n            **key,\n            'num_rois': len(roi_masks),\n            'segmented_mask': final_mask\n        })\n        \n        # Insert part entries (one per ROI)\n        for roi_idx, mask in enumerate(roi_masks):\n            # Compute center of mass\n            cy, cx = ndimage.center_of_mass(mask)\n            self.Roi.insert1({\n                **key,\n                'roi_idx': roi_idx,\n                'mask': mask,\n                'center_x': cx,\n                'center_y': cy\n            })\n        \n        print(f\"Scan {key['scan_idx']}, params {key['seg_param_id']}: {len(roi_masks)} ROIs\")\n</pre> @schema class Segmentation(dj.Computed):     definition = \"\"\"     -&gt; AverageFrame     -&gt; SegmentationParam     ---     num_rois : int16          # number of detected ROIs     segmented_mask :    # labeled mask image     \"\"\"      class Roi(dj.Part):         definition = \"\"\"         -&gt; master         roi_idx : int16         ---         mask :          # binary mask for this ROI         center_x : float32    # ROI center x coordinate         center_y : float32    # ROI center y coordinate         \"\"\"      def make(self, key):         # Fetch inputs         avg_frame = (AverageFrame &amp; key).fetch1('average_frame')         threshold, min_size = (SegmentationParam &amp; key).fetch1('threshold', 'min_size')                  # Threshold to get binary mask         binary_mask = avg_frame &gt; threshold                  # Label connected components         labeled, num_labels = ndimage.label(binary_mask)                  # Filter by size and extract ROIs         roi_masks = []         for i in range(1, num_labels + 1):  # 0 is background             roi_mask = (labeled == i)             if roi_mask.sum() &gt;= min_size:                 roi_masks.append(roi_mask)                  # Re-label the filtered mask         final_mask = np.zeros_like(labeled)         for i, mask in enumerate(roi_masks, 1):             final_mask[mask] = i                  # Insert master entry         self.insert1({             **key,             'num_rois': len(roi_masks),             'segmented_mask': final_mask         })                  # Insert part entries (one per ROI)         for roi_idx, mask in enumerate(roi_masks):             # Compute center of mass             cy, cx = ndimage.center_of_mass(mask)             self.Roi.insert1({                 **key,                 'roi_idx': roi_idx,                 'mask': mask,                 'center_x': cx,                 'center_y': cy             })                  print(f\"Scan {key['scan_idx']}, params {key['seg_param_id']}: {len(roi_masks)} ROIs\") In\u00a0[9]: Copied! <pre># Populate all AverageFrame x SegmentationParam combinations\nSegmentation.populate(display_progress=True)\n</pre> # Populate all AverageFrame x SegmentationParam combinations Segmentation.populate(display_progress=True) <pre>\rSegmentation:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>\rSegmentation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;00:00, 138.79it/s]</pre> <pre>Scan 1, params 1: 6 ROIs\nScan 2, params 1: 6 ROIs\nScan 3, params 1: 9 ROIs\nScan 1, params 2: 3 ROIs\nScan 2, params 2: 2 ROIs\nScan 3, params 2: 3 ROIs\n</pre> <pre>\n</pre> Out[9]: <pre>{'success_count': 6, 'error_list': []}</pre> In\u00a0[10]: Copied! <pre># View results summary\nSegmentation()\n</pre> # View results summary Segmentation() Out[10]: None <p>mouse_id</p> None <p>session_date</p> None <p>scan_idx</p> None <p>seg_param_id</p> None <p>num_rois</p> number of detected ROIs <p>segmented_mask</p> labeled mask image 0 2017-05-15 1 1 6 &lt;blob&gt;0 2017-05-15 1 2 3 &lt;blob&gt;0 2017-05-15 2 1 6 &lt;blob&gt;0 2017-05-15 2 2 2 &lt;blob&gt;0 2017-05-15 3 1 9 &lt;blob&gt;0 2017-05-15 3 2 3 &lt;blob&gt; <p>Total: 6</p> In\u00a0[11]: Copied! <pre># View individual ROIs\nSegmentation.Roi()\n</pre> # View individual ROIs Segmentation.Roi() Out[11]: None <p>mouse_id</p> None <p>session_date</p> None <p>scan_idx</p> None <p>seg_param_id</p> None <p>roi_idx</p> <p>mask</p> binary mask for this ROI <p>center_x</p> ROI center x coordinate <p>center_y</p> ROI center y coordinate 0 2017-05-15 1 1 0 &lt;blob&gt; 109.95531 2.26815650 2017-05-15 1 1 1 &lt;blob&gt; 62.761467 45.289910 2017-05-15 1 1 2 &lt;blob&gt; 1.6376811 54.7246360 2017-05-15 1 1 3 &lt;blob&gt; 108.045456 81.2954560 2017-05-15 1 1 4 &lt;blob&gt; 123.35514 100.00 2017-05-15 1 1 5 &lt;blob&gt; 75.53095 110.158190 2017-05-15 1 2 0 &lt;blob&gt; 109.314514 1.58064520 2017-05-15 1 2 1 &lt;blob&gt; 62.8247 46.2310750 2017-05-15 1 2 2 &lt;blob&gt; 74.85655 110.278590 2017-05-15 2 1 0 &lt;blob&gt; 85.938774 6.00680260 2017-05-15 2 1 1 &lt;blob&gt; 113.816246 12.760320 2017-05-15 2 1 2 &lt;blob&gt; 70.36826 70.053894 <p>...</p> <p>Total: 29</p> In\u00a0[12]: Copied! <pre># Compare segmentation with different parameters for scan 1\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\nkey = {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 1}\navg_frame = (AverageFrame &amp; key).fetch1('average_frame')\n\naxes[0].imshow(avg_frame, cmap='gray')\naxes[0].set_title('Average Frame')\naxes[0].axis('off')\n\nfor ax, param_id in zip(axes[1:], [1, 2]):\n    seg_key = {**key, 'seg_param_id': param_id}\n    mask, num_rois = (Segmentation &amp; seg_key).fetch1('segmented_mask', 'num_rois')\n    threshold = (SegmentationParam &amp; {'seg_param_id': param_id}).fetch1('threshold')\n    \n    ax.imshow(avg_frame, cmap='gray')\n    ax.imshow(mask, cmap='tab10', alpha=0.5 * (mask &gt; 0))\n    ax.set_title(f'Threshold={threshold}: {num_rois} ROIs')\n    ax.axis('off')\n\nplt.tight_layout()\n</pre> # Compare segmentation with different parameters for scan 1 fig, axes = plt.subplots(1, 3, figsize=(12, 4))  key = {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 1} avg_frame = (AverageFrame &amp; key).fetch1('average_frame')  axes[0].imshow(avg_frame, cmap='gray') axes[0].set_title('Average Frame') axes[0].axis('off')  for ax, param_id in zip(axes[1:], [1, 2]):     seg_key = {**key, 'seg_param_id': param_id}     mask, num_rois = (Segmentation &amp; seg_key).fetch1('segmented_mask', 'num_rois')     threshold = (SegmentationParam &amp; {'seg_param_id': param_id}).fetch1('threshold')          ax.imshow(avg_frame, cmap='gray')     ax.imshow(mask, cmap='tab10', alpha=0.5 * (mask &gt; 0))     ax.set_title(f'Threshold={threshold}: {num_rois} ROIs')     ax.axis('off')  plt.tight_layout() In\u00a0[13]: Copied! <pre>@schema\nclass Fluorescence(dj.Imported):\n    definition = \"\"\"\n    -&gt; Segmentation\n    ---\n    timestamps : &lt;blob&gt;    # time for each frame (seconds)\n    \"\"\"\n\n    class Trace(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        -&gt; Segmentation.Roi\n        ---\n        trace : &lt;blob&gt;     # fluorescence trace (mean within ROI mask)\n        \"\"\"\n\n    def make(self, key):\n        # Get scan info and load movie\n        file_name, fps = (Scan &amp; key).fetch1('file_name', 'fps')\n        movie = io.imread(DATA_DIR / file_name)\n        n_frames = movie.shape[0]\n        \n        # Create time axis\n        timestamps = np.arange(n_frames) / fps\n        \n        # Insert master entry\n        self.insert1({**key, 'timestamps': timestamps})\n        \n        # Extract trace for each ROI\n        for roi_key in (Segmentation.Roi &amp; key).keys():\n            mask = (Segmentation.Roi &amp; roi_key).fetch1('mask')\n            \n            # Compute mean fluorescence within mask for each frame\n            trace = np.array([frame[mask].mean() for frame in movie])\n            \n            self.Trace.insert1({**roi_key, 'trace': trace})\n        \n        n_rois = len(Segmentation.Roi &amp; key)\n        print(f\"Extracted {n_rois} traces from {file_name}\")\n</pre> @schema class Fluorescence(dj.Imported):     definition = \"\"\"     -&gt; Segmentation     ---     timestamps :     # time for each frame (seconds)     \"\"\"      class Trace(dj.Part):         definition = \"\"\"         -&gt; master         -&gt; Segmentation.Roi         ---         trace :      # fluorescence trace (mean within ROI mask)         \"\"\"      def make(self, key):         # Get scan info and load movie         file_name, fps = (Scan &amp; key).fetch1('file_name', 'fps')         movie = io.imread(DATA_DIR / file_name)         n_frames = movie.shape[0]                  # Create time axis         timestamps = np.arange(n_frames) / fps                  # Insert master entry         self.insert1({**key, 'timestamps': timestamps})                  # Extract trace for each ROI         for roi_key in (Segmentation.Roi &amp; key).keys():             mask = (Segmentation.Roi &amp; roi_key).fetch1('mask')                          # Compute mean fluorescence within mask for each frame             trace = np.array([frame[mask].mean() for frame in movie])                          self.Trace.insert1({**roi_key, 'trace': trace})                  n_rois = len(Segmentation.Roi &amp; key)         print(f\"Extracted {n_rois} traces from {file_name}\") In\u00a0[14]: Copied! <pre>Fluorescence.populate(display_progress=True)\n</pre> Fluorescence.populate(display_progress=True) <pre>\rFluorescence:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>\rFluorescence: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;00:00, 63.92it/s]</pre> <pre>Extracted 6 traces from example_scan_01.tif\nExtracted 6 traces from example_scan_02.tif\nExtracted 9 traces from example_scan_03.tif\nExtracted 3 traces from example_scan_01.tif\nExtracted 2 traces from example_scan_02.tif\nExtracted 3 traces from example_scan_03.tif\n</pre> <pre>\n</pre> Out[14]: <pre>{'success_count': 6, 'error_list': []}</pre> In\u00a0[15]: Copied! <pre># Plot traces for one segmentation result\nkey = {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 1, 'seg_param_id': 2}\n\ntimestamps = (Fluorescence &amp; key).fetch1('timestamps')\ntraces = (Fluorescence.Trace &amp; key).to_arrays('trace')\n\nplt.figure(figsize=(12, 4))\nfor i, trace in enumerate(traces):\n    plt.plot(timestamps, trace + i * 20, label=f'ROI {i}')  # offset for visibility\n\nplt.xlabel('Time (s)')\nplt.ylabel('Fluorescence (offset)')\nplt.title('Fluorescence Traces')\nplt.legend()\nplt.tight_layout()\n</pre> # Plot traces for one segmentation result key = {'mouse_id': 0, 'session_date': '2017-05-15', 'scan_idx': 1, 'seg_param_id': 2}  timestamps = (Fluorescence &amp; key).fetch1('timestamps') traces = (Fluorescence.Trace &amp; key).to_arrays('trace')  plt.figure(figsize=(12, 4)) for i, trace in enumerate(traces):     plt.plot(timestamps, trace + i * 20, label=f'ROI {i}')  # offset for visibility  plt.xlabel('Time (s)') plt.ylabel('Fluorescence (offset)') plt.title('Fluorescence Traces') plt.legend() plt.tight_layout() In\u00a0[16]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[16]: <p>Legend:</p> <ul> <li>Green rectangles: Manual tables (user-entered data)</li> <li>Gray rectangles: Lookup tables (parameters)</li> <li>Blue ovals: Imported tables (data from files)</li> <li>Red ovals: Computed tables (derived from other tables)</li> <li>Plain text: Part tables (detailed results)</li> </ul> In\u00a0[17]: Copied! <pre># Cleanup: drop schema for re-running\nschema.drop(prompt=False)\n</pre> # Cleanup: drop schema for re-running schema.drop(prompt=False)"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#calcium-imaging-pipeline","title":"Calcium Imaging Pipeline\u00b6","text":"<p>This tutorial builds a complete calcium imaging analysis pipeline using DataJoint. You'll learn to:</p> <ul> <li>Import raw imaging data from TIFF files</li> <li>Segment cells using parameterized detection</li> <li>Extract fluorescence traces from detected ROIs</li> <li>Use Lookup tables for analysis parameters</li> <li>Use Part tables for one-to-many results</li> </ul>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#the-pipeline","title":"The Pipeline\u00b6","text":"<p>Legend: Green = Manual, Gray = Lookup, Blue = Imported, Red = Computed, White = Part</p> <p>Each scan produces a TIFF movie. We compute an average frame, segment cells using threshold-based detection, and extract fluorescence traces for each detected ROI.</p>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#manual-tables-experiment-metadata","title":"Manual Tables: Experiment Metadata\u00b6","text":"<p>We start with tables for subjects, sessions, and scan metadata. These are Manual tables - data entered by experimenters or recording systems.</p>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#insert-sample-data","title":"Insert Sample Data\u00b6","text":""},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#imported-table-average-frame","title":"Imported Table: Average Frame\u00b6","text":"<p>An Imported table pulls data from external files. Here we load each TIFF movie and compute the average frame across all time points.</p> <p>The <code>make()</code> method defines how to compute one entry. DataJoint's <code>populate()</code> calls it for each pending entry.</p>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#lookup-table-segmentation-parameters","title":"Lookup Table: Segmentation Parameters\u00b6","text":"<p>A Lookup table stores parameter sets that don't change often. This lets us run the same analysis with different parameters and compare results.</p> <p>Our cell segmentation has two parameters:</p> <ul> <li><code>threshold</code>: intensity threshold for detecting bright regions</li> <li><code>min_size</code>: minimum blob size (filters out noise)</li> </ul>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#computed-table-with-part-table-segmentation","title":"Computed Table with Part Table: Segmentation\u00b6","text":"<p>A Computed table derives data from other DataJoint tables. Here, <code>Segmentation</code> depends on both <code>AverageFrame</code> and <code>SegmentationParam</code> - DataJoint will compute all combinations.</p> <p>Since each segmentation produces multiple ROIs, we use a Part table (<code>Roi</code>) to store the individual masks. The master table stores summary info; part tables store detailed results.</p>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#visualize-segmentation-results","title":"Visualize Segmentation Results\u00b6","text":""},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#fluorescence-trace-extraction","title":"Fluorescence Trace Extraction\u00b6","text":"<p>Now we extract the fluorescence time series for each ROI. This requires going back to the raw TIFF movie, so we use an Imported table.</p> <p>The master table (<code>Fluorescence</code>) stores shared time axis; the part table (<code>Trace</code>) stores each ROI's trace.</p>"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#visualize-fluorescence-traces","title":"Visualize Fluorescence Traces\u00b6","text":""},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#pipeline-diagram","title":"Pipeline Diagram\u00b6","text":""},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#summary","title":"Summary\u00b6","text":"<p>This pipeline demonstrates key DataJoint patterns for imaging analysis:</p> Concept Example Purpose Manual tables <code>Mouse</code>, <code>Session</code>, <code>Scan</code> Store experiment metadata Imported tables <code>AverageFrame</code>, <code>Fluorescence</code> Load data from external files Computed tables <code>Segmentation</code> Derive data from other tables Lookup tables <code>SegmentationParam</code> Store analysis parameters Part tables <code>Roi</code>, <code>Trace</code> Store one-to-many results <code>populate()</code> Auto-compute missing entries Automatic pipeline execution"},{"location":"tutorials/domain/calcium-imaging/calcium-imaging/#key-benefits","title":"Key Benefits\u00b6","text":"<ol> <li>Parameter tracking: Different segmentation parameters stored alongside results</li> <li>Reproducibility: Re-run <code>populate()</code> to recompute after changes</li> <li>Data integrity: Foreign keys ensure consistent relationships</li> <li>Provenance: Clear lineage from raw data to final traces</li> </ol>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/","title":"Electrophysiology Pipeline","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nschema = dj.Schema('tutorial_electrophysiology')\n\n# Data directory (relative to this notebook)\nDATA_DIR = Path('./data')\n</pre> import datajoint as dj import numpy as np import matplotlib.pyplot as plt from pathlib import Path  schema = dj.Schema('tutorial_electrophysiology')  # Data directory (relative to this notebook) DATA_DIR = Path('./data') <pre>[2026-01-27 15:29:53] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int32\n    ---\n    dob : date\n    sex : enum('M', 'F', 'unknown')\n    \"\"\"\n\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Mouse\n    session_date : date\n    ---\n    experimenter : varchar(100)\n    \"\"\"\n</pre> @schema class Mouse(dj.Manual):     definition = \"\"\"     mouse_id : int32     ---     dob : date     sex : enum('M', 'F', 'unknown')     \"\"\"   @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Mouse     session_date : date     ---     experimenter : varchar(100)     \"\"\" In\u00a0[3]: Copied! <pre># Insert mice\nMouse.insert([\n    {'mouse_id': 0, 'dob': '2017-03-01', 'sex': 'M'},\n    {'mouse_id': 5, 'dob': '2016-12-25', 'sex': 'F'},\n    {'mouse_id': 100, 'dob': '2017-05-12', 'sex': 'F'},\n], skip_duplicates=True)\n\n# Insert sessions (matching our data files)\nSession.insert([\n    {'mouse_id': 0, 'session_date': '2017-05-15', 'experimenter': 'Alice'},\n    {'mouse_id': 0, 'session_date': '2017-05-19', 'experimenter': 'Alice'},\n    {'mouse_id': 5, 'session_date': '2017-01-05', 'experimenter': 'Bob'},\n    {'mouse_id': 100, 'session_date': '2017-05-25', 'experimenter': 'Carol'},\n    {'mouse_id': 100, 'session_date': '2017-06-01', 'experimenter': 'Carol'},\n], skip_duplicates=True)\n\nSession()\n</pre> # Insert mice Mouse.insert([     {'mouse_id': 0, 'dob': '2017-03-01', 'sex': 'M'},     {'mouse_id': 5, 'dob': '2016-12-25', 'sex': 'F'},     {'mouse_id': 100, 'dob': '2017-05-12', 'sex': 'F'}, ], skip_duplicates=True)  # Insert sessions (matching our data files) Session.insert([     {'mouse_id': 0, 'session_date': '2017-05-15', 'experimenter': 'Alice'},     {'mouse_id': 0, 'session_date': '2017-05-19', 'experimenter': 'Alice'},     {'mouse_id': 5, 'session_date': '2017-01-05', 'experimenter': 'Bob'},     {'mouse_id': 100, 'session_date': '2017-05-25', 'experimenter': 'Carol'},     {'mouse_id': 100, 'session_date': '2017-06-01', 'experimenter': 'Carol'}, ], skip_duplicates=True)  Session() Out[3]: None <p>mouse_id</p> None <p>session_date</p> <p>experimenter</p> 0 2017-05-15 Alice0 2017-05-19 Alice5 2017-01-05 Bob100 2017-05-25 Carol100 2017-06-01 Carol <p>Total: 5</p> In\u00a0[4]: Copied! <pre>@schema\nclass Neuron(dj.Imported):\n    definition = \"\"\"\n    -&gt; Session\n    neuron_id : int16\n    ---\n    activity : &lt;blob&gt;    # neural activity trace\n    \"\"\"\n\n    def make(self, key):\n        # Construct filename from key\n        filename = f\"data_{key['mouse_id']}_{key['session_date']}.npy\"\n        filepath = DATA_DIR / filename\n        \n        # Load data (shape: n_neurons x n_timepoints)\n        data = np.load(filepath)\n        \n        # Insert one row per neuron\n        for neuron_id, activity in enumerate(data):\n            self.insert1({\n                **key,\n                'neuron_id': neuron_id,\n                'activity': activity\n            })\n        \n        print(f\"Imported {len(data)} neuron(s) from {filename}\")\n</pre> @schema class Neuron(dj.Imported):     definition = \"\"\"     -&gt; Session     neuron_id : int16     ---     activity :     # neural activity trace     \"\"\"      def make(self, key):         # Construct filename from key         filename = f\"data_{key['mouse_id']}_{key['session_date']}.npy\"         filepath = DATA_DIR / filename                  # Load data (shape: n_neurons x n_timepoints)         data = np.load(filepath)                  # Insert one row per neuron         for neuron_id, activity in enumerate(data):             self.insert1({                 **key,                 'neuron_id': neuron_id,                 'activity': activity             })                  print(f\"Imported {len(data)} neuron(s) from {filename}\") In\u00a0[5]: Copied! <pre>Neuron.populate(display_progress=True)\n</pre> Neuron.populate(display_progress=True) <pre>\rNeuron:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\rNeuron: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 182.65it/s]</pre> <pre>Imported 1 neuron(s) from data_0_2017-05-15.npy\nImported 1 neuron(s) from data_0_2017-05-19.npy\nImported 1 neuron(s) from data_5_2017-01-05.npy\nImported 1 neuron(s) from data_100_2017-05-25.npy\nImported 1 neuron(s) from data_100_2017-06-01.npy\n</pre> <pre>\n</pre> Out[5]: <pre>{'success_count': 5, 'error_list': []}</pre> In\u00a0[6]: Copied! <pre>Neuron()\n</pre> Neuron() Out[6]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> <p>activity</p> neural activity trace 0 2017-05-15 0 &lt;blob&gt;0 2017-05-19 0 &lt;blob&gt;5 2017-01-05 0 &lt;blob&gt;100 2017-05-25 0 &lt;blob&gt;100 2017-06-01 0 &lt;blob&gt; <p>Total: 5</p> In\u00a0[7]: Copied! <pre>fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n\nfor ax, key in zip(axes.ravel(), Neuron.keys()):\n    activity = (Neuron &amp; key).fetch1('activity')\n    ax.plot(activity)\n    ax.set_title(f\"Mouse {key['mouse_id']}, {key['session_date']}\")\n    ax.set_xlabel('Time bin')\n    ax.set_ylabel('Activity')\n\n# Hide unused subplot\naxes[1, 2].axis('off')\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(2, 3, figsize=(12, 6))  for ax, key in zip(axes.ravel(), Neuron.keys()):     activity = (Neuron &amp; key).fetch1('activity')     ax.plot(activity)     ax.set_title(f\"Mouse {key['mouse_id']}, {key['session_date']}\")     ax.set_xlabel('Time bin')     ax.set_ylabel('Activity')  # Hide unused subplot axes[1, 2].axis('off') plt.tight_layout() In\u00a0[8]: Copied! <pre>@schema\nclass ActivityStats(dj.Computed):\n    definition = \"\"\"\n    -&gt; Neuron\n    ---\n    mean_activity : float32\n    std_activity : float32\n    max_activity : float32\n    \"\"\"\n\n    def make(self, key):\n        activity = (Neuron &amp; key).fetch1('activity')\n        \n        self.insert1({\n            **key,\n            'mean_activity': activity.mean(),\n            'std_activity': activity.std(),\n            'max_activity': activity.max()\n        })\n</pre> @schema class ActivityStats(dj.Computed):     definition = \"\"\"     -&gt; Neuron     ---     mean_activity : float32     std_activity : float32     max_activity : float32     \"\"\"      def make(self, key):         activity = (Neuron &amp; key).fetch1('activity')                  self.insert1({             **key,             'mean_activity': activity.mean(),             'std_activity': activity.std(),             'max_activity': activity.max()         }) In\u00a0[9]: Copied! <pre>ActivityStats.populate(display_progress=True)\nActivityStats()\n</pre> ActivityStats.populate(display_progress=True) ActivityStats() <pre>\rActivityStats:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\rActivityStats: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 530.68it/s]</pre> <pre>\n</pre> Out[9]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>mean_activity</p> <p>std_activity</p> <p>max_activity</p> 0 2017-05-15 0 0.20735653 0.40086678 2.4816060 2017-05-19 0 0.1327401 0.2914616 1.82805415 2017-01-05 0 0.089178555 0.23641196 1.3738891100 2017-05-25 0 0.21906962 0.32878324 1.7638324100 2017-06-01 0 0.08732656 0.23785825 1.324542 <p>Total: 5</p> In\u00a0[10]: Copied! <pre>@schema\nclass SpikeParams(dj.Lookup):\n    definition = \"\"\"\n    spike_param_id : int16\n    ---\n    threshold : float32    # spike detection threshold\n    \"\"\"\n    \n    contents = [\n        {'spike_param_id': 1, 'threshold': 0.5},\n        {'spike_param_id': 2, 'threshold': 0.9},\n    ]\n\nSpikeParams()\n</pre> @schema class SpikeParams(dj.Lookup):     definition = \"\"\"     spike_param_id : int16     ---     threshold : float32    # spike detection threshold     \"\"\"          contents = [         {'spike_param_id': 1, 'threshold': 0.5},         {'spike_param_id': 2, 'threshold': 0.9},     ]  SpikeParams() Out[10]: None <p>spike_param_id</p> <p>threshold</p> spike detection threshold 1 0.52 0.9 <p>Total: 2</p> In\u00a0[11]: Copied! <pre>@schema\nclass Spikes(dj.Computed):\n    definition = \"\"\"\n    -&gt; Neuron\n    -&gt; SpikeParams\n    ---\n    spike_times : &lt;blob&gt;    # indices of detected spikes\n    spike_count : int32     # total number of spikes\n    \"\"\"\n\n    class Waveform(dj.Part):\n        definition = \"\"\"\n        -&gt; master\n        spike_idx : int32\n        ---\n        waveform : &lt;blob&gt;    # activity around spike (\u00b140 samples)\n        \"\"\"\n\n    def make(self, key):\n        # Fetch inputs\n        activity = (Neuron &amp; key).fetch1('activity')\n        threshold = (SpikeParams &amp; key).fetch1('threshold')\n        \n        # Detect threshold crossings (rising edge)\n        above_threshold = (activity &gt; threshold).astype(int)\n        rising_edge = np.diff(above_threshold) &gt; 0\n        spike_times = np.where(rising_edge)[0] + 1  # +1 to get crossing point\n        \n        # Insert master entry\n        self.insert1({\n            **key,\n            'spike_times': spike_times,\n            'spike_count': len(spike_times)\n        })\n        \n        # Extract and insert waveforms\n        window = 40  # samples before and after spike\n        for spike_idx, t in enumerate(spike_times):\n            # Skip spikes too close to edges\n            if t &lt; window or t &gt;= len(activity) - window:\n                continue\n            \n            waveform = activity[t - window : t + window]\n            self.Waveform.insert1({\n                **key,\n                'spike_idx': spike_idx,\n                'waveform': waveform\n            })\n</pre> @schema class Spikes(dj.Computed):     definition = \"\"\"     -&gt; Neuron     -&gt; SpikeParams     ---     spike_times :     # indices of detected spikes     spike_count : int32     # total number of spikes     \"\"\"      class Waveform(dj.Part):         definition = \"\"\"         -&gt; master         spike_idx : int32         ---         waveform :     # activity around spike (\u00b140 samples)         \"\"\"      def make(self, key):         # Fetch inputs         activity = (Neuron &amp; key).fetch1('activity')         threshold = (SpikeParams &amp; key).fetch1('threshold')                  # Detect threshold crossings (rising edge)         above_threshold = (activity &gt; threshold).astype(int)         rising_edge = np.diff(above_threshold) &gt; 0         spike_times = np.where(rising_edge)[0] + 1  # +1 to get crossing point                  # Insert master entry         self.insert1({             **key,             'spike_times': spike_times,             'spike_count': len(spike_times)         })                  # Extract and insert waveforms         window = 40  # samples before and after spike         for spike_idx, t in enumerate(spike_times):             # Skip spikes too close to edges             if t &lt; window or t &gt;= len(activity) - window:                 continue                          waveform = activity[t - window : t + window]             self.Waveform.insert1({                 **key,                 'spike_idx': spike_idx,                 'waveform': waveform             }) In\u00a0[12]: Copied! <pre>Spikes.populate(display_progress=True)\n</pre> Spikes.populate(display_progress=True) <pre>\rSpikes:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>\rSpikes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00&lt;00:00, 120.69it/s]</pre> <pre>\n</pre> Out[12]: <pre>{'success_count': 10, 'error_list': []}</pre> In\u00a0[13]: Copied! <pre># View spike counts for each neuron \u00d7 parameter combination\nSpikes.proj('spike_count')\n</pre> # View spike counts for each neuron \u00d7 parameter combination Spikes.proj('spike_count') Out[13]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>spike_param_id</p> None <p>spike_times</p> indices of detected spikes <p>spike_count</p> total number of spikes 0 2017-05-15 0 1 &lt;blob&gt; 260 2017-05-15 0 2 &lt;blob&gt; 270 2017-05-19 0 1 &lt;blob&gt; 240 2017-05-19 0 2 &lt;blob&gt; 215 2017-01-05 0 1 &lt;blob&gt; 185 2017-01-05 0 2 &lt;blob&gt; 14100 2017-05-25 0 1 &lt;blob&gt; 41100 2017-05-25 0 2 &lt;blob&gt; 35100 2017-06-01 0 1 &lt;blob&gt; 18100 2017-06-01 0 2 &lt;blob&gt; 15 <p>Total: 10</p> In\u00a0[14]: Copied! <pre># Pick one neuron to visualize\nneuron_key = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0}\nactivity = (Neuron &amp; neuron_key).fetch1('activity')\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n\nfor ax, param_id in zip(axes, [1, 2]):\n    key = {**neuron_key, 'spike_param_id': param_id}\n    spike_times, spike_count = (Spikes &amp; key).fetch1('spike_times', 'spike_count')\n    threshold = (SpikeParams &amp; {'spike_param_id': param_id}).fetch1('threshold')\n    \n    ax.plot(activity, 'b-', alpha=0.7, label='Activity')\n    ax.axhline(threshold, color='r', linestyle='--', label=f'Threshold={threshold}')\n    ax.scatter(spike_times, activity[spike_times], color='red', s=20, zorder=5)\n    ax.set_title(f'Threshold={threshold}: {spike_count} spikes detected')\n    ax.set_ylabel('Activity')\n    ax.legend(loc='upper right')\n\naxes[1].set_xlabel('Time bin')\nplt.tight_layout()\n</pre> # Pick one neuron to visualize neuron_key = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0} activity = (Neuron &amp; neuron_key).fetch1('activity')  fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)  for ax, param_id in zip(axes, [1, 2]):     key = {**neuron_key, 'spike_param_id': param_id}     spike_times, spike_count = (Spikes &amp; key).fetch1('spike_times', 'spike_count')     threshold = (SpikeParams &amp; {'spike_param_id': param_id}).fetch1('threshold')          ax.plot(activity, 'b-', alpha=0.7, label='Activity')     ax.axhline(threshold, color='r', linestyle='--', label=f'Threshold={threshold}')     ax.scatter(spike_times, activity[spike_times], color='red', s=20, zorder=5)     ax.set_title(f'Threshold={threshold}: {spike_count} spikes detected')     ax.set_ylabel('Activity')     ax.legend(loc='upper right')  axes[1].set_xlabel('Time bin') plt.tight_layout() In\u00a0[15]: Copied! <pre># Get waveforms for one neuron with threshold=0.5\nkey = {'mouse_id': 100, 'session_date': '2017-05-25', 'neuron_id': 0, 'spike_param_id': 1}\nwaveforms = (Spikes.Waveform &amp; key).to_arrays('waveform')\n\nif len(waveforms) &gt; 0:\n    waveform_matrix = np.vstack(waveforms)\n    \n    plt.figure(figsize=(8, 4))\n    # Plot individual waveforms (light)\n    for wf in waveform_matrix:\n        plt.plot(wf, 'b-', alpha=0.2)\n    # Plot mean waveform (bold)\n    plt.plot(waveform_matrix.mean(axis=0), 'r-', linewidth=2, label='Mean waveform')\n    plt.axvline(40, color='k', linestyle='--', alpha=0.5, label='Spike time')\n    plt.xlabel('Sample (relative to spike)')\n    plt.ylabel('Activity')\n    plt.title(f'Spike Waveforms (n={len(waveforms)})')\n    plt.legend()\nelse:\n    print(\"No waveforms found for this key\")\n</pre> # Get waveforms for one neuron with threshold=0.5 key = {'mouse_id': 100, 'session_date': '2017-05-25', 'neuron_id': 0, 'spike_param_id': 1} waveforms = (Spikes.Waveform &amp; key).to_arrays('waveform')  if len(waveforms) &gt; 0:     waveform_matrix = np.vstack(waveforms)          plt.figure(figsize=(8, 4))     # Plot individual waveforms (light)     for wf in waveform_matrix:         plt.plot(wf, 'b-', alpha=0.2)     # Plot mean waveform (bold)     plt.plot(waveform_matrix.mean(axis=0), 'r-', linewidth=2, label='Mean waveform')     plt.axvline(40, color='k', linestyle='--', alpha=0.5, label='Spike time')     plt.xlabel('Sample (relative to spike)')     plt.ylabel('Activity')     plt.title(f'Spike Waveforms (n={len(waveforms)})')     plt.legend() else:     print(\"No waveforms found for this key\") In\u00a0[16]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[16]: In\u00a0[17]: Copied! <pre># Find neurons with high spike counts (threshold=0.5)\n(Spikes &amp; 'spike_param_id = 1' &amp; 'spike_count &gt; 20').proj('spike_count')\n</pre> # Find neurons with high spike counts (threshold=0.5) (Spikes &amp; 'spike_param_id = 1' &amp; 'spike_count &gt; 20').proj('spike_count') Out[17]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>spike_param_id</p> None <p>spike_times</p> indices of detected spikes <p>spike_count</p> total number of spikes 0 2017-05-15 0 1 &lt;blob&gt; 260 2017-05-19 0 1 &lt;blob&gt; 24100 2017-05-25 0 1 &lt;blob&gt; 41 <p>Total: 3</p> In\u00a0[18]: Copied! <pre># Join with Mouse to see which mice have most spikes\n(Mouse * Session * Spikes &amp; 'spike_param_id = 1').proj('spike_count')\n</pre> # Join with Mouse to see which mice have most spikes (Mouse * Session * Spikes &amp; 'spike_param_id = 1').proj('spike_count') Out[18]: <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>spike_param_id</p> None <p>spike_count</p> total number of spikes 0 2017-05-15 0 1 260 2017-05-19 0 1 245 2017-01-05 0 1 18100 2017-05-25 0 1 41100 2017-06-01 0 1 18 <p>Total: 5</p> In\u00a0[19]: Copied! <pre># Cleanup: drop schema for re-running\nschema.drop(prompt=False)\n</pre> # Cleanup: drop schema for re-running schema.drop(prompt=False)"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#electrophysiology-pipeline","title":"Electrophysiology Pipeline\u00b6","text":"<p>This tutorial builds an electrophysiology analysis pipeline using DataJoint. You'll learn to:</p> <ul> <li>Import neural recordings from data files</li> <li>Compute activity statistics</li> <li>Detect spikes using parameterized thresholds</li> <li>Extract waveforms using Part tables</li> </ul>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#the-pipeline","title":"The Pipeline\u00b6","text":"<p>Legend: Green = Manual, Gray = Lookup, Blue = Imported, Red = Computed, White = Part</p> <p>Each session records from neurons. We compute statistics, detect spikes with configurable thresholds, and extract spike waveforms.</p>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/electrophysiology/#manual-tables-experiment-metadata","title":"Manual Tables: Experiment Metadata\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/electrophysiology/#insert-sample-data","title":"Insert Sample Data\u00b6","text":"<p>Our data files follow the naming convention <code>data_{mouse_id}_{session_date}.npy</code>.</p>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#imported-table-neuron-activity","title":"Imported Table: Neuron Activity\u00b6","text":"<p>Each data file contains recordings from one or more neurons. We import each neuron's activity trace.</p>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#visualize-neural-activity","title":"Visualize Neural Activity\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/electrophysiology/#computed-table-activity-statistics","title":"Computed Table: Activity Statistics\u00b6","text":"<p>For each neuron, compute basic statistics of the activity trace.</p>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#lookup-table-spike-detection-parameters","title":"Lookup Table: Spike Detection Parameters\u00b6","text":"<p>Spike detection depends on threshold choice. Using a Lookup table, we can run detection with multiple thresholds and compare results.</p>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#computed-table-with-part-table-spike-detection","title":"Computed Table with Part Table: Spike Detection\u00b6","text":"<p>Detect spikes by finding threshold crossings. Store:</p> <ul> <li>Master table: spike count and binary spike array</li> <li>Part table: waveform for each spike</li> </ul>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#compare-detection-thresholds","title":"Compare Detection Thresholds\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/electrophysiology/#average-waveform","title":"Average Waveform\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/electrophysiology/#pipeline-diagram","title":"Pipeline Diagram\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/electrophysiology/#querying-results","title":"Querying Results\u00b6","text":"<p>DataJoint makes it easy to query across the pipeline.</p>"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#summary","title":"Summary\u00b6","text":"<p>This pipeline demonstrates key patterns for electrophysiology analysis:</p> Concept Example Purpose Imported tables <code>Neuron</code> Load data from files Computed tables <code>ActivityStats</code>, <code>Spikes</code> Derive results Lookup tables <code>SpikeParams</code> Parameterize analysis Part tables <code>Waveform</code> Store variable-length results Multi-parent keys <code>Spikes</code> Compute all Neuron \u00d7 Param combinations"},{"location":"tutorials/domain/electrophysiology/electrophysiology/#key-benefits","title":"Key Benefits\u00b6","text":"<ol> <li>Parameter comparison: Different thresholds stored alongside results</li> <li>Automatic tracking: <code>populate()</code> knows what's computed vs. pending</li> <li>Cascading: Delete a parameter set, all derived results cascade</li> <li>Provenance: Trace any spike back to its source recording</li> </ol>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/","title":"Electrophysiology Pipeline with Object Storage","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport tempfile\n\n# Create a temporary directory for object storage\nSTORE_PATH = tempfile.mkdtemp(prefix='dj_ephys_')\n\n# Configure object storage with partitioning by mouse_id, session_date, and neuron_id\ndj.config.stores['ephys'] = {\n    'protocol': 'file',\n    'location': STORE_PATH,\n    'partition_pattern': '{mouse_id}/{session_date}/{neuron_id}',  # Partition by subject, session, and neuron\n}\n\nschema = dj.Schema('tutorial_electrophysiology_npy')\n\n# Data directory (relative to this notebook)\nDATA_DIR = Path('./data')\n\nprint(f\"Store configured at: {STORE_PATH}\")\nprint(\"Partitioning: {mouse_id}/{session_date}/{neuron_id}\")\n</pre> import datajoint as dj import numpy as np import matplotlib.pyplot as plt from pathlib import Path import tempfile  # Create a temporary directory for object storage STORE_PATH = tempfile.mkdtemp(prefix='dj_ephys_')  # Configure object storage with partitioning by mouse_id, session_date, and neuron_id dj.config.stores['ephys'] = {     'protocol': 'file',     'location': STORE_PATH,     'partition_pattern': '{mouse_id}/{session_date}/{neuron_id}',  # Partition by subject, session, and neuron }  schema = dj.Schema('tutorial_electrophysiology_npy')  # Data directory (relative to this notebook) DATA_DIR = Path('./data')  print(f\"Store configured at: {STORE_PATH}\") print(\"Partitioning: {mouse_id}/{session_date}/{neuron_id}\") <pre>[2026-01-27 15:30:00] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> <pre>Store configured at: /var/folders/cn/dpwf5t7j3gd8gzyw2r7dhm8r0000gn/T/dj_ephys_d4v7e7iq\nPartitioning: {mouse_id}/{session_date}/{neuron_id}\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Mouse(dj.Manual):\n    definition = \"\"\"\n    mouse_id : int32\n    ---\n    dob : date\n    sex : enum('M', 'F', 'unknown')\n    \"\"\"\n\n\n@schema\nclass Session(dj.Manual):\n    definition = \"\"\"\n    -&gt; Mouse\n    session_date : date\n    ---\n    experimenter : varchar(100)\n    \"\"\"\n</pre> @schema class Mouse(dj.Manual):     definition = \"\"\"     mouse_id : int32     ---     dob : date     sex : enum('M', 'F', 'unknown')     \"\"\"   @schema class Session(dj.Manual):     definition = \"\"\"     -&gt; Mouse     session_date : date     ---     experimenter : varchar(100)     \"\"\" In\u00a0[3]: Copied! <pre># Insert mice\nMouse.insert([\n    {'mouse_id': 0, 'dob': '2017-03-01', 'sex': 'M'},\n    {'mouse_id': 5, 'dob': '2016-12-25', 'sex': 'F'},\n    {'mouse_id': 100, 'dob': '2017-05-12', 'sex': 'F'},\n], skip_duplicates=True)\n\n# Insert sessions (matching our data files)\nSession.insert([\n    {'mouse_id': 0, 'session_date': '2017-05-15', 'experimenter': 'Alice'},\n    {'mouse_id': 0, 'session_date': '2017-05-19', 'experimenter': 'Alice'},\n    {'mouse_id': 5, 'session_date': '2017-01-05', 'experimenter': 'Bob'},\n    {'mouse_id': 100, 'session_date': '2017-05-25', 'experimenter': 'Carol'},\n    {'mouse_id': 100, 'session_date': '2017-06-01', 'experimenter': 'Carol'},\n], skip_duplicates=True)\n\nSession()\n</pre> # Insert mice Mouse.insert([     {'mouse_id': 0, 'dob': '2017-03-01', 'sex': 'M'},     {'mouse_id': 5, 'dob': '2016-12-25', 'sex': 'F'},     {'mouse_id': 100, 'dob': '2017-05-12', 'sex': 'F'}, ], skip_duplicates=True)  # Insert sessions (matching our data files) Session.insert([     {'mouse_id': 0, 'session_date': '2017-05-15', 'experimenter': 'Alice'},     {'mouse_id': 0, 'session_date': '2017-05-19', 'experimenter': 'Alice'},     {'mouse_id': 5, 'session_date': '2017-01-05', 'experimenter': 'Bob'},     {'mouse_id': 100, 'session_date': '2017-05-25', 'experimenter': 'Carol'},     {'mouse_id': 100, 'session_date': '2017-06-01', 'experimenter': 'Carol'}, ], skip_duplicates=True)  Session() Out[3]: None <p>mouse_id</p> None <p>session_date</p> <p>experimenter</p> 0 2017-05-15 Alice0 2017-05-19 Alice5 2017-01-05 Bob100 2017-05-25 Carol100 2017-06-01 Carol <p>Total: 5</p> In\u00a0[4]: Copied! <pre>@schema\nclass Neuron(dj.Imported):\n    definition = \"\"\"\n    -&gt; Session\n    neuron_id : int16\n    ---\n    activity : &lt;npy@ephys&gt;    # neural activity trace (lazy loading)\n    \"\"\"\n\n    def make(self, key):\n        # Construct filename from key\n        filename = f\"data_{key['mouse_id']}_{key['session_date']}.npy\"\n        filepath = DATA_DIR / filename\n        \n        # Load data (shape: n_neurons x n_timepoints)\n        data = np.load(filepath)\n        \n        # Insert one row per neuron\n        for neuron_id, activity in enumerate(data):\n            self.insert1({\n                **key,\n                'neuron_id': neuron_id,\n                'activity': activity\n            })\n        \n        print(f\"Imported {len(data)} neuron(s) from {filename}\")\n</pre> @schema class Neuron(dj.Imported):     definition = \"\"\"     -&gt; Session     neuron_id : int16     ---     activity :     # neural activity trace (lazy loading)     \"\"\"      def make(self, key):         # Construct filename from key         filename = f\"data_{key['mouse_id']}_{key['session_date']}.npy\"         filepath = DATA_DIR / filename                  # Load data (shape: n_neurons x n_timepoints)         data = np.load(filepath)                  # Insert one row per neuron         for neuron_id, activity in enumerate(data):             self.insert1({                 **key,                 'neuron_id': neuron_id,                 'activity': activity             })                  print(f\"Imported {len(data)} neuron(s) from {filename}\") In\u00a0[5]: Copied! <pre>Neuron.populate(display_progress=True)\n</pre> Neuron.populate(display_progress=True) <pre>\rNeuron:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\rNeuron: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 149.23it/s]</pre> <pre>Imported 1 neuron(s) from data_0_2017-05-15.npy\nImported 1 neuron(s) from data_0_2017-05-19.npy\nImported 1 neuron(s) from data_5_2017-01-05.npy\nImported 1 neuron(s) from data_100_2017-05-25.npy\nImported 1 neuron(s) from data_100_2017-06-01.npy\n</pre> <pre>\n</pre> Out[5]: <pre>{'success_count': 5, 'error_list': []}</pre> In\u00a0[6]: Copied! <pre>Neuron()\n</pre> Neuron() Out[6]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> <p>activity</p> neural activity trace (lazy loading) 0 2017-05-15 0 &lt;npy&gt;0 2017-05-19 0 &lt;npy&gt;5 2017-01-05 0 &lt;npy&gt;100 2017-05-25 0 &lt;npy&gt;100 2017-06-01 0 &lt;npy&gt; <p>Total: 5</p> In\u00a0[7]: Copied! <pre># Fetch returns NpyRef, not the array\nkey = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0}\nref = (Neuron &amp; key).fetch1('activity')\n\nprint(f\"Type: {type(ref).__name__}\")\nprint(f\"Shape: {ref.shape} (no download!)\")\nprint(f\"Dtype: {ref.dtype}\")\nprint(f\"Is loaded: {ref.is_loaded}\")\n</pre> # Fetch returns NpyRef, not the array key = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0} ref = (Neuron &amp; key).fetch1('activity')  print(f\"Type: {type(ref).__name__}\") print(f\"Shape: {ref.shape} (no download!)\") print(f\"Dtype: {ref.dtype}\") print(f\"Is loaded: {ref.is_loaded}\") <pre>Type: NpyRef\nShape: (1000,) (no download!)\nDtype: float64\nIs loaded: False\n</pre> In\u00a0[8]: Copied! <pre># Explicitly load when ready\nactivity = ref.load()\nprint(f\"Loaded: {activity.shape}, is_loaded: {ref.is_loaded}\")\n</pre> # Explicitly load when ready activity = ref.load() print(f\"Loaded: {activity.shape}, is_loaded: {ref.is_loaded}\") <pre>Loaded: (1000,), is_loaded: True\n</pre> In\u00a0[9]: Copied! <pre># Memory-mapped loading - efficient for large arrays\nkey = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0}\nref = (Neuron &amp; key).fetch1('activity')\n\n# Load as memory-mapped array (read-only)\nmmap_arr = ref.load(mmap_mode='r')\n\nprint(f\"Type: {type(mmap_arr).__name__}\")\nprint(f\"Shape: {mmap_arr.shape}\")\n\n# Random access only reads the needed portion from disk\nslice_data = mmap_arr[100:200]\nprint(f\"Slice [100:200]: {slice_data[:5]}...\")\n</pre> # Memory-mapped loading - efficient for large arrays key = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0} ref = (Neuron &amp; key).fetch1('activity')  # Load as memory-mapped array (read-only) mmap_arr = ref.load(mmap_mode='r')  print(f\"Type: {type(mmap_arr).__name__}\") print(f\"Shape: {mmap_arr.shape}\")  # Random access only reads the needed portion from disk slice_data = mmap_arr[100:200] print(f\"Slice [100:200]: {slice_data[:5]}...\") <pre>Type: memmap\nShape: (1000,)\nSlice [100:200]: [-0.09536487  0.00759549 -0.14405436 -0.08785159 -0.14242823]...\n</pre> In\u00a0[10]: Copied! <pre>fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n\nfor ax, key in zip(axes.ravel(), Neuron.keys()):\n    # fetch1 returns NpyRef, but plotting works via __array__\n    activity = (Neuron &amp; key).fetch1('activity')\n    ax.plot(np.asarray(activity))  # Convert to array for plotting\n    ax.set_title(f\"Mouse {key['mouse_id']}, {key['session_date']}\")\n    ax.set_xlabel('Time bin')\n    ax.set_ylabel('Activity')\n\n# Hide unused subplot\naxes[1, 2].axis('off')\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(2, 3, figsize=(12, 6))  for ax, key in zip(axes.ravel(), Neuron.keys()):     # fetch1 returns NpyRef, but plotting works via __array__     activity = (Neuron &amp; key).fetch1('activity')     ax.plot(np.asarray(activity))  # Convert to array for plotting     ax.set_title(f\"Mouse {key['mouse_id']}, {key['session_date']}\")     ax.set_xlabel('Time bin')     ax.set_ylabel('Activity')  # Hide unused subplot axes[1, 2].axis('off') plt.tight_layout() In\u00a0[11]: Copied! <pre>@schema\nclass ActivityStats(dj.Computed):\n    definition = \"\"\"\n    -&gt; Neuron\n    ---\n    mean_activity : float32\n    std_activity : float32\n    max_activity : float32\n    \"\"\"\n\n    def make(self, key):\n        # fetch1 returns NpyRef, but np.mean/std/max work via __array__\n        activity = (Neuron &amp; key).fetch1('activity')\n        \n        self.insert1({\n            **key,\n            'mean_activity': np.mean(activity),  # Auto-loads via __array__\n            'std_activity': np.std(activity),\n            'max_activity': np.max(activity)\n        })\n</pre> @schema class ActivityStats(dj.Computed):     definition = \"\"\"     -&gt; Neuron     ---     mean_activity : float32     std_activity : float32     max_activity : float32     \"\"\"      def make(self, key):         # fetch1 returns NpyRef, but np.mean/std/max work via __array__         activity = (Neuron &amp; key).fetch1('activity')                  self.insert1({             **key,             'mean_activity': np.mean(activity),  # Auto-loads via __array__             'std_activity': np.std(activity),             'max_activity': np.max(activity)         }) In\u00a0[12]: Copied! <pre>ActivityStats.populate(display_progress=True)\nActivityStats()\n</pre> ActivityStats.populate(display_progress=True) ActivityStats() <pre>\rActivityStats:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\rActivityStats: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00&lt;00:00, 501.46it/s]</pre> <pre>\n</pre> Out[12]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>mean_activity</p> <p>std_activity</p> <p>max_activity</p> 0 2017-05-15 0 0.20735653 0.40086678 2.4816060 2017-05-19 0 0.1327401 0.2914616 1.82805415 2017-01-05 0 0.089178555 0.23641196 1.3738891100 2017-05-25 0 0.21906962 0.32878324 1.7638324100 2017-06-01 0 0.08732656 0.23785825 1.324542 <p>Total: 5</p> In\u00a0[13]: Copied! <pre>@schema\nclass SpikeParams(dj.Lookup):\n    definition = \"\"\"\n    spike_param_id : int16\n    ---\n    threshold : float32    # spike detection threshold\n    \"\"\"\n    \n    contents = [\n        {'spike_param_id': 1, 'threshold': 0.5},\n        {'spike_param_id': 2, 'threshold': 0.9},\n    ]\n\nSpikeParams()\n</pre> @schema class SpikeParams(dj.Lookup):     definition = \"\"\"     spike_param_id : int16     ---     threshold : float32    # spike detection threshold     \"\"\"          contents = [         {'spike_param_id': 1, 'threshold': 0.5},         {'spike_param_id': 2, 'threshold': 0.9},     ]  SpikeParams() Out[13]: None <p>spike_param_id</p> <p>threshold</p> spike detection threshold 1 0.52 0.9 <p>Total: 2</p> In\u00a0[14]: Copied! <pre>@schema\nclass Spikes(dj.Computed):\n    definition = \"\"\"\n    -&gt; Neuron\n    -&gt; SpikeParams\n    ---\n    spike_times : &lt;npy@ephys&gt;    # indices of detected spikes\n    spike_count : int64         # total number of spikes\n    waveforms : &lt;npy@ephys&gt;      # all waveforms stacked (n_spikes x window_size)\n    \"\"\"\n\n    def make(self, key):\n        # Fetch inputs - activity is NpyRef\n        activity_ref = (Neuron &amp; key).fetch1('activity')\n        threshold = (SpikeParams &amp; key).fetch1('threshold')\n        \n        # Load activity for processing\n        activity = activity_ref.load()\n        \n        # Detect threshold crossings (rising edge)\n        above_threshold = (activity &gt; threshold).astype(int)\n        rising_edge = np.diff(above_threshold) &gt; 0\n        spike_times = np.where(rising_edge)[0] + 1  # +1 to get crossing point\n        \n        # Extract waveforms for all spikes\n        window = 40  # samples before and after spike\n        waveforms = []\n        for t in spike_times:\n            # Skip spikes too close to edges\n            if t &lt; window or t &gt;= len(activity) - window:\n                continue\n            waveforms.append(activity[t - window : t + window])\n        \n        # Stack into 2D array (n_spikes x window_size)\n        waveforms = np.vstack(waveforms) if waveforms else np.empty((0, 2 * window))\n        \n        # Insert entry with all data\n        self.insert1({\n            **key,\n            'spike_times': spike_times,\n            'spike_count': len(spike_times),\n            'waveforms': waveforms,  # All waveforms as single array\n        })\n</pre> @schema class Spikes(dj.Computed):     definition = \"\"\"     -&gt; Neuron     -&gt; SpikeParams     ---     spike_times :     # indices of detected spikes     spike_count : int64         # total number of spikes     waveforms :       # all waveforms stacked (n_spikes x window_size)     \"\"\"      def make(self, key):         # Fetch inputs - activity is NpyRef         activity_ref = (Neuron &amp; key).fetch1('activity')         threshold = (SpikeParams &amp; key).fetch1('threshold')                  # Load activity for processing         activity = activity_ref.load()                  # Detect threshold crossings (rising edge)         above_threshold = (activity &gt; threshold).astype(int)         rising_edge = np.diff(above_threshold) &gt; 0         spike_times = np.where(rising_edge)[0] + 1  # +1 to get crossing point                  # Extract waveforms for all spikes         window = 40  # samples before and after spike         waveforms = []         for t in spike_times:             # Skip spikes too close to edges             if t &lt; window or t &gt;= len(activity) - window:                 continue             waveforms.append(activity[t - window : t + window])                  # Stack into 2D array (n_spikes x window_size)         waveforms = np.vstack(waveforms) if waveforms else np.empty((0, 2 * window))                  # Insert entry with all data         self.insert1({             **key,             'spike_times': spike_times,             'spike_count': len(spike_times),             'waveforms': waveforms,  # All waveforms as single array         }) In\u00a0[15]: Copied! <pre>Spikes.populate(display_progress=True)\n</pre> Spikes.populate(display_progress=True) <pre>\rSpikes:   0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>\rSpikes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:00&lt;00:00, 350.05it/s]</pre> <pre>\n</pre> Out[15]: <pre>{'success_count': 10, 'error_list': []}</pre> In\u00a0[16]: Copied! <pre># View spike counts for each neuron x parameter combination\nSpikes.proj('spike_count')\n</pre> # View spike counts for each neuron x parameter combination Spikes.proj('spike_count') Out[16]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>spike_param_id</p> None <p>spike_times</p> indices of detected spikes <p>spike_count</p> total number of spikes <p>waveforms</p> all waveforms stacked (n_spikes x window_size) 0 2017-05-15 0 1 &lt;npy&gt; 26 &lt;npy&gt;0 2017-05-15 0 2 &lt;npy&gt; 27 &lt;npy&gt;0 2017-05-19 0 1 &lt;npy&gt; 24 &lt;npy&gt;0 2017-05-19 0 2 &lt;npy&gt; 21 &lt;npy&gt;5 2017-01-05 0 1 &lt;npy&gt; 18 &lt;npy&gt;5 2017-01-05 0 2 &lt;npy&gt; 14 &lt;npy&gt;100 2017-05-25 0 1 &lt;npy&gt; 41 &lt;npy&gt;100 2017-05-25 0 2 &lt;npy&gt; 35 &lt;npy&gt;100 2017-06-01 0 1 &lt;npy&gt; 18 &lt;npy&gt;100 2017-06-01 0 2 &lt;npy&gt; 15 &lt;npy&gt; <p>Total: 10</p> In\u00a0[17]: Copied! <pre># Pick one neuron to visualize\nneuron_key = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0}\nactivity = (Neuron &amp; neuron_key).fetch1('activity').load()  # Explicit load\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n\nfor ax, param_id in zip(axes, [1, 2]):\n    key = {**neuron_key, 'spike_param_id': param_id}\n    spike_times_ref = (Spikes &amp; key).fetch1('spike_times')\n    spike_times = spike_times_ref.load()  # Load spike times\n    spike_count = (Spikes &amp; key).fetch1('spike_count')\n    threshold = (SpikeParams &amp; {'spike_param_id': param_id}).fetch1('threshold')\n    \n    ax.plot(activity, 'b-', alpha=0.7, label='Activity')\n    ax.axhline(threshold, color='r', linestyle='--', label=f'Threshold={threshold}')\n    ax.scatter(spike_times, activity[spike_times], color='red', s=20, zorder=5)\n    ax.set_title(f'Threshold={threshold}: {spike_count} spikes detected')\n    ax.set_ylabel('Activity')\n    ax.legend(loc='upper right')\n\naxes[1].set_xlabel('Time bin')\nplt.tight_layout()\n</pre> # Pick one neuron to visualize neuron_key = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0} activity = (Neuron &amp; neuron_key).fetch1('activity').load()  # Explicit load  fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)  for ax, param_id in zip(axes, [1, 2]):     key = {**neuron_key, 'spike_param_id': param_id}     spike_times_ref = (Spikes &amp; key).fetch1('spike_times')     spike_times = spike_times_ref.load()  # Load spike times     spike_count = (Spikes &amp; key).fetch1('spike_count')     threshold = (SpikeParams &amp; {'spike_param_id': param_id}).fetch1('threshold')          ax.plot(activity, 'b-', alpha=0.7, label='Activity')     ax.axhline(threshold, color='r', linestyle='--', label=f'Threshold={threshold}')     ax.scatter(spike_times, activity[spike_times], color='red', s=20, zorder=5)     ax.set_title(f'Threshold={threshold}: {spike_count} spikes detected')     ax.set_ylabel('Activity')     ax.legend(loc='upper right')  axes[1].set_xlabel('Time bin') plt.tight_layout() In\u00a0[18]: Copied! <pre># Get waveforms for one neuron with threshold=0.5\nkey = {'mouse_id': 100, 'session_date': '2017-05-25', 'neuron_id': 0, 'spike_param_id': 1}\n\n# Fetch waveforms NpyRef directly from Spikes table\nwaveforms_ref = (Spikes &amp; key).fetch1('waveforms')\n\n# Load the waveform matrix\nwaveform_matrix = waveforms_ref.load()\n\nif len(waveform_matrix) &gt; 0:\n    plt.figure(figsize=(8, 4))\n    # Plot individual waveforms (light)\n    for wf in waveform_matrix:\n        plt.plot(wf, 'b-', alpha=0.2)\n    # Plot mean waveform (bold)\n    plt.plot(waveform_matrix.mean(axis=0), 'r-', linewidth=2, label='Mean waveform')\n    plt.axvline(40, color='k', linestyle='--', alpha=0.5, label='Spike time')\n    plt.xlabel('Sample (relative to spike)')\n    plt.ylabel('Activity')\n    plt.title(f'Spike Waveforms (n={len(waveform_matrix)})')\n    plt.legend()\nelse:\n    print(\"No waveforms found for this key\")\n</pre> # Get waveforms for one neuron with threshold=0.5 key = {'mouse_id': 100, 'session_date': '2017-05-25', 'neuron_id': 0, 'spike_param_id': 1}  # Fetch waveforms NpyRef directly from Spikes table waveforms_ref = (Spikes &amp; key).fetch1('waveforms')  # Load the waveform matrix waveform_matrix = waveforms_ref.load()  if len(waveform_matrix) &gt; 0:     plt.figure(figsize=(8, 4))     # Plot individual waveforms (light)     for wf in waveform_matrix:         plt.plot(wf, 'b-', alpha=0.2)     # Plot mean waveform (bold)     plt.plot(waveform_matrix.mean(axis=0), 'r-', linewidth=2, label='Mean waveform')     plt.axvline(40, color='k', linestyle='--', alpha=0.5, label='Spike time')     plt.xlabel('Sample (relative to spike)')     plt.ylabel('Activity')     plt.title(f'Spike Waveforms (n={len(waveform_matrix)})')     plt.legend() else:     print(\"No waveforms found for this key\") In\u00a0[19]: Copied! <pre># Fetch all neurons - returns NpyRefs, NOT arrays\nall_neurons = Neuron.to_dicts()\nprint(f\"Fetched {len(all_neurons)} neurons\\n\")\n\n# Inspect metadata without downloading\nfor neuron in all_neurons:\n    ref = neuron['activity']\n    print(f\"Mouse {neuron['mouse_id']}, {neuron['session_date']}: \"\n          f\"shape={ref.shape}, loaded={ref.is_loaded}\")\n</pre> # Fetch all neurons - returns NpyRefs, NOT arrays all_neurons = Neuron.to_dicts() print(f\"Fetched {len(all_neurons)} neurons\\n\")  # Inspect metadata without downloading for neuron in all_neurons:     ref = neuron['activity']     print(f\"Mouse {neuron['mouse_id']}, {neuron['session_date']}: \"           f\"shape={ref.shape}, loaded={ref.is_loaded}\") <pre>Fetched 5 neurons\n\nMouse 0, 2017-05-15: shape=(1000,), loaded=False\nMouse 0, 2017-05-19: shape=(1000,), loaded=False\nMouse 5, 2017-01-05: shape=(1000,), loaded=False\nMouse 100, 2017-05-25: shape=(1000,), loaded=False\nMouse 100, 2017-06-01: shape=(1000,), loaded=False\n</pre> In\u00a0[20]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[20]: In\u00a0[21]: Copied! <pre># Find neurons with high spike counts (threshold=0.5)\n(Spikes &amp; 'spike_param_id = 1' &amp; 'spike_count &gt; 20').proj('spike_count')\n</pre> # Find neurons with high spike counts (threshold=0.5) (Spikes &amp; 'spike_param_id = 1' &amp; 'spike_count &gt; 20').proj('spike_count') Out[21]: None <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>spike_param_id</p> None <p>spike_times</p> indices of detected spikes <p>spike_count</p> total number of spikes <p>waveforms</p> all waveforms stacked (n_spikes x window_size) 0 2017-05-15 0 1 &lt;npy&gt; 26 &lt;npy&gt;0 2017-05-19 0 1 &lt;npy&gt; 24 &lt;npy&gt;100 2017-05-25 0 1 &lt;npy&gt; 41 &lt;npy&gt; <p>Total: 3</p> In\u00a0[22]: Copied! <pre># Join with Mouse to see which mice have most spikes\n(Mouse * Session * Spikes &amp; 'spike_param_id = 1').proj('spike_count')\n</pre> # Join with Mouse to see which mice have most spikes (Mouse * Session * Spikes &amp; 'spike_param_id = 1').proj('spike_count') Out[22]: <p>mouse_id</p> None <p>session_date</p> None <p>neuron_id</p> None <p>spike_param_id</p> None <p>spike_count</p> total number of spikes 0 2017-05-15 0 1 260 2017-05-19 0 1 245 2017-01-05 0 1 18100 2017-05-25 0 1 41100 2017-06-01 0 1 18 <p>Total: 5</p> In\u00a0[23]: Copied! <pre># Explore the store directory structure\nfrom pathlib import Path\n\nprint(f\"Store location: {STORE_PATH}\\n\")\nprint(\"Directory structure (one subject shown in full):\")\n\ndef print_tree(directory, prefix=\"\", max_depth=7, current_depth=0, limit_items=True):\n    \"\"\"Print directory tree with limited depth.\"\"\"\n    if current_depth &gt;= max_depth:\n        return\n    \n    try:\n        entries = sorted(Path(directory).iterdir())\n    except PermissionError:\n        return\n    \n    dirs = [e for e in entries if e.is_dir()]\n    files = [e for e in entries if e.is_file()]\n    \n    # At depth 0 (root), only show first subject in detail\n    if current_depth == 0 and limit_items:\n        dirs = dirs[:1]  # Only show first mouse_id\n    \n    # Show directories\n    for i, d in enumerate(dirs):\n        is_last_dir = (i == len(dirs) - 1) and len(files) == 0\n        connector = \"\u2514\u2500\u2500 \" if is_last_dir else \"\u251c\u2500\u2500 \"\n        print(f\"{prefix}{connector}{d.name}/\")\n        \n        extension = \"    \" if is_last_dir else \"\u2502   \"\n        print_tree(d, prefix + extension, max_depth, current_depth + 1, limit_items=False)\n    \n    if current_depth == 0 and limit_items and len(sorted(Path(directory).iterdir(), key=lambda x: x.is_file())) &gt; 1:\n        total_mice = len([e for e in Path(directory).iterdir() if e.is_dir()])\n        if total_mice &gt; 1:\n            print(f\"{prefix}\u2514\u2500\u2500 ... and {total_mice - 1} more subjects (mouse_id=5, mouse_id=100)\")\n    \n    # Show files\n    for i, f in enumerate(files):\n        is_last = i == len(files) - 1\n        connector = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n        size_kb = f.stat().st_size / 1024\n        print(f\"{prefix}{connector}{f.name} ({size_kb:.1f} KB)\")\n\nprint_tree(STORE_PATH)\n</pre> # Explore the store directory structure from pathlib import Path  print(f\"Store location: {STORE_PATH}\\n\") print(\"Directory structure (one subject shown in full):\")  def print_tree(directory, prefix=\"\", max_depth=7, current_depth=0, limit_items=True):     \"\"\"Print directory tree with limited depth.\"\"\"     if current_depth &gt;= max_depth:         return          try:         entries = sorted(Path(directory).iterdir())     except PermissionError:         return          dirs = [e for e in entries if e.is_dir()]     files = [e for e in entries if e.is_file()]          # At depth 0 (root), only show first subject in detail     if current_depth == 0 and limit_items:         dirs = dirs[:1]  # Only show first mouse_id          # Show directories     for i, d in enumerate(dirs):         is_last_dir = (i == len(dirs) - 1) and len(files) == 0         connector = \"\u2514\u2500\u2500 \" if is_last_dir else \"\u251c\u2500\u2500 \"         print(f\"{prefix}{connector}{d.name}/\")                  extension = \"    \" if is_last_dir else \"\u2502   \"         print_tree(d, prefix + extension, max_depth, current_depth + 1, limit_items=False)          if current_depth == 0 and limit_items and len(sorted(Path(directory).iterdir(), key=lambda x: x.is_file())) &gt; 1:         total_mice = len([e for e in Path(directory).iterdir() if e.is_dir()])         if total_mice &gt; 1:             print(f\"{prefix}\u2514\u2500\u2500 ... and {total_mice - 1} more subjects (mouse_id=5, mouse_id=100)\")          # Show files     for i, f in enumerate(files):         is_last = i == len(files) - 1         connector = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"         size_kb = f.stat().st_size / 1024         print(f\"{prefix}{connector}{f.name} ({size_kb:.1f} KB)\")  print_tree(STORE_PATH) <pre>Store location: /var/folders/cn/dpwf5t7j3gd8gzyw2r7dhm8r0000gn/T/dj_ephys_d4v7e7iq\n\nDirectory structure (one subject shown in full):\n\u2514\u2500\u2500 mouse_id=0/\n    \u251c\u2500\u2500 session_date=2017-05-15/\n    \u2502   \u2514\u2500\u2500 neuron_id=0/\n    \u2502       \u2514\u2500\u2500 tutorial_electrophysiology_npy/\n    \u2502           \u251c\u2500\u2500 __spikes/\n    \u2502           \u2502   \u251c\u2500\u2500 spike_param_id=1/\n    \u2502           \u2502   \u2502   \u251c\u2500\u2500 spike_times_BXKKFICZ.npy (0.3 KB)\n    \u2502           \u2502   \u2502   \u2514\u2500\u2500 waveforms_kUwdzj78.npy (15.1 KB)\n    \u2502           \u2502   \u2514\u2500\u2500 spike_param_id=2/\n    \u2502           \u2502       \u251c\u2500\u2500 spike_times_QW40D7lg.npy (0.3 KB)\n    \u2502           \u2502       \u2514\u2500\u2500 waveforms_szEnIhMx.npy (15.8 KB)\n    \u2502           \u2514\u2500\u2500 _neuron/\n    \u2502               \u2514\u2500\u2500 activity_ezivz3DF.npy (7.9 KB)\n    \u2514\u2500\u2500 session_date=2017-05-19/\n        \u2514\u2500\u2500 neuron_id=0/\n            \u2514\u2500\u2500 tutorial_electrophysiology_npy/\n                \u251c\u2500\u2500 __spikes/\n                \u2502   \u251c\u2500\u2500 spike_param_id=1/\n                \u2502   \u2502   \u251c\u2500\u2500 spike_times_cmgrNWWT.npy (0.3 KB)\n                \u2502   \u2502   \u2514\u2500\u2500 waveforms_7qeTl_Dp.npy (14.5 KB)\n                \u2502   \u2514\u2500\u2500 spike_param_id=2/\n                \u2502       \u251c\u2500\u2500 spike_times_BEf94y5U.npy (0.3 KB)\n                \u2502       \u2514\u2500\u2500 waveforms_jcSQoe85.npy (12.6 KB)\n                \u2514\u2500\u2500 _neuron/\n                    \u2514\u2500\u2500 activity_l4RFNege.npy (7.9 KB)\n\u2514\u2500\u2500 ... and 2 more subjects (mouse_id=5, mouse_id=100)\n</pre> In\u00a0[24]: Copied! <pre># Get the actual path from an NpyRef\nkey = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0}\nref = (Neuron &amp; key).fetch1('activity')\n\nprint(f\"NpyRef path (relative): {ref.path}\")\nprint(f\"Full path: {Path(STORE_PATH) / ref.path}\\n\")\n\n# Load directly with NumPy - bypass the database!\ndirect_path = Path(STORE_PATH) / ref.path\nactivity_direct = np.load(direct_path)\nprint(f\"Loaded array: shape={activity_direct.shape}, dtype={activity_direct.dtype}\")\nprint(f\"First 5 values: {activity_direct[:5]}\")\n</pre> # Get the actual path from an NpyRef key = {'mouse_id': 0, 'session_date': '2017-05-15', 'neuron_id': 0} ref = (Neuron &amp; key).fetch1('activity')  print(f\"NpyRef path (relative): {ref.path}\") print(f\"Full path: {Path(STORE_PATH) / ref.path}\\n\")  # Load directly with NumPy - bypass the database! direct_path = Path(STORE_PATH) / ref.path activity_direct = np.load(direct_path) print(f\"Loaded array: shape={activity_direct.shape}, dtype={activity_direct.dtype}\") print(f\"First 5 values: {activity_direct[:5]}\") <pre>NpyRef path (relative): mouse_id=0/session_date=2017-05-15/neuron_id=0/tutorial_electrophysiology_npy/_neuron/activity_ezivz3DF.npy\nFull path: /var/folders/cn/dpwf5t7j3gd8gzyw2r7dhm8r0000gn/T/dj_ephys_d4v7e7iq/mouse_id=0/session_date=2017-05-15/neuron_id=0/tutorial_electrophysiology_npy/_neuron/activity_ezivz3DF.npy\n\nLoaded array: shape=(1000,), dtype=float64\nFirst 5 values: [0.35788741 0.44753156 0.19641299 0.39111449 0.17669518]\n</pre> In\u00a0[25]: Copied! <pre># Find all .npy files for a specific mouse using filesystem tools\n# This works without any database query!\n\nstore_path = Path(STORE_PATH)\nall_npy_files = list(store_path.rglob(\"*.npy\"))\n\nprint(f\"All .npy files in store ({len(all_npy_files)} total):\\n\")\nfor f in sorted(all_npy_files)[:8]:\n    rel_path = f.relative_to(STORE_PATH)\n    size_kb = f.stat().st_size / 1024\n    print(f\"  {rel_path} ({size_kb:.1f} KB)\")\nif len(all_npy_files) &gt; 8:\n    print(f\"  ... and {len(all_npy_files) - 8} more files\")\n</pre> # Find all .npy files for a specific mouse using filesystem tools # This works without any database query!  store_path = Path(STORE_PATH) all_npy_files = list(store_path.rglob(\"*.npy\"))  print(f\"All .npy files in store ({len(all_npy_files)} total):\\n\") for f in sorted(all_npy_files)[:8]:     rel_path = f.relative_to(STORE_PATH)     size_kb = f.stat().st_size / 1024     print(f\"  {rel_path} ({size_kb:.1f} KB)\") if len(all_npy_files) &gt; 8:     print(f\"  ... and {len(all_npy_files) - 8} more files\") <pre>All .npy files in store (25 total):\n\n  mouse_id=0/session_date=2017-05-15/neuron_id=0/tutorial_electrophysiology_npy/__spikes/spike_param_id=1/spike_times_BXKKFICZ.npy (0.3 KB)\n  mouse_id=0/session_date=2017-05-15/neuron_id=0/tutorial_electrophysiology_npy/__spikes/spike_param_id=1/waveforms_kUwdzj78.npy (15.1 KB)\n  mouse_id=0/session_date=2017-05-15/neuron_id=0/tutorial_electrophysiology_npy/__spikes/spike_param_id=2/spike_times_QW40D7lg.npy (0.3 KB)\n  mouse_id=0/session_date=2017-05-15/neuron_id=0/tutorial_electrophysiology_npy/__spikes/spike_param_id=2/waveforms_szEnIhMx.npy (15.8 KB)\n  mouse_id=0/session_date=2017-05-15/neuron_id=0/tutorial_electrophysiology_npy/_neuron/activity_ezivz3DF.npy (7.9 KB)\n  mouse_id=0/session_date=2017-05-19/neuron_id=0/tutorial_electrophysiology_npy/__spikes/spike_param_id=1/spike_times_cmgrNWWT.npy (0.3 KB)\n  mouse_id=0/session_date=2017-05-19/neuron_id=0/tutorial_electrophysiology_npy/__spikes/spike_param_id=1/waveforms_7qeTl_Dp.npy (14.5 KB)\n  mouse_id=0/session_date=2017-05-19/neuron_id=0/tutorial_electrophysiology_npy/__spikes/spike_param_id=2/spike_times_BEf94y5U.npy (0.3 KB)\n  ... and 17 more files\n</pre> In\u00a0[26]: Copied! <pre># Cleanup: drop schema and remove temporary store\nschema.drop(prompt=False)\nimport shutil\nshutil.rmtree(STORE_PATH, ignore_errors=True)\n</pre> # Cleanup: drop schema and remove temporary store schema.drop(prompt=False) import shutil shutil.rmtree(STORE_PATH, ignore_errors=True)"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#electrophysiology-pipeline-with-object-storage","title":"Electrophysiology Pipeline with Object Storage\u00b6","text":"<p>This tutorial builds an electrophysiology analysis pipeline using DataJoint with the <code>&lt;npy@&gt;</code> codec for efficient array storage. You'll learn to:</p> <ul> <li>Configure object storage for neural data</li> <li>Import neural recordings using <code>&lt;npy@&gt;</code> (lazy loading)</li> <li>Compute activity statistics</li> <li>Detect spikes using parameterized thresholds</li> <li>Extract waveforms as stacked arrays</li> <li>Use memory mapping for efficient random access</li> <li>Access files directly without database queries</li> </ul>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#the-pipeline","title":"The Pipeline\u00b6","text":"<p>Legend: Green = Manual, Gray = Lookup, Blue = Imported, Red = Computed</p> <p>Each session records from neurons. We compute statistics, detect spikes with configurable thresholds, and extract spike waveforms.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#why-npy-instead-of-blob","title":"Why <code>&lt;npy@&gt;</code> Instead of <code>&lt;blob@&gt;</code>?\u00b6","text":"Feature <code>&lt;npy@&gt;</code> <code>&lt;blob@&gt;</code> Lazy loading Yes - inspect shape/dtype without download No - always downloads Memory mapping Yes - random access via <code>mmap_mode</code> No Format Portable <code>.npy</code> files DataJoint serialization Bulk fetch Safe - returns references Downloads everything Direct access Yes - navigable file paths No - hash-addressed"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#setup","title":"Setup\u00b6","text":"<p>First, configure object storage for the <code>&lt;npy@&gt;</code> codec.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#partitioning-by-subject-session-and-neuron","title":"Partitioning by Subject, Session, and Neuron\u00b6","text":"<p>We've configured the store with <code>partition_pattern: '{mouse_id}/{session_date}/{neuron_id}'</code>. This organizes storage by the complete experimental hierarchy\u2014grouping all data for each individual neuron together at the top of the directory structure.</p> <p>Without partitioning:</p> <pre><code>{store}/{schema}/{table}/{mouse_id=X}/{session_date=Y}/{neuron_id=Z}/file.npy\n</code></pre> <p>With partitioning:</p> <pre><code>{store}/{mouse_id=X}/{session_date=Y}/{neuron_id=Z}/{schema}/{table}/file.npy\n</code></pre> <p>Partitioning moves the specified primary key attributes to the front of the path, making it easy to:</p> <ul> <li>Browse by experimental hierarchy - Navigate: subject \u2192 session \u2192 neuron</li> <li>Selective sync - Copy all data for one neuron: <code>rsync mouse_id=100/session_date=2017-05-25/neuron_id=0/ backup/</code></li> <li>Efficient queries - Filesystem can quickly locate specific neurons</li> <li>Publication-ready - Export complete hierarchies to data repositories</li> </ul> <p>The remaining primary key attributes (like <code>spike_param_id</code> in the Spikes table) stay in their normal position after the schema/table path.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#manual-tables-experiment-metadata","title":"Manual Tables: Experiment Metadata\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#insert-sample-data","title":"Insert Sample Data\u00b6","text":"<p>Our data files follow the naming convention <code>data_{mouse_id}_{session_date}.npy</code>.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#imported-table-neuron-activity-with-npy","title":"Imported Table: Neuron Activity with <code>&lt;npy@&gt;</code>\u00b6","text":"<p>Each data file contains recordings from one or more neurons. We import each neuron's activity trace using <code>&lt;npy@ephys&gt;</code> for schema-addressed object storage.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#lazy-loading-with-npyref","title":"Lazy Loading with NpyRef\u00b6","text":"<p>When fetching <code>&lt;npy@&gt;</code> attributes, you get an <code>NpyRef</code> that provides metadata without downloading the array.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#memory-mapping-for-large-arrays","title":"Memory Mapping for Large Arrays\u00b6","text":"<p>For very large arrays, use <code>mmap_mode</code> to access data without loading it all into memory. This is especially efficient for local filesystem stores.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#visualize-neural-activity","title":"Visualize Neural Activity\u00b6","text":"<p>NpyRef works transparently with NumPy functions via <code>__array__</code>.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#computed-table-activity-statistics","title":"Computed Table: Activity Statistics\u00b6","text":"<p>For each neuron, compute basic statistics of the activity trace. NumPy functions work directly with NpyRef.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#lookup-table-spike-detection-parameters","title":"Lookup Table: Spike Detection Parameters\u00b6","text":"<p>Spike detection depends on threshold choice. Using a Lookup table, we can run detection with multiple thresholds and compare results.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#computed-table-spike-detection","title":"Computed Table: Spike Detection\u00b6","text":"<p>Detect spikes by finding threshold crossings. Store spike times and all waveforms as <code>&lt;npy@&gt;</code> arrays.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#compare-detection-thresholds","title":"Compare Detection Thresholds\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#average-waveform","title":"Average Waveform\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#bulk-fetch-with-lazy-loading","title":"Bulk Fetch with Lazy Loading\u00b6","text":"<p>Fetching many rows returns NpyRefs - inspect metadata before downloading.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#pipeline-diagram","title":"Pipeline Diagram\u00b6","text":""},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#querying-results","title":"Querying Results\u00b6","text":"<p>DataJoint makes it easy to query across the pipeline.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#summary","title":"Summary\u00b6","text":"<p>This pipeline demonstrates key patterns for electrophysiology analysis with object storage:</p> Concept Example Purpose Object storage <code>&lt;npy@ephys&gt;</code> Store arrays in file/S3/MinIO Lazy loading <code>NpyRef</code> Inspect shape/dtype without download Memory mapping <code>ref.load(mmap_mode='r')</code> Random access to large arrays Imported tables <code>Neuron</code> Load data from files Computed tables <code>ActivityStats</code>, <code>Spikes</code> Derive results Lookup tables <code>SpikeParams</code> Parameterize analysis Array attributes <code>waveforms</code> Store multi-spike data as single array"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#key-benefits-of-npy","title":"Key Benefits of <code>&lt;npy@&gt;</code>\u00b6","text":"<ol> <li>Lazy loading: Inspect array metadata without downloading</li> <li>Memory mapping: Random access to large arrays via <code>mmap_mode</code></li> <li>Safe bulk fetch: Fetching 1000 rows doesn't download 1000 arrays</li> <li>NumPy integration: <code>np.mean(ref)</code> auto-downloads via <code>__array__</code></li> <li>Portable format: <code>.npy</code> files readable by NumPy, MATLAB, etc.</li> <li>Schema-addressed: Files organized by schema/table/key</li> <li>Direct access: Navigate and load files without database queries</li> </ol>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#direct-file-access-navigating-the-store","title":"Direct File Access: Navigating the Store\u00b6","text":"<p>A key advantage of <code>&lt;npy@&gt;</code> is schema-addressed storage: files are organized in a predictable directory structure that mirrors your database schema. This means you can navigate and access data files directly\u2014without querying the database.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#store-directory-structure-with-partitioning","title":"Store Directory Structure with Partitioning\u00b6","text":"<p>This tutorial uses <code>partition_pattern: '{mouse_id}/{session_date}/{neuron_id}'</code> to organize files by the complete experimental hierarchy:</p> <pre><code>{store}/{mouse_id=X}/{session_date=Y}/{neuron_id=Z}/{schema}/{table}/{remaining_key}/{file}.npy\n</code></pre> <p>Without partitioning, the structure would be:</p> <pre><code>{store}/{schema}/{table}/{mouse_id=X}/{session_date=Y}/{neuron_id=Z}/{remaining_key}/{file}.npy\n</code></pre> <p>Partitioning moves the experimental hierarchy to the top of the path, creating a browsable structure that matches how you think about your data:</p> <ol> <li>Navigate to a subject (mouse_id=100)</li> <li>Navigate to a session (session_date=2017-05-25)</li> <li>Navigate to a neuron (neuron_id=0)</li> <li>See all data for that neuron organized by table</li> </ol> <p>This structure enables:</p> <ul> <li>Direct file access for external tools (MATLAB, Julia, shell scripts)</li> <li>Browsable data organized by subject/session/neuron</li> <li>Selective backup/sync - Copy entire subjects, sessions, or individual neurons</li> <li>Debugging by inspecting raw files in the experimental hierarchy</li> </ul>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#use-cases-for-direct-access","title":"Use Cases for Direct Access\u00b6","text":"<p>External tools: Load data in MATLAB, Julia, or R without DataJoint:</p> <pre>% MATLAB - use the path from NpyRef or navigate the store\nactivity = readNPY('store/schema/table/key_hash/activity.npy');\n</pre> <p>Shell scripting: Process files with command-line tools:</p> <pre># List all .npy files in store\nfind $STORE -name \"*.npy\"\n\n# Backup the entire store\nrsync -av $STORE/ backup/\n</pre> <p>Disaster recovery: If the database is lost, the store contains all array data in standard <code>.npy</code> format. The path structure and JSON metadata in the database can help reconstruct mappings.</p>"},{"location":"tutorials/domain/electrophysiology/ephys-with-npy/#publishing-to-data-repositories","title":"Publishing to Data Repositories\u00b6","text":"<p>Many scientific data repositories accept structured file-folder hierarchies\u2014exactly what <code>&lt;npy@&gt;</code> provides. The schema-addressed storage format makes your data publication-ready:</p> Repository Accepted Formats Schema-Addressed Benefit DANDI NWB, folders Export subject/session hierarchy OpenNeuro BIDS folders Map to BIDS-like structure Figshare Any files/folders Upload store directly Zenodo Any files/folders Archive with DOI OSF Any files/folders Version-controlled sharing <p>Export for publication:</p> <pre># Export one subject's data for publication\nsubject_dir = Path(STORE_PATH) / \"schema\" / \"table\" / \"objects\" / \"mouse_id=100\"\n\n# Copy to publication directory\nimport shutil\nshutil.copytree(subject_dir, \"publication_data/mouse_100/\")\n\n# The resulting structure is self-documenting:\n# publication_data/\n#   mouse_100/\n#     session_date=2017-05-25/\n#       neuron_id=0/\n#         activity_xyz.npy\n#         spike_times_abc.npy\n#         waveforms_def.npy\n</pre> <p>Key advantages for publishing:</p> <ol> <li>Self-documenting paths: Primary key values are in folder names\u2014no lookup table needed</li> <li>Standard format: <code>.npy</code> files are readable by NumPy, MATLAB, Julia, R, and most analysis tools</li> <li>Selective export: Copy only specific subjects, sessions, or tables</li> <li>Reproducibility: Published data has the same structure as your working pipeline</li> <li>Metadata preservation: Path encodes experimental metadata (subject, session, parameters)</li> </ol>"},{"location":"tutorials/examples/blob-detection/","title":"Blob Detection Pipeline","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.feature import blob_doh\nfrom skimage.color import rgb2gray\n\n# Create a schema - this is our database namespace\nschema = dj.Schema('tutorial_blobs')\n</pre> import datajoint as dj import matplotlib.pyplot as plt from skimage import data from skimage.feature import blob_doh from skimage.color import rgb2gray  # Create a schema - this is our database namespace schema = dj.Schema('tutorial_blobs') <pre>[2026-01-27 15:30:07] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Image(dj.Manual):\n    definition = \"\"\"\n    # Images for blob detection\n    image_id : int16\n    ---\n    image_name : varchar(100)\n    image : &lt;blob&gt;              # serialized numpy array\n    \"\"\"\n</pre> @schema class Image(dj.Manual):     definition = \"\"\"     # Images for blob detection     image_id : int16     ---     image_name : varchar(100)     image :               # serialized numpy array     \"\"\" <p>Now let's insert two sample images from scikit-image:</p> In\u00a0[3]: Copied! <pre># Insert sample images\nImage.insert([\n    {'image_id': 1, 'image_name': 'Hubble Deep Field', \n     'image': rgb2gray(data.hubble_deep_field())},\n    {'image_id': 2, 'image_name': 'Human Mitosis', \n     'image': data.human_mitosis() / 255.0},\n], skip_duplicates=True)\n\nImage()\n</pre> # Insert sample images Image.insert([     {'image_id': 1, 'image_name': 'Hubble Deep Field',       'image': rgb2gray(data.hubble_deep_field())},     {'image_id': 2, 'image_name': 'Human Mitosis',       'image': data.human_mitosis() / 255.0}, ], skip_duplicates=True)  Image() Out[3]: Images for blob detection <p>image_id</p> <p>image_name</p> <p>image</p> serialized numpy array 1 Hubble Deep Field &lt;blob&gt;2 Human Mitosis &lt;blob&gt; <p>Total: 2</p> In\u00a0[4]: Copied! <pre># Visualize the images\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\nfor ax, row in zip(axes, Image()):\n    ax.imshow(row['image'], cmap='gray_r')\n    ax.set_title(row['image_name'])\n    ax.axis('off')\nplt.tight_layout()\n</pre> # Visualize the images fig, axes = plt.subplots(1, 2, figsize=(10, 5)) for ax, row in zip(axes, Image()):     ax.imshow(row['image'], cmap='gray_r')     ax.set_title(row['image_name'])     ax.axis('off') plt.tight_layout() In\u00a0[5]: Copied! <pre>@schema\nclass DetectionParams(dj.Lookup):\n    definition = \"\"\"\n    # Blob detection parameter sets\n    params_id : int16\n    ---\n    min_sigma : float32         # minimum blob size\n    max_sigma : float32         # maximum blob size  \n    threshold : float32         # detection sensitivity\n    \"\"\"\n    \n    # Pre-populate with parameter sets to try\n    contents = [\n        {'params_id': 1, 'min_sigma': 2.0, 'max_sigma': 6.0, 'threshold': 0.001},\n        {'params_id': 2, 'min_sigma': 3.0, 'max_sigma': 8.0, 'threshold': 0.002},\n        {'params_id': 3, 'min_sigma': 4.0, 'max_sigma': 20.0, 'threshold': 0.01},\n    ]\n\nDetectionParams()\n</pre> @schema class DetectionParams(dj.Lookup):     definition = \"\"\"     # Blob detection parameter sets     params_id : int16     ---     min_sigma : float32         # minimum blob size     max_sigma : float32         # maximum blob size       threshold : float32         # detection sensitivity     \"\"\"          # Pre-populate with parameter sets to try     contents = [         {'params_id': 1, 'min_sigma': 2.0, 'max_sigma': 6.0, 'threshold': 0.001},         {'params_id': 2, 'min_sigma': 3.0, 'max_sigma': 8.0, 'threshold': 0.002},         {'params_id': 3, 'min_sigma': 4.0, 'max_sigma': 20.0, 'threshold': 0.01},     ]  DetectionParams() Out[5]: Blob detection parameter sets <p>params_id</p> <p>min_sigma</p> minimum blob size <p>max_sigma</p> maximum blob size <p>threshold</p> detection sensitivity 1 2.0 6.0 0.0012 3.0 8.0 0.0023 4.0 20.0 0.01 <p>Total: 3</p> In\u00a0[6]: Copied! <pre>@schema\nclass Detection(dj.Computed):\n    definition = \"\"\"\n    # Blob detection results\n    -&gt; Image                    # depends on Image\n    -&gt; DetectionParams          # depends on DetectionParams\n    ---\n    num_blobs : int32          # number of blobs detected\n    \"\"\"\n    \n    class Blob(dj.Part):\n        definition = \"\"\"\n        # Individual detected blobs\n        -&gt; master\n        blob_idx : int32\n        ---\n        x : float32             # x coordinate\n        y : float32             # y coordinate  \n        radius : float32        # blob radius\n        \"\"\"\n    \n    def make(self, key):\n        # Fetch the image and parameters\n        img = (Image &amp; key).fetch1('image')\n        params = (DetectionParams &amp; key).fetch1()\n        \n        # Run blob detection\n        blobs = blob_doh(\n            img,\n            min_sigma=params['min_sigma'],\n            max_sigma=params['max_sigma'],\n            threshold=params['threshold']\n        )\n        \n        # Insert master row\n        self.insert1({**key, 'num_blobs': len(blobs)})\n        \n        # Insert part rows (all blobs for this detection)\n        self.Blob.insert([\n            {**key, 'blob_idx': i, 'x': x, 'y': y, 'radius': r}\n            for i, (x, y, r) in enumerate(blobs)\n        ])\n</pre> @schema class Detection(dj.Computed):     definition = \"\"\"     # Blob detection results     -&gt; Image                    # depends on Image     -&gt; DetectionParams          # depends on DetectionParams     ---     num_blobs : int32          # number of blobs detected     \"\"\"          class Blob(dj.Part):         definition = \"\"\"         # Individual detected blobs         -&gt; master         blob_idx : int32         ---         x : float32             # x coordinate         y : float32             # y coordinate           radius : float32        # blob radius         \"\"\"          def make(self, key):         # Fetch the image and parameters         img = (Image &amp; key).fetch1('image')         params = (DetectionParams &amp; key).fetch1()                  # Run blob detection         blobs = blob_doh(             img,             min_sigma=params['min_sigma'],             max_sigma=params['max_sigma'],             threshold=params['threshold']         )                  # Insert master row         self.insert1({**key, 'num_blobs': len(blobs)})                  # Insert part rows (all blobs for this detection)         self.Blob.insert([             {**key, 'blob_idx': i, 'x': x, 'y': y, 'radius': r}             for i, (x, y, r) in enumerate(blobs)         ]) In\u00a0[7]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[7]: <p>The diagram shows:</p> <ul> <li>Green = Manual tables (user-entered data)</li> <li>Gray = Lookup tables (reference data)</li> <li>Red = Computed tables (derived data)</li> <li>Edges = Dependencies (foreign keys), always flow top-to-bottom</li> </ul> In\u00a0[8]: Copied! <pre># Run all pending computations\nDetection.populate(display_progress=True)\n</pre> # Run all pending computations Detection.populate(display_progress=True) <pre>\rDetection:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>\rDetection:  17%|\u2588\u258b        | 1/6 [00:00&lt;00:03,  1.27it/s]</pre> <pre>\rDetection:  33%|\u2588\u2588\u2588\u258e      | 2/6 [00:01&lt;00:01,  2.15it/s]</pre> <pre>\rDetection:  50%|\u2588\u2588\u2588\u2588\u2588     | 3/6 [00:01&lt;00:01,  2.17it/s]</pre> <pre>\rDetection:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 4/6 [00:01&lt;00:00,  2.85it/s]</pre> <pre>\rDetection:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 5/6 [00:01&lt;00:00,  2.91it/s]</pre> <pre>\rDetection: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:02&lt;00:00,  3.55it/s]</pre> <pre>\rDetection: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:02&lt;00:00,  2.78it/s]</pre> <pre>\n</pre> Out[8]: <pre>{'success_count': 6, 'error_list': []}</pre> In\u00a0[9]: Copied! <pre># View results summary\nDetection()\n</pre> # View results summary Detection() Out[9]: Blob detection results <p>image_id</p> None <p>params_id</p> None <p>num_blobs</p> number of blobs detected 1 1 19211 2 9711 3 2292 1 3642 2 2322 3 11 <p>Total: 6</p> <p>We computed 6 results: 2 images \u00d7 3 parameter sets. Each shows how many blobs were detected.</p> In\u00a0[10]: Copied! <pre>fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n\nfor ax, key in zip(axes.ravel(),\n                   Detection.keys(order_by='image_id, params_id')):\n    # Get image and detection info in one fetch\n    name, img, num_blobs = (Detection * Image &amp; key).fetch1(\n        'image_name', 'image', 'num_blobs')\n    \n    ax.imshow(img, cmap='gray_r')\n    \n    # Get all blob coordinates in one query\n    x, y, r = (Detection.Blob &amp; key).to_arrays('x', 'y', 'radius')\n    for xi, yi, ri in zip(x, y, r):\n        circle = plt.Circle((yi, xi), ri * 1.2,\n                            color='red', fill=False, alpha=0.6)\n        ax.add_patch(circle)\n    \n    ax.set_title(f\"{name}\\nParams {key['params_id']}: {num_blobs} blobs\",\n                 fontsize=10)\n    ax.axis('off')\n\nplt.tight_layout()\n</pre> fig, axes = plt.subplots(2, 3, figsize=(12, 8))  for ax, key in zip(axes.ravel(),                    Detection.keys(order_by='image_id, params_id')):     # Get image and detection info in one fetch     name, img, num_blobs = (Detection * Image &amp; key).fetch1(         'image_name', 'image', 'num_blobs')          ax.imshow(img, cmap='gray_r')          # Get all blob coordinates in one query     x, y, r = (Detection.Blob &amp; key).to_arrays('x', 'y', 'radius')     for xi, yi, ri in zip(x, y, r):         circle = plt.Circle((yi, xi), ri * 1.2,                             color='red', fill=False, alpha=0.6)         ax.add_patch(circle)          ax.set_title(f\"{name}\\nParams {key['params_id']}: {num_blobs} blobs\",                  fontsize=10)     ax.axis('off')  plt.tight_layout() In\u00a0[11]: Copied! <pre># Find detections with fewer than 300 blobs\nDetection &amp; 'num_blobs &lt; 300'\n</pre> # Find detections with fewer than 300 blobs Detection &amp; 'num_blobs &lt; 300' Out[11]: Blob detection results <p>image_id</p> None <p>params_id</p> None <p>num_blobs</p> number of blobs detected 1 3 2292 2 2322 3 11 <p>Total: 3</p> In\u00a0[12]: Copied! <pre># Join to see image names with blob counts\n(Image * Detection).proj('image_name', 'num_blobs')\n</pre> # Join to see image names with blob counts (Image * Detection).proj('image_name', 'num_blobs') Out[12]: <p>image_id</p> None <p>params_id</p> None <p>num_blobs</p> number of blobs detected <p>image_name</p> 1 1 1921 Hubble Deep Field1 2 971 Hubble Deep Field1 3 229 Hubble Deep Field2 1 364 Human Mitosis2 2 232 Human Mitosis2 3 11 Human Mitosis <p>Total: 6</p> In\u00a0[13]: Copied! <pre>@schema\nclass SelectedDetection(dj.Manual):\n    definition = \"\"\"\n    # Best detection for each image\n    -&gt; Image\n    ---\n    -&gt; Detection\n    \"\"\"\n\n# Select params 3 for Hubble (fewer, larger blobs)\n# Select params 1 for Mitosis (many small spots)\nSelectedDetection.insert([\n    {'image_id': 1, 'params_id': 3},\n    {'image_id': 2, 'params_id': 1},\n], skip_duplicates=True)\n\nSelectedDetection()\n</pre> @schema class SelectedDetection(dj.Manual):     definition = \"\"\"     # Best detection for each image     -&gt; Image     ---     -&gt; Detection     \"\"\"  # Select params 3 for Hubble (fewer, larger blobs) # Select params 1 for Mitosis (many small spots) SelectedDetection.insert([     {'image_id': 1, 'params_id': 3},     {'image_id': 2, 'params_id': 1}, ], skip_duplicates=True)  SelectedDetection() Out[13]: Best detection for each image <p>image_id</p> None <p>params_id</p> None 1 32 1 <p>Total: 2</p> In\u00a0[14]: Copied! <pre># View the final schema with selections\ndj.Diagram(schema)\n</pre> # View the final schema with selections dj.Diagram(schema) Out[14]: In\u00a0[15]: Copied! <pre># Cleanup: drop the schema for re-running the tutorial\nschema.drop(prompt=False)\n</pre> # Cleanup: drop the schema for re-running the tutorial schema.drop(prompt=False)"},{"location":"tutorials/examples/blob-detection/#blob-detection-pipeline","title":"Blob Detection Pipeline\u00b6","text":"<p>This tutorial introduces DataJoint through a real image analysis pipeline that detects bright blobs in astronomical and biological images. By the end, you'll understand:</p> <ul> <li>Schemas \u2014 Namespaces that group related tables</li> <li>Table types \u2014 Manual, Lookup, and Computed tables</li> <li>Dependencies \u2014 How tables relate through foreign keys</li> <li>Computation \u2014 Automatic population of derived data</li> <li>Master-Part \u2014 Atomic insertion of hierarchical results</li> </ul>"},{"location":"tutorials/examples/blob-detection/#the-problem","title":"The Problem\u00b6","text":"<p>We have images and want to detect bright spots (blobs) in them. Different detection parameters work better for different images, so we need to:</p> <ol> <li>Store our images</li> <li>Define parameter sets to try</li> <li>Run detection for each image \u00d7 parameter combination</li> <li>Store and visualize results</li> <li>Select the best parameters for each image</li> </ol> <p>This is a computational workflow \u2014 a series of steps where each step depends on previous results. DataJoint makes these workflows reproducible and manageable.</p>"},{"location":"tutorials/examples/blob-detection/#setup","title":"Setup\u00b6","text":"<p>First, let's import our tools and create a schema (database namespace) for this project.</p>"},{"location":"tutorials/examples/blob-detection/#manual-tables-storing-raw-data","title":"Manual Tables: Storing Raw Data\u00b6","text":"<p>A Manual table stores data that users enter directly \u2014 it's the starting point of your pipeline. Here we define an <code>Image</code> table to store our sample images.</p> <p>The <code>definition</code> string specifies:</p> <ul> <li>Primary key (above <code>---</code>): attributes that uniquely identify each row</li> <li>Secondary attributes (below <code>---</code>): additional data for each row</li> </ul>"},{"location":"tutorials/examples/blob-detection/#lookup-tables-parameter-sets","title":"Lookup Tables: Parameter Sets\u00b6","text":"<p>A Lookup table stores reference data that doesn't change often \u2014 things like experimental protocols, parameter configurations, or categorical options.</p> <p>For blob detection, we'll try different parameter combinations to find what works best for each image type.</p>"},{"location":"tutorials/examples/blob-detection/#computed-tables-automatic-processing","title":"Computed Tables: Automatic Processing\u00b6","text":"<p>A Computed table automatically derives data from other tables. You define:</p> <ol> <li>Dependencies (using <code>-&gt;</code>) \u2014 which tables provide input</li> <li><code>make()</code> method \u2014 how to compute results for one input combination</li> </ol> <p>DataJoint then handles:</p> <ul> <li>Determining what needs to be computed</li> <li>Running computations (optionally in parallel)</li> <li>Tracking what's done vs. pending</li> </ul>"},{"location":"tutorials/examples/blob-detection/#master-part-structure","title":"Master-Part Structure\u00b6","text":"<p>Our detection produces multiple blobs per image. We use a master-part structure:</p> <ul> <li>Master (<code>Detection</code>): One row per job, stores summary (blob count)</li> <li>Part (<code>Detection.Blob</code>): One row per blob, stores details (x, y, radius)</li> </ul> <p>Both are inserted atomically \u2014 if anything fails, the whole transaction rolls back.</p>"},{"location":"tutorials/examples/blob-detection/#viewing-the-schema","title":"Viewing the Schema\u00b6","text":"<p>DataJoint can visualize the relationships between tables:</p>"},{"location":"tutorials/examples/blob-detection/#running-the-pipeline","title":"Running the Pipeline\u00b6","text":"<p>Call <code>populate()</code> to run all pending computations. DataJoint automatically determines what needs to be computed: every combination of <code>Image</code> \u00d7 <code>DetectionParams</code> that doesn't already have a <code>Detection</code> result.</p>"},{"location":"tutorials/examples/blob-detection/#visualizing-results","title":"Visualizing Results\u00b6","text":"<p>Let's see how different parameters affect detection:</p>"},{"location":"tutorials/examples/blob-detection/#querying-results","title":"Querying Results\u00b6","text":"<p>DataJoint's query language makes it easy to explore results:</p>"},{"location":"tutorials/examples/blob-detection/#storing-selections","title":"Storing Selections\u00b6","text":"<p>After reviewing the results, we can record which parameter set works best for each image. This is another Manual table that references our computed results:</p>"},{"location":"tutorials/examples/blob-detection/#key-concepts-recap","title":"Key Concepts Recap\u00b6","text":"Concept What It Does Example Schema Groups related tables <code>schema = dj.Schema('tutorial_blobs')</code> Manual Table Stores user-entered data <code>Image</code>, <code>SelectedDetection</code> Lookup Table Stores reference/config data <code>DetectionParams</code> Computed Table Derives data automatically <code>Detection</code> Part Table Stores detailed results with master <code>Detection.Blob</code> Foreign Key (<code>-&gt;</code>) Creates dependency <code>-&gt; Image</code> <code>populate()</code> Runs pending computations <code>Detection.populate()</code> Restriction (<code>&amp;</code>) Filters rows <code>Detection &amp; 'num_blobs &lt; 300'</code> Join (<code>*</code>) Combines tables <code>Image * Detection</code>"},{"location":"tutorials/examples/blob-detection/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Schema Design \u2014 Learn table types and relationships in depth</li> <li>Queries \u2014 Master DataJoint's query operators</li> <li>Computation \u2014 Build complex computational workflows</li> </ul>"},{"location":"tutorials/examples/fractal-pipeline/","title":"Fractal Image Pipeline","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nschema = dj.Schema('tutorial_fractal')\n</pre> import datajoint as dj import numpy as np from matplotlib import pyplot as plt  schema = dj.Schema('tutorial_fractal') <pre>[2026-01-27 15:30:21] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>def julia(c, size=256, center=(0.0, 0.0), zoom=1.0, iters=256):\n    \"\"\"Generate a Julia set image.\"\"\"\n    x, y = np.meshgrid(\n        np.linspace(-1, 1, size) / zoom + center[0],\n        np.linspace(-1, 1, size) / zoom + center[1]\n    )\n    z = x + 1j * y\n    img = np.zeros(z.shape)\n    mask = np.ones(z.shape, dtype=bool)\n    for _ in range(iters):\n        z[mask] = z[mask] ** 2 + c\n        mask = np.abs(z) &lt; 2\n        img += mask\n    return img\n</pre> def julia(c, size=256, center=(0.0, 0.0), zoom=1.0, iters=256):     \"\"\"Generate a Julia set image.\"\"\"     x, y = np.meshgrid(         np.linspace(-1, 1, size) / zoom + center[0],         np.linspace(-1, 1, size) / zoom + center[1]     )     z = x + 1j * y     img = np.zeros(z.shape)     mask = np.ones(z.shape, dtype=bool)     for _ in range(iters):         z[mask] = z[mask] ** 2 + c         mask = np.abs(z) &lt; 2         img += mask     return img In\u00a0[3]: Copied! <pre># Example fractal\nplt.imshow(julia(-0.4 + 0.6j), cmap='magma')\nplt.axis('off');\n</pre> # Example fractal plt.imshow(julia(-0.4 + 0.6j), cmap='magma') plt.axis('off'); In\u00a0[4]: Copied! <pre>@schema\nclass JuliaSpec(dj.Manual):\n    \"\"\"Parameters for generating Julia fractals.\"\"\"\n    definition = \"\"\"\n    spec_id : int16\n    ---\n    c_real : float64          # Real part of c\n    c_imag : float64          # Imaginary part of c  \n    noise_level = 50 : float64\n    \"\"\"\n</pre> @schema class JuliaSpec(dj.Manual):     \"\"\"Parameters for generating Julia fractals.\"\"\"     definition = \"\"\"     spec_id : int16     ---     c_real : float64          # Real part of c     c_imag : float64          # Imaginary part of c       noise_level = 50 : float64     \"\"\" In\u00a0[5]: Copied! <pre>@schema\nclass JuliaImage(dj.Computed):\n    \"\"\"Generated fractal images with noise.\"\"\"\n    definition = \"\"\"\n    -&gt; JuliaSpec\n    ---\n    image : &lt;blob&gt;            # Generated fractal image\n    \"\"\"\n    \n    def make(self, key):\n        spec = (JuliaSpec &amp; key).fetch1()\n        img = julia(spec['c_real'] + 1j * spec['c_imag'])\n        img += np.random.randn(*img.shape) * spec['noise_level']\n        self.insert1({**key, 'image': img.astype(np.float32)})\n</pre> @schema class JuliaImage(dj.Computed):     \"\"\"Generated fractal images with noise.\"\"\"     definition = \"\"\"     -&gt; JuliaSpec     ---     image :             # Generated fractal image     \"\"\"          def make(self, key):         spec = (JuliaSpec &amp; key).fetch1()         img = julia(spec['c_real'] + 1j * spec['c_imag'])         img += np.random.randn(*img.shape) * spec['noise_level']         self.insert1({**key, 'image': img.astype(np.float32)}) In\u00a0[6]: Copied! <pre>from skimage import filters, restoration\nfrom skimage.morphology import disk\n\n@schema\nclass DenoiseMethod(dj.Lookup):\n    \"\"\"Image denoising algorithms.\"\"\"\n    definition = \"\"\"\n    method_id : int16\n    ---\n    method_name : varchar(20)\n    params : &lt;blob&gt;\n    \"\"\"\n    contents = [\n        [0, 'gaussian', {'sigma': 1.8}],\n        [1, 'median', {'radius': 3}],\n        [2, 'tv', {'weight': 20.0}],\n    ]\n</pre> from skimage import filters, restoration from skimage.morphology import disk  @schema class DenoiseMethod(dj.Lookup):     \"\"\"Image denoising algorithms.\"\"\"     definition = \"\"\"     method_id : int16     ---     method_name : varchar(20)     params :      \"\"\"     contents = [         [0, 'gaussian', {'sigma': 1.8}],         [1, 'median', {'radius': 3}],         [2, 'tv', {'weight': 20.0}],     ] In\u00a0[7]: Copied! <pre>@schema\nclass Denoised(dj.Computed):\n    \"\"\"Denoised images: each image \u00d7 each method.\"\"\"\n    definition = \"\"\"\n    -&gt; JuliaImage\n    -&gt; DenoiseMethod\n    ---\n    denoised : &lt;blob&gt;\n    \"\"\"\n    \n    def make(self, key):\n        img = (JuliaImage &amp; key).fetch1('image')\n        method, params = (DenoiseMethod &amp; key).fetch1('method_name', 'params')\n        \n        if method == 'gaussian':\n            result = filters.gaussian(img, **params)\n        elif method == 'median':\n            result = filters.median(img, disk(params['radius']))\n        elif method == 'tv':\n            result = restoration.denoise_tv_chambolle(img, **params)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n            \n        self.insert1({**key, 'denoised': result.astype(np.float32)})\n</pre> @schema class Denoised(dj.Computed):     \"\"\"Denoised images: each image \u00d7 each method.\"\"\"     definition = \"\"\"     -&gt; JuliaImage     -&gt; DenoiseMethod     ---     denoised :      \"\"\"          def make(self, key):         img = (JuliaImage &amp; key).fetch1('image')         method, params = (DenoiseMethod &amp; key).fetch1('method_name', 'params')                  if method == 'gaussian':             result = filters.gaussian(img, **params)         elif method == 'median':             result = filters.median(img, disk(params['radius']))         elif method == 'tv':             result = restoration.denoise_tv_chambolle(img, **params)         else:             raise ValueError(f\"Unknown method: {method}\")                      self.insert1({**key, 'denoised': result.astype(np.float32)}) In\u00a0[8]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[8]: In\u00a0[9]: Copied! <pre># Define fractal parameters\nJuliaSpec.insert([\n    {'spec_id': 0, 'c_real': -0.4, 'c_imag': 0.6},\n    {'spec_id': 1, 'c_real': -0.74543, 'c_imag': 0.11301},\n    {'spec_id': 2, 'c_real': -0.1, 'c_imag': 0.651},\n    {'spec_id': 3, 'c_real': -0.835, 'c_imag': -0.2321},\n])\nJuliaSpec()\n</pre> # Define fractal parameters JuliaSpec.insert([     {'spec_id': 0, 'c_real': -0.4, 'c_imag': 0.6},     {'spec_id': 1, 'c_real': -0.74543, 'c_imag': 0.11301},     {'spec_id': 2, 'c_real': -0.1, 'c_imag': 0.651},     {'spec_id': 3, 'c_real': -0.835, 'c_imag': -0.2321}, ]) JuliaSpec() Out[9]: None <p>spec_id</p> <p>c_real</p> Real part of c <p>c_imag</p> Imaginary part of c <p>noise_level</p> 0 -0.4 0.6 50.01 -0.74543 0.11301 50.02 -0.1 0.651 50.03 -0.835 -0.2321 50.0 <p>Total: 4</p> In\u00a0[10]: Copied! <pre># Generate all fractal images\nJuliaImage.populate(display_progress=True)\n</pre> # Generate all fractal images JuliaImage.populate(display_progress=True) <pre>\rJuliaImage:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre> <pre>\rJuliaImage:  25%|\u2588\u2588\u258c       | 1/4 [00:00&lt;00:00,  9.67it/s]</pre> <pre>\rJuliaImage:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:00&lt;00:00, 10.73it/s]</pre> <pre>\rJuliaImage: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00, 11.30it/s]</pre> <pre>\n</pre> Out[10]: <pre>{'success_count': 4, 'error_list': []}</pre> In\u00a0[11]: Copied! <pre># View generated images\nfig, axes = plt.subplots(1, 4, figsize=(12, 3))\nfor ax, row in zip(axes, JuliaImage()):\n    ax.imshow(row['image'], cmap='magma')\n    ax.set_title(f\"spec_id={row['spec_id']}\")\n    ax.axis('off')\nplt.tight_layout()\n</pre> # View generated images fig, axes = plt.subplots(1, 4, figsize=(12, 3)) for ax, row in zip(axes, JuliaImage()):     ax.imshow(row['image'], cmap='magma')     ax.set_title(f\"spec_id={row['spec_id']}\")     ax.axis('off') plt.tight_layout() In\u00a0[12]: Copied! <pre># Apply all denoising methods to all images\nDenoised.populate(display_progress=True)\n</pre> # Apply all denoising methods to all images Denoised.populate(display_progress=True) <pre>\rDenoised:   0%|          | 0/12 [00:00&lt;?, ?it/s]</pre> <pre>\rDenoised:  33%|\u2588\u2588\u2588\u258e      | 4/12 [00:00&lt;00:00, 35.70it/s]</pre> <pre>\rDenoised:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 8/12 [00:00&lt;00:00, 34.68it/s]</pre> <pre>\rDenoised: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:00&lt;00:00, 35.27it/s]</pre> <pre>\rDenoised: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:00&lt;00:00, 35.16it/s]</pre> <pre>\n</pre> Out[12]: <pre>{'success_count': 12, 'error_list': []}</pre> In\u00a0[13]: Copied! <pre># 4 images \u00d7 3 methods = 12 results\nprint(f\"JuliaImage: {len(JuliaImage())} rows\")\nprint(f\"DenoiseMethod: {len(DenoiseMethod())} rows\")\nprint(f\"Denoised: {len(Denoised())} rows\")\n</pre> # 4 images \u00d7 3 methods = 12 results print(f\"JuliaImage: {len(JuliaImage())} rows\") print(f\"DenoiseMethod: {len(DenoiseMethod())} rows\") print(f\"Denoised: {len(Denoised())} rows\") <pre>JuliaImage: 4 rows\nDenoiseMethod: 3 rows\nDenoised: 12 rows\n</pre> In\u00a0[14]: Copied! <pre># Compare denoising methods on one image\nspec_id = 0\noriginal = (JuliaImage &amp; {'spec_id': spec_id}).fetch1('image')\n\nfig, axes = plt.subplots(1, 4, figsize=(14, 3.5))\naxes[0].imshow(original, cmap='magma')\naxes[0].set_title('Original (noisy)')\n\nfor ax, method_id in zip(axes[1:], [0, 1, 2]):\n    result = (Denoised &amp; {'spec_id': spec_id, 'method_id': method_id}).fetch1('denoised')\n    method_name = (DenoiseMethod &amp; {'method_id': method_id}).fetch1('method_name')\n    ax.imshow(result, cmap='magma')\n    ax.set_title(method_name)\n\nfor ax in axes:\n    ax.axis('off')\nplt.tight_layout()\n</pre> # Compare denoising methods on one image spec_id = 0 original = (JuliaImage &amp; {'spec_id': spec_id}).fetch1('image')  fig, axes = plt.subplots(1, 4, figsize=(14, 3.5)) axes[0].imshow(original, cmap='magma') axes[0].set_title('Original (noisy)')  for ax, method_id in zip(axes[1:], [0, 1, 2]):     result = (Denoised &amp; {'spec_id': spec_id, 'method_id': method_id}).fetch1('denoised')     method_name = (DenoiseMethod &amp; {'method_id': method_id}).fetch1('method_name')     ax.imshow(result, cmap='magma')     ax.set_title(method_name)  for ax in axes:     ax.axis('off') plt.tight_layout() In\u00a0[15]: Copied! <pre># Add a new spec \u2014 populate only computes what's missing\nJuliaSpec.insert1({'spec_id': 4, 'c_real': -0.7, 'c_imag': 0.27})\n\nJuliaImage.populate(display_progress=True)  # Only spec_id=4\nDenoised.populate(display_progress=True)    # Only spec_id=4 \u00d7 3 methods\n</pre> # Add a new spec \u2014 populate only computes what's missing JuliaSpec.insert1({'spec_id': 4, 'c_real': -0.7, 'c_imag': 0.27})  JuliaImage.populate(display_progress=True)  # Only spec_id=4 Denoised.populate(display_progress=True)    # Only spec_id=4 \u00d7 3 methods <pre>\rJuliaImage:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rJuliaImage: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 10.53it/s]</pre> <pre>\n</pre> <pre>\rDenoised:   0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>\rDenoised: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 34.60it/s]</pre> <pre>\n</pre> Out[15]: <pre>{'success_count': 3, 'error_list': []}</pre> In\u00a0[16]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/examples/fractal-pipeline/#fractal-image-pipeline","title":"Fractal Image Pipeline\u00b6","text":"<p>This tutorial demonstrates computed tables by building an image processing pipeline for Julia fractals.</p> <p>You'll learn:</p> <ul> <li>Manual tables: Parameters you define (experimental configurations)</li> <li>Lookup tables: Fixed reference data (processing methods)</li> <li>Computed tables: Automatically generated results via <code>populate()</code></li> <li>Many-to-many pipelines: Processing every combination of inputs \u00d7 methods</li> </ul>"},{"location":"tutorials/examples/fractal-pipeline/#julia-set-generator","title":"Julia Set Generator\u00b6","text":"<p>Julia sets are fractals generated by iterating $f(z) = z^2 + c$ for each point in the complex plane. Points that don't escape to infinity form intricate patterns.</p>"},{"location":"tutorials/examples/fractal-pipeline/#pipeline-architecture","title":"Pipeline Architecture\u00b6","text":"<p>We'll build a pipeline with four tables:</p> <ul> <li>JuliaSpec (Manual): Parameters we define for fractal generation</li> <li>JuliaImage (Computed): Generated from specs</li> <li>DenoiseMethod (Lookup): Fixed set of denoising algorithms</li> <li>Denoised (Computed): Each image \u00d7 each method</li> </ul> <p>After defining all tables, we'll visualize the schema with <code>dj.Diagram(schema)</code>.</p>"},{"location":"tutorials/examples/fractal-pipeline/#running-the-pipeline","title":"Running the Pipeline\u00b6","text":"<ol> <li>Insert specs into Manual table</li> <li>Call <code>populate()</code> on Computed tables</li> </ol>"},{"location":"tutorials/examples/fractal-pipeline/#key-points","title":"Key Points\u00b6","text":"Table Type Populated By Use For Manual <code>insert()</code> Experimental parameters, user inputs Lookup <code>contents</code> attribute Fixed reference data, method catalogs Computed <code>populate()</code> Derived results, processed outputs <p>The pipeline automatically:</p> <ul> <li>Tracks dependencies (can't process an image that doesn't exist)</li> <li>Skips already-computed results (idempotent)</li> <li>Computes all combinations when multiple tables converge</li> </ul>"},{"location":"tutorials/examples/hotel-reservations/","title":"Hotel Reservation System","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport datetime\nimport random\n\n# Clean start\nschema = dj.Schema('tutorial_hotel')\nschema.drop(prompt=False)\nschema = dj.Schema('tutorial_hotel')\n</pre> import datajoint as dj import datetime import random  # Clean start schema = dj.Schema('tutorial_hotel') schema.drop(prompt=False) schema = dj.Schema('tutorial_hotel') <pre>[2026-01-27 15:30:29] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Room(dj.Lookup):\n    definition = \"\"\"\n    # Hotel rooms\n    room : int16                    # room number\n    ---\n    room_type : enum('Deluxe', 'Suite')\n    \"\"\"\n    contents = [\n        {'room': i, 'room_type': 'Suite' if i % 5 == 0 else 'Deluxe'}\n        for i in range(1, 21)  # 20 rooms\n    ]\n</pre> @schema class Room(dj.Lookup):     definition = \"\"\"     # Hotel rooms     room : int16                    # room number     ---     room_type : enum('Deluxe', 'Suite')     \"\"\"     contents = [         {'room': i, 'room_type': 'Suite' if i % 5 == 0 else 'Deluxe'}         for i in range(1, 21)  # 20 rooms     ] In\u00a0[3]: Copied! <pre>@schema\nclass RoomAvailable(dj.Manual):\n    definition = \"\"\"\n    # Room availability and pricing by date\n    -&gt; Room\n    date : date\n    ---\n    price : decimal(6, 2)           # price per night\n    \"\"\"\n</pre> @schema class RoomAvailable(dj.Manual):     definition = \"\"\"     # Room availability and pricing by date     -&gt; Room     date : date     ---     price : decimal(6, 2)           # price per night     \"\"\" In\u00a0[4]: Copied! <pre>@schema\nclass Guest(dj.Manual):\n    definition = \"\"\"\n    # Hotel guests\n    guest_id : int64               # auto-assigned guest ID\n    ---\n    guest_name : varchar(60)\n    index(guest_name)\n    \"\"\"\n</pre> @schema class Guest(dj.Manual):     definition = \"\"\"     # Hotel guests     guest_id : int64               # auto-assigned guest ID     ---     guest_name : varchar(60)     index(guest_name)     \"\"\" In\u00a0[5]: Copied! <pre>@schema\nclass Reservation(dj.Manual):\n    definition = \"\"\"\n    # Room reservations (one per room per night)\n    -&gt; RoomAvailable\n    ---\n    -&gt; Guest\n    credit_card : varchar(80)       # encrypted card info\n    \"\"\"\n</pre> @schema class Reservation(dj.Manual):     definition = \"\"\"     # Room reservations (one per room per night)     -&gt; RoomAvailable     ---     -&gt; Guest     credit_card : varchar(80)       # encrypted card info     \"\"\" In\u00a0[6]: Copied! <pre>@schema\nclass CheckIn(dj.Manual):\n    definition = \"\"\"\n    # Check-in records (requires reservation)\n    -&gt; Reservation\n    \"\"\"\n</pre> @schema class CheckIn(dj.Manual):     definition = \"\"\"     # Check-in records (requires reservation)     -&gt; Reservation     \"\"\" In\u00a0[7]: Copied! <pre>@schema\nclass CheckOut(dj.Manual):\n    definition = \"\"\"\n    # Check-out records (requires check-in)\n    -&gt; CheckIn\n    \"\"\"\n</pre> @schema class CheckOut(dj.Manual):     definition = \"\"\"     # Check-out records (requires check-in)     -&gt; CheckIn     \"\"\" In\u00a0[8]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[8]: In\u00a0[9]: Copied! <pre>random.seed(42)\nstart_date = datetime.date.today()\ndays = 30\n\nfor day in range(days):\n    date = start_date + datetime.timedelta(days=day)\n    # Weekend prices are higher\n    is_weekend = date.weekday() &gt;= 5\n    base_price = 200 if is_weekend else 150\n    \n    RoomAvailable.insert(\n        {\n            'room': room['room'],\n            'date': date,\n            'price': base_price + random.randint(-30, 50)\n        }\n        for room in Room.to_dicts()\n    )\n\nprint(f\"Created {len(RoomAvailable())} room-night records\")\nRoomAvailable() &amp; {'room': 1}\n</pre> random.seed(42) start_date = datetime.date.today() days = 30  for day in range(days):     date = start_date + datetime.timedelta(days=day)     # Weekend prices are higher     is_weekend = date.weekday() &gt;= 5     base_price = 200 if is_weekend else 150          RoomAvailable.insert(         {             'room': room['room'],             'date': date,             'price': base_price + random.randint(-30, 50)         }         for room in Room.to_dicts()     )  print(f\"Created {len(RoomAvailable())} room-night records\") RoomAvailable() &amp; {'room': 1} <pre>Created 600 room-night records\n</pre> Out[9]: Room availability and pricing by date <p>room</p> None <p>date</p> <p>price</p> price per night 1 2026-01-27 134.001 2026-01-28 145.001 2026-01-29 164.001 2026-01-30 130.001 2026-01-31 204.001 2026-02-01 187.001 2026-02-02 174.001 2026-02-03 120.001 2026-02-04 122.001 2026-02-05 197.001 2026-02-06 195.001 2026-02-07 230.00 <p>...</p> <p>Total: 30</p> In\u00a0[10]: Copied! <pre># Domain-specific exceptions\nclass HotelError(Exception):\n    pass\n\nclass RoomNotAvailable(HotelError):\n    pass\n\nclass RoomAlreadyReserved(HotelError):\n    pass\n\nclass NoReservation(HotelError):\n    pass\n\nclass NotCheckedIn(HotelError):\n    pass\n\nclass AlreadyProcessed(HotelError):\n    pass\n</pre> # Domain-specific exceptions class HotelError(Exception):     pass  class RoomNotAvailable(HotelError):     pass  class RoomAlreadyReserved(HotelError):     pass  class NoReservation(HotelError):     pass  class NotCheckedIn(HotelError):     pass  class AlreadyProcessed(HotelError):     pass In\u00a0[11]: Copied! <pre>def reserve_room(room, date, guest_name, credit_card):\n    \"\"\"\n    Make a reservation. Creates guest record if needed.\n    \n    Raises\n    ------\n    RoomNotAvailable\n        If room is not available on that date\n    RoomAlreadyReserved\n        If room is already reserved for that date\n    \"\"\"\n    # Find or create guest\n    guests = list((Guest &amp; {'guest_name': guest_name}).keys())\n    if guests:\n        guest_key = guests[0]\n    else:\n        guest_key = {'guest_id': random.randint(1, 2**31)}\n        Guest.insert1({**guest_key, 'guest_name': guest_name})\n    \n    try:\n        Reservation.insert1({\n            'room': room,\n            'date': date,\n            **guest_key,\n            'credit_card': credit_card\n        })\n    except dj.errors.DuplicateError:\n        raise RoomAlreadyReserved(\n            f\"Room {room} already reserved for {date}\") from None\n    except dj.errors.IntegrityError:\n        raise RoomNotAvailable(\n            f\"Room {room} not available on {date}\") from None\n</pre> def reserve_room(room, date, guest_name, credit_card):     \"\"\"     Make a reservation. Creates guest record if needed.          Raises     ------     RoomNotAvailable         If room is not available on that date     RoomAlreadyReserved         If room is already reserved for that date     \"\"\"     # Find or create guest     guests = list((Guest &amp; {'guest_name': guest_name}).keys())     if guests:         guest_key = guests[0]     else:         guest_key = {'guest_id': random.randint(1, 2**31)}         Guest.insert1({**guest_key, 'guest_name': guest_name})          try:         Reservation.insert1({             'room': room,             'date': date,             **guest_key,             'credit_card': credit_card         })     except dj.errors.DuplicateError:         raise RoomAlreadyReserved(             f\"Room {room} already reserved for {date}\") from None     except dj.errors.IntegrityError:         raise RoomNotAvailable(             f\"Room {room} not available on {date}\") from None In\u00a0[12]: Copied! <pre>def check_in(room, date):\n    \"\"\"\n    Check in a guest. Requires existing reservation.\n    \n    Raises\n    ------\n    NoReservation\n        If no reservation exists for this room/date\n    AlreadyProcessed\n        If guest already checked in\n    \"\"\"\n    try:\n        CheckIn.insert1({'room': room, 'date': date})\n    except dj.errors.DuplicateError:\n        raise AlreadyProcessed(\n            f\"Room {room} already checked in for {date}\") from None\n    except dj.errors.IntegrityError:\n        raise NoReservation(\n            f\"No reservation for room {room} on {date}\") from None\n</pre> def check_in(room, date):     \"\"\"     Check in a guest. Requires existing reservation.          Raises     ------     NoReservation         If no reservation exists for this room/date     AlreadyProcessed         If guest already checked in     \"\"\"     try:         CheckIn.insert1({'room': room, 'date': date})     except dj.errors.DuplicateError:         raise AlreadyProcessed(             f\"Room {room} already checked in for {date}\") from None     except dj.errors.IntegrityError:         raise NoReservation(             f\"No reservation for room {room} on {date}\") from None In\u00a0[13]: Copied! <pre>def check_out(room, date):\n    \"\"\"\n    Check out a guest. Requires prior check-in.\n    \n    Raises\n    ------\n    NotCheckedIn\n        If guest hasn't checked in\n    AlreadyProcessed\n        If guest already checked out\n    \"\"\"\n    try:\n        CheckOut.insert1({'room': room, 'date': date})\n    except dj.errors.DuplicateError:\n        raise AlreadyProcessed(\n            f\"Room {room} already checked out for {date}\") from None\n    except dj.errors.IntegrityError:\n        raise NotCheckedIn(\n            f\"Room {room} not checked in for {date}\") from None\n</pre> def check_out(room, date):     \"\"\"     Check out a guest. Requires prior check-in.          Raises     ------     NotCheckedIn         If guest hasn't checked in     AlreadyProcessed         If guest already checked out     \"\"\"     try:         CheckOut.insert1({'room': room, 'date': date})     except dj.errors.DuplicateError:         raise AlreadyProcessed(             f\"Room {room} already checked out for {date}\") from None     except dj.errors.IntegrityError:         raise NotCheckedIn(             f\"Room {room} not checked in for {date}\") from None In\u00a0[14]: Copied! <pre># Successful reservation\ntomorrow = start_date + datetime.timedelta(days=1)\nreserve_room(1, tomorrow, 'Alice Smith', '4111-1111-1111-1111')\nprint(f\"Reserved room 1 for {tomorrow}\")\n\nReservation()\n</pre> # Successful reservation tomorrow = start_date + datetime.timedelta(days=1) reserve_room(1, tomorrow, 'Alice Smith', '4111-1111-1111-1111') print(f\"Reserved room 1 for {tomorrow}\")  Reservation() <pre>Reserved room 1 for 2026-01-28\n</pre> Out[14]: Room reservations (one per room per night) <p>room</p> None <p>date</p> None <p>guest_id</p> None <p>credit_card</p> encrypted card info 1 2026-01-28 1282531583 4111-1111-1111-1111 <p>Total: 1</p> In\u00a0[15]: Copied! <pre># Try to double-book the same room \u2014 fails!\ntry:\n    reserve_room(1, tomorrow, 'Bob Jones', '5555-5555-5555-5555')\nexcept RoomAlreadyReserved as e:\n    print(f\"Blocked: {e}\")\n</pre> # Try to double-book the same room \u2014 fails! try:     reserve_room(1, tomorrow, 'Bob Jones', '5555-5555-5555-5555') except RoomAlreadyReserved as e:     print(f\"Blocked: {e}\") <pre>Blocked: Room 1 already reserved for 2026-01-28\n</pre> In\u00a0[16]: Copied! <pre># Try to reserve unavailable date \u2014 fails!\nfar_future = start_date + datetime.timedelta(days=365)\ntry:\n    reserve_room(1, far_future, 'Carol White', '6666-6666-6666-6666')\nexcept RoomNotAvailable as e:\n    print(f\"Blocked: {e}\")\n</pre> # Try to reserve unavailable date \u2014 fails! far_future = start_date + datetime.timedelta(days=365) try:     reserve_room(1, far_future, 'Carol White', '6666-6666-6666-6666') except RoomNotAvailable as e:     print(f\"Blocked: {e}\") <pre>Blocked: Room 1 not available on 2027-01-27\n</pre> In\u00a0[17]: Copied! <pre># Try to check in without reservation \u2014 fails!\ntry:\n    check_in(2, tomorrow)  # Room 2 has no reservation\nexcept NoReservation as e:\n    print(f\"Blocked: {e}\")\n</pre> # Try to check in without reservation \u2014 fails! try:     check_in(2, tomorrow)  # Room 2 has no reservation except NoReservation as e:     print(f\"Blocked: {e}\") <pre>Blocked: No reservation for room 2 on 2026-01-28\n</pre> In\u00a0[18]: Copied! <pre># Successful check-in (has reservation)\ncheck_in(1, tomorrow)\nprint(f\"Checked in room 1 for {tomorrow}\")\n\nCheckIn()\n</pre> # Successful check-in (has reservation) check_in(1, tomorrow) print(f\"Checked in room 1 for {tomorrow}\")  CheckIn() <pre>Checked in room 1 for 2026-01-28\n</pre> Out[18]: Check-in records (requires reservation) <p>room</p> None <p>date</p> None 1 2026-01-28 <p>Total: 1</p> In\u00a0[19]: Copied! <pre># Try to check out without checking in \u2014 fails!\n# First make a reservation for room 3\nreserve_room(3, tomorrow, 'David Brown', '7777-7777-7777-7777')\n\ntry:\n    check_out(3, tomorrow)  # Reserved but not checked in\nexcept NotCheckedIn as e:\n    print(f\"Blocked: {e}\")\n</pre> # Try to check out without checking in \u2014 fails! # First make a reservation for room 3 reserve_room(3, tomorrow, 'David Brown', '7777-7777-7777-7777')  try:     check_out(3, tomorrow)  # Reserved but not checked in except NotCheckedIn as e:     print(f\"Blocked: {e}\") <pre>Blocked: Room 3 not checked in for 2026-01-28\n</pre> In\u00a0[20]: Copied! <pre># Successful check-out (was checked in)\ncheck_out(1, tomorrow)\nprint(f\"Checked out room 1 for {tomorrow}\")\n\nCheckOut()\n</pre> # Successful check-out (was checked in) check_out(1, tomorrow) print(f\"Checked out room 1 for {tomorrow}\")  CheckOut() <pre>Checked out room 1 for 2026-01-28\n</pre> Out[20]: Check-out records (requires check-in) <p>room</p> None <p>date</p> None 1 2026-01-28 <p>Total: 1</p> In\u00a0[21]: Copied! <pre># Available rooms (not reserved) for tomorrow\navailable = (RoomAvailable &amp; {'date': tomorrow}) - Reservation\nprint(f\"Available rooms for {tomorrow}: {len(available)}\")\navailable\n</pre> # Available rooms (not reserved) for tomorrow available = (RoomAvailable &amp; {'date': tomorrow}) - Reservation print(f\"Available rooms for {tomorrow}: {len(available)}\") available <pre>Available rooms for 2026-01-28: 18\n</pre> Out[21]: Room availability and pricing by date <p>room</p> None <p>date</p> <p>price</p> price per night 2 2026-01-28 189.004 2026-01-28 148.005 2026-01-28 177.006 2026-01-28 195.007 2026-01-28 155.008 2026-01-28 120.009 2026-01-28 140.0010 2026-01-28 174.0011 2026-01-28 163.0012 2026-01-28 155.0013 2026-01-28 139.0014 2026-01-28 147.00 <p>...</p> <p>Total: 18</p> In\u00a0[22]: Copied! <pre># Guests currently checked in (checked in but not out)\ncurrently_in = (CheckIn - CheckOut) * Reservation * Guest\ncurrently_in.proj('guest_name', 'room', 'date')\n</pre> # Guests currently checked in (checked in but not out) currently_in = (CheckIn - CheckOut) * Reservation * Guest currently_in.proj('guest_name', 'room', 'date') Out[22]: <p>room</p> None <p>date</p> None <p>guest_name</p> <p>Total: 0</p> In\u00a0[23]: Copied! <pre># Reservations without check-in (no-shows or upcoming)\nnot_checked_in = Reservation - CheckIn\n(not_checked_in * Guest).proj('guest_name', 'room', 'date')\n</pre> # Reservations without check-in (no-shows or upcoming) not_checked_in = Reservation - CheckIn (not_checked_in * Guest).proj('guest_name', 'room', 'date') Out[23]: <p>room</p> None <p>date</p> None <p>guest_name</p> 3 2026-01-28 David Brown <p>Total: 1</p> In\u00a0[24]: Copied! <pre># Revenue by room type using aggr\ndj.U('room_type').aggr(\n    Room * RoomAvailable * Reservation,\n    total_revenue='SUM(price)',\n    reservations='COUNT(*)'\n)\n</pre> # Revenue by room type using aggr dj.U('room_type').aggr(     Room * RoomAvailable * Reservation,     total_revenue='SUM(price)',     reservations='COUNT(*)' ) Out[24]: <p>room_type</p> <p>total_revenue</p> calculated attribute <p>reservations</p> calculated attribute Deluxe 318.00 2 <p>Total: 1</p> In\u00a0[25]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/examples/hotel-reservations/#hotel-reservation-system","title":"Hotel Reservation System\u00b6","text":"<p>This example demonstrates how DataJoint's schema design enforces business rules automatically. You'll learn:</p> <ul> <li>Workflow dependencies \u2014 Tables that enforce operational sequences</li> <li>Business rule enforcement \u2014 Using referential integrity as validation</li> <li>Temporal data \u2014 Room availability and pricing by date</li> <li>Error handling \u2014 Converting database errors to domain exceptions</li> </ul>"},{"location":"tutorials/examples/hotel-reservations/#the-problem","title":"The Problem\u00b6","text":"<p>A hotel needs to enforce these business rules:</p> <ol> <li>Rooms have types (Deluxe, Suite) with varying prices by date</li> <li>Guests can only reserve rooms that are available</li> <li>Each room can have at most one reservation per night</li> <li>Guests must have a reservation before checking in</li> <li>Guests must check in before checking out</li> </ol> <p>Traditional approaches validate these rules in application code. With DataJoint, the schema itself enforces them through foreign keys and unique constraints.</p>"},{"location":"tutorials/examples/hotel-reservations/#schema-design","title":"Schema Design\u00b6","text":"<p>The schema forms a workflow: <code>Room \u2192 RoomAvailable \u2192 Reservation \u2192 CheckIn \u2192 CheckOut</code></p> <p>Each arrow represents a foreign key dependency. You cannot insert a child record without a valid parent.</p>"},{"location":"tutorials/examples/hotel-reservations/#how-the-schema-enforces-rules","title":"How the Schema Enforces Rules\u00b6","text":"Business Rule Schema Enforcement Room must exist <code>Reservation -&gt; RoomAvailable -&gt; Room</code> Room must be available on date <code>Reservation -&gt; RoomAvailable</code> One reservation per room/night <code>RoomAvailable</code> is primary key of <code>Reservation</code> Must reserve before check-in <code>CheckIn -&gt; Reservation</code> Must check-in before check-out <code>CheckOut -&gt; CheckIn</code> <p>The database rejects invalid operations \u2014 no application code needed.</p>"},{"location":"tutorials/examples/hotel-reservations/#populate-room-availability","title":"Populate Room Availability\u00b6","text":"<p>Make rooms available for the next 30 days with random pricing:</p>"},{"location":"tutorials/examples/hotel-reservations/#business-operations","title":"Business Operations\u00b6","text":"<p>These functions wrap database operations and convert constraint violations into meaningful domain errors:</p>"},{"location":"tutorials/examples/hotel-reservations/#demo-business-rule-enforcement","title":"Demo: Business Rule Enforcement\u00b6","text":"<p>Let's see the schema enforce our business rules:</p>"},{"location":"tutorials/examples/hotel-reservations/#useful-queries","title":"Useful Queries\u00b6","text":"<p>The workflow structure enables powerful queries:</p>"},{"location":"tutorials/examples/hotel-reservations/#key-concepts","title":"Key Concepts\u00b6","text":"Concept How It's Used Workflow Dependencies <code>CheckOut -&gt; CheckIn -&gt; Reservation -&gt; RoomAvailable</code> Unique Constraints One reservation per room/night (primary key) Referential Integrity Can't reserve unavailable room, can't check in without reservation Error Translation Database exceptions \u2192 domain-specific errors <p>The schema is the business logic. Application code just translates errors.</p>"},{"location":"tutorials/examples/hotel-reservations/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>University Database \u2014 Academic records with many-to-many relationships</li> <li>Languages &amp; Proficiency \u2014 International standards and lookup tables</li> <li>Data Entry \u2014 Insert patterns and transactions</li> </ul>"},{"location":"tutorials/examples/languages/","title":"Languages and Proficiency","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nfrom faker import Faker\n\ndj.config['display.limit'] = 8\n\n# Clean start\nschema = dj.Schema('tutorial_languages')\nschema.drop(prompt=False)\nschema = dj.Schema('tutorial_languages')\n</pre> import datajoint as dj import numpy as np from faker import Faker  dj.config['display.limit'] = 8  # Clean start schema = dj.Schema('tutorial_languages') schema.drop(prompt=False) schema = dj.Schema('tutorial_languages') <pre>[2026-01-27 15:30:33] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Language(dj.Lookup):\n    definition = \"\"\"\n    # ISO 639-1 language codes\n    lang_code : char(2)             # two-letter code (en, es, ja)\n    ---\n    language : varchar(30)          # full name\n    native_name : varchar(50)       # name in native script\n    \"\"\"\n    contents = [\n        ('ar', 'Arabic', '\u0627\u0644\u0639\u0631\u0628\u064a\u0629'),\n        ('de', 'German', 'Deutsch'),\n        ('en', 'English', 'English'),\n        ('es', 'Spanish', 'Espa\u00f1ol'),\n        ('fr', 'French', 'Fran\u00e7ais'),\n        ('hi', 'Hindi', '\u0939\u093f\u0928\u094d\u0926\u0940'),\n        ('ja', 'Japanese', '\u65e5\u672c\u8a9e'),\n        ('ko', 'Korean', '\ud55c\uad6d\uc5b4'),\n        ('pt', 'Portuguese', 'Portugu\u00eas'),\n        ('uk', 'Ukrainian', '\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430'),\n        ('zh', 'Chinese', '\u4e2d\u6587'),\n    ]\n</pre> @schema class Language(dj.Lookup):     definition = \"\"\"     # ISO 639-1 language codes     lang_code : char(2)             # two-letter code (en, es, ja)     ---     language : varchar(30)          # full name     native_name : varchar(50)       # name in native script     \"\"\"     contents = [         ('ar', 'Arabic', '\u0627\u0644\u0639\u0631\u0628\u064a\u0629'),         ('de', 'German', 'Deutsch'),         ('en', 'English', 'English'),         ('es', 'Spanish', 'Espa\u00f1ol'),         ('fr', 'French', 'Fran\u00e7ais'),         ('hi', 'Hindi', '\u0939\u093f\u0928\u094d\u0926\u0940'),         ('ja', 'Japanese', '\u65e5\u672c\u8a9e'),         ('ko', 'Korean', '\ud55c\uad6d\uc5b4'),         ('pt', 'Portuguese', 'Portugu\u00eas'),         ('uk', 'Ukrainian', '\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430'),         ('zh', 'Chinese', '\u4e2d\u6587'),     ] In\u00a0[3]: Copied! <pre>@schema\nclass CEFRLevel(dj.Lookup):\n    definition = \"\"\"\n    # CEFR proficiency levels\n    cefr_level : char(2)            # A1, A2, B1, B2, C1, C2\n    ---\n    level_name : varchar(20)        # descriptive name\n    category : enum('Basic', 'Independent', 'Proficient')\n    description : varchar(100)      # can-do summary\n    \"\"\"\n    contents = [\n        ('A1', 'Beginner', 'Basic',\n         'Can use familiar everyday expressions'),\n        ('A2', 'Elementary', 'Basic',\n         'Can communicate in simple routine tasks'),\n        ('B1', 'Intermediate', 'Independent',\n         'Can deal with most travel situations'),\n        ('B2', 'Upper Intermediate', 'Independent',\n         'Can interact with fluency and spontaneity'),\n        ('C1', 'Advanced', 'Proficient',\n         'Can express ideas fluently for professional use'),\n        ('C2', 'Mastery', 'Proficient',\n         'Can understand virtually everything'),\n    ]\n</pre> @schema class CEFRLevel(dj.Lookup):     definition = \"\"\"     # CEFR proficiency levels     cefr_level : char(2)            # A1, A2, B1, B2, C1, C2     ---     level_name : varchar(20)        # descriptive name     category : enum('Basic', 'Independent', 'Proficient')     description : varchar(100)      # can-do summary     \"\"\"     contents = [         ('A1', 'Beginner', 'Basic',          'Can use familiar everyday expressions'),         ('A2', 'Elementary', 'Basic',          'Can communicate in simple routine tasks'),         ('B1', 'Intermediate', 'Independent',          'Can deal with most travel situations'),         ('B2', 'Upper Intermediate', 'Independent',          'Can interact with fluency and spontaneity'),         ('C1', 'Advanced', 'Proficient',          'Can express ideas fluently for professional use'),         ('C2', 'Mastery', 'Proficient',          'Can understand virtually everything'),     ] In\u00a0[4]: Copied! <pre>print(\"Languages:\")\nprint(Language())\nprint(\"\\nCEFR Levels:\")\nprint(CEFRLevel())\n</pre> print(\"Languages:\") print(Language()) print(\"\\nCEFR Levels:\") print(CEFRLevel()) <pre>Languages:\n*lang_code    language     native_name   \n+-----------+ +----------+ +------------+\nar            Arabic       \u0627\u0644\u0639\u0631\u0628\u064a\u0629       \nde            German       Deutsch       \nen            English      English       \nes            Spanish      Espa\u00f1ol       \nfr            French       Fran\u00e7ais      \nhi            Hindi        \u0939\u093f\u0928\u094d\u0926\u0940        \nja            Japanese     \u65e5\u672c\u8a9e           \nko            Korean       \ud55c\uad6d\uc5b4           \n   ...\n (Total: 11)\n\n\nCEFR Levels:\n*cefr_level    level_name     category       description   \n+------------+ +------------+ +------------+ +------------+\nA1             Beginner       Basic          Can use famili\nA2             Elementary     Basic          Can communicat\nB1             Intermediate   Independent    Can deal with \nB2             Upper Intermed Independent    Can interact w\nC1             Advanced       Proficient     Can express id\nC2             Mastery        Proficient     Can understand\n (Total: 6)\n\n</pre> In\u00a0[5]: Copied! <pre>@schema\nclass Person(dj.Manual):\n    definition = \"\"\"\n    # People with language skills\n    person_id : int32               # unique identifier\n    ---\n    name : varchar(60)\n    date_of_birth : date\n    \"\"\"\n</pre> @schema class Person(dj.Manual):     definition = \"\"\"     # People with language skills     person_id : int32               # unique identifier     ---     name : varchar(60)     date_of_birth : date     \"\"\" In\u00a0[6]: Copied! <pre>@schema\nclass Proficiency(dj.Manual):\n    definition = \"\"\"\n    # Language proficiency (many-to-many: person &lt;-&gt; language)\n    -&gt; Person\n    -&gt; Language\n    ---\n    -&gt; CEFRLevel\n    \"\"\"\n</pre> @schema class Proficiency(dj.Manual):     definition = \"\"\"     # Language proficiency (many-to-many: person &lt;-&gt; language)     -&gt; Person     -&gt; Language     ---     -&gt; CEFRLevel     \"\"\" In\u00a0[7]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[7]: <p>Reading the diagram:</p> <ul> <li>Gray tables (Language, CEFRLevel) are Lookup tables</li> <li>Green table (Person) is Manual</li> <li>Solid lines indicate foreign keys in the primary key (many-to-many)</li> <li>Dashed line indicates foreign key in secondary attributes (reference)</li> </ul> In\u00a0[8]: Copied! <pre>np.random.seed(42)\nfake = Faker()\nfake.seed_instance(42)\n\n# Generate 200 people\nn_people = 200\nPerson.insert(\n    {\n        'person_id': i,\n        'name': fake.name(),\n        'date_of_birth': fake.date_of_birth(\n            minimum_age=18, maximum_age=70)\n    }\n    for i in range(n_people)\n)\n\nprint(f\"Created {len(Person())} people\")\nPerson()\n</pre> np.random.seed(42) fake = Faker() fake.seed_instance(42)  # Generate 200 people n_people = 200 Person.insert(     {         'person_id': i,         'name': fake.name(),         'date_of_birth': fake.date_of_birth(             minimum_age=18, maximum_age=70)     }     for i in range(n_people) )  print(f\"Created {len(Person())} people\") Person() <pre>Created 200 people\n</pre> Out[8]: People with language skills <p>person_id</p> unique identifier <p>name</p> <p>date_of_birth</p> 0 Allison Hill 1966-11-251 Megan Mcclain 1959-09-052 Allen Robinson 1981-11-083 Cristian Santos 1983-12-144 Kevin Pacheco 1955-06-015 Melissa Peterson 1963-04-246 Gabrielle Davis 1960-03-137 Lindsey Roman 1993-09-30 <p>...</p> <p>Total: 200</p> In\u00a0[9]: Copied! <pre># Assign random language proficiencies\nlang_keys = list(Language.keys())\ncefr_keys = list(CEFRLevel.keys())\n\n# More people at intermediate levels than extremes\ncefr_weights = [0.08, 0.12, 0.20, 0.25, 0.20, 0.15]\navg_languages = 2.5\n\nfor person_key in Person.keys():\n    n_langs = np.random.poisson(avg_languages)\n    if n_langs &gt; 0:\n        selected_langs = np.random.choice(\n            len(lang_keys), min(n_langs, len(lang_keys)), replace=False)\n        Proficiency.insert(\n            {\n                **person_key,\n                **lang_keys[i],\n                **np.random.choice(cefr_keys, p=cefr_weights)\n            }\n            for i in selected_langs\n        )\n\nprint(f\"Created {len(Proficiency())} proficiency records\")\nProficiency()\n</pre> # Assign random language proficiencies lang_keys = list(Language.keys()) cefr_keys = list(CEFRLevel.keys())  # More people at intermediate levels than extremes cefr_weights = [0.08, 0.12, 0.20, 0.25, 0.20, 0.15] avg_languages = 2.5  for person_key in Person.keys():     n_langs = np.random.poisson(avg_languages)     if n_langs &gt; 0:         selected_langs = np.random.choice(             len(lang_keys), min(n_langs, len(lang_keys)), replace=False)         Proficiency.insert(             {                 **person_key,                 **lang_keys[i],                 **np.random.choice(cefr_keys, p=cefr_weights)             }             for i in selected_langs         )  print(f\"Created {len(Proficiency())} proficiency records\") Proficiency() <pre>Created 527 proficiency records\n</pre> Out[9]: Language proficiency (many-to-many: person &lt;-&gt; language) <p>person_id</p> None <p>lang_code</p> None <p>cefr_level</p> None 0 ar A10 de B20 hi C20 uk B23 de B23 uk A14 hi B24 pt C1 <p>...</p> <p>Total: 527</p> In\u00a0[10]: Copied! <pre># Proficient English speakers (C1 or C2)\nproficient_english = (\n    Person.proj('name') &amp; \n    (Proficiency &amp; {'lang_code': 'en'} &amp; \"cefr_level &gt;= 'C1'\")\n)\nprint(f\"Proficient English speakers: {len(proficient_english)}\")\nproficient_english\n</pre> # Proficient English speakers (C1 or C2) proficient_english = (     Person.proj('name') &amp;      (Proficiency &amp; {'lang_code': 'en'} &amp; \"cefr_level &gt;= 'C1'\") ) print(f\"Proficient English speakers: {len(proficient_english)}\") proficient_english <pre>Proficient English speakers: 18\n</pre> Out[10]: People with language skills <p>person_id</p> unique identifier <p>name</p> <p>date_of_birth</p> 22 Brian Burton bytes32 Elizabeth Brown bytes33 Angelica Tucker bytes37 Zachary Santos bytes38 Barbara Walker bytes42 Timothy Duncan bytes53 Whitney Peters bytes67 Teresa Taylor bytes <p>...</p> <p>Total: 18</p> In\u00a0[11]: Copied! <pre># People who speak BOTH English AND Spanish\nbilingual = (\n    Person.proj('name') &amp; \n    (Proficiency &amp; {'lang_code': 'en'}) &amp; \n    (Proficiency &amp; {'lang_code': 'es'})\n)\nprint(f\"English + Spanish speakers: {len(bilingual)}\")\nbilingual\n</pre> # People who speak BOTH English AND Spanish bilingual = (     Person.proj('name') &amp;      (Proficiency &amp; {'lang_code': 'en'}) &amp;      (Proficiency &amp; {'lang_code': 'es'}) ) print(f\"English + Spanish speakers: {len(bilingual)}\") bilingual <pre>English + Spanish speakers: 6\n</pre> Out[11]: People with language skills <p>person_id</p> unique identifier <p>name</p> <p>date_of_birth</p> 38 Barbara Walker bytes67 Teresa Taylor bytes77 Richard Henson bytes113 Denise Jones bytes122 Michael Powell bytes137 Lindsay Martinez bytes <p>Total: 6</p> In\u00a0[12]: Copied! <pre># People who speak English OR Spanish\neither = (\n    Person.proj('name') &amp; \n    (Proficiency &amp; \"lang_code in ('en', 'es')\")\n)\nprint(f\"English or Spanish speakers: {len(either)}\")\neither\n</pre> # People who speak English OR Spanish either = (     Person.proj('name') &amp;      (Proficiency &amp; \"lang_code in ('en', 'es')\") ) print(f\"English or Spanish speakers: {len(either)}\") either <pre>English or Spanish speakers: 79\n</pre> Out[12]: People with language skills <p>person_id</p> unique identifier <p>name</p> <p>date_of_birth</p> 6 Gabrielle Davis bytes11 David Garcia bytes12 Holly Wood bytes17 Daniel Hahn bytes19 Derek Wright bytes20 Kevin Hurst bytes22 Brian Burton bytes27 Sherri Baker bytes <p>...</p> <p>Total: 79</p> In\u00a0[13]: Copied! <pre># People who speak 4+ languages\npolyglots = Person.aggr(\n    Proficiency,\n    'name',\n    n_languages='COUNT(lang_code)',\n    languages='GROUP_CONCAT(lang_code)'\n) &amp; 'n_languages &gt;= 4'\n\nprint(f\"Polyglots (4+ languages): {len(polyglots)}\")\npolyglots\n</pre> # People who speak 4+ languages polyglots = Person.aggr(     Proficiency,     'name',     n_languages='COUNT(lang_code)',     languages='GROUP_CONCAT(lang_code)' ) &amp; 'n_languages &gt;= 4'  print(f\"Polyglots (4+ languages): {len(polyglots)}\") polyglots <pre>Polyglots (4+ languages): 56\n</pre> Out[13]: <p>person_id</p> unique identifier <p>name</p> <p>n_languages</p> calculated attribute <p>languages</p> calculated attribute 0 Allison Hill 4 ar,hi,uk,de6 Gabrielle Davis 5 hi,de,uk,fr,en8 Valerie Gray 4 hi,zh,ko,de9 Lisa Hensley 4 zh,pt,ar,ja11 David Garcia 4 es,ko,zh,hi12 Holly Wood 4 pt,en,uk,de14 Nicholas Martin 5 zh,pt,hi,fr,ko15 Margaret Hawkins DDS 4 ja,fr,uk,de <p>...</p> <p>Total: 56</p> In\u00a0[14]: Copied! <pre># Top 5 polyglots\ntop_polyglots = Person.aggr(\n    Proficiency,\n    'name',\n    n_languages='COUNT(lang_code)'\n) &amp; dj.Top(5, order_by='n_languages DESC')\n\ntop_polyglots\n</pre> # Top 5 polyglots top_polyglots = Person.aggr(     Proficiency,     'name',     n_languages='COUNT(lang_code)' ) &amp; dj.Top(5, order_by='n_languages DESC')  top_polyglots Out[14]: <p>person_id</p> unique identifier <p>name</p> <p>n_languages</p> calculated attribute 58 Bryan Zamora 720 Kevin Hurst 7116 Joshua Perry 742 Timothy Duncan 777 Richard Henson 7 <p>Total: 5</p> In\u00a0[15]: Copied! <pre># Number of speakers per language\nspeakers_per_lang = Language.aggr(\n    Proficiency,\n    'language',\n    n_speakers='COUNT(person_id)'\n)\nspeakers_per_lang\n</pre> # Number of speakers per language speakers_per_lang = Language.aggr(     Proficiency,     'language',     n_speakers='COUNT(person_id)' ) speakers_per_lang Out[15]: <p>lang_code</p> two-letter code (en, es, ja) <p>language</p> full name <p>n_speakers</p> calculated attribute ar Arabic 41de German 55en English 45es Spanish 40fr French 49hi Hindi 54ja Japanese 47ko Korean 47 <p>...</p> <p>Total: 11</p> In\u00a0[16]: Copied! <pre># CEFR level distribution for English\nenglish_levels = CEFRLevel.aggr(\n    Proficiency &amp; {'lang_code': 'en'},\n    'level_name',\n    n_speakers='COUNT(person_id)'\n)\nenglish_levels\n</pre> # CEFR level distribution for English english_levels = CEFRLevel.aggr(     Proficiency &amp; {'lang_code': 'en'},     'level_name',     n_speakers='COUNT(person_id)' ) english_levels Out[16]: <p>cefr_level</p> A1, A2, B1, B2, C1, C2 <p>level_name</p> descriptive name <p>n_speakers</p> calculated attribute A1 Beginner 1A2 Elementary 5B1 Intermediate 8B2 Upper Intermediate 13C1 Advanced 13C2 Mastery 5 <p>Total: 6</p> In\u00a0[17]: Copied! <pre># Full profile: person + language + proficiency details\nfull_profile = (\n    Person * Proficiency * Language * CEFRLevel\n).proj('name', 'language', 'level_name', 'category')\n\n# Show profile for person_id=0\nfull_profile &amp; {'person_id': 0}\n</pre> # Full profile: person + language + proficiency details full_profile = (     Person * Proficiency * Language * CEFRLevel ).proj('name', 'language', 'level_name', 'category')  # Show profile for person_id=0 full_profile &amp; {'person_id': 0} Out[17]: <p>person_id</p> None <p>lang_code</p> None <p>name</p> <p>language</p> full name <p>level_name</p> descriptive name <p>category</p> 0 ar Allison Hill Arabic Beginner Basic0 de Allison Hill German Upper Intermediate Independent0 hi Allison Hill Hindi Mastery Proficient0 uk Allison Hill Ukrainian Upper Intermediate Independent <p>Total: 4</p> In\u00a0[18]: Copied! <pre># Find people with C1+ proficiency in multiple languages\nadvanced_polyglots = Person.aggr(\n    Proficiency &amp; \"cefr_level &gt;= 'C1'\",\n    'name',\n    n_advanced='COUNT(*)'\n) &amp; 'n_advanced &gt;= 2'\n\nprint(f\"Advanced in 2+ languages: {len(advanced_polyglots)}\")\nadvanced_polyglots\n</pre> # Find people with C1+ proficiency in multiple languages advanced_polyglots = Person.aggr(     Proficiency &amp; \"cefr_level &gt;= 'C1'\",     'name',     n_advanced='COUNT(*)' ) &amp; 'n_advanced &gt;= 2'  print(f\"Advanced in 2+ languages: {len(advanced_polyglots)}\") advanced_polyglots <pre>Advanced in 2+ languages: 49\n</pre> Out[18]: <p>person_id</p> unique identifier <p>name</p> <p>n_advanced</p> calculated attribute 6 Gabrielle Davis 27 Lindsey Roman 214 Nicholas Martin 319 Derek Wright 220 Kevin Hurst 325 Melanie Herrera 227 Sherri Baker 229 Lisa Hernandez 2 <p>...</p> <p>Total: 49</p> In\u00a0[19]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/examples/languages/#languages-and-proficiency","title":"Languages and Proficiency\u00b6","text":"<p>This example demonstrates many-to-many relationships using an association table with international standards. You'll learn:</p> <ul> <li>Many-to-many relationships \u2014 People speak multiple languages; languages have multiple speakers</li> <li>Lookup tables \u2014 Standardized reference data (ISO language codes, CEFR levels)</li> <li>Association tables \u2014 Linking entities with additional attributes</li> <li>Complex queries \u2014 Aggregations, filtering, and joins</li> </ul>"},{"location":"tutorials/examples/languages/#international-standards","title":"International Standards\u00b6","text":"<p>This example uses two widely-adopted standards:</p> <ul> <li>ISO 639-1 \u2014 Two-letter language codes (<code>en</code>, <code>es</code>, <code>ja</code>)</li> <li>CEFR \u2014 Common European Framework of Reference for language proficiency (A1\u2013C2)</li> </ul> <p>Using international standards ensures data consistency and enables integration with external systems.</p>"},{"location":"tutorials/examples/languages/#lookup-tables","title":"Lookup Tables\u00b6","text":"<p>Lookup tables store standardized reference data that rarely changes. The <code>contents</code> attribute pre-populates them when the schema is created.</p>"},{"location":"tutorials/examples/languages/#entity-and-association-tables","title":"Entity and Association Tables\u00b6","text":"<ul> <li>Person \u2014 The main entity</li> <li>Proficiency \u2014 Association table linking Person, Language, and CEFRLevel</li> </ul> <p>The association table's primary key includes both Person and Language, creating the many-to-many relationship.</p>"},{"location":"tutorials/examples/languages/#populate-sample-data","title":"Populate Sample Data\u00b6","text":""},{"location":"tutorials/examples/languages/#query-examples","title":"Query Examples\u00b6","text":""},{"location":"tutorials/examples/languages/#finding-speakers","title":"Finding Speakers\u00b6","text":""},{"location":"tutorials/examples/languages/#aggregations","title":"Aggregations\u00b6","text":""},{"location":"tutorials/examples/languages/#joining-tables","title":"Joining Tables\u00b6","text":""},{"location":"tutorials/examples/languages/#key-concepts","title":"Key Concepts\u00b6","text":"Pattern Implementation Many-to-many <code>Proficiency</code> links <code>Person</code> and <code>Language</code> Lookup tables <code>Language</code> and <code>CEFRLevel</code> with <code>contents</code> Association data <code>cefr_level</code> stored in the association table Standards ISO 639-1 codes, CEFR levels"},{"location":"tutorials/examples/languages/#benefits-of-lookup-tables","title":"Benefits of Lookup Tables\u00b6","text":"<ol> <li>Data consistency \u2014 Only valid codes can be used</li> <li>Rich metadata \u2014 Full names, descriptions stored once</li> <li>Easy updates \u2014 Change \"Espa\u00f1ol\" to \"Spanish\" in one place</li> <li>Self-documenting \u2014 <code>Language()</code> shows all valid options</li> </ol>"},{"location":"tutorials/examples/languages/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>University Database \u2014 Academic records</li> <li>Hotel Reservations \u2014 Workflow dependencies</li> <li>Queries Tutorial \u2014 Query operators in depth</li> </ul>"},{"location":"tutorials/examples/university/","title":"University Database","text":"In\u00a0[1]: Copied! <pre>import datajoint as dj\nimport numpy as np\nfrom datetime import date\n\nschema = dj.Schema('tutorial_university')\n</pre> import datajoint as dj import numpy as np from datetime import date  schema = dj.Schema('tutorial_university') <pre>[2026-01-27 15:30:37] DataJoint 2.1.0a9 connected to postgres@127.0.0.1:5432\n</pre> In\u00a0[2]: Copied! <pre>@schema\nclass Student(dj.Manual):\n    definition = \"\"\"\n    student_id : int64           # university-wide ID\n    ---\n    first_name : varchar(40)\n    last_name : varchar(40)\n    sex : enum('F', 'M', 'U')\n    date_of_birth : date\n    home_city : varchar(60)\n    home_state : char(2)          # US state code\n    \"\"\"\n</pre> @schema class Student(dj.Manual):     definition = \"\"\"     student_id : int64           # university-wide ID     ---     first_name : varchar(40)     last_name : varchar(40)     sex : enum('F', 'M', 'U')     date_of_birth : date     home_city : varchar(60)     home_state : char(2)          # US state code     \"\"\" In\u00a0[3]: Copied! <pre>@schema\nclass Department(dj.Manual):\n    definition = \"\"\"\n    dept : varchar(6)   # e.g. BIOL, CS, MATH\n    ---\n    dept_name : varchar(200)\n    \"\"\"\n</pre> @schema class Department(dj.Manual):     definition = \"\"\"     dept : varchar(6)   # e.g. BIOL, CS, MATH     ---     dept_name : varchar(200)     \"\"\" In\u00a0[4]: Copied! <pre>@schema\nclass StudentMajor(dj.Manual):\n    definition = \"\"\"\n    -&gt; Student\n    ---\n    -&gt; Department\n    declare_date : date\n    \"\"\"\n</pre> @schema class StudentMajor(dj.Manual):     definition = \"\"\"     -&gt; Student     ---     -&gt; Department     declare_date : date     \"\"\" In\u00a0[5]: Copied! <pre>@schema\nclass Course(dj.Manual):\n    definition = \"\"\"\n    -&gt; Department\n    course : int16               # course number, e.g. 1010\n    ---\n    course_name : varchar(200)\n    credits : decimal(3,1)\n    \"\"\"\n</pre> @schema class Course(dj.Manual):     definition = \"\"\"     -&gt; Department     course : int16               # course number, e.g. 1010     ---     course_name : varchar(200)     credits : decimal(3,1)     \"\"\" In\u00a0[6]: Copied! <pre>@schema\nclass Term(dj.Manual):\n    definition = \"\"\"\n    term_year : int16\n    term : enum('Spring', 'Summer', 'Fall')\n    \"\"\"\n</pre> @schema class Term(dj.Manual):     definition = \"\"\"     term_year : int16     term : enum('Spring', 'Summer', 'Fall')     \"\"\" In\u00a0[7]: Copied! <pre>@schema\nclass Section(dj.Manual):\n    definition = \"\"\"\n    -&gt; Course\n    -&gt; Term\n    section : char(1)\n    ---\n    auditorium : varchar(12)\n    \"\"\"\n</pre> @schema class Section(dj.Manual):     definition = \"\"\"     -&gt; Course     -&gt; Term     section : char(1)     ---     auditorium : varchar(12)     \"\"\" In\u00a0[8]: Copied! <pre>@schema\nclass Enroll(dj.Manual):\n    definition = \"\"\"\n    -&gt; Student\n    -&gt; Section\n    \"\"\"\n</pre> @schema class Enroll(dj.Manual):     definition = \"\"\"     -&gt; Student     -&gt; Section     \"\"\" In\u00a0[9]: Copied! <pre>@schema\nclass LetterGrade(dj.Lookup):\n    definition = \"\"\"\n    grade : char(2)\n    ---\n    points : decimal(3,2)\n    \"\"\"\n    contents = [\n        ['A',  4.00], ['A-', 3.67],\n        ['B+', 3.33], ['B',  3.00], ['B-', 2.67],\n        ['C+', 2.33], ['C',  2.00], ['C-', 1.67],\n        ['D+', 1.33], ['D',  1.00],\n        ['F',  0.00]\n    ]\n</pre> @schema class LetterGrade(dj.Lookup):     definition = \"\"\"     grade : char(2)     ---     points : decimal(3,2)     \"\"\"     contents = [         ['A',  4.00], ['A-', 3.67],         ['B+', 3.33], ['B',  3.00], ['B-', 2.67],         ['C+', 2.33], ['C',  2.00], ['C-', 1.67],         ['D+', 1.33], ['D',  1.00],         ['F',  0.00]     ] In\u00a0[10]: Copied! <pre>@schema\nclass Grade(dj.Manual):\n    definition = \"\"\"\n    -&gt; Enroll\n    ---\n    -&gt; LetterGrade\n    \"\"\"\n</pre> @schema class Grade(dj.Manual):     definition = \"\"\"     -&gt; Enroll     ---     -&gt; LetterGrade     \"\"\" In\u00a0[11]: Copied! <pre>dj.Diagram(schema)\n</pre> dj.Diagram(schema) Out[11]: In\u00a0[12]: Copied! <pre>import faker\nimport random\n\nfake = faker.Faker()\nfaker.Faker.seed(42)\nrandom.seed(42)\n</pre> import faker import random  fake = faker.Faker() faker.Faker.seed(42) random.seed(42) In\u00a0[13]: Copied! <pre>def generate_students(n=500):\n    \"\"\"Generate n student records.\"\"\"\n    fake_name = {'F': fake.name_female, 'M': fake.name_male}\n    for student_id in range(1000, 1000 + n):\n        sex = random.choice(['F', 'M'])\n        name = fake_name[sex]().split()[:2]\n        yield {\n            'student_id': student_id,\n            'first_name': name[0],\n            'last_name': name[-1],\n            'sex': sex,\n            'date_of_birth': fake.date_between(\n                start_date='-35y', end_date='-17y'),\n            'home_city': fake.city(),\n            'home_state': fake.state_abbr()\n        }\n\nStudent.insert(generate_students(500))\nprint(f\"Inserted {len(Student())} students\")\n</pre> def generate_students(n=500):     \"\"\"Generate n student records.\"\"\"     fake_name = {'F': fake.name_female, 'M': fake.name_male}     for student_id in range(1000, 1000 + n):         sex = random.choice(['F', 'M'])         name = fake_name[sex]().split()[:2]         yield {             'student_id': student_id,             'first_name': name[0],             'last_name': name[-1],             'sex': sex,             'date_of_birth': fake.date_between(                 start_date='-35y', end_date='-17y'),             'home_city': fake.city(),             'home_state': fake.state_abbr()         }  Student.insert(generate_students(500)) print(f\"Inserted {len(Student())} students\") <pre>Inserted 500 students\n</pre> In\u00a0[14]: Copied! <pre># Departments\nDepartment.insert([\n    {'dept': 'CS', 'dept_name': 'Computer Science'},\n    {'dept': 'BIOL', 'dept_name': 'Life Sciences'},\n    {'dept': 'PHYS', 'dept_name': 'Physics'},\n    {'dept': 'MATH', 'dept_name': 'Mathematics'},\n])\n\n# Assign majors to ~75% of students\nstudents = Student.keys()\ndepts = Department.keys()\nStudentMajor.insert(\n    {\n        **s, **random.choice(depts),\n        'declare_date': fake.date_between(start_date='-4y')\n    }\n    for s in students if random.random() &lt; 0.75\n)\nprint(f\"{len(StudentMajor())} students declared majors\")\n</pre> # Departments Department.insert([     {'dept': 'CS', 'dept_name': 'Computer Science'},     {'dept': 'BIOL', 'dept_name': 'Life Sciences'},     {'dept': 'PHYS', 'dept_name': 'Physics'},     {'dept': 'MATH', 'dept_name': 'Mathematics'}, ])  # Assign majors to ~75% of students students = Student.keys() depts = Department.keys() StudentMajor.insert(     {         **s, **random.choice(depts),         'declare_date': fake.date_between(start_date='-4y')     }     for s in students if random.random() &lt; 0.75 ) print(f\"{len(StudentMajor())} students declared majors\") <pre>378 students declared majors\n</pre> In\u00a0[15]: Copied! <pre># Course catalog\nCourse.insert([\n    ['BIOL', 1010, 'Biology in the 21st Century', 3],\n    ['BIOL', 2020, 'Principles of Cell Biology', 3],\n    ['BIOL', 2325, 'Human Anatomy', 4],\n    ['BIOL', 2420, 'Human Physiology', 4],\n    ['PHYS', 2210, 'Physics for Scientists I', 4],\n    ['PHYS', 2220, 'Physics for Scientists II', 4],\n    ['PHYS', 2060, 'Quantum Mechanics', 3],\n    ['MATH', 1210, 'Calculus I', 4],\n    ['MATH', 1220, 'Calculus II', 4],\n    ['MATH', 2270, 'Linear Algebra', 4],\n    ['MATH', 2280, 'Differential Equations', 4],\n    ['CS', 1410, 'Intro to Object-Oriented Programming', 4],\n    ['CS', 2420, 'Data Structures &amp; Algorithms', 4],\n    ['CS', 3500, 'Software Practice', 4],\n    ['CS', 3810, 'Computer Organization', 4],\n])\nprint(f\"{len(Course())} courses in catalog\")\n</pre> # Course catalog Course.insert([     ['BIOL', 1010, 'Biology in the 21st Century', 3],     ['BIOL', 2020, 'Principles of Cell Biology', 3],     ['BIOL', 2325, 'Human Anatomy', 4],     ['BIOL', 2420, 'Human Physiology', 4],     ['PHYS', 2210, 'Physics for Scientists I', 4],     ['PHYS', 2220, 'Physics for Scientists II', 4],     ['PHYS', 2060, 'Quantum Mechanics', 3],     ['MATH', 1210, 'Calculus I', 4],     ['MATH', 1220, 'Calculus II', 4],     ['MATH', 2270, 'Linear Algebra', 4],     ['MATH', 2280, 'Differential Equations', 4],     ['CS', 1410, 'Intro to Object-Oriented Programming', 4],     ['CS', 2420, 'Data Structures &amp; Algorithms', 4],     ['CS', 3500, 'Software Practice', 4],     ['CS', 3810, 'Computer Organization', 4], ]) print(f\"{len(Course())} courses in catalog\") <pre>15 courses in catalog\n</pre> In\u00a0[16]: Copied! <pre># Academic terms 2020-2024\nTerm.insert(\n    {'term_year': year, 'term': term}\n    for year in range(2020, 2025)\n    for term in ['Spring', 'Summer', 'Fall']\n)\n\n# Create sections for each course-term with 1-3 sections\nfor course in Course.keys():\n    for term in Term.keys():\n        for sec in 'abc'[:random.randint(1, 3)]:\n            if random.random() &lt; 0.7:  # Not every course every term\n                Section.insert1({\n                    **course, **term,\n                    'section': sec,\n                    'auditorium': f\"{random.choice('ABCDEF')}\"\n                                  f\"{random.randint(100, 400)}\"\n                }, skip_duplicates=True)\n\nprint(f\"{len(Section())} sections created\")\n</pre> # Academic terms 2020-2024 Term.insert(     {'term_year': year, 'term': term}     for year in range(2020, 2025)     for term in ['Spring', 'Summer', 'Fall'] )  # Create sections for each course-term with 1-3 sections for course in Course.keys():     for term in Term.keys():         for sec in 'abc'[:random.randint(1, 3)]:             if random.random() &lt; 0.7:  # Not every course every term                 Section.insert1({                     **course, **term,                     'section': sec,                     'auditorium': f\"{random.choice('ABCDEF')}\"                                   f\"{random.randint(100, 400)}\"                 }, skip_duplicates=True)  print(f\"{len(Section())} sections created\") <pre>339 sections created\n</pre> In\u00a0[17]: Copied! <pre># Enroll students in courses\nterms = Term.keys()\nfor student in Student.keys():\n    # Each student enrolls over 2-6 random terms\n    student_terms = random.sample(terms, k=random.randint(2, 6))\n    for term in student_terms:\n        # Take 2-4 courses per term\n        available = (Section &amp; term).keys()\n        if available:\n            n_courses = min(random.randint(2, 4), len(available))\n            for section in random.sample(available, k=n_courses):\n                Enroll.insert1(\n                    {**student, **section}, skip_duplicates=True)\n\nprint(f\"{len(Enroll())} enrollments\")\n</pre> # Enroll students in courses terms = Term.keys() for student in Student.keys():     # Each student enrolls over 2-6 random terms     student_terms = random.sample(terms, k=random.randint(2, 6))     for term in student_terms:         # Take 2-4 courses per term         available = (Section &amp; term).keys()         if available:             n_courses = min(random.randint(2, 4), len(available))             for section in random.sample(available, k=n_courses):                 Enroll.insert1(                     {**student, **section}, skip_duplicates=True)  print(f\"{len(Enroll())} enrollments\") <pre>5787 enrollments\n</pre> In\u00a0[18]: Copied! <pre># Assign grades to ~90% of enrollments (some incomplete)\ngrades = LetterGrade.to_arrays('grade')\n# Weight toward B/C range\nweights = [5, 8, 10, 15, 12, 10, 15, 10, 5, 5, 5]\n\nfor enroll in Enroll.keys():\n    if random.random() &lt; 0.9:\n        Grade.insert1({**enroll, 'grade': random.choices(grades, weights=weights)[0]})\n\nprint(f\"{len(Grade())} grades assigned\")\n</pre> # Assign grades to ~90% of enrollments (some incomplete) grades = LetterGrade.to_arrays('grade') # Weight toward B/C range weights = [5, 8, 10, 15, 12, 10, 15, 10, 5, 5, 5]  for enroll in Enroll.keys():     if random.random() &lt; 0.9:         Grade.insert1({**enroll, 'grade': random.choices(grades, weights=weights)[0]})  print(f\"{len(Grade())} grades assigned\") <pre>5178 grades assigned\n</pre> In\u00a0[19]: Copied! <pre>dj.config['display.limit'] = 8  # Limit preview rows\n</pre> dj.config['display.limit'] = 8  # Limit preview rows In\u00a0[20]: Copied! <pre># Students from California\nStudent &amp; {'home_state': 'CA'}\n</pre> # Students from California Student &amp; {'home_state': 'CA'} Out[20]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1073 Kyle Martinez M 2008-10-11 West Amandastad CA1092 Christina Wilson F 1995-02-12 West Margaret CA1157 Kelly Foster F 1996-08-27 Jeffreyburgh CA1194 Nicholas Buck M 2002-02-20 Hillton CA1203 Heather Armstrong F 1999-06-26 New Nicole CA <p>Total: 5</p> In\u00a0[21]: Copied! <pre># Female students NOT from California\n(Student &amp; {'sex': 'F'}) - {'home_state': 'CA'}\n</pre> # Female students NOT from California (Student &amp; {'sex': 'F'}) - {'home_state': 'CA'} Out[21]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison Hill F 1995-02-02 Lake Joyside CO1001 Amanda Davis F 1995-04-05 New Jamesside MT1003 Tina Rogers F 1992-09-27 West Melanieview AS1004 Julia Martinez F 2007-09-03 New Kellystad OK1005 Andrea Stanley F 2004-12-26 Port Jesseville MS1006 Madison Diaz F 1997-09-25 Lake Joseph TX1007 Lisa Jackson F 2004-03-12 South Noah SC1009 Christine Hahn F 2006-11-05 Jasonfort MO <p>...</p> <p>Total: 258</p> In\u00a0[22]: Copied! <pre># SQL-style string conditions\nStudent &amp; \"home_state IN ('CA', 'TX', 'NY')\"\n</pre> # SQL-style string conditions Student &amp; \"home_state IN ('CA', 'TX', 'NY')\" Out[22]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1006 Madison Diaz F 1997-09-25 Lake Joseph TX1014 Ashley Graham F 1999-03-28 Teresaburgh NY1073 Kyle Martinez M 2008-10-11 West Amandastad CA1092 Christina Wilson F 1995-02-12 West Margaret CA1136 Beth Fisher F 1993-02-12 South Shanestad NY1149 Emily Nguyen F 2008-01-06 North Ericton TX1157 Kelly Foster F 1996-08-27 Jeffreyburgh CA1163 Tiffany Stevenson F 1991-10-23 West Cassidy NY <p>...</p> <p>Total: 22</p> In\u00a0[23]: Copied! <pre># OR conditions using a list\nStudent &amp; [{'home_state': 'CA'}, {'home_state': 'TX'}]\n</pre> # OR conditions using a list Student &amp; [{'home_state': 'CA'}, {'home_state': 'TX'}] Out[23]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1006 Madison Diaz F 1997-09-25 Lake Joseph TX1073 Kyle Martinez M 2008-10-11 West Amandastad CA1092 Christina Wilson F 1995-02-12 West Margaret CA1149 Emily Nguyen F 2008-01-06 North Ericton TX1157 Kelly Foster F 1996-08-27 Jeffreyburgh CA1194 Nicholas Buck M 2002-02-20 Hillton CA1203 Heather Armstrong F 1999-06-26 New Nicole CA1267 Angela Cole F 1997-10-01 North Joseph TX <p>...</p> <p>Total: 14</p> In\u00a0[24]: Copied! <pre># Students majoring in Computer Science\nStudent &amp; (StudentMajor &amp; {'dept': 'CS'})\n</pre> # Students majoring in Computer Science Student &amp; (StudentMajor &amp; {'dept': 'CS'}) Out[24]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1004 Julia Martinez F 2007-09-03 New Kellystad OK1005 Andrea Stanley F 2004-12-26 Port Jesseville MS1007 Lisa Jackson F 2004-03-12 South Noah SC1009 Christine Hahn F 2006-11-05 Jasonfort MO1011 Heidi Osborn F 1993-07-24 Daviston VI1013 Kristen Dunlap F 2003-04-26 Lake Nicoleview WI1017 Sandra Parker F 1992-02-11 North Jessicaland WV1020 Jessica Holmes F 1993-04-01 North Matthew OR <p>...</p> <p>Total: 79</p> In\u00a0[25]: Copied! <pre># Students who have NOT taken any Math courses\nStudent - (Enroll &amp; {'dept': 'MATH'})\n</pre> # Students who have NOT taken any Math courses Student - (Enroll &amp; {'dept': 'MATH'}) Out[25]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1022 Jose Ramirez M 1999-10-23 Lake Selena OK1059 Stephanie Simmons F 2007-05-24 Merrittfort NV1061 Donna Nelson F 1996-10-28 East Edwardfurt UT1064 Scott Ashley M 1998-08-12 South Jeffrey NE1096 Nicole Ruiz F 2002-10-19 Howardshire NE1098 Jennifer Simpson F 2002-12-30 Cervantesside AZ1124 Tonya Taylor F 2008-06-19 Coffeyside MN1151 Ashley Gregory F 2001-09-30 Brandonfurt PA <p>...</p> <p>Total: 33</p> In\u00a0[26]: Copied! <pre># Students with ungraded enrollments (enrolled but no grade yet)\nStudent &amp; (Enroll - Grade)\n</pre> # Students with ungraded enrollments (enrolled but no grade yet) Student &amp; (Enroll - Grade) Out[26]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison Hill F 1995-02-02 Lake Joyside CO1002 Kevin Pacheco M 1991-03-10 Lake Roberto KY1003 Tina Rogers F 1992-09-27 West Melanieview AS1004 Julia Martinez F 2007-09-03 New Kellystad OK1005 Andrea Stanley F 2004-12-26 Port Jesseville MS1006 Madison Diaz F 1997-09-25 Lake Joseph TX1007 Lisa Jackson F 2004-03-12 South Noah SC1008 Daniel Nguyen M 2004-11-24 East Lydiamouth MO <p>...</p> <p>Total: 348</p> In\u00a0[27]: Copied! <pre># All-A students: have grades AND no non-A grades\nall_a = (Student &amp; Grade) - (Grade - {'grade': 'A'})\nall_a\n</pre> # All-A students: have grades AND no non-A grades all_a = (Student &amp; Grade) - (Grade - {'grade': 'A'}) all_a Out[27]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code <p>Total: 0</p> In\u00a0[28]: Copied! <pre># Select specific attributes\nStudent.proj('first_name', 'last_name')\n</pre> # Select specific attributes Student.proj('first_name', 'last_name') Out[28]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison Hill bytes bytes bytes bytes1001 Amanda Davis bytes bytes bytes bytes1002 Kevin Pacheco bytes bytes bytes bytes1003 Tina Rogers bytes bytes bytes bytes1004 Julia Martinez bytes bytes bytes bytes1005 Andrea Stanley bytes bytes bytes bytes1006 Madison Diaz bytes bytes bytes bytes1007 Lisa Jackson bytes bytes bytes bytes <p>...</p> <p>Total: 500</p> In\u00a0[29]: Copied! <pre># Computed attribute: full name\nStudent.proj(full_name=\"CONCAT(first_name, ' ', last_name)\")\n</pre> # Computed attribute: full name Student.proj(full_name=\"CONCAT(first_name, ' ', last_name)\") Out[29]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 bytes bytes bytes bytes bytes bytes1001 bytes bytes bytes bytes bytes bytes1002 bytes bytes bytes bytes bytes bytes1003 bytes bytes bytes bytes bytes bytes1004 bytes bytes bytes bytes bytes bytes1005 bytes bytes bytes bytes bytes bytes1006 bytes bytes bytes bytes bytes bytes1007 bytes bytes bytes bytes bytes bytes <p>...</p> <p>Total: 500</p> In\u00a0[30]: Copied! <pre># Calculate age in years\nStudent.proj('first_name', 'last_name', \n             age='TIMESTAMPDIFF(YEAR, date_of_birth, CURDATE())')\n</pre> # Calculate age in years Student.proj('first_name', 'last_name',               age='TIMESTAMPDIFF(YEAR, date_of_birth, CURDATE())') Out[30]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison Hill bytes bytes bytes bytes1001 Amanda Davis bytes bytes bytes bytes1002 Kevin Pacheco bytes bytes bytes bytes1003 Tina Rogers bytes bytes bytes bytes1004 Julia Martinez bytes bytes bytes bytes1005 Andrea Stanley bytes bytes bytes bytes1006 Madison Diaz bytes bytes bytes bytes1007 Lisa Jackson bytes bytes bytes bytes <p>...</p> <p>Total: 500</p> In\u00a0[31]: Copied! <pre># Keep all attributes plus computed ones with ...\nStudent.proj(..., age='TIMESTAMPDIFF(YEAR, date_of_birth, CURDATE())')\n</pre> # Keep all attributes plus computed ones with ... Student.proj(..., age='TIMESTAMPDIFF(YEAR, date_of_birth, CURDATE())') Out[31]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison Hill F 1995-02-02 Lake Joyside CO1001 Amanda Davis F 1995-04-05 New Jamesside MT1002 Kevin Pacheco M 1991-03-10 Lake Roberto KY1003 Tina Rogers F 1992-09-27 West Melanieview AS1004 Julia Martinez F 2007-09-03 New Kellystad OK1005 Andrea Stanley F 2004-12-26 Port Jesseville MS1006 Madison Diaz F 1997-09-25 Lake Joseph TX1007 Lisa Jackson F 2004-03-12 South Noah SC <p>...</p> <p>Total: 500</p> In\u00a0[32]: Copied! <pre># Exclude specific attributes with -\nStudent.proj(..., '-date_of_birth')\n</pre> # Exclude specific attributes with - Student.proj(..., '-date_of_birth') Out[32]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison Hill F bytes Lake Joyside CO1001 Amanda Davis F bytes New Jamesside MT1002 Kevin Pacheco M bytes Lake Roberto KY1003 Tina Rogers F bytes West Melanieview AS1004 Julia Martinez F bytes New Kellystad OK1005 Andrea Stanley F bytes Port Jesseville MS1006 Madison Diaz F bytes Lake Joseph TX1007 Lisa Jackson F bytes South Noah SC <p>...</p> <p>Total: 500</p> In\u00a0[33]: Copied! <pre># Rename attribute\nStudent.proj('first_name', family_name='last_name')\n</pre> # Rename attribute Student.proj('first_name', family_name='last_name') Out[33]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison bytes bytes bytes bytes bytes1001 Amanda bytes bytes bytes bytes bytes1002 Kevin bytes bytes bytes bytes bytes1003 Tina bytes bytes bytes bytes bytes1004 Julia bytes bytes bytes bytes bytes1005 Andrea bytes bytes bytes bytes bytes1006 Madison bytes bytes bytes bytes bytes1007 Lisa bytes bytes bytes bytes bytes <p>...</p> <p>Total: 500</p> In\u00a0[34]: Copied! <pre># All unique first names\ndj.U('first_name') &amp; Student\n</pre> # All unique first names dj.U('first_name') &amp; Student Out[34]: <p>first_name</p> AaronAdamAdrianaAlejandroAlexanderAlexisAllisonAmanda <p>...</p> <p>Total: 246</p> In\u00a0[35]: Copied! <pre># All unique home states of enrolled students\ndj.U('home_state') &amp; (Student &amp; Enroll)\n</pre> # All unique home states of enrolled students dj.U('home_state') &amp; (Student &amp; Enroll) Out[35]: <p>home_state</p> US state code AKALARASAZCACOCT <p>...</p> <p>Total: 59</p> In\u00a0[36]: Copied! <pre># Birth years of students in CS courses\ndj.U('birth_year') &amp; (\n    Student.proj(birth_year='YEAR(date_of_birth)') &amp; (Enroll &amp; {'dept': 'CS'})\n)\n</pre> # Birth years of students in CS courses dj.U('birth_year') &amp; (     Student.proj(birth_year='YEAR(date_of_birth)') &amp; (Enroll &amp; {'dept': 'CS'}) ) Out[36]: <p>birth_year</p> calculated attribute 19911992199319941995199619971998 <p>...</p> <p>Total: 19</p> In\u00a0[37]: Copied! <pre># Students with their declared majors\nStudent.proj('first_name', 'last_name') * StudentMajor\n</pre> # Students with their declared majors Student.proj('first_name', 'last_name') * StudentMajor Out[37]: <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>dept</p> None <p>declare_date</p> 1000 Allison Hill MATH 2025-06-191001 Amanda Davis MATH 2023-07-031002 Kevin Pacheco MATH 2024-01-311003 Tina Rogers MATH 2023-03-261004 Julia Martinez CS 2024-10-251005 Andrea Stanley CS 2024-07-011007 Lisa Jackson CS 2023-03-141009 Christine Hahn CS 2025-10-11 <p>...</p> <p>Total: 378</p> In\u00a0[38]: Copied! <pre># Courses with department names\nCourse * Department.proj('dept_name')\n</pre> # Courses with department names Course * Department.proj('dept_name') Out[38]: <p>dept</p> None <p>course</p> course number, e.g. 1010 <p>course_name</p> <p>credits</p> <p>dept_name</p> BIOL 1010 Biology in the 21st Century 3.0 Life SciencesBIOL 2020 Principles of Cell Biology 3.0 Life SciencesBIOL 2325 Human Anatomy 4.0 Life SciencesBIOL 2420 Human Physiology 4.0 Life SciencesCS 1410 Intro to Object-Oriented Programming 4.0 Computer ScienceCS 2420 Data Structures &amp; Algorithms 4.0 Computer ScienceCS 3500 Software Practice 4.0 Computer ScienceCS 3810 Computer Organization 4.0 Computer Science <p>...</p> <p>Total: 15</p> In\u00a0[39]: Copied! <pre># Left join: all students, including those without majors (NULL for unmatched)\nStudent.proj('first_name', 'last_name').join(StudentMajor, left=True)\n</pre> # Left join: all students, including those without majors (NULL for unmatched) Student.proj('first_name', 'last_name').join(StudentMajor, left=True) Out[39]: <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>dept</p> None <p>declare_date</p> 1000 Allison Hill MATH 2025-06-191001 Amanda Davis MATH 2023-07-031002 Kevin Pacheco MATH 2024-01-311003 Tina Rogers MATH 2023-03-261004 Julia Martinez CS 2024-10-251005 Andrea Stanley CS 2024-07-011006 Madison Diaz None None1007 Lisa Jackson CS 2023-03-14 <p>...</p> <p>Total: 500</p> In\u00a0[40]: Copied! <pre># Multi-table join: grades with student names and course info\n(Student.proj('first_name', 'last_name') \n * Grade \n * Course.proj('course_name', 'credits'))\n</pre> # Multi-table join: grades with student names and course info (Student.proj('first_name', 'last_name')   * Grade   * Course.proj('course_name', 'credits')) Out[40]: <p>student_id</p> None <p>dept</p> None <p>course</p> None <p>term_year</p> None <p>term</p> None <p>section</p> None <p>grade</p> None <p>first_name</p> <p>last_name</p> <p>course_name</p> <p>credits</p> 1000 BIOL 1010 2022 Fall c D+ Allison Hill Biology in the 21st Century 3.01000 BIOL 1010 2024 Summer a C- Allison Hill Biology in the 21st Century 3.01000 BIOL 2325 2020 Fall a C  Allison Hill Human Anatomy 4.01000 CS 2420 2023 Spring a B- Allison Hill Data Structures &amp; Algorithms 4.01000 CS 3500 2022 Fall b C  Allison Hill Software Practice 4.01000 MATH 1220 2024 Summer a C- Allison Hill Calculus II 4.01000 PHYS 2210 2020 Fall a C- Allison Hill Physics for Scientists I 4.01000 PHYS 2210 2023 Spring a D+ Allison Hill Physics for Scientists I 4.0 <p>...</p> <p>Total: 5178</p> In\u00a0[41]: Copied! <pre># Number of students per department\nDepartment.aggr(StudentMajor, n_students='COUNT(*)')\n</pre> # Number of students per department Department.aggr(StudentMajor, n_students='COUNT(*)') Out[41]: <p>dept</p> e.g. BIOL, CS, MATH <p>n_students</p> calculated attribute BIOL 90CS 79MATH 116PHYS 93 <p>Total: 4</p> In\u00a0[42]: Copied! <pre># Breakdown by sex per department\nDepartment.aggr(\n    StudentMajor * Student,\n    n_female=\"SUM(sex='F')\",\n    n_male=\"SUM(sex='M')\"\n)\n</pre> # Breakdown by sex per department Department.aggr(     StudentMajor * Student,     n_female=\"SUM(sex='F')\",     n_male=\"SUM(sex='M')\" ) Out[42]: <p>dept</p> e.g. BIOL, CS, MATH <p>n_female</p> calculated attribute <p>n_male</p> calculated attribute BIOL 43 47CS 43 36MATH 56 60PHYS 51 42 <p>Total: 4</p> In\u00a0[43]: Copied! <pre># Enrollment counts per course (with course name)\nCourse.aggr(Enroll, ..., n_enrolled='COUNT(*)')\n</pre> # Enrollment counts per course (with course name) Course.aggr(Enroll, ..., n_enrolled='COUNT(*)') Out[43]: <p>dept</p> None <p>course</p> course number, e.g. 1010 <p>course_name</p> <p>credits</p> <p>n_enrolled</p> calculated attribute BIOL 1010 Biology in the 21st Century 3.0 392BIOL 2020 Principles of Cell Biology 3.0 393BIOL 2325 Human Anatomy 4.0 420BIOL 2420 Human Physiology 4.0 453CS 1410 Intro to Object-Oriented Programming 4.0 384CS 2420 Data Structures &amp; Algorithms 4.0 392CS 3500 Software Practice 4.0 436CS 3810 Computer Organization 4.0 457 <p>...</p> <p>Total: 15</p> In\u00a0[44]: Copied! <pre># Average grade points per course\nCourse.aggr(\n    Grade * LetterGrade,\n    'course_name',\n    avg_gpa='AVG(points)',\n    n_grades='COUNT(*)'\n)\n</pre> # Average grade points per course Course.aggr(     Grade * LetterGrade,     'course_name',     avg_gpa='AVG(points)',     n_grades='COUNT(*)' ) Out[44]: <p>dept</p> None <p>course</p> course number, e.g. 1010 <p>course_name</p> <p>avg_gpa</p> calculated attribute <p>n_grades</p> calculated attribute BIOL 1010 Biology in the 21st Century 2.3946327683615819 354BIOL 2020 Principles of Cell Biology 2.4106956521739130 345BIOL 2325 Human Anatomy 2.3396325459317585 381BIOL 2420 Human Physiology 2.3905459057071960 403CS 1410 Intro to Object-Oriented Programming 2.3971176470588235 340CS 2420 Data Structures &amp; Algorithms 2.4033333333333333 354CS 3500 Software Practice 2.3978663239074550 389CS 3810 Computer Organization 2.4337226277372263 411 <p>...</p> <p>Total: 15</p> In\u00a0[45]: Copied! <pre># Student GPA: weighted average of grade points by credits\nstudent_gpa = Student.aggr(\n    Grade * LetterGrade * Course,\n    'first_name', 'last_name',\n    total_credits='SUM(credits)',\n    gpa='SUM(points * credits) / SUM(credits)'\n)\nstudent_gpa\n</pre> # Student GPA: weighted average of grade points by credits student_gpa = Student.aggr(     Grade * LetterGrade * Course,     'first_name', 'last_name',     total_credits='SUM(credits)',     gpa='SUM(points * credits) / SUM(credits)' ) student_gpa Out[45]: <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>total_credits</p> calculated attribute <p>gpa</p> calculated attribute 1000 Allison Hill 30.0 1.81200000000000001001 Amanda Davis 41.0 2.30024390243902441002 Kevin Pacheco 32.0 2.94875000000000001003 Tina Rogers 35.0 2.12342857142857141004 Julia Martinez 35.0 2.40028571428571431005 Andrea Stanley 54.0 2.20444444444444441006 Madison Diaz 74.0 2.50972972972972971007 Lisa Jackson 14.0 2.3564285714285714 <p>...</p> <p>Total: 500</p> In\u00a0[46]: Copied! <pre># Top 5 students by GPA (with at least 12 credits)\nstudent_gpa &amp; 'total_credits &gt;= 12' &amp; dj.Top(5, order_by='gpa DESC')\n</pre> # Top 5 students by GPA (with at least 12 credits) student_gpa &amp; 'total_credits &gt;= 12' &amp; dj.Top(5, order_by='gpa DESC') Out[46]: <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>total_credits</p> calculated attribute <p>gpa</p> calculated attribute 1167 Diane Cummings 24.0 3.27833333333333331210 Patricia Baker 23.0 3.30434782608695651222 Andrew Taylor 12.0 3.55333333333333331230 Megan Dillon 23.0 3.66826086956521741375 Ryan Lamb 12.0 3.3333333333333333 <p>Total: 5</p> In\u00a0[47]: Copied! <pre># Students who have taken courses in ALL departments\n# (i.e., no department exists where they haven't enrolled)\nall_depts = Student - (\n    Student.proj() * Department - Enroll.proj('student_id', 'dept')\n)\nall_depts.proj('first_name', 'last_name')\n</pre> # Students who have taken courses in ALL departments # (i.e., no department exists where they haven't enrolled) all_depts = Student - (     Student.proj() * Department - Enroll.proj('student_id', 'dept') ) all_depts.proj('first_name', 'last_name') Out[47]: None <p>student_id</p> university-wide ID <p>first_name</p> <p>last_name</p> <p>sex</p> <p>date_of_birth</p> <p>home_city</p> <p>home_state</p> US state code 1000 Allison Hill bytes bytes bytes bytes1001 Amanda Davis bytes bytes bytes bytes1002 Kevin Pacheco bytes bytes bytes bytes1004 Julia Martinez bytes bytes bytes bytes1005 Andrea Stanley bytes bytes bytes bytes1006 Madison Diaz bytes bytes bytes bytes1007 Lisa Jackson bytes bytes bytes bytes1008 Daniel Nguyen bytes bytes bytes bytes <p>...</p> <p>Total: 364</p> In\u00a0[48]: Copied! <pre># Most popular courses (by enrollment) per department\ncourse_enrollment = Course.aggr(Enroll, ..., n='COUNT(*)')\n\n# For each department, find the max enrollment\nmax_per_dept = Department.aggr(course_enrollment, max_n='MAX(n)')\n\n# Join to find courses matching the max\ncourse_enrollment * max_per_dept &amp; 'n = max_n'\n</pre> # Most popular courses (by enrollment) per department course_enrollment = Course.aggr(Enroll, ..., n='COUNT(*)')  # For each department, find the max enrollment max_per_dept = Department.aggr(course_enrollment, max_n='MAX(n)')  # Join to find courses matching the max course_enrollment * max_per_dept &amp; 'n = max_n' Out[48]: <p>dept</p> None <p>course</p> course number, e.g. 1010 <p>course_name</p> <p>credits</p> <p>n</p> calculated attribute <p>max_n</p> calculated attribute BIOL 2420 Human Physiology 4.0 453 453CS 3810 Computer Organization 4.0 457 457MATH 1220 Calculus II 4.0 425 425PHYS 2210 Physics for Scientists I 4.0 349 349 <p>Total: 4</p> In\u00a0[49]: Copied! <pre># Grade distribution: count of each grade across all courses\nLetterGrade.aggr(Grade, ..., count='COUNT(*)') &amp; 'count &gt; 0'\n</pre> # Grade distribution: count of each grade across all courses LetterGrade.aggr(Grade, ..., count='COUNT(*)') &amp; 'count &gt; 0' Out[49]: <p>grade</p> <p>points</p> <p>count</p> calculated attribute A  4.00 259A- 3.67 400B  3.00 767B+ 3.33 521B- 2.67 586C  2.00 760C+ 2.33 553C- 1.67 566 <p>...</p> <p>Total: 11</p> In\u00a0[50]: Copied! <pre># Fetch as numpy recarray\ndata = (Student &amp; {'home_state': 'CA'}).to_arrays()\nprint(f\"Type: {type(data).__name__}, shape: {data.shape}\")\ndata[:3]\n</pre> # Fetch as numpy recarray data = (Student &amp; {'home_state': 'CA'}).to_arrays() print(f\"Type: {type(data).__name__}, shape: {data.shape}\") data[:3] <pre>Type: ndarray, shape: (5,)\n</pre> Out[50]: <pre>array([(1073, 'Kyle', 'Martinez', 'M', datetime.date(2008, 10, 11), 'West Amandastad', 'CA'),\n       (1092, 'Christina', 'Wilson', 'F', datetime.date(1995, 2, 12), 'West Margaret', 'CA'),\n       (1157, 'Kelly', 'Foster', 'F', datetime.date(1996, 8, 27), 'Jeffreyburgh', 'CA')],\n      dtype=[('student_id', '&lt;i8'), ('first_name', 'O'), ('last_name', 'O'), ('sex', 'O'), ('date_of_birth', 'O'), ('home_city', 'O'), ('home_state', 'O')])</pre> In\u00a0[51]: Copied! <pre># Fetch as list of dicts\n(Student &amp; {'home_state': 'CA'}).to_dicts(limit=3)\n</pre> # Fetch as list of dicts (Student &amp; {'home_state': 'CA'}).to_dicts(limit=3) Out[51]: <pre>[{'student_id': 1073,\n  'first_name': 'Kyle',\n  'last_name': 'Martinez',\n  'sex': 'M',\n  'date_of_birth': datetime.date(2008, 10, 11),\n  'home_city': 'West Amandastad',\n  'home_state': 'CA'},\n {'student_id': 1092,\n  'first_name': 'Christina',\n  'last_name': 'Wilson',\n  'sex': 'F',\n  'date_of_birth': datetime.date(1995, 2, 12),\n  'home_city': 'West Margaret',\n  'home_state': 'CA'},\n {'student_id': 1157,\n  'first_name': 'Kelly',\n  'last_name': 'Foster',\n  'sex': 'F',\n  'date_of_birth': datetime.date(1996, 8, 27),\n  'home_city': 'Jeffreyburgh',\n  'home_state': 'CA'}]</pre> In\u00a0[52]: Copied! <pre># Fetch specific attributes as arrays\nfirst_names, last_names = (Student &amp; {'home_state': 'CA'}).to_arrays('first_name', 'last_name')\nlist(zip(first_names, last_names))[:5]\n</pre> # Fetch specific attributes as arrays first_names, last_names = (Student &amp; {'home_state': 'CA'}).to_arrays('first_name', 'last_name') list(zip(first_names, last_names))[:5] Out[52]: <pre>[('Kyle', 'Martinez'),\n ('Christina', 'Wilson'),\n ('Kelly', 'Foster'),\n ('Nicholas', 'Buck'),\n ('Heather', 'Armstrong')]</pre> In\u00a0[53]: Copied! <pre># Fetch single row with fetch1\nstudent = (Student &amp; {'student_id': 1000}).fetch1()\nprint(f\"{student['first_name']} {student['last_name']} from {student['home_city']}, {student['home_state']}\")\n</pre> # Fetch single row with fetch1 student = (Student &amp; {'student_id': 1000}).fetch1() print(f\"{student['first_name']} {student['last_name']} from {student['home_city']}, {student['home_state']}\") <pre>Allison Hill from Lake Joyside, CO\n</pre> In\u00a0[54]: Copied! <pre># Fetch as pandas DataFrame\n(student_gpa &amp; 'total_credits &gt;= 12').to_pandas().sort_values('gpa', ascending=False).head(10)\n</pre> # Fetch as pandas DataFrame (student_gpa &amp; 'total_credits &gt;= 12').to_pandas().sort_values('gpa', ascending=False).head(10) Out[54]: first_name last_name total_credits gpa student_id 1230 Megan Dillon 23.0 3.6682608695652174 1222 Andrew Taylor 12.0 3.5533333333333333 1375 Ryan Lamb 12.0 3.3333333333333333 1210 Patricia Baker 23.0 3.3043478260869565 1167 Diane Cummings 24.0 3.2783333333333333 1059 Stephanie Simmons 21.0 3.2057142857142857 1471 Jenna Bryant 16.0 3.1650000000000000 1311 Matthew Byrd 19.0 3.1236842105263158 1048 Jessica Chandler 31.0 3.0880645161290323 1277 Brenda Mccoy 19.0 3.0863157894736842 In\u00a0[55]: Copied! <pre># Cleanup\nschema.drop(prompt=False)\n</pre> # Cleanup schema.drop(prompt=False)"},{"location":"tutorials/examples/university/#university-database","title":"University Database\u00b6","text":"<p>This tutorial builds a complete university registration system to demonstrate:</p> <ul> <li>Schema design with realistic relationships</li> <li>Data population using Faker for synthetic data</li> <li>Rich query patterns from simple to complex</li> </ul> <p>University databases are classic examples because everyone understands students, courses, enrollments, and grades. The domain naturally demonstrates:</p> <ul> <li>One-to-many relationships (department \u2192 courses)</li> <li>Many-to-many relationships (students \u2194 courses via enrollments)</li> <li>Workflow dependencies (enrollment requires both student and section to exist)</li> </ul>"},{"location":"tutorials/examples/university/#schema-design","title":"Schema Design\u00b6","text":"<p>Our university schema models:</p> Table Purpose <code>Student</code> Student records with contact info <code>Department</code> Academic departments <code>StudentMajor</code> Student-declared majors <code>Course</code> Course catalog <code>Term</code> Academic terms (Spring/Summer/Fall) <code>Section</code> Course offerings in specific terms <code>Enroll</code> Student enrollments in sections <code>LetterGrade</code> Grade scale (lookup) <code>Grade</code> Assigned grades"},{"location":"tutorials/examples/university/#populate-with-synthetic-data","title":"Populate with Synthetic Data\u00b6","text":"<p>We use Faker to generate realistic student data.</p>"},{"location":"tutorials/examples/university/#querying-data","title":"Querying Data\u00b6","text":"<p>DataJoint queries are composable expressions. Displaying a query shows a preview; use <code>fetch()</code> to retrieve data.</p>"},{"location":"tutorials/examples/university/#restriction-and-","title":"Restriction (<code>&amp;</code> and <code>-</code>)\u00b6","text":"<p>Filter rows using <code>&amp;</code> (keep matching) or <code>-</code> (remove matching).</p>"},{"location":"tutorials/examples/university/#subqueries-in-restrictions","title":"Subqueries in Restrictions\u00b6","text":"<p>Use another query as a restriction condition.</p>"},{"location":"tutorials/examples/university/#projection-proj","title":"Projection (<code>.proj()</code>)\u00b6","text":"<p>Select, rename, or compute attributes.</p>"},{"location":"tutorials/examples/university/#universal-set-dju","title":"Universal Set (<code>dj.U()</code>)\u00b6","text":"<p>The universal set <code>dj.U()</code> extracts unique values of specified attributes.</p>"},{"location":"tutorials/examples/university/#join","title":"Join (<code>*</code>)\u00b6","text":"<p>Combine tables on matching attributes.</p>"},{"location":"tutorials/examples/university/#aggregation-aggr","title":"Aggregation (<code>.aggr()</code>)\u00b6","text":"<p>Group rows and compute aggregate statistics.</p>"},{"location":"tutorials/examples/university/#complex-queries","title":"Complex Queries\u00b6","text":"<p>Combine operators to answer complex questions.</p>"},{"location":"tutorials/examples/university/#fetching-results","title":"Fetching Results\u00b6","text":"<p>Use the fetch methods to retrieve data into Python:</p> <ul> <li><code>to_dicts()</code> \u2014 list of dictionaries</li> <li><code>to_arrays()</code> \u2014 numpy arrays</li> <li><code>to_pandas()</code> \u2014 pandas DataFrame</li> <li><code>fetch1()</code> \u2014 single row (query must return exactly one row)</li> </ul>"},{"location":"tutorials/examples/university/#summary","title":"Summary\u00b6","text":"<p>This tutorial demonstrated:</p> Operation Syntax Purpose Restriction <code>A &amp; cond</code> Keep matching rows Anti-restriction <code>A - cond</code> Remove matching rows Projection <code>A.proj(...)</code> Select/compute attributes Join <code>A * B</code> Combine tables Left join <code>A.join(B, left=True)</code> Keep all rows from A Aggregation <code>A.aggr(B, ...)</code> Group and aggregate Universal <code>dj.U('attr') &amp; A</code> Unique values Top <code>A &amp; dj.Top(n, order_by=...)</code> Limit/order results Fetch keys <code>A.keys()</code> Primary key dicts Fetch arrays <code>A.to_arrays(...)</code> Numpy arrays Fetch dicts <code>A.to_dicts()</code> List of dicts Fetch pandas <code>A.to_pandas()</code> DataFrame Fetch one <code>A.fetch1()</code> Single row dict"}]}